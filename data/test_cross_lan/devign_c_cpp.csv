func,func_cleaned,project,target
"int qemu_opts_id_wellformed(const char *id)

{

    int i;



    if (!qemu_isalpha(id[0])) {

        return 0;

    }

    for (i = 1; id[i]; i++) {

        if (!qemu_isalnum(id[i]) && !strchr(""-._"", id[i])) {

            return 0;

        }

    }

    return 1;

}
","int qemu_opts_id_wellformed(const char *id) {
int i; if (!qemu_isalpha(id[0])) {
return 0; } for (i = 1; id[i]; i++) {
if (!qemu_isalnum(id[i]) && !strchr(""-._"", id[i])) {
return 0; } } return 1; } ",qemu,0
"void qed_release(BDRVQEDState *s)

{

    aio_context_release(bdrv_get_aio_context(s->bs));

}
","void qed_release(BDRVQEDState *s) {
aio_context_release(bdrv_get_aio_context(s->bs)); } ",qemu,0
"static int ppc_hash64_pte_prot(PowerPCCPU *cpu,

                               ppc_slb_t *slb, ppc_hash_pte64_t pte)

{

    CPUPPCState *env = &cpu->env;

    unsigned pp, key;

    /* Some pp bit combinations have undefined behaviour, so default

     * to no access in those cases */

    int prot = 0;



    key = !!(msr_pr ? (slb->vsid & SLB_VSID_KP)

             : (slb->vsid & SLB_VSID_KS));

    pp = (pte.pte1 & HPTE64_R_PP) | ((pte.pte1 & HPTE64_R_PP0) >> 61);



    if (key == 0) {

        switch (pp) {

        case 0x0:

        case 0x1:

        case 0x2:

            prot = PAGE_READ | PAGE_WRITE | PAGE_EXEC;

            break;



        case 0x3:

        case 0x6:

            prot = PAGE_READ | PAGE_EXEC;

            break;

        }

    } else {

        switch (pp) {

        case 0x0:

        case 0x6:

            break;



        case 0x1:

        case 0x3:

            prot = PAGE_READ | PAGE_EXEC;

            break;



        case 0x2:

            prot = PAGE_READ | PAGE_WRITE | PAGE_EXEC;

            break;

        }

    }



    /* No execute if either noexec or guarded bits set */

    if (!(pte.pte1 & HPTE64_R_N) || (pte.pte1 & HPTE64_R_G)

        || (slb->vsid & SLB_VSID_N)) {

        prot |= PAGE_EXEC;

    }



    return prot;

}
","static int ppc_hash64_pte_prot(PowerPCCPU *cpu, ppc_slb_t *slb, ppc_hash_pte64_t pte) {
CPUPPCState *env = &cpu->env; unsigned pp, key; /* Some pp bit combinations have undefined behaviour, so default * to no access in those cases */ int prot = 0; key = !!(msr_pr ? (slb->vsid & SLB_VSID_KP) : (slb->vsid & SLB_VSID_KS)); pp = (pte.pte1 & HPTE64_R_PP) | ((pte.pte1 & HPTE64_R_PP0) >> 61); if (key == 0) {
switch (pp) {
case 0x0: case 0x1: case 0x2: prot = PAGE_READ | PAGE_WRITE | PAGE_EXEC; break; case 0x3: case 0x6: prot = PAGE_READ | PAGE_EXEC; break; } } else {
switch (pp) {
case 0x0: case 0x6: break; case 0x1: case 0x3: prot = PAGE_READ | PAGE_EXEC; break; case 0x2: prot = PAGE_READ | PAGE_WRITE | PAGE_EXEC; break; } } /* No execute if either noexec or guarded bits set */ if (!(pte.pte1 & HPTE64_R_N) || (pte.pte1 & HPTE64_R_G) || (slb->vsid & SLB_VSID_N)) {
prot |= PAGE_EXEC; } return prot; } ",qemu,1
"void s390_io_interrupt(S390CPU *cpu, uint16_t subchannel_id,

                       uint16_t subchannel_nr, uint32_t io_int_parm,

                       uint32_t io_int_word)

{

    if (kvm_enabled()) {

        kvm_s390_io_interrupt(cpu, subchannel_id, subchannel_nr, io_int_parm,

                              io_int_word);

    } else {

        cpu_inject_io(cpu, subchannel_id, subchannel_nr, io_int_parm,

                      io_int_word);

    }

}
","void s390_io_interrupt(S390CPU *cpu, uint16_t subchannel_id, uint16_t subchannel_nr, uint32_t io_int_parm, uint32_t io_int_word) {
if (kvm_enabled()) {
kvm_s390_io_interrupt(cpu, subchannel_id, subchannel_nr, io_int_parm, io_int_word); } else {
cpu_inject_io(cpu, subchannel_id, subchannel_nr, io_int_parm, io_int_word); } } ",qemu,0
"static uint32_t timer_int_route(struct HPETTimer *timer)

{

    uint32_t route;

    route = (timer->config & HPET_TN_INT_ROUTE_MASK) >> HPET_TN_INT_ROUTE_SHIFT;

    return route;

}
","static uint32_t timer_int_route(struct HPETTimer *timer) {
uint32_t route; route = (timer->config & HPET_TN_INT_ROUTE_MASK) >> HPET_TN_INT_ROUTE_SHIFT; return route; } ",qemu,1
"static int disas_cp15_insn(CPUState *env, DisasContext *s, uint32_t insn)

{

    uint32_t rd;

    TCGv tmp, tmp2;



    /* M profile cores use memory mapped registers instead of cp15.  */

    if (arm_feature(env, ARM_FEATURE_M))

	return 1;



    if ((insn & (1 << 25)) == 0) {

        if (insn & (1 << 20)) {

            /* mrrc */

            return 1;

        }

        /* mcrr.  Used for block cache operations, so implement as no-op.  */

        return 0;

    }

    if ((insn & (1 << 4)) == 0) {

        /* cdp */

        return 1;

    }

    if (IS_USER(s) && !cp15_user_ok(insn)) {

        return 1;

    }



    /* Pre-v7 versions of the architecture implemented WFI via coprocessor

     * instructions rather than a separate instruction.

     */

    if ((insn & 0x0fff0fff) == 0x0e070f90) {

        /* 0,c7,c0,4: Standard v6 WFI (also used in some pre-v6 cores).

         * In v7, this must NOP.

         */

        if (!arm_feature(env, ARM_FEATURE_V7)) {

            /* Wait for interrupt.  */

            gen_set_pc_im(s->pc);

            s->is_jmp = DISAS_WFI;

        }

        return 0;

    }



    if ((insn & 0x0fff0fff) == 0x0e070f58) {

        /* 0,c7,c8,2: Not all pre-v6 cores implemented this WFI,

         * so this is slightly over-broad.

         */

        if (!arm_feature(env, ARM_FEATURE_V6)) {

            /* Wait for interrupt.  */

            gen_set_pc_im(s->pc);

            s->is_jmp = DISAS_WFI;

            return 0;

        }

        /* Otherwise fall through to handle via helper function.

         * In particular, on v7 and some v6 cores this is one of

         * the VA-PA registers.

         */

    }



    rd = (insn >> 12) & 0xf;



    if (cp15_tls_load_store(env, s, insn, rd))

        return 0;



    tmp2 = tcg_const_i32(insn);

    if (insn & ARM_CP_RW_BIT) {

        tmp = new_tmp();

        gen_helper_get_cp15(tmp, cpu_env, tmp2);

        /* If the destination register is r15 then sets condition codes.  */

        if (rd != 15)

            store_reg(s, rd, tmp);

        else

            dead_tmp(tmp);

    } else {

        tmp = load_reg(s, rd);

        gen_helper_set_cp15(cpu_env, tmp2, tmp);

        dead_tmp(tmp);

        /* Normally we would always end the TB here, but Linux

         * arch/arm/mach-pxa/sleep.S expects two instructions following

         * an MMU enable to execute from cache.  Imitate this behaviour.  */

        if (!arm_feature(env, ARM_FEATURE_XSCALE) ||

                (insn & 0x0fff0fff) != 0x0e010f10)

            gen_lookup_tb(s);

    }

    tcg_temp_free_i32(tmp2);

    return 0;

}
","static int disas_cp15_insn(CPUState *env, DisasContext *s, uint32_t insn) {
uint32_t rd; TCGv tmp, tmp2; /* M profile cores use memory mapped registers instead of cp15. */ if (arm_feature(env, ARM_FEATURE_M)) return 1; if ((insn & (1 << 25)) == 0) {
if (insn & (1 << 20)) {
/* mrrc */ return 1; } /* mcrr. Used for block cache operations, so implement as no-op. */ return 0; } if ((insn & (1 << 4)) == 0) {
/* cdp */ return 1; } if (IS_USER(s) && !cp15_user_ok(insn)) {
return 1; } /* Pre-v7 versions of the architecture implemented WFI via coprocessor * instructions rather than a separate instruction. */ if ((insn & 0x0fff0fff) == 0x0e070f90) {
/* 0,c7,c0,4: Standard v6 WFI (also used in some pre-v6 cores). * In v7, this must NOP. */ if (!arm_feature(env, ARM_FEATURE_V7)) {
/* Wait for interrupt. */ gen_set_pc_im(s->pc); s->is_jmp = DISAS_WFI; } return 0; } if ((insn & 0x0fff0fff) == 0x0e070f58) {
/* 0,c7,c8,2: Not all pre-v6 cores implemented this WFI, * so this is slightly over-broad. */ if (!arm_feature(env, ARM_FEATURE_V6)) {
/* Wait for interrupt. */ gen_set_pc_im(s->pc); s->is_jmp = DISAS_WFI; return 0; } /* Otherwise fall through to handle via helper function. * In particular, on v7 and some v6 cores this is one of * the VA-PA registers. */ } rd = (insn >> 12) & 0xf; if (cp15_tls_load_store(env, s, insn, rd)) return 0; tmp2 = tcg_const_i32(insn); if (insn & ARM_CP_RW_BIT) {
tmp = new_tmp(); gen_helper_get_cp15(tmp, cpu_env, tmp2); /* If the destination register is r15 then sets condition codes. */ if (rd != 15) store_reg(s, rd, tmp); else dead_tmp(tmp); } else {
tmp = load_reg(s, rd); gen_helper_set_cp15(cpu_env, tmp2, tmp); dead_tmp(tmp); /* Normally we would always end the TB here, but Linux * arch/arm/mach-pxa/sleep.S expects two instructions following * an MMU enable to execute from cache. Imitate this behaviour. */ if (!arm_feature(env, ARM_FEATURE_XSCALE) || (insn & 0x0fff0fff) != 0x0e010f10) gen_lookup_tb(s); } tcg_temp_free_i32(tmp2); return 0; } ",qemu,1
"static int mode_sense_page(SCSIDiskState *s, int page, uint8_t **p_outbuf,

                           int page_control)

{

    static const int mode_sense_valid[0x3f] = {

        [MODE_PAGE_HD_GEOMETRY]            = (1 << TYPE_DISK),

        [MODE_PAGE_FLEXIBLE_DISK_GEOMETRY] = (1 << TYPE_DISK),

        [MODE_PAGE_CACHING]                = (1 << TYPE_DISK) | (1 << TYPE_ROM),

        [MODE_PAGE_CAPABILITIES]           = (1 << TYPE_ROM),

    };



    BlockDriverState *bdrv = s->bs;

    int cylinders, heads, secs;

    uint8_t *p = *p_outbuf;



    if ((mode_sense_valid[page] & (1 << s->qdev.type)) == 0) {

        return -1;

    }



    p[0] = page;



    /*

     * If Changeable Values are requested, a mask denoting those mode parameters

     * that are changeable shall be returned. As we currently don't support

     * parameter changes via MODE_SELECT all bits are returned set to zero.

     * The buffer was already menset to zero by the caller of this function.

     */

    switch (page) {

    case MODE_PAGE_HD_GEOMETRY:

        p[1] = 0x16;

        if (page_control == 1) { /* Changeable Values */

            break;

        }

        /* if a geometry hint is available, use it */

        bdrv_get_geometry_hint(bdrv, &cylinders, &heads, &secs);

        p[2] = (cylinders >> 16) & 0xff;

        p[3] = (cylinders >> 8) & 0xff;

        p[4] = cylinders & 0xff;

        p[5] = heads & 0xff;

        /* Write precomp start cylinder, disabled */

        p[6] = (cylinders >> 16) & 0xff;

        p[7] = (cylinders >> 8) & 0xff;

        p[8] = cylinders & 0xff;

        /* Reduced current start cylinder, disabled */

        p[9] = (cylinders >> 16) & 0xff;

        p[10] = (cylinders >> 8) & 0xff;

        p[11] = cylinders & 0xff;

        /* Device step rate [ns], 200ns */

        p[12] = 0;

        p[13] = 200;

        /* Landing zone cylinder */

        p[14] = 0xff;

        p[15] =  0xff;

        p[16] = 0xff;

        /* Medium rotation rate [rpm], 5400 rpm */

        p[20] = (5400 >> 8) & 0xff;

        p[21] = 5400 & 0xff;

        break;



    case MODE_PAGE_FLEXIBLE_DISK_GEOMETRY:

        p[1] = 0x1e;

        if (page_control == 1) { /* Changeable Values */

            break;

        }

        /* Transfer rate [kbit/s], 5Mbit/s */

        p[2] = 5000 >> 8;

        p[3] = 5000 & 0xff;

        /* if a geometry hint is available, use it */

        bdrv_get_geometry_hint(bdrv, &cylinders, &heads, &secs);

        p[4] = heads & 0xff;

        p[5] = secs & 0xff;

        p[6] = s->cluster_size * 2;

        p[8] = (cylinders >> 8) & 0xff;

        p[9] = cylinders & 0xff;

        /* Write precomp start cylinder, disabled */

        p[10] = (cylinders >> 8) & 0xff;

        p[11] = cylinders & 0xff;

        /* Reduced current start cylinder, disabled */

        p[12] = (cylinders >> 8) & 0xff;

        p[13] = cylinders & 0xff;

        /* Device step rate [100us], 100us */

        p[14] = 0;

        p[15] = 1;

        /* Device step pulse width [us], 1us */

        p[16] = 1;

        /* Device head settle delay [100us], 100us */

        p[17] = 0;

        p[18] = 1;

        /* Motor on delay [0.1s], 0.1s */

        p[19] = 1;

        /* Motor off delay [0.1s], 0.1s */

        p[20] = 1;

        /* Medium rotation rate [rpm], 5400 rpm */

        p[28] = (5400 >> 8) & 0xff;

        p[29] = 5400 & 0xff;

        break;



    case MODE_PAGE_CACHING:

        p[0] = 8;

        p[1] = 0x12;

        if (page_control == 1) { /* Changeable Values */

            break;

        }

        if (bdrv_enable_write_cache(s->bs)) {

            p[2] = 4; /* WCE */

        }

        break;



    case MODE_PAGE_CAPABILITIES:

        p[1] = 0x14;

        if (page_control == 1) { /* Changeable Values */

            break;

        }

        p[2] = 3; // CD-R & CD-RW read

        p[3] = 0; // Writing not supported

        p[4] = 0x7f; /* Audio, composite, digital out,

                        mode 2 form 1&2, multi session */

        p[5] = 0xff; /* CD DA, DA accurate, RW supported,

                        RW corrected, C2 errors, ISRC,

                        UPC, Bar code */

        p[6] = 0x2d | (s->tray_locked ? 2 : 0);

        /* Locking supported, jumper present, eject, tray */

        p[7] = 0; /* no volume & mute control, no

                     changer */

        p[8] = (50 * 176) >> 8; // 50x read speed

        p[9] = (50 * 176) & 0xff;

        p[10] = 0 >> 8; // No volume

        p[11] = 0 & 0xff;

        p[12] = 2048 >> 8; // 2M buffer

        p[13] = 2048 & 0xff;

        p[14] = (16 * 176) >> 8; // 16x read speed current

        p[15] = (16 * 176) & 0xff;

        p[18] = (16 * 176) >> 8; // 16x write speed

        p[19] = (16 * 176) & 0xff;

        p[20] = (16 * 176) >> 8; // 16x write speed current

        p[21] = (16 * 176) & 0xff;

        break;



    default:

        return -1;

    }



    *p_outbuf += p[1] + 2;

    return p[1] + 2;

}
","static int mode_sense_page(SCSIDiskState *s, int page, uint8_t **p_outbuf, int page_control) {
static const int mode_sense_valid[0x3f] = {
[MODE_PAGE_HD_GEOMETRY] = (1 << TYPE_DISK), [MODE_PAGE_FLEXIBLE_DISK_GEOMETRY] = (1 << TYPE_DISK), [MODE_PAGE_CACHING] = (1 << TYPE_DISK) | (1 << TYPE_ROM), [MODE_PAGE_CAPABILITIES] = (1 << TYPE_ROM), }; BlockDriverState *bdrv = s->bs; int cylinders, heads, secs; uint8_t *p = *p_outbuf; if ((mode_sense_valid[page] & (1 << s->qdev.type)) == 0) {
return -1; } p[0] = page; /* * If Changeable Values are requested, a mask denoting those mode parameters * that are changeable shall be returned. As we currently don't support * parameter changes via MODE_SELECT all bits are returned set to zero. * The buffer was already menset to zero by the caller of this function. */ switch (page) {
case MODE_PAGE_HD_GEOMETRY: p[1] = 0x16; if (page_control == 1) {
/* Changeable Values */ break; } /* if a geometry hint is available, use it */ bdrv_get_geometry_hint(bdrv, &cylinders, &heads, &secs); p[2] = (cylinders >> 16) & 0xff; p[3] = (cylinders >> 8) & 0xff; p[4] = cylinders & 0xff; p[5] = heads & 0xff; /* Write precomp start cylinder, disabled */ p[6] = (cylinders >> 16) & 0xff; p[7] = (cylinders >> 8) & 0xff; p[8] = cylinders & 0xff; /* Reduced current start cylinder, disabled */ p[9] = (cylinders >> 16) & 0xff; p[10] = (cylinders >> 8) & 0xff; p[11] = cylinders & 0xff; /* Device step rate [ns], 200ns */ p[12] = 0; p[13] = 200; /* Landing zone cylinder */ p[14] = 0xff; p[15] = 0xff; p[16] = 0xff; /* Medium rotation rate [rpm], 5400 rpm */ p[20] = (5400 >> 8) & 0xff; p[21] = 5400 & 0xff; break; case MODE_PAGE_FLEXIBLE_DISK_GEOMETRY: p[1] = 0x1e; if (page_control == 1) {
/* Changeable Values */ break; } /* Transfer rate [kbit/s], 5Mbit/s */ p[2] = 5000 >> 8; p[3] = 5000 & 0xff; /* if a geometry hint is available, use it */ bdrv_get_geometry_hint(bdrv, &cylinders, &heads, &secs); p[4] = heads & 0xff; p[5] = secs & 0xff; p[6] = s->cluster_size * 2; p[8] = (cylinders >> 8) & 0xff; p[9] = cylinders & 0xff; /* Write precomp start cylinder, disabled */ p[10] = (cylinders >> 8) & 0xff; p[11] = cylinders & 0xff; /* Reduced current start cylinder, disabled */ p[12] = (cylinders >> 8) & 0xff; p[13] = cylinders & 0xff; /* Device step rate [100us], 100us */ p[14] = 0; p[15] = 1; /* Device step pulse width [us], 1us */ p[16] = 1; /* Device head settle delay [100us], 100us */ p[17] = 0; p[18] = 1; /* Motor on delay [0.1s], 0.1s */ p[19] = 1; /* Motor off delay [0.1s], 0.1s */ p[20] = 1; /* Medium rotation rate [rpm], 5400 rpm */ p[28] = (5400 >> 8) & 0xff; p[29] = 5400 & 0xff; break; case MODE_PAGE_CACHING: p[0] = 8; p[1] = 0x12; if (page_control == 1) {
/* Changeable Values */ break; } if (bdrv_enable_write_cache(s->bs)) {
p[2] = 4; /* WCE */ } break; case MODE_PAGE_CAPABILITIES: p[1] = 0x14; if (page_control == 1) {
/* Changeable Values */ break; } p[2] = 3; // CD-R & CD-RW read p[3] = 0; // Writing not supported p[4] = 0x7f; /* Audio, composite, digital out, mode 2 form 1&2, multi session */ p[5] = 0xff; /* CD DA, DA accurate, RW supported, RW corrected, C2 errors, ISRC, UPC, Bar code */ p[6] = 0x2d | (s->tray_locked ? 2 : 0); /* Locking supported, jumper present, eject, tray */ p[7] = 0; /* no volume & mute control, no changer */ p[8] = (50 * 176) >> 8; // 50x read speed p[9] = (50 * 176) & 0xff; p[10] = 0 >> 8; // No volume p[11] = 0 & 0xff; p[12] = 2048 >> 8; // 2M buffer p[13] = 2048 & 0xff; p[14] = (16 * 176) >> 8; // 16x read speed current p[15] = (16 * 176) & 0xff; p[18] = (16 * 176) >> 8; // 16x write speed p[19] = (16 * 176) & 0xff; p[20] = (16 * 176) >> 8; // 16x write speed current p[21] = (16 * 176) & 0xff; break; default: return -1; } *p_outbuf += p[1] + 2; return p[1] + 2; } ",qemu,0
"static void vfio_amd_xgbe_class_init(ObjectClass *klass, void *data)

{

    DeviceClass *dc = DEVICE_CLASS(klass);

    VFIOAmdXgbeDeviceClass *vcxc =

        VFIO_AMD_XGBE_DEVICE_CLASS(klass);

    vcxc->parent_realize = dc->realize;

    dc->realize = amd_xgbe_realize;

    dc->desc = ""VFIO AMD XGBE"";

    dc->vmsd = &vfio_platform_amd_xgbe_vmstate;



}","static void vfio_amd_xgbe_class_init(ObjectClass *klass, void *data) {
DeviceClass *dc = DEVICE_CLASS(klass); VFIOAmdXgbeDeviceClass *vcxc = VFIO_AMD_XGBE_DEVICE_CLASS(klass); vcxc->parent_realize = dc->realize; dc->realize = amd_xgbe_realize; dc->desc = ""VFIO AMD XGBE""; dc->vmsd = &vfio_platform_amd_xgbe_vmstate; }",qemu,1
"static int tusb6010_init(SysBusDevice *dev)

{

    TUSBState *s = FROM_SYSBUS(TUSBState, dev);

    qemu_irq *musb_irqs;

    int i;

    s->otg_timer = qemu_new_timer_ns(vm_clock, tusb_otg_tick, s);

    s->pwr_timer = qemu_new_timer_ns(vm_clock, tusb_power_tick, s);

    memory_region_init_io(&s->iomem[1], &tusb_async_ops, s, ""tusb-async"",

                          UINT32_MAX);

    sysbus_init_mmio_region(dev, &s->iomem[0]);

    sysbus_init_mmio_region(dev, &s->iomem[1]);

    sysbus_init_irq(dev, &s->irq);

    qdev_init_gpio_in(&dev->qdev, tusb6010_irq, musb_irq_max + 1);

    musb_irqs = g_new0(qemu_irq, musb_irq_max);

    for (i = 0; i < musb_irq_max; i++) {

        musb_irqs[i] = qdev_get_gpio_in(&dev->qdev, i + 1);

    }

    s->musb = musb_init(musb_irqs);

    return 0;

}
","static int tusb6010_init(SysBusDevice *dev) {
TUSBState *s = FROM_SYSBUS(TUSBState, dev); qemu_irq *musb_irqs; int i; s->otg_timer = qemu_new_timer_ns(vm_clock, tusb_otg_tick, s); s->pwr_timer = qemu_new_timer_ns(vm_clock, tusb_power_tick, s); memory_region_init_io(&s->iomem[1], &tusb_async_ops, s, ""tusb-async"", UINT32_MAX); sysbus_init_mmio_region(dev, &s->iomem[0]); sysbus_init_mmio_region(dev, &s->iomem[1]); sysbus_init_irq(dev, &s->irq); qdev_init_gpio_in(&dev->qdev, tusb6010_irq, musb_irq_max + 1); musb_irqs = g_new0(qemu_irq, musb_irq_max); for (i = 0; i < musb_irq_max; i++) {
musb_irqs[i] = qdev_get_gpio_in(&dev->qdev, i + 1); } s->musb = musb_init(musb_irqs); return 0; } ",qemu,0
"static void rtl8139_io_writew(void *opaque, uint8_t addr, uint32_t val)

{

    RTL8139State *s = opaque;



    addr &= 0xfe;



    switch (addr)

    {

        case IntrMask:

            rtl8139_IntrMask_write(s, val);

            break;



        case IntrStatus:

            rtl8139_IntrStatus_write(s, val);

            break;



        case MultiIntr:

            rtl8139_MultiIntr_write(s, val);

            break;



        case RxBufPtr:

            rtl8139_RxBufPtr_write(s, val);

            break;



        case BasicModeCtrl:

            rtl8139_BasicModeCtrl_write(s, val);

            break;

        case BasicModeStatus:

            rtl8139_BasicModeStatus_write(s, val);

            break;

        case NWayAdvert:

            DPRINTF(""NWayAdvert write(w) val=0x%04x\n"", val);

            s->NWayAdvert = val;

            break;

        case NWayLPAR:

            DPRINTF(""forbidden NWayLPAR write(w) val=0x%04x\n"", val);

            break;

        case NWayExpansion:

            DPRINTF(""NWayExpansion write(w) val=0x%04x\n"", val);

            s->NWayExpansion = val;

            break;



        case CpCmd:

            rtl8139_CpCmd_write(s, val);

            break;



        case IntrMitigate:

            rtl8139_IntrMitigate_write(s, val);

            break;



        default:

            DPRINTF(""ioport write(w) addr=0x%x val=0x%04x via write(b)\n"",

                addr, val);



            rtl8139_io_writeb(opaque, addr, val & 0xff);

            rtl8139_io_writeb(opaque, addr + 1, (val >> 8) & 0xff);

            break;

    }

}
","static void rtl8139_io_writew(void *opaque, uint8_t addr, uint32_t val) {
RTL8139State *s = opaque; addr &= 0xfe; switch (addr) {
case IntrMask: rtl8139_IntrMask_write(s, val); break; case IntrStatus: rtl8139_IntrStatus_write(s, val); break; case MultiIntr: rtl8139_MultiIntr_write(s, val); break; case RxBufPtr: rtl8139_RxBufPtr_write(s, val); break; case BasicModeCtrl: rtl8139_BasicModeCtrl_write(s, val); break; case BasicModeStatus: rtl8139_BasicModeStatus_write(s, val); break; case NWayAdvert: DPRINTF(""NWayAdvert write(w) val=0x%04x\n"", val); s->NWayAdvert = val; break; case NWayLPAR: DPRINTF(""forbidden NWayLPAR write(w) val=0x%04x\n"", val); break; case NWayExpansion: DPRINTF(""NWayExpansion write(w) val=0x%04x\n"", val); s->NWayExpansion = val; break; case CpCmd: rtl8139_CpCmd_write(s, val); break; case IntrMitigate: rtl8139_IntrMitigate_write(s, val); break; default: DPRINTF(""ioport write(w) addr=0x%x val=0x%04x via write(b)\n"", addr, val); rtl8139_io_writeb(opaque, addr, val & 0xff); rtl8139_io_writeb(opaque, addr + 1, (val >> 8) & 0xff); break; } } ",qemu,1
"int cpu_get_dump_info(ArchDumpInfo *info,

                      const struct GuestPhysBlockList *guest_phys_blocks)

{

    PowerPCCPU *cpu = POWERPC_CPU(first_cpu);

    PowerPCCPUClass *pcc = POWERPC_CPU_GET_CLASS(cpu);



    info->d_machine = EM_PPC64;

    info->d_class = ELFCLASS64;

    if ((*pcc->interrupts_big_endian)(cpu)) {

        info->d_endian = ELFDATA2MSB;

    } else {

        info->d_endian = ELFDATA2LSB;









    return 0;
","int cpu_get_dump_info(ArchDumpInfo *info, const struct GuestPhysBlockList *guest_phys_blocks) {
PowerPCCPU *cpu = POWERPC_CPU(first_cpu); PowerPCCPUClass *pcc = POWERPC_CPU_GET_CLASS(cpu); info->d_machine = EM_PPC64; info->d_class = ELFCLASS64; if ((*pcc->interrupts_big_endian)(cpu)) {
info->d_endian = ELFDATA2MSB; } else {
info->d_endian = ELFDATA2LSB; return 0; ",qemu,1
"int bdrv_flush(BlockDriverState *bs)

{

    Coroutine *co;

    RwCo rwco = {

        .bs = bs,

        .ret = NOT_DONE,

    };



    if (qemu_in_coroutine()) {

        /* Fast-path if already in coroutine context */

        bdrv_flush_co_entry(&rwco);

    } else {

        AioContext *aio_context = bdrv_get_aio_context(bs);



        co = qemu_coroutine_create(bdrv_flush_co_entry);

        qemu_coroutine_enter(co, &rwco);

        while (rwco.ret == NOT_DONE) {

            aio_poll(aio_context, true);

        }

    }



    return rwco.ret;

}
","int bdrv_flush(BlockDriverState *bs) {
Coroutine *co; RwCo rwco = {
.bs = bs, .ret = NOT_DONE, }; if (qemu_in_coroutine()) {
/* Fast-path if already in coroutine context */ bdrv_flush_co_entry(&rwco); } else {
AioContext *aio_context = bdrv_get_aio_context(bs); co = qemu_coroutine_create(bdrv_flush_co_entry); qemu_coroutine_enter(co, &rwco); while (rwco.ret == NOT_DONE) {
aio_poll(aio_context, true); } } return rwco.ret; } ",qemu,0
"static GThread *trace_thread_create(GThreadFunc fn)

{

    GThread *thread;

#ifndef _WIN32

    sigset_t set, oldset;



    sigfillset(&set);

    pthread_sigmask(SIG_SETMASK, &set, &oldset);

#endif

    thread = g_thread_create(writeout_thread, NULL, FALSE, NULL);

#ifndef _WIN32

    pthread_sigmask(SIG_SETMASK, &oldset, NULL);

#endif



    return thread;

}
","static GThread *trace_thread_create(GThreadFunc fn) {
GThread *thread; #ifndef _WIN32 sigset_t set, oldset; sigfillset(&set); pthread_sigmask(SIG_SETMASK, &set, &oldset); #endif thread = g_thread_create(writeout_thread, NULL, FALSE, NULL); #ifndef _WIN32 pthread_sigmask(SIG_SETMASK, &oldset, NULL); #endif return thread; } ",qemu,1
"static QPCIDevice *get_device(void)

{

    QPCIDevice *dev;

    QPCIBus *pcibus;



    pcibus = qpci_init_pc();

    dev = NULL;

    qpci_device_foreach(pcibus, 0x1af4, 0x1110, save_fn, &dev);

    g_assert(dev != NULL);



    return dev;

}
","static QPCIDevice *get_device(void) {
QPCIDevice *dev; QPCIBus *pcibus; pcibus = qpci_init_pc(); dev = NULL; qpci_device_foreach(pcibus, 0x1af4, 0x1110, save_fn, &dev); g_assert(dev != NULL); return dev; } ",qemu,1
"static void check_suspend_mode(GuestSuspendMode mode, Error **errp)

{

    SYSTEM_POWER_CAPABILITIES sys_pwr_caps;

    Error *local_err = NULL;



    if (error_is_set(errp)) {

        return;

    }

    ZeroMemory(&sys_pwr_caps, sizeof(sys_pwr_caps));

    if (!GetPwrCapabilities(&sys_pwr_caps)) {

        error_set(&local_err, QERR_QGA_COMMAND_FAILED,

                  ""failed to determine guest suspend capabilities"");

        goto out;

    }



    switch (mode) {

    case GUEST_SUSPEND_MODE_DISK:

        if (!sys_pwr_caps.SystemS4) {

            error_set(&local_err, QERR_QGA_COMMAND_FAILED,

                      ""suspend-to-disk not supported by OS"");

        }

        break;

    case GUEST_SUSPEND_MODE_RAM:

        if (!sys_pwr_caps.SystemS3) {

            error_set(&local_err, QERR_QGA_COMMAND_FAILED,

                      ""suspend-to-ram not supported by OS"");

        }

        break;

    default:

        error_set(&local_err, QERR_INVALID_PARAMETER_VALUE, ""mode"",

                  ""GuestSuspendMode"");

    }



out:

    if (local_err) {

        error_propagate(errp, local_err);

    }

}
","static void check_suspend_mode(GuestSuspendMode mode, Error **errp) {
SYSTEM_POWER_CAPABILITIES sys_pwr_caps; Error *local_err = NULL; if (error_is_set(errp)) {
return; } ZeroMemory(&sys_pwr_caps, sizeof(sys_pwr_caps)); if (!GetPwrCapabilities(&sys_pwr_caps)) {
error_set(&local_err, QERR_QGA_COMMAND_FAILED, ""failed to determine guest suspend capabilities""); goto out; } switch (mode) {
case GUEST_SUSPEND_MODE_DISK: if (!sys_pwr_caps.SystemS4) {
error_set(&local_err, QERR_QGA_COMMAND_FAILED, ""suspend-to-disk not supported by OS""); } break; case GUEST_SUSPEND_MODE_RAM: if (!sys_pwr_caps.SystemS3) {
error_set(&local_err, QERR_QGA_COMMAND_FAILED, ""suspend-to-ram not supported by OS""); } break; default: error_set(&local_err, QERR_INVALID_PARAMETER_VALUE, ""mode"", ""GuestSuspendMode""); } out: if (local_err) {
error_propagate(errp, local_err); } } ",qemu,1
"int ga_install_service(const char *path, const char *logfile,

                       const char *state_dir)

{

    int ret = EXIT_FAILURE;

    SC_HANDLE manager;

    SC_HANDLE service;

    TCHAR module_fname[MAX_PATH];

    GString *cmdline;

    SERVICE_DESCRIPTION desc = { (char *)QGA_SERVICE_DESCRIPTION };



    if (GetModuleFileName(NULL, module_fname, MAX_PATH) == 0) {

        printf_win_error(""No full path to service's executable"");

        return EXIT_FAILURE;

    }



    cmdline = g_string_new(module_fname);

    g_string_append(cmdline, "" -d"");



    if (path) {

        g_string_append_printf(cmdline, "" -p %s"", path);

    }

    if (logfile) {

        g_string_append_printf(cmdline, "" -l %s -v"", logfile);

    }

    if (state_dir) {

        g_string_append_printf(cmdline, "" -t %s"", state_dir);

    }



    g_debug(""service's cmdline: %s"", cmdline->str);



    manager = OpenSCManager(NULL, NULL, SC_MANAGER_ALL_ACCESS);

    if (manager == NULL) {

        printf_win_error(""No handle to service control manager"");

        goto out_strings;

    }



    service = CreateService(manager, QGA_SERVICE_NAME, QGA_SERVICE_DISPLAY_NAME,

        SERVICE_ALL_ACCESS, SERVICE_WIN32_OWN_PROCESS, SERVICE_AUTO_START,

        SERVICE_ERROR_NORMAL, cmdline->str, NULL, NULL, NULL, NULL, NULL);

    if (service == NULL) {

        printf_win_error(""Failed to install service"");

        goto out_manager;

    }



    ChangeServiceConfig2(service, SERVICE_CONFIG_DESCRIPTION, &desc);

    fprintf(stderr, ""Service was installed successfully.\n"");

    ret = EXIT_SUCCESS;

    CloseServiceHandle(service);



out_manager:

    CloseServiceHandle(manager);



out_strings:

    g_string_free(cmdline, TRUE);

    return ret;

}
","int ga_install_service(const char *path, const char *logfile, const char *state_dir) {
int ret = EXIT_FAILURE; SC_HANDLE manager; SC_HANDLE service; TCHAR module_fname[MAX_PATH]; GString *cmdline; SERVICE_DESCRIPTION desc = {
(char *)QGA_SERVICE_DESCRIPTION }; if (GetModuleFileName(NULL, module_fname, MAX_PATH) == 0) {
printf_win_error(""No full path to service's executable""); return EXIT_FAILURE; } cmdline = g_string_new(module_fname); g_string_append(cmdline, "" -d""); if (path) {
g_string_append_printf(cmdline, "" -p %s"", path); } if (logfile) {
g_string_append_printf(cmdline, "" -l %s -v"", logfile); } if (state_dir) {
g_string_append_printf(cmdline, "" -t %s"", state_dir); } g_debug(""service's cmdline: %s"", cmdline->str); manager = OpenSCManager(NULL, NULL, SC_MANAGER_ALL_ACCESS); if (manager == NULL) {
printf_win_error(""No handle to service control manager""); goto out_strings; } service = CreateService(manager, QGA_SERVICE_NAME, QGA_SERVICE_DISPLAY_NAME, SERVICE_ALL_ACCESS, SERVICE_WIN32_OWN_PROCESS, SERVICE_AUTO_START, SERVICE_ERROR_NORMAL, cmdline->str, NULL, NULL, NULL, NULL, NULL); if (service == NULL) {
printf_win_error(""Failed to install service""); goto out_manager; } ChangeServiceConfig2(service, SERVICE_CONFIG_DESCRIPTION, &desc); fprintf(stderr, ""Service was installed successfully.\n""); ret = EXIT_SUCCESS; CloseServiceHandle(service); out_manager: CloseServiceHandle(manager); out_strings: g_string_free(cmdline, TRUE); return ret; } ",qemu,1
"QObject *object_property_get_qobject(Object *obj, const char *name,

                                     Error **errp)

{

    QObject *ret = NULL;

    Error *local_err = NULL;

    Visitor *v;



    v = qmp_output_visitor_new(&ret);

    object_property_get(obj, v, name, &local_err);

    if (!local_err) {

        visit_complete(v, &ret);

    }

    error_propagate(errp, local_err);

    visit_free(v);

    return ret;

}
","QObject *object_property_get_qobject(Object *obj, const char *name, Error **errp) {
QObject *ret = NULL; Error *local_err = NULL; Visitor *v; v = qmp_output_visitor_new(&ret); object_property_get(obj, v, name, &local_err); if (!local_err) {
visit_complete(v, &ret); } error_propagate(errp, local_err); visit_free(v); return ret; } ",qemu,0
"void arm_cpu_do_interrupt(CPUState *cs)

{

    ARMCPU *cpu = ARM_CPU(cs);

    CPUARMState *env = &cpu->env;

    uint32_t addr;

    uint32_t mask;

    int new_mode;

    uint32_t offset;

    uint32_t moe;



    assert(!IS_M(env));



    arm_log_exception(cs->exception_index);



    if (arm_is_psci_call(cpu, cs->exception_index)) {

        arm_handle_psci_call(cpu);

        qemu_log_mask(CPU_LOG_INT, ""...handled as PSCI call\n"");

        return;

    }



    /* If this is a debug exception we must update the DBGDSCR.MOE bits */

    switch (env->exception.syndrome >> ARM_EL_EC_SHIFT) {

    case EC_BREAKPOINT:

    case EC_BREAKPOINT_SAME_EL:

        moe = 1;

        break;

    case EC_WATCHPOINT:

    case EC_WATCHPOINT_SAME_EL:

        moe = 10;

        break;

    case EC_AA32_BKPT:

        moe = 3;

        break;

    case EC_VECTORCATCH:

        moe = 5;

        break;

    default:

        moe = 0;

        break;

    }



    if (moe) {

        env->cp15.mdscr_el1 = deposit64(env->cp15.mdscr_el1, 2, 4, moe);

    }



    /* TODO: Vectored interrupt controller.  */

    switch (cs->exception_index) {

    case EXCP_UDEF:

        new_mode = ARM_CPU_MODE_UND;

        addr = 0x04;

        mask = CPSR_I;

        if (env->thumb)

            offset = 2;

        else

            offset = 4;

        break;

    case EXCP_SWI:

        if (semihosting_enabled) {

            /* Check for semihosting interrupt.  */

            if (env->thumb) {

                mask = arm_lduw_code(env, env->regs[15] - 2, env->bswap_code)

                    & 0xff;

            } else {

                mask = arm_ldl_code(env, env->regs[15] - 4, env->bswap_code)

                    & 0xffffff;

            }

            /* Only intercept calls from privileged modes, to provide some

               semblance of security.  */

            if (((mask == 0x123456 && !env->thumb)

                    || (mask == 0xab && env->thumb))

                  && (env->uncached_cpsr & CPSR_M) != ARM_CPU_MODE_USR) {

                env->regs[0] = do_arm_semihosting(env);

                qemu_log_mask(CPU_LOG_INT, ""...handled as semihosting call\n"");

                return;

            }

        }

        new_mode = ARM_CPU_MODE_SVC;

        addr = 0x08;

        mask = CPSR_I;

        /* The PC already points to the next instruction.  */

        offset = 0;

        break;

    case EXCP_BKPT:

        /* See if this is a semihosting syscall.  */

        if (env->thumb && semihosting_enabled) {

            mask = arm_lduw_code(env, env->regs[15], env->bswap_code) & 0xff;

            if (mask == 0xab

                  && (env->uncached_cpsr & CPSR_M) != ARM_CPU_MODE_USR) {

                env->regs[15] += 2;

                env->regs[0] = do_arm_semihosting(env);

                qemu_log_mask(CPU_LOG_INT, ""...handled as semihosting call\n"");

                return;

            }

        }

        env->exception.fsr = 2;

        /* Fall through to prefetch abort.  */

    case EXCP_PREFETCH_ABORT:

        env->cp15.ifsr_el2 = env->exception.fsr;

        env->cp15.far_el[1] = deposit64(env->cp15.far_el[1], 32, 32,

                                        env->exception.vaddress);

        qemu_log_mask(CPU_LOG_INT, ""...with IFSR 0x%x IFAR 0x%x\n"",

                      env->cp15.ifsr_el2, (uint32_t)env->exception.vaddress);

        new_mode = ARM_CPU_MODE_ABT;

        addr = 0x0c;

        mask = CPSR_A | CPSR_I;

        offset = 4;

        break;

    case EXCP_DATA_ABORT:

        env->cp15.esr_el[1] = env->exception.fsr;

        env->cp15.far_el[1] = deposit64(env->cp15.far_el[1], 0, 32,

                                        env->exception.vaddress);

        qemu_log_mask(CPU_LOG_INT, ""...with DFSR 0x%x DFAR 0x%x\n"",

                      (uint32_t)env->cp15.esr_el[1],

                      (uint32_t)env->exception.vaddress);

        new_mode = ARM_CPU_MODE_ABT;

        addr = 0x10;

        mask = CPSR_A | CPSR_I;

        offset = 8;

        break;

    case EXCP_IRQ:

        new_mode = ARM_CPU_MODE_IRQ;

        addr = 0x18;

        /* Disable IRQ and imprecise data aborts.  */

        mask = CPSR_A | CPSR_I;

        offset = 4;

        if (env->cp15.scr_el3 & SCR_IRQ) {

            /* IRQ routed to monitor mode */

            new_mode = ARM_CPU_MODE_MON;

            mask |= CPSR_F;

        }

        break;

    case EXCP_FIQ:

        new_mode = ARM_CPU_MODE_FIQ;

        addr = 0x1c;

        /* Disable FIQ, IRQ and imprecise data aborts.  */

        mask = CPSR_A | CPSR_I | CPSR_F;

        if (env->cp15.scr_el3 & SCR_FIQ) {

            /* FIQ routed to monitor mode */

            new_mode = ARM_CPU_MODE_MON;

        }

        offset = 4;

        break;

    case EXCP_SMC:

        new_mode = ARM_CPU_MODE_MON;

        addr = 0x08;

        mask = CPSR_A | CPSR_I | CPSR_F;

        offset = 0;

        break;

    default:

        cpu_abort(cs, ""Unhandled exception 0x%x\n"", cs->exception_index);

        return; /* Never happens.  Keep compiler happy.  */

    }



    if (new_mode == ARM_CPU_MODE_MON) {

        addr += env->cp15.mvbar;

    } else if (A32_BANKED_CURRENT_REG_GET(env, sctlr) & SCTLR_V) {

        /* High vectors. When enabled, base address cannot be remapped. */

        addr += 0xffff0000;

    } else {

        /* ARM v7 architectures provide a vector base address register to remap

         * the interrupt vector table.

         * This register is only followed in non-monitor mode, and is banked.

         * Note: only bits 31:5 are valid.

         */

        addr += env->cp15.vbar_el[1];

    }



    if ((env->uncached_cpsr & CPSR_M) == ARM_CPU_MODE_MON) {

        env->cp15.scr_el3 &= ~SCR_NS;

    }



    switch_mode (env, new_mode);

    /* For exceptions taken to AArch32 we must clear the SS bit in both

     * PSTATE and in the old-state value we save to SPSR_<mode>, so zero it now.

     */

    env->uncached_cpsr &= ~PSTATE_SS;

    env->spsr = cpsr_read(env);

    /* Clear IT bits.  */

    env->condexec_bits = 0;

    /* Switch to the new mode, and to the correct instruction set.  */

    env->uncached_cpsr = (env->uncached_cpsr & ~CPSR_M) | new_mode;

    env->daif |= mask;

    /* this is a lie, as the was no c1_sys on V4T/V5, but who cares

     * and we should just guard the thumb mode on V4 */

    if (arm_feature(env, ARM_FEATURE_V4T)) {

        env->thumb = (A32_BANKED_CURRENT_REG_GET(env, sctlr) & SCTLR_TE) != 0;

    }

    env->regs[14] = env->regs[15] + offset;

    env->regs[15] = addr;

    cs->interrupt_request |= CPU_INTERRUPT_EXITTB;

}
","void arm_cpu_do_interrupt(CPUState *cs) {
ARMCPU *cpu = ARM_CPU(cs); CPUARMState *env = &cpu->env; uint32_t addr; uint32_t mask; int new_mode; uint32_t offset; uint32_t moe; assert(!IS_M(env)); arm_log_exception(cs->exception_index); if (arm_is_psci_call(cpu, cs->exception_index)) {
arm_handle_psci_call(cpu); qemu_log_mask(CPU_LOG_INT, ""...handled as PSCI call\n""); return; } /* If this is a debug exception we must update the DBGDSCR.MOE bits */ switch (env->exception.syndrome >> ARM_EL_EC_SHIFT) {
case EC_BREAKPOINT: case EC_BREAKPOINT_SAME_EL: moe = 1; break; case EC_WATCHPOINT: case EC_WATCHPOINT_SAME_EL: moe = 10; break; case EC_AA32_BKPT: moe = 3; break; case EC_VECTORCATCH: moe = 5; break; default: moe = 0; break; } if (moe) {
env->cp15.mdscr_el1 = deposit64(env->cp15.mdscr_el1, 2, 4, moe); } /* TODO: Vectored interrupt controller. */ switch (cs->exception_index) {
case EXCP_UDEF: new_mode = ARM_CPU_MODE_UND; addr = 0x04; mask = CPSR_I; if (env->thumb) offset = 2; else offset = 4; break; case EXCP_SWI: if (semihosting_enabled) {
/* Check for semihosting interrupt. */ if (env->thumb) {
mask = arm_lduw_code(env, env->regs[15] - 2, env->bswap_code) & 0xff; } else {
mask = arm_ldl_code(env, env->regs[15] - 4, env->bswap_code) & 0xffffff; } /* Only intercept calls from privileged modes, to provide some semblance of security. */ if (((mask == 0x123456 && !env->thumb) || (mask == 0xab && env->thumb)) && (env->uncached_cpsr & CPSR_M) != ARM_CPU_MODE_USR) {
env->regs[0] = do_arm_semihosting(env); qemu_log_mask(CPU_LOG_INT, ""...handled as semihosting call\n""); return; } } new_mode = ARM_CPU_MODE_SVC; addr = 0x08; mask = CPSR_I; /* The PC already points to the next instruction. */ offset = 0; break; case EXCP_BKPT: /* See if this is a semihosting syscall. */ if (env->thumb && semihosting_enabled) {
mask = arm_lduw_code(env, env->regs[15], env->bswap_code) & 0xff; if (mask == 0xab && (env->uncached_cpsr & CPSR_M) != ARM_CPU_MODE_USR) {
env->regs[15] += 2; env->regs[0] = do_arm_semihosting(env); qemu_log_mask(CPU_LOG_INT, ""...handled as semihosting call\n""); return; } } env->exception.fsr = 2; /* Fall through to prefetch abort. */ case EXCP_PREFETCH_ABORT: env->cp15.ifsr_el2 = env->exception.fsr; env->cp15.far_el[1] = deposit64(env->cp15.far_el[1], 32, 32, env->exception.vaddress); qemu_log_mask(CPU_LOG_INT, ""...with IFSR 0x%x IFAR 0x%x\n"", env->cp15.ifsr_el2, (uint32_t)env->exception.vaddress); new_mode = ARM_CPU_MODE_ABT; addr = 0x0c; mask = CPSR_A | CPSR_I; offset = 4; break; case EXCP_DATA_ABORT: env->cp15.esr_el[1] = env->exception.fsr; env->cp15.far_el[1] = deposit64(env->cp15.far_el[1], 0, 32, env->exception.vaddress); qemu_log_mask(CPU_LOG_INT, ""...with DFSR 0x%x DFAR 0x%x\n"", (uint32_t)env->cp15.esr_el[1], (uint32_t)env->exception.vaddress); new_mode = ARM_CPU_MODE_ABT; addr = 0x10; mask = CPSR_A | CPSR_I; offset = 8; break; case EXCP_IRQ: new_mode = ARM_CPU_MODE_IRQ; addr = 0x18; /* Disable IRQ and imprecise data aborts. */ mask = CPSR_A | CPSR_I; offset = 4; if (env->cp15.scr_el3 & SCR_IRQ) {
/* IRQ routed to monitor mode */ new_mode = ARM_CPU_MODE_MON; mask |= CPSR_F; } break; case EXCP_FIQ: new_mode = ARM_CPU_MODE_FIQ; addr = 0x1c; /* Disable FIQ, IRQ and imprecise data aborts. */ mask = CPSR_A | CPSR_I | CPSR_F; if (env->cp15.scr_el3 & SCR_FIQ) {
/* FIQ routed to monitor mode */ new_mode = ARM_CPU_MODE_MON; } offset = 4; break; case EXCP_SMC: new_mode = ARM_CPU_MODE_MON; addr = 0x08; mask = CPSR_A | CPSR_I | CPSR_F; offset = 0; break; default: cpu_abort(cs, ""Unhandled exception 0x%x\n"", cs->exception_index); return; /* Never happens. Keep compiler happy. */ } if (new_mode == ARM_CPU_MODE_MON) {
addr += env->cp15.mvbar; } else if (A32_BANKED_CURRENT_REG_GET(env, sctlr) & SCTLR_V) {
/* High vectors. When enabled, base address cannot be remapped. */ addr += 0xffff0000; } else {
/* ARM v7 architectures provide a vector base address register to remap * the interrupt vector table. * This register is only followed in non-monitor mode, and is banked. * Note: only bits 31:5 are valid. */ addr += env->cp15.vbar_el[1]; } if ((env->uncached_cpsr & CPSR_M) == ARM_CPU_MODE_MON) {
env->cp15.scr_el3 &= ~SCR_NS; } switch_mode (env, new_mode); /* For exceptions taken to AArch32 we must clear the SS bit in both * PSTATE and in the old-state value we save to SPSR_<mode>, so zero it now. */ env->uncached_cpsr &= ~PSTATE_SS; env->spsr = cpsr_read(env); /* Clear IT bits. */ env->condexec_bits = 0; /* Switch to the new mode, and to the correct instruction set. */ env->uncached_cpsr = (env->uncached_cpsr & ~CPSR_M) | new_mode; env->daif |= mask; /* this is a lie, as the was no c1_sys on V4T/V5, but who cares * and we should just guard the thumb mode on V4 */ if (arm_feature(env, ARM_FEATURE_V4T)) {
env->thumb = (A32_BANKED_CURRENT_REG_GET(env, sctlr) & SCTLR_TE) != 0; } env->regs[14] = env->regs[15] + offset; env->regs[15] = addr; cs->interrupt_request |= CPU_INTERRUPT_EXITTB; } ",qemu,0
"void input_type_enum(Visitor *v, int *obj, const char *strings[],

                     const char *kind, const char *name,

                     Error **errp)

{

    int64_t value = 0;

    char *enum_str;



    assert(strings);



    visit_type_str(v, &enum_str, name, errp);

    if (error_is_set(errp)) {

        return;

    }



    while (strings[value] != NULL) {

        if (strcmp(strings[value], enum_str) == 0) {

            break;

        }

        value++;

    }



    if (strings[value] == NULL) {

        error_set(errp, QERR_INVALID_PARAMETER, name ? name : ""null"");

        g_free(enum_str);

        return;

    }



    g_free(enum_str);

    *obj = value;

}
","void input_type_enum(Visitor *v, int *obj, const char *strings[], const char *kind, const char *name, Error **errp) {
int64_t value = 0; char *enum_str; assert(strings); visit_type_str(v, &enum_str, name, errp); if (error_is_set(errp)) {
return; } while (strings[value] != NULL) {
if (strcmp(strings[value], enum_str) == 0) {
break; } value++; } if (strings[value] == NULL) {
error_set(errp, QERR_INVALID_PARAMETER, name ? name : ""null""); g_free(enum_str); return; } g_free(enum_str); *obj = value; } ",qemu,0
"static void do_vm_stop(RunState state)

{

    if (runstate_is_running()) {

        cpu_disable_ticks();

        pause_all_vcpus();

        runstate_set(state);

        vm_state_notify(0, state);

        qemu_aio_flush();

        bdrv_flush_all();

        monitor_protocol_event(QEVENT_STOP, NULL);

    }

}
","static void do_vm_stop(RunState state) {
if (runstate_is_running()) {
cpu_disable_ticks(); pause_all_vcpus(); runstate_set(state); vm_state_notify(0, state); qemu_aio_flush(); bdrv_flush_all(); monitor_protocol_event(QEVENT_STOP, NULL); } } ",qemu,0
"static int nbd_establish_connection(BlockDriverState *bs)

{

    BDRVNBDState *s = bs->opaque;

    int sock;

    int ret;

    off_t size;

    size_t blocksize;



    if (s->host_spec[0] == '/') {

        sock = unix_socket_outgoing(s->host_spec);

    } else {

        sock = tcp_socket_outgoing_spec(s->host_spec);

    }



    /* Failed to establish connection */

    if (sock < 0) {

        logout(""Failed to establish connection to NBD server\n"");

        return -errno;

    }



    /* NBD handshake */

    ret = nbd_receive_negotiate(sock, s->export_name, &s->nbdflags, &size,

                                &blocksize);

    if (ret < 0) {

        logout(""Failed to negotiate with the NBD server\n"");

        closesocket(sock);

        return ret;

    }



    /* Now that we're connected, set the socket to be non-blocking and

     * kick the reply mechanism.  */

    socket_set_nonblock(sock);

    qemu_aio_set_fd_handler(s->sock, nbd_reply_ready, NULL,

                            nbd_have_request, s);



    s->sock = sock;

    s->size = size;

    s->blocksize = blocksize;



    logout(""Established connection with NBD server\n"");

    return 0;

}
","static int nbd_establish_connection(BlockDriverState *bs) {
BDRVNBDState *s = bs->opaque; int sock; int ret; off_t size; size_t blocksize; if (s->host_spec[0] == '/') {
sock = unix_socket_outgoing(s->host_spec); } else {
sock = tcp_socket_outgoing_spec(s->host_spec); } /* Failed to establish connection */ if (sock < 0) {
logout(""Failed to establish connection to NBD server\n""); return -errno; } /* NBD handshake */ ret = nbd_receive_negotiate(sock, s->export_name, &s->nbdflags, &size, &blocksize); if (ret < 0) {
logout(""Failed to negotiate with the NBD server\n""); closesocket(sock); return ret; } /* Now that we're connected, set the socket to be non-blocking and * kick the reply mechanism. */ socket_set_nonblock(sock); qemu_aio_set_fd_handler(s->sock, nbd_reply_ready, NULL, nbd_have_request, s); s->sock = sock; s->size = size; s->blocksize = blocksize; logout(""Established connection with NBD server\n""); return 0; } ",qemu,1
"int pcilg_service_call(S390CPU *cpu, uint8_t r1, uint8_t r2)

{

    CPUS390XState *env = &cpu->env;

    S390PCIBusDevice *pbdev;

    uint64_t offset;

    uint64_t data;

    MemoryRegion *mr;

    uint8_t len;

    uint32_t fh;

    uint8_t pcias;



    cpu_synchronize_state(CPU(cpu));



    if (env->psw.mask & PSW_MASK_PSTATE) {

        program_interrupt(env, PGM_PRIVILEGED, 4);

        return 0;

    }



    if (r2 & 0x1) {

        program_interrupt(env, PGM_SPECIFICATION, 4);

        return 0;

    }



    fh = env->regs[r2] >> 32;

    pcias = (env->regs[r2] >> 16) & 0xf;

    len = env->regs[r2] & 0xf;

    offset = env->regs[r2 + 1];



    pbdev = s390_pci_find_dev_by_fh(fh);

    if (!pbdev) {

        DPRINTF(""pcilg no pci dev\n"");

        setcc(cpu, ZPCI_PCI_LS_INVAL_HANDLE);

        return 0;

    }



    switch (pbdev->state) {

    case ZPCI_FS_RESERVED:

    case ZPCI_FS_STANDBY:

    case ZPCI_FS_DISABLED:

    case ZPCI_FS_PERMANENT_ERROR:

        setcc(cpu, ZPCI_PCI_LS_INVAL_HANDLE);

        return 0;

    case ZPCI_FS_ERROR:

        setcc(cpu, ZPCI_PCI_LS_ERR);

        s390_set_status_code(env, r2, ZPCI_PCI_ST_BLOCKED);

        return 0;

    default:

        break;

    }



    if (pcias < 6) {

        if ((8 - (offset & 0x7)) < len) {

            program_interrupt(env, PGM_OPERAND, 4);

            return 0;

        }

        mr = pbdev->pdev->io_regions[pcias].memory;

        memory_region_dispatch_read(mr, offset, &data, len,

                                    MEMTXATTRS_UNSPECIFIED);

    } else if (pcias == 15) {

        if ((4 - (offset & 0x3)) < len) {

            program_interrupt(env, PGM_OPERAND, 4);

            return 0;

        }

        data =  pci_host_config_read_common(

                   pbdev->pdev, offset, pci_config_size(pbdev->pdev), len);



        switch (len) {

        case 1:

            break;

        case 2:

            data = bswap16(data);

            break;

        case 4:

            data = bswap32(data);

            break;

        case 8:

            data = bswap64(data);

            break;

        default:

            program_interrupt(env, PGM_OPERAND, 4);

            return 0;

        }

    } else {

        DPRINTF(""invalid space\n"");

        setcc(cpu, ZPCI_PCI_LS_ERR);

        s390_set_status_code(env, r2, ZPCI_PCI_ST_INVAL_AS);

        return 0;

    }



    env->regs[r1] = data;

    setcc(cpu, ZPCI_PCI_LS_OK);

    return 0;

}
","int pcilg_service_call(S390CPU *cpu, uint8_t r1, uint8_t r2) {
CPUS390XState *env = &cpu->env; S390PCIBusDevice *pbdev; uint64_t offset; uint64_t data; MemoryRegion *mr; uint8_t len; uint32_t fh; uint8_t pcias; cpu_synchronize_state(CPU(cpu)); if (env->psw.mask & PSW_MASK_PSTATE) {
program_interrupt(env, PGM_PRIVILEGED, 4); return 0; } if (r2 & 0x1) {
program_interrupt(env, PGM_SPECIFICATION, 4); return 0; } fh = env->regs[r2] >> 32; pcias = (env->regs[r2] >> 16) & 0xf; len = env->regs[r2] & 0xf; offset = env->regs[r2 + 1]; pbdev = s390_pci_find_dev_by_fh(fh); if (!pbdev) {
DPRINTF(""pcilg no pci dev\n""); setcc(cpu, ZPCI_PCI_LS_INVAL_HANDLE); return 0; } switch (pbdev->state) {
case ZPCI_FS_RESERVED: case ZPCI_FS_STANDBY: case ZPCI_FS_DISABLED: case ZPCI_FS_PERMANENT_ERROR: setcc(cpu, ZPCI_PCI_LS_INVAL_HANDLE); return 0; case ZPCI_FS_ERROR: setcc(cpu, ZPCI_PCI_LS_ERR); s390_set_status_code(env, r2, ZPCI_PCI_ST_BLOCKED); return 0; default: break; } if (pcias < 6) {
if ((8 - (offset & 0x7)) < len) {
program_interrupt(env, PGM_OPERAND, 4); return 0; } mr = pbdev->pdev->io_regions[pcias].memory; memory_region_dispatch_read(mr, offset, &data, len, MEMTXATTRS_UNSPECIFIED); } else if (pcias == 15) {
if ((4 - (offset & 0x3)) < len) {
program_interrupt(env, PGM_OPERAND, 4); return 0; } data = pci_host_config_read_common( pbdev->pdev, offset, pci_config_size(pbdev->pdev), len); switch (len) {
case 1: break; case 2: data = bswap16(data); break; case 4: data = bswap32(data); break; case 8: data = bswap64(data); break; default: program_interrupt(env, PGM_OPERAND, 4); return 0; } } else {
DPRINTF(""invalid space\n""); setcc(cpu, ZPCI_PCI_LS_ERR); s390_set_status_code(env, r2, ZPCI_PCI_ST_INVAL_AS); return 0; } env->regs[r1] = data; setcc(cpu, ZPCI_PCI_LS_OK); return 0; } ",qemu,1
"static int vvfat_open(BlockDriverState *bs, QDict *options, int flags,

                      Error **errp)

{

    BDRVVVFATState *s = bs->opaque;

    int cyls, heads, secs;

    bool floppy;

    const char *dirname, *label;

    QemuOpts *opts;

    Error *local_err = NULL;

    int ret;



#ifdef DEBUG

    vvv = s;

#endif



    opts = qemu_opts_create(&runtime_opts, NULL, 0, &error_abort);

    qemu_opts_absorb_qdict(opts, options, &local_err);

    if (local_err) {

        error_propagate(errp, local_err);

        ret = -EINVAL;

        goto fail;

    }



    dirname = qemu_opt_get(opts, ""dir"");

    if (!dirname) {

        error_setg(errp, ""vvfat block driver requires a 'dir' option"");

        ret = -EINVAL;

        goto fail;

    }



    s->fat_type = qemu_opt_get_number(opts, ""fat-type"", 0);

    floppy = qemu_opt_get_bool(opts, ""floppy"", false);



    memset(s->volume_label, ' ', sizeof(s->volume_label));

    label = qemu_opt_get(opts, ""label"");

    if (label) {

        size_t label_length = strlen(label);

        if (label_length > 11) {

            error_setg(errp, ""vvfat label cannot be longer than 11 bytes"");

            ret = -EINVAL;

            goto fail;

        }

        memcpy(s->volume_label, label, label_length);

    } else {

        memcpy(s->volume_label, ""QEMU VVFAT"", 10);

    }



    if (floppy) {

        /* 1.44MB or 2.88MB floppy.  2.88MB can be FAT12 (default) or FAT16. */

        if (!s->fat_type) {

            s->fat_type = 12;

            secs = 36;

            s->sectors_per_cluster = 2;

        } else {

            secs = s->fat_type == 12 ? 18 : 36;

            s->sectors_per_cluster = 1;

        }

        cyls = 80;

        heads = 2;

    } else {

        /* 32MB or 504MB disk*/

        if (!s->fat_type) {

            s->fat_type = 16;

        }

        s->offset_to_bootsector = 0x3f;

        cyls = s->fat_type == 12 ? 64 : 1024;

        heads = 16;

        secs = 63;

    }



    switch (s->fat_type) {

    case 32:

            fprintf(stderr, ""Big fat greek warning: FAT32 has not been tested. ""

                ""You are welcome to do so!\n"");

        break;

    case 16:

    case 12:

        break;

    default:

        error_setg(errp, ""Valid FAT types are only 12, 16 and 32"");

        ret = -EINVAL;

        goto fail;

    }





    s->bs = bs;



    /* LATER TODO: if FAT32, adjust */

    s->sectors_per_cluster=0x10;



    s->current_cluster=0xffffffff;



    s->qcow = NULL;

    s->qcow_filename = NULL;

    s->fat2 = NULL;

    s->downcase_short_names = 1;



    fprintf(stderr, ""vvfat %s chs %d,%d,%d\n"",

            dirname, cyls, heads, secs);



    s->sector_count = cyls * heads * secs - s->offset_to_bootsector;



    if (qemu_opt_get_bool(opts, ""rw"", false)) {

        if (!bdrv_is_read_only(bs)) {

            ret = enable_write_target(bs, errp);

            if (ret < 0) {

                goto fail;

            }

        } else {

            ret = -EPERM;

            error_setg(errp,

                       ""Unable to set VVFAT to 'rw' when drive is read-only"");

            goto fail;

        }

    } else  {

        /* read only is the default for safety */

        ret = bdrv_set_read_only(bs, true, &local_err);

        if (ret < 0) {

            error_propagate(errp, local_err);

            goto fail;

        }

    }



    bs->total_sectors = cyls * heads * secs;



    if (init_directories(s, dirname, heads, secs, errp)) {

        ret = -EIO;

        goto fail;

    }



    s->sector_count = s->offset_to_root_dir

                    + s->sectors_per_cluster * s->cluster_count;



    /* Disable migration when vvfat is used rw */

    if (s->qcow) {

        error_setg(&s->migration_blocker,

                   ""The vvfat (rw) format used by node '%s' ""

                   ""does not support live migration"",

                   bdrv_get_device_or_node_name(bs));

        ret = migrate_add_blocker(s->migration_blocker, &local_err);

        if (local_err) {

            error_propagate(errp, local_err);

            error_free(s->migration_blocker);

            goto fail;

        }

    }



    if (s->offset_to_bootsector > 0) {

        init_mbr(s, cyls, heads, secs);

    }



    qemu_co_mutex_init(&s->lock);



    ret = 0;

fail:

    qemu_opts_del(opts);

    return ret;

}
","static int vvfat_open(BlockDriverState *bs, QDict *options, int flags, Error **errp) {
BDRVVVFATState *s = bs->opaque; int cyls, heads, secs; bool floppy; const char *dirname, *label; QemuOpts *opts; Error *local_err = NULL; int ret; #ifdef DEBUG vvv = s; #endif opts = qemu_opts_create(&runtime_opts, NULL, 0, &error_abort); qemu_opts_absorb_qdict(opts, options, &local_err); if (local_err) {
error_propagate(errp, local_err); ret = -EINVAL; goto fail; } dirname = qemu_opt_get(opts, ""dir""); if (!dirname) {
error_setg(errp, ""vvfat block driver requires a 'dir' option""); ret = -EINVAL; goto fail; } s->fat_type = qemu_opt_get_number(opts, ""fat-type"", 0); floppy = qemu_opt_get_bool(opts, ""floppy"", false); memset(s->volume_label, ' ', sizeof(s->volume_label)); label = qemu_opt_get(opts, ""label""); if (label) {
size_t label_length = strlen(label); if (label_length > 11) {
error_setg(errp, ""vvfat label cannot be longer than 11 bytes""); ret = -EINVAL; goto fail; } memcpy(s->volume_label, label, label_length); } else {
memcpy(s->volume_label, ""QEMU VVFAT"", 10); } if (floppy) {
/* 1.44MB or 2.88MB floppy. 2.88MB can be FAT12 (default) or FAT16. */ if (!s->fat_type) {
s->fat_type = 12; secs = 36; s->sectors_per_cluster = 2; } else {
secs = s->fat_type == 12 ? 18 : 36; s->sectors_per_cluster = 1; } cyls = 80; heads = 2; } else {
/* 32MB or 504MB disk*/ if (!s->fat_type) {
s->fat_type = 16; } s->offset_to_bootsector = 0x3f; cyls = s->fat_type == 12 ? 64 : 1024; heads = 16; secs = 63; } switch (s->fat_type) {
case 32: fprintf(stderr, ""Big fat greek warning: FAT32 has not been tested. "" ""You are welcome to do so!\n""); break; case 16: case 12: break; default: error_setg(errp, ""Valid FAT types are only 12, 16 and 32""); ret = -EINVAL; goto fail; } s->bs = bs; /* LATER TODO: if FAT32, adjust */ s->sectors_per_cluster=0x10; s->current_cluster=0xffffffff; s->qcow = NULL; s->qcow_filename = NULL; s->fat2 = NULL; s->downcase_short_names = 1; fprintf(stderr, ""vvfat %s chs %d,%d,%d\n"", dirname, cyls, heads, secs); s->sector_count = cyls * heads * secs - s->offset_to_bootsector; if (qemu_opt_get_bool(opts, ""rw"", false)) {
if (!bdrv_is_read_only(bs)) {
ret = enable_write_target(bs, errp); if (ret < 0) {
goto fail; } } else {
ret = -EPERM; error_setg(errp, ""Unable to set VVFAT to 'rw' when drive is read-only""); goto fail; } } else {
/* read only is the default for safety */ ret = bdrv_set_read_only(bs, true, &local_err); if (ret < 0) {
error_propagate(errp, local_err); goto fail; } } bs->total_sectors = cyls * heads * secs; if (init_directories(s, dirname, heads, secs, errp)) {
ret = -EIO; goto fail; } s->sector_count = s->offset_to_root_dir + s->sectors_per_cluster * s->cluster_count; /* Disable migration when vvfat is used rw */ if (s->qcow) {
error_setg(&s->migration_blocker, ""The vvfat (rw) format used by node '%s' "" ""does not support live migration"", bdrv_get_device_or_node_name(bs)); ret = migrate_add_blocker(s->migration_blocker, &local_err); if (local_err) {
error_propagate(errp, local_err); error_free(s->migration_blocker); goto fail; } } if (s->offset_to_bootsector > 0) {
init_mbr(s, cyls, heads, secs); } qemu_co_mutex_init(&s->lock); ret = 0; fail: qemu_opts_del(opts); return ret; } ",qemu,0
"static int get_phys_addr_v5(CPUARMState *env, uint32_t address, int access_type,

                            int is_user, hwaddr *phys_ptr,

                            int *prot, target_ulong *page_size)

{

    CPUState *cs = CPU(arm_env_get_cpu(env));

    int code;

    uint32_t table;

    uint32_t desc;

    int type;

    int ap;

    int domain = 0;

    int domain_prot;

    hwaddr phys_addr;



    /* Pagetable walk.  */

    /* Lookup l1 descriptor.  */

    if (!get_level1_table_address(env, &table, address)) {

        /* Section translation fault if page walk is disabled by PD0 or PD1 */

        code = 5;

        goto do_fault;

    }

    desc = ldl_phys(cs->as, table);

    type = (desc & 3);

    domain = (desc >> 5) & 0x0f;

    domain_prot = (env->cp15.c3 >> (domain * 2)) & 3;

    if (type == 0) {

        /* Section translation fault.  */

        code = 5;

        goto do_fault;

    }

    if (domain_prot == 0 || domain_prot == 2) {

        if (type == 2)

            code = 9; /* Section domain fault.  */

        else

            code = 11; /* Page domain fault.  */

        goto do_fault;

    }

    if (type == 2) {

        /* 1Mb section.  */

        phys_addr = (desc & 0xfff00000) | (address & 0x000fffff);

        ap = (desc >> 10) & 3;

        code = 13;

        *page_size = 1024 * 1024;

    } else {

        /* Lookup l2 entry.  */

	if (type == 1) {

	    /* Coarse pagetable.  */

	    table = (desc & 0xfffffc00) | ((address >> 10) & 0x3fc);

	} else {

	    /* Fine pagetable.  */

	    table = (desc & 0xfffff000) | ((address >> 8) & 0xffc);

	}

        desc = ldl_phys(cs->as, table);

        switch (desc & 3) {

        case 0: /* Page translation fault.  */

            code = 7;

            goto do_fault;

        case 1: /* 64k page.  */

            phys_addr = (desc & 0xffff0000) | (address & 0xffff);

            ap = (desc >> (4 + ((address >> 13) & 6))) & 3;

            *page_size = 0x10000;

            break;

        case 2: /* 4k page.  */

            phys_addr = (desc & 0xfffff000) | (address & 0xfff);

            ap = (desc >> (4 + ((address >> 9) & 6))) & 3;

            *page_size = 0x1000;

            break;

        case 3: /* 1k page.  */

	    if (type == 1) {

		if (arm_feature(env, ARM_FEATURE_XSCALE)) {

		    phys_addr = (desc & 0xfffff000) | (address & 0xfff);

		} else {

		    /* Page translation fault.  */

		    code = 7;

		    goto do_fault;

		}

	    } else {

		phys_addr = (desc & 0xfffffc00) | (address & 0x3ff);

	    }

            ap = (desc >> 4) & 3;

            *page_size = 0x400;

            break;

        default:

            /* Never happens, but compiler isn't smart enough to tell.  */

            abort();

        }

        code = 15;

    }

    *prot = check_ap(env, ap, domain_prot, access_type, is_user);

    if (!*prot) {

        /* Access permission fault.  */

        goto do_fault;

    }

    *prot |= PAGE_EXEC;

    *phys_ptr = phys_addr;

    return 0;

do_fault:

    return code | (domain << 4);

}
","static int get_phys_addr_v5(CPUARMState *env, uint32_t address, int access_type, int is_user, hwaddr *phys_ptr, int *prot, target_ulong *page_size) {
CPUState *cs = CPU(arm_env_get_cpu(env)); int code; uint32_t table; uint32_t desc; int type; int ap; int domain = 0; int domain_prot; hwaddr phys_addr; /* Pagetable walk. */ /* Lookup l1 descriptor. */ if (!get_level1_table_address(env, &table, address)) {
/* Section translation fault if page walk is disabled by PD0 or PD1 */ code = 5; goto do_fault; } desc = ldl_phys(cs->as, table); type = (desc & 3); domain = (desc >> 5) & 0x0f; domain_prot = (env->cp15.c3 >> (domain * 2)) & 3; if (type == 0) {
/* Section translation fault. */ code = 5; goto do_fault; } if (domain_prot == 0 || domain_prot == 2) {
if (type == 2) code = 9; /* Section domain fault. */ else code = 11; /* Page domain fault. */ goto do_fault; } if (type == 2) {
/* 1Mb section. */ phys_addr = (desc & 0xfff00000) | (address & 0x000fffff); ap = (desc >> 10) & 3; code = 13; *page_size = 1024 * 1024; } else {
/* Lookup l2 entry. */ if (type == 1) {
/* Coarse pagetable. */ table = (desc & 0xfffffc00) | ((address >> 10) & 0x3fc); } else {
/* Fine pagetable. */ table = (desc & 0xfffff000) | ((address >> 8) & 0xffc); } desc = ldl_phys(cs->as, table); switch (desc & 3) {
case 0: /* Page translation fault. */ code = 7; goto do_fault; case 1: /* 64k page. */ phys_addr = (desc & 0xffff0000) | (address & 0xffff); ap = (desc >> (4 + ((address >> 13) & 6))) & 3; *page_size = 0x10000; break; case 2: /* 4k page. */ phys_addr = (desc & 0xfffff000) | (address & 0xfff); ap = (desc >> (4 + ((address >> 9) & 6))) & 3; *page_size = 0x1000; break; case 3: /* 1k page. */ if (type == 1) {
if (arm_feature(env, ARM_FEATURE_XSCALE)) {
phys_addr = (desc & 0xfffff000) | (address & 0xfff); } else {
/* Page translation fault. */ code = 7; goto do_fault; } } else {
phys_addr = (desc & 0xfffffc00) | (address & 0x3ff); } ap = (desc >> 4) & 3; *page_size = 0x400; break; default: /* Never happens, but compiler isn't smart enough to tell. */ abort(); } code = 15; } *prot = check_ap(env, ap, domain_prot, access_type, is_user); if (!*prot) {
/* Access permission fault. */ goto do_fault; } *prot |= PAGE_EXEC; *phys_ptr = phys_addr; return 0; do_fault: return code | (domain << 4); } ",qemu,0
"static int stellaris_sys_init(uint32_t base, qemu_irq irq,

                              stellaris_board_info * board,

                              uint8_t *macaddr)

{

    ssys_state *s;



    s = (ssys_state *)g_malloc0(sizeof(ssys_state));

    s->irq = irq;

    s->board = board;

    /* Most devices come preprogrammed with a MAC address in the user data. */

    s->user0 = macaddr[0] | (macaddr[1] << 8) | (macaddr[2] << 16);

    s->user1 = macaddr[3] | (macaddr[4] << 8) | (macaddr[5] << 16);



    memory_region_init_io(&s->iomem, NULL, &ssys_ops, s, ""ssys"", 0x00001000);

    memory_region_add_subregion(get_system_memory(), base, &s->iomem);

    ssys_reset(s);

    vmstate_register(NULL, -1, &vmstate_stellaris_sys, s);

    return 0;

}
","static int stellaris_sys_init(uint32_t base, qemu_irq irq, stellaris_board_info * board, uint8_t *macaddr) {
ssys_state *s; s = (ssys_state *)g_malloc0(sizeof(ssys_state)); s->irq = irq; s->board = board; /* Most devices come preprogrammed with a MAC address in the user data. */ s->user0 = macaddr[0] | (macaddr[1] << 8) | (macaddr[2] << 16); s->user1 = macaddr[3] | (macaddr[4] << 8) | (macaddr[5] << 16); memory_region_init_io(&s->iomem, NULL, &ssys_ops, s, ""ssys"", 0x00001000); memory_region_add_subregion(get_system_memory(), base, &s->iomem); ssys_reset(s); vmstate_register(NULL, -1, &vmstate_stellaris_sys, s); return 0; } ",qemu,1
"static void dump_regs(TCGContext *s)

{

    TCGTemp *ts;

    int i;

    char buf[64];



    for(i = 0; i < s->nb_temps; i++) {

        ts = &s->temps[i];

        printf(""  %10s: "", tcg_get_arg_str_idx(s, buf, sizeof(buf), i));

        switch(ts->val_type) {

        case TEMP_VAL_REG:

            printf(""%s"", tcg_target_reg_names[ts->reg]);

            break;

        case TEMP_VAL_MEM:

            printf(""%d(%s)"", (int)ts->mem_offset, tcg_target_reg_names[ts->mem_reg]);

            break;

        case TEMP_VAL_CONST:

            printf(""$0x%"" TCG_PRIlx, ts->val);

            break;

        case TEMP_VAL_DEAD:

            printf(""D"");

            break;

        default:

            printf(""???"");

            break;

        }

        printf(""\n"");

    }



    for(i = 0; i < TCG_TARGET_NB_REGS; i++) {

        if (s->reg_to_temp[i] >= 0) {

            printf(""%s: %s\n"", 

                   tcg_target_reg_names[i], 

                   tcg_get_arg_str_idx(s, buf, sizeof(buf), s->reg_to_temp[i]));

        }

    }

}
","static void dump_regs(TCGContext *s) {
TCGTemp *ts; int i; char buf[64]; for(i = 0; i < s->nb_temps; i++) {
ts = &s->temps[i]; printf("" %10s: "", tcg_get_arg_str_idx(s, buf, sizeof(buf), i)); switch(ts->val_type) {
case TEMP_VAL_REG: printf(""%s"", tcg_target_reg_names[ts->reg]); break; case TEMP_VAL_MEM: printf(""%d(%s)"", (int)ts->mem_offset, tcg_target_reg_names[ts->mem_reg]); break; case TEMP_VAL_CONST: printf(""$0x%"" TCG_PRIlx, ts->val); break; case TEMP_VAL_DEAD: printf(""D""); break; default: printf(""???""); break; } printf(""\n""); } for(i = 0; i < TCG_TARGET_NB_REGS; i++) {
if (s->reg_to_temp[i] >= 0) {
printf(""%s: %s\n"", tcg_target_reg_names[i], tcg_get_arg_str_idx(s, buf, sizeof(buf), s->reg_to_temp[i])); } } } ",qemu,0
"void visit_type_enum(Visitor *v, const char *name, int *obj,

                     const char *const strings[], Error **errp)

{

    assert(obj && strings);

    if (v->type == VISITOR_INPUT) {

        input_type_enum(v, name, obj, strings, errp);

    } else if (v->type == VISITOR_OUTPUT) {

        output_type_enum(v, name, obj, strings, errp);

    }

}
","void visit_type_enum(Visitor *v, const char *name, int *obj, const char *const strings[], Error **errp) {
assert(obj && strings); if (v->type == VISITOR_INPUT) {
input_type_enum(v, name, obj, strings, errp); } else if (v->type == VISITOR_OUTPUT) {
output_type_enum(v, name, obj, strings, errp); } } ",qemu,1
"char *socket_address_to_string(struct SocketAddressLegacy *addr, Error **errp)

{

    char *buf;

    InetSocketAddress *inet;



    switch (addr->type) {

    case SOCKET_ADDRESS_LEGACY_KIND_INET:

        inet = addr->u.inet.data;

        if (strchr(inet->host, ':') == NULL) {

            buf = g_strdup_printf(""%s:%s"", inet->host, inet->port);

        } else {

            buf = g_strdup_printf(""[%s]:%s"", inet->host, inet->port);

        }

        break;



    case SOCKET_ADDRESS_LEGACY_KIND_UNIX:

        buf = g_strdup(addr->u.q_unix.data->path);

        break;



    case SOCKET_ADDRESS_LEGACY_KIND_FD:

        buf = g_strdup(addr->u.fd.data->str);

        break;



    case SOCKET_ADDRESS_LEGACY_KIND_VSOCK:

        buf = g_strdup_printf(""%s:%s"",

                              addr->u.vsock.data->cid,

                              addr->u.vsock.data->port);

        break;



    default:

        abort();

    }

    return buf;

}
","char *socket_address_to_string(struct SocketAddressLegacy *addr, Error **errp) {
char *buf; InetSocketAddress *inet; switch (addr->type) {
case SOCKET_ADDRESS_LEGACY_KIND_INET: inet = addr->u.inet.data; if (strchr(inet->host, ':') == NULL) {
buf = g_strdup_printf(""%s:%s"", inet->host, inet->port); } else {
buf = g_strdup_printf(""[%s]:%s"", inet->host, inet->port); } break; case SOCKET_ADDRESS_LEGACY_KIND_UNIX: buf = g_strdup(addr->u.q_unix.data->path); break; case SOCKET_ADDRESS_LEGACY_KIND_FD: buf = g_strdup(addr->u.fd.data->str); break; case SOCKET_ADDRESS_LEGACY_KIND_VSOCK: buf = g_strdup_printf(""%s:%s"", addr->u.vsock.data->cid, addr->u.vsock.data->port); break; default: abort(); } return buf; } ",qemu,0
"static void pcnet_transmit(PCNetState *s)

{

    target_phys_addr_t xmit_cxda = 0;

    int count = CSR_XMTRL(s)-1;

    s->xmit_pos = -1;



    if (!CSR_TXON(s)) {

        s->csr[0] &= ~0x0008;

        return;

    }



    s->tx_busy = 1;



    txagain:

    if (pcnet_tdte_poll(s)) {

        struct pcnet_TMD tmd;



        TMDLOAD(&tmd, PHYSADDR(s,CSR_CXDA(s)));



#ifdef PCNET_DEBUG_TMD

        printf(""  TMDLOAD 0x%08x\n"", PHYSADDR(s,CSR_CXDA(s)));

        PRINT_TMD(&tmd);

#endif

        if (GET_FIELD(tmd.status, TMDS, STP)) {

            s->xmit_pos = 0;

            xmit_cxda = PHYSADDR(s,CSR_CXDA(s));

        }

        if (!GET_FIELD(tmd.status, TMDS, ENP)) {

            int bcnt = 4096 - GET_FIELD(tmd.length, TMDL, BCNT);

            s->phys_mem_read(s->dma_opaque, PHYSADDR(s, tmd.tbadr),

                             s->buffer + s->xmit_pos, bcnt, CSR_BSWP(s));

            s->xmit_pos += bcnt;

        } else if (s->xmit_pos >= 0) {

            int bcnt = 4096 - GET_FIELD(tmd.length, TMDL, BCNT);

            s->phys_mem_read(s->dma_opaque, PHYSADDR(s, tmd.tbadr),

                             s->buffer + s->xmit_pos, bcnt, CSR_BSWP(s));

            s->xmit_pos += bcnt;

#ifdef PCNET_DEBUG

            printf(""pcnet_transmit size=%d\n"", s->xmit_pos);

#endif

            if (CSR_LOOP(s))

                pcnet_receive(s, s->buffer, s->xmit_pos);

            else

                if (s->vc)

                    qemu_send_packet(s->vc, s->buffer, s->xmit_pos);



            s->csr[0] &= ~0x0008;   /* clear TDMD */

            s->csr[4] |= 0x0004;    /* set TXSTRT */

            s->xmit_pos = -1;

        }



        SET_FIELD(&tmd.status, TMDS, OWN, 0);

        TMDSTORE(&tmd, PHYSADDR(s,CSR_CXDA(s)));

        if (!CSR_TOKINTD(s) || (CSR_LTINTEN(s) && GET_FIELD(tmd.status, TMDS, LTINT)))

            s->csr[0] |= 0x0200;    /* set TINT */



        if (CSR_XMTRC(s)<=1)

            CSR_XMTRC(s) = CSR_XMTRL(s);

        else

            CSR_XMTRC(s)--;

        if (count--)

            goto txagain;



    } else

    if (s->xmit_pos >= 0) {

        struct pcnet_TMD tmd;

        TMDLOAD(&tmd, PHYSADDR(s,xmit_cxda));

        SET_FIELD(&tmd.misc, TMDM, BUFF, 1);

        SET_FIELD(&tmd.misc, TMDM, UFLO, 1);

        SET_FIELD(&tmd.status, TMDS, ERR, 1);

        SET_FIELD(&tmd.status, TMDS, OWN, 0);

        TMDSTORE(&tmd, PHYSADDR(s,xmit_cxda));

        s->csr[0] |= 0x0200;    /* set TINT */

        if (!CSR_DXSUFLO(s)) {

            s->csr[0] &= ~0x0010;

        } else

        if (count--)

          goto txagain;

    }



    s->tx_busy = 0;

}
","static void pcnet_transmit(PCNetState *s) {
target_phys_addr_t xmit_cxda = 0; int count = CSR_XMTRL(s)-1; s->xmit_pos = -1; if (!CSR_TXON(s)) {
s->csr[0] &= ~0x0008; return; } s->tx_busy = 1; txagain: if (pcnet_tdte_poll(s)) {
struct pcnet_TMD tmd; TMDLOAD(&tmd, PHYSADDR(s,CSR_CXDA(s))); #ifdef PCNET_DEBUG_TMD printf("" TMDLOAD 0x%08x\n"", PHYSADDR(s,CSR_CXDA(s))); PRINT_TMD(&tmd); #endif if (GET_FIELD(tmd.status, TMDS, STP)) {
s->xmit_pos = 0; xmit_cxda = PHYSADDR(s,CSR_CXDA(s)); } if (!GET_FIELD(tmd.status, TMDS, ENP)) {
int bcnt = 4096 - GET_FIELD(tmd.length, TMDL, BCNT); s->phys_mem_read(s->dma_opaque, PHYSADDR(s, tmd.tbadr), s->buffer + s->xmit_pos, bcnt, CSR_BSWP(s)); s->xmit_pos += bcnt; } else if (s->xmit_pos >= 0) {
int bcnt = 4096 - GET_FIELD(tmd.length, TMDL, BCNT); s->phys_mem_read(s->dma_opaque, PHYSADDR(s, tmd.tbadr), s->buffer + s->xmit_pos, bcnt, CSR_BSWP(s)); s->xmit_pos += bcnt; #ifdef PCNET_DEBUG printf(""pcnet_transmit size=%d\n"", s->xmit_pos); #endif if (CSR_LOOP(s)) pcnet_receive(s, s->buffer, s->xmit_pos); else if (s->vc) qemu_send_packet(s->vc, s->buffer, s->xmit_pos); s->csr[0] &= ~0x0008; /* clear TDMD */ s->csr[4] |= 0x0004; /* set TXSTRT */ s->xmit_pos = -1; } SET_FIELD(&tmd.status, TMDS, OWN, 0); TMDSTORE(&tmd, PHYSADDR(s,CSR_CXDA(s))); if (!CSR_TOKINTD(s) || (CSR_LTINTEN(s) && GET_FIELD(tmd.status, TMDS, LTINT))) s->csr[0] |= 0x0200; /* set TINT */ if (CSR_XMTRC(s)<=1) CSR_XMTRC(s) = CSR_XMTRL(s); else CSR_XMTRC(s)--; if (count--) goto txagain; } else if (s->xmit_pos >= 0) {
struct pcnet_TMD tmd; TMDLOAD(&tmd, PHYSADDR(s,xmit_cxda)); SET_FIELD(&tmd.misc, TMDM, BUFF, 1); SET_FIELD(&tmd.misc, TMDM, UFLO, 1); SET_FIELD(&tmd.status, TMDS, ERR, 1); SET_FIELD(&tmd.status, TMDS, OWN, 0); TMDSTORE(&tmd, PHYSADDR(s,xmit_cxda)); s->csr[0] |= 0x0200; /* set TINT */ if (!CSR_DXSUFLO(s)) {
s->csr[0] &= ~0x0010; } else if (count--) goto txagain; } s->tx_busy = 0; } ",qemu,0
"static void coroutine_fn v9fs_xattrwalk(void *opaque)

{

    int64_t size;

    V9fsString name;

    ssize_t err = 0;

    size_t offset = 7;

    int32_t fid, newfid;

    V9fsFidState *file_fidp;

    V9fsFidState *xattr_fidp = NULL;

    V9fsPDU *pdu = opaque;

    V9fsState *s = pdu->s;



    v9fs_string_init(&name);

    err = pdu_unmarshal(pdu, offset, ""dds"", &fid, &newfid, &name);

    if (err < 0) {

        goto out_nofid;

    }

    trace_v9fs_xattrwalk(pdu->tag, pdu->id, fid, newfid, name.data);



    file_fidp = get_fid(pdu, fid);

    if (file_fidp == NULL) {

        err = -ENOENT;

        goto out_nofid;

    }

    xattr_fidp = alloc_fid(s, newfid);

    if (xattr_fidp == NULL) {

        err = -EINVAL;

        goto out;

    }

    v9fs_path_copy(&xattr_fidp->path, &file_fidp->path);

    if (!v9fs_string_size(&name)) {

        /*

         * listxattr request. Get the size first

         */

        size = v9fs_co_llistxattr(pdu, &xattr_fidp->path, NULL, 0);

        if (size < 0) {

            err = size;

            clunk_fid(s, xattr_fidp->fid);

            goto out;

        }

        /*

         * Read the xattr value

         */

        xattr_fidp->fs.xattr.len = size;

        xattr_fidp->fid_type = P9_FID_XATTR;

        xattr_fidp->fs.xattr.xattrwalk_fid = true;

        if (size) {

            xattr_fidp->fs.xattr.value = g_malloc(size);

            err = v9fs_co_llistxattr(pdu, &xattr_fidp->path,

                                     xattr_fidp->fs.xattr.value,

                                     xattr_fidp->fs.xattr.len);

            if (err < 0) {

                clunk_fid(s, xattr_fidp->fid);

                goto out;

            }

        }

        err = pdu_marshal(pdu, offset, ""q"", size);

        if (err < 0) {

            goto out;

        }

        err += offset;

    } else {

        /*

         * specific xattr fid. We check for xattr

         * presence also collect the xattr size

         */

        size = v9fs_co_lgetxattr(pdu, &xattr_fidp->path,

                                 &name, NULL, 0);

        if (size < 0) {

            err = size;

            clunk_fid(s, xattr_fidp->fid);

            goto out;

        }

        /*

         * Read the xattr value

         */

        xattr_fidp->fs.xattr.len = size;

        xattr_fidp->fid_type = P9_FID_XATTR;

        xattr_fidp->fs.xattr.xattrwalk_fid = true;

        if (size) {

            xattr_fidp->fs.xattr.value = g_malloc(size);

            err = v9fs_co_lgetxattr(pdu, &xattr_fidp->path,

                                    &name, xattr_fidp->fs.xattr.value,

                                    xattr_fidp->fs.xattr.len);

            if (err < 0) {

                clunk_fid(s, xattr_fidp->fid);

                goto out;

            }

        }

        err = pdu_marshal(pdu, offset, ""q"", size);

        if (err < 0) {

            goto out;

        }

        err += offset;

    }

    trace_v9fs_xattrwalk_return(pdu->tag, pdu->id, size);

out:

    put_fid(pdu, file_fidp);

    if (xattr_fidp) {

        put_fid(pdu, xattr_fidp);

    }

out_nofid:

    pdu_complete(pdu, err);

    v9fs_string_free(&name);

}
","static void coroutine_fn v9fs_xattrwalk(void *opaque) {
int64_t size; V9fsString name; ssize_t err = 0; size_t offset = 7; int32_t fid, newfid; V9fsFidState *file_fidp; V9fsFidState *xattr_fidp = NULL; V9fsPDU *pdu = opaque; V9fsState *s = pdu->s; v9fs_string_init(&name); err = pdu_unmarshal(pdu, offset, ""dds"", &fid, &newfid, &name); if (err < 0) {
goto out_nofid; } trace_v9fs_xattrwalk(pdu->tag, pdu->id, fid, newfid, name.data); file_fidp = get_fid(pdu, fid); if (file_fidp == NULL) {
err = -ENOENT; goto out_nofid; } xattr_fidp = alloc_fid(s, newfid); if (xattr_fidp == NULL) {
err = -EINVAL; goto out; } v9fs_path_copy(&xattr_fidp->path, &file_fidp->path); if (!v9fs_string_size(&name)) {
/* * listxattr request. Get the size first */ size = v9fs_co_llistxattr(pdu, &xattr_fidp->path, NULL, 0); if (size < 0) {
err = size; clunk_fid(s, xattr_fidp->fid); goto out; } /* * Read the xattr value */ xattr_fidp->fs.xattr.len = size; xattr_fidp->fid_type = P9_FID_XATTR; xattr_fidp->fs.xattr.xattrwalk_fid = true; if (size) {
xattr_fidp->fs.xattr.value = g_malloc(size); err = v9fs_co_llistxattr(pdu, &xattr_fidp->path, xattr_fidp->fs.xattr.value, xattr_fidp->fs.xattr.len); if (err < 0) {
clunk_fid(s, xattr_fidp->fid); goto out; } } err = pdu_marshal(pdu, offset, ""q"", size); if (err < 0) {
goto out; } err += offset; } else {
/* * specific xattr fid. We check for xattr * presence also collect the xattr size */ size = v9fs_co_lgetxattr(pdu, &xattr_fidp->path, &name, NULL, 0); if (size < 0) {
err = size; clunk_fid(s, xattr_fidp->fid); goto out; } /* * Read the xattr value */ xattr_fidp->fs.xattr.len = size; xattr_fidp->fid_type = P9_FID_XATTR; xattr_fidp->fs.xattr.xattrwalk_fid = true; if (size) {
xattr_fidp->fs.xattr.value = g_malloc(size); err = v9fs_co_lgetxattr(pdu, &xattr_fidp->path, &name, xattr_fidp->fs.xattr.value, xattr_fidp->fs.xattr.len); if (err < 0) {
clunk_fid(s, xattr_fidp->fid); goto out; } } err = pdu_marshal(pdu, offset, ""q"", size); if (err < 0) {
goto out; } err += offset; } trace_v9fs_xattrwalk_return(pdu->tag, pdu->id, size); out: put_fid(pdu, file_fidp); if (xattr_fidp) {
put_fid(pdu, xattr_fidp); } out_nofid: pdu_complete(pdu, err); v9fs_string_free(&name); } ",qemu,1
"static void set_mem_path(Object *o, const char *str, Error **errp)

{

    HostMemoryBackend *backend = MEMORY_BACKEND(o);

    HostMemoryBackendFile *fb = MEMORY_BACKEND_FILE(o);



    if (memory_region_size(&backend->mr)) {

        error_setg(errp, ""cannot change property value"");

        return;

    }

    if (fb->mem_path) {

        g_free(fb->mem_path);

    }

    fb->mem_path = g_strdup(str);

}
","static void set_mem_path(Object *o, const char *str, Error **errp) {
HostMemoryBackend *backend = MEMORY_BACKEND(o); HostMemoryBackendFile *fb = MEMORY_BACKEND_FILE(o); if (memory_region_size(&backend->mr)) {
error_setg(errp, ""cannot change property value""); return; } if (fb->mem_path) {
g_free(fb->mem_path); } fb->mem_path = g_strdup(str); } ",qemu,0
"static void gen_waiti(DisasContext *dc, uint32_t imm4)

{

    TCGv_i32 pc = tcg_const_i32(dc->next_pc);

    TCGv_i32 intlevel = tcg_const_i32(imm4);



    if (dc->tb->cflags & CF_USE_ICOUNT) {

        gen_io_start();

    }

    gen_helper_waiti(cpu_env, pc, intlevel);

    if (dc->tb->cflags & CF_USE_ICOUNT) {

        gen_io_end();

    }

    tcg_temp_free(pc);

    tcg_temp_free(intlevel);

    gen_jumpi_check_loop_end(dc, 0);

}
","static void gen_waiti(DisasContext *dc, uint32_t imm4) {
TCGv_i32 pc = tcg_const_i32(dc->next_pc); TCGv_i32 intlevel = tcg_const_i32(imm4); if (dc->tb->cflags & CF_USE_ICOUNT) {
gen_io_start(); } gen_helper_waiti(cpu_env, pc, intlevel); if (dc->tb->cflags & CF_USE_ICOUNT) {
gen_io_end(); } tcg_temp_free(pc); tcg_temp_free(intlevel); gen_jumpi_check_loop_end(dc, 0); } ",qemu,1
"static uint64_t omap_os_timer_read(void *opaque, target_phys_addr_t addr,

                                   unsigned size)

{

    struct omap_32khz_timer_s *s = (struct omap_32khz_timer_s *) opaque;

    int offset = addr & OMAP_MPUI_REG_MASK;



    if (size != 4) {

        return omap_badwidth_read32(opaque, addr);

    }



    switch (offset) {

    case 0x00:	/* TVR */

        return s->timer.reset_val;



    case 0x04:	/* TCR */

        return omap_timer_read(&s->timer);



    case 0x08:	/* CR */

        return (s->timer.ar << 3) | (s->timer.it_ena << 2) | s->timer.st;



    default:

        break;

    }

    OMAP_BAD_REG(addr);

    return 0;

}
","static uint64_t omap_os_timer_read(void *opaque, target_phys_addr_t addr, unsigned size) {
struct omap_32khz_timer_s *s = (struct omap_32khz_timer_s *) opaque; int offset = addr & OMAP_MPUI_REG_MASK; if (size != 4) {
return omap_badwidth_read32(opaque, addr); } switch (offset) {
case 0x00: /* TVR */ return s->timer.reset_val; case 0x04: /* TCR */ return omap_timer_read(&s->timer); case 0x08: /* CR */ return (s->timer.ar << 3) | (s->timer.it_ena << 2) | s->timer.st; default: break; } OMAP_BAD_REG(addr); return 0; } ",qemu,0
"static void test_acpi_one(const char *params, test_data *data)

{

    char *args;

    uint8_t signature_low;

    uint8_t signature_high;

    uint16_t signature;

    int i;

    const char *device = """";



    if (!g_strcmp0(data->machine, MACHINE_Q35)) {

        device = "",id=hd -device ide-hd,drive=hd"";

    }



    args = g_strdup_printf(""-net none -display none %s -drive file=%s%s,"",

                           params ? params : """", disk, device);

    qtest_start(args);



   /* Wait at most 1 minute */

#define TEST_DELAY (1 * G_USEC_PER_SEC / 10)

#define TEST_CYCLES MAX((60 * G_USEC_PER_SEC / TEST_DELAY), 1)



    /* Poll until code has run and modified memory.  Once it has we know BIOS

     * initialization is done.  TODO: check that IP reached the halt

     * instruction.

     */

    for (i = 0; i < TEST_CYCLES; ++i) {

        signature_low = readb(BOOT_SECTOR_ADDRESS + SIGNATURE_OFFSET);

        signature_high = readb(BOOT_SECTOR_ADDRESS + SIGNATURE_OFFSET + 1);

        signature = (signature_high << 8) | signature_low;

        if (signature == SIGNATURE) {

            break;

        }

        g_usleep(TEST_DELAY);

    }

    g_assert_cmphex(signature, ==, SIGNATURE);



    test_acpi_rsdp_address(data);

    test_acpi_rsdp_table(data);

    test_acpi_rsdt_table(data);

    test_acpi_fadt_table(data);

    test_acpi_facs_table(data);

    test_acpi_dsdt_table(data);

    test_acpi_tables(data);



    if (iasl) {

        if (getenv(ACPI_REBUILD_EXPECTED_AML)) {

            dump_aml_files(data, true);

        } else {

            test_acpi_asl(data);

        }

    }



    test_smbios_ep_address(data);

    test_smbios_ep_table(data);

    test_smbios_structs(data);



    qtest_quit(global_qtest);

    g_free(args);

}
","static void test_acpi_one(const char *params, test_data *data) {
char *args; uint8_t signature_low; uint8_t signature_high; uint16_t signature; int i; const char *device = """"; if (!g_strcmp0(data->machine, MACHINE_Q35)) {
device = "",id=hd -device ide-hd,drive=hd""; } args = g_strdup_printf(""-net none -display none %s -drive file=%s%s,"", params ? params : """", disk, device); qtest_start(args); /* Wait at most 1 minute */ #define TEST_DELAY (1 * G_USEC_PER_SEC / 10) #define TEST_CYCLES MAX((60 * G_USEC_PER_SEC / TEST_DELAY), 1) /* Poll until code has run and modified memory. Once it has we know BIOS * initialization is done. TODO: check that IP reached the halt * instruction. */ for (i = 0; i < TEST_CYCLES; ++i) {
signature_low = readb(BOOT_SECTOR_ADDRESS + SIGNATURE_OFFSET); signature_high = readb(BOOT_SECTOR_ADDRESS + SIGNATURE_OFFSET + 1); signature = (signature_high << 8) | signature_low; if (signature == SIGNATURE) {
break; } g_usleep(TEST_DELAY); } g_assert_cmphex(signature, ==, SIGNATURE); test_acpi_rsdp_address(data); test_acpi_rsdp_table(data); test_acpi_rsdt_table(data); test_acpi_fadt_table(data); test_acpi_facs_table(data); test_acpi_dsdt_table(data); test_acpi_tables(data); if (iasl) {
if (getenv(ACPI_REBUILD_EXPECTED_AML)) {
dump_aml_files(data, true); } else {
test_acpi_asl(data); } } test_smbios_ep_address(data); test_smbios_ep_table(data); test_smbios_structs(data); qtest_quit(global_qtest); g_free(args); } ",qemu,1
"static int tcg_cpu_exec(CPUState *cpu)

{

    int ret;

#ifdef CONFIG_PROFILER

    int64_t ti;

#endif



#ifdef CONFIG_PROFILER

    ti = profile_getclock();

#endif

    if (use_icount) {

        int64_t count;

        int64_t deadline;

        int decr;

        timers_state.qemu_icount -= (cpu->icount_decr.u16.low

                                    + cpu->icount_extra);

        cpu->icount_decr.u16.low = 0;

        cpu->icount_extra = 0;

        deadline = qemu_clock_deadline_ns_all(QEMU_CLOCK_VIRTUAL);



        /* Maintain prior (possibly buggy) behaviour where if no deadline

         * was set (as there is no QEMU_CLOCK_VIRTUAL timer) or it is more than

         * INT32_MAX nanoseconds ahead, we still use INT32_MAX

         * nanoseconds.

         */

        if ((deadline < 0) || (deadline > INT32_MAX)) {

            deadline = INT32_MAX;

        }



        count = qemu_icount_round(deadline);

        timers_state.qemu_icount += count;

        decr = (count > 0xffff) ? 0xffff : count;

        count -= decr;

        cpu->icount_decr.u16.low = decr;

        cpu->icount_extra = count;

    }

    ret = cpu_exec(cpu);

#ifdef CONFIG_PROFILER

    tcg_time += profile_getclock() - ti;

#endif

    if (use_icount) {

        /* Fold pending instructions back into the

           instruction counter, and clear the interrupt flag.  */

        timers_state.qemu_icount -= (cpu->icount_decr.u16.low

                        + cpu->icount_extra);

        cpu->icount_decr.u32 = 0;

        cpu->icount_extra = 0;

    }

    return ret;

}
","static int tcg_cpu_exec(CPUState *cpu) {
int ret; #ifdef CONFIG_PROFILER int64_t ti; #endif #ifdef CONFIG_PROFILER ti = profile_getclock(); #endif if (use_icount) {
int64_t count; int64_t deadline; int decr; timers_state.qemu_icount -= (cpu->icount_decr.u16.low + cpu->icount_extra); cpu->icount_decr.u16.low = 0; cpu->icount_extra = 0; deadline = qemu_clock_deadline_ns_all(QEMU_CLOCK_VIRTUAL); /* Maintain prior (possibly buggy) behaviour where if no deadline * was set (as there is no QEMU_CLOCK_VIRTUAL timer) or it is more than * INT32_MAX nanoseconds ahead, we still use INT32_MAX * nanoseconds. */ if ((deadline < 0) || (deadline > INT32_MAX)) {
deadline = INT32_MAX; } count = qemu_icount_round(deadline); timers_state.qemu_icount += count; decr = (count > 0xffff) ? 0xffff : count; count -= decr; cpu->icount_decr.u16.low = decr; cpu->icount_extra = count; } ret = cpu_exec(cpu); #ifdef CONFIG_PROFILER tcg_time += profile_getclock() - ti; #endif if (use_icount) {
/* Fold pending instructions back into the instruction counter, and clear the interrupt flag. */ timers_state.qemu_icount -= (cpu->icount_decr.u16.low + cpu->icount_extra); cpu->icount_decr.u32 = 0; cpu->icount_extra = 0; } return ret; } ",qemu,0
"static MTPData *usb_mtp_get_partial_object(MTPState *s, MTPControl *c,

                                           MTPObject *o)

{

    MTPData *d = usb_mtp_data_alloc(c);

    off_t offset;



    trace_usb_mtp_op_get_partial_object(s->dev.addr, o->handle, o->path,

                                        c->argv[1], c->argv[2]);



    d->fd = open(o->path, O_RDONLY);

    if (d->fd == -1) {


        return NULL;

    }



    offset = c->argv[1];

    if (offset > o->stat.st_size) {

        offset = o->stat.st_size;

    }

    lseek(d->fd, offset, SEEK_SET);



    d->length = c->argv[2];

    if (d->length > o->stat.st_size - offset) {

        d->length = o->stat.st_size - offset;

    }



    return d;

}","static MTPData *usb_mtp_get_partial_object(MTPState *s, MTPControl *c, MTPObject *o) {
MTPData *d = usb_mtp_data_alloc(c); off_t offset; trace_usb_mtp_op_get_partial_object(s->dev.addr, o->handle, o->path, c->argv[1], c->argv[2]); d->fd = open(o->path, O_RDONLY); if (d->fd == -1) {
return NULL; } offset = c->argv[1]; if (offset > o->stat.st_size) {
offset = o->stat.st_size; } lseek(d->fd, offset, SEEK_SET); d->length = c->argv[2]; if (d->length > o->stat.st_size - offset) {
d->length = o->stat.st_size - offset; } return d; }",qemu,1
"void cpu_exec_init(CPUArchState *env)

{

    CPUState *cpu = ENV_GET_CPU(env);

    CPUClass *cc = CPU_GET_CLASS(cpu);

    CPUState *some_cpu;

    int cpu_index;



#if defined(CONFIG_USER_ONLY)

    cpu_list_lock();

#endif

    cpu_index = 0;

    CPU_FOREACH(some_cpu) {

        cpu_index++;

    }

    cpu->cpu_index = cpu_index;

    cpu->numa_node = 0;

    QTAILQ_INIT(&cpu->breakpoints);

    QTAILQ_INIT(&cpu->watchpoints);

#ifndef CONFIG_USER_ONLY

    cpu->as = &address_space_memory;

    cpu->thread_id = qemu_get_thread_id();


#endif

    QTAILQ_INSERT_TAIL(&cpus, cpu, node);

#if defined(CONFIG_USER_ONLY)

    cpu_list_unlock();

#endif

    if (qdev_get_vmsd(DEVICE(cpu)) == NULL) {

        vmstate_register(NULL, cpu_index, &vmstate_cpu_common, cpu);

    }

#if defined(CPU_SAVE_VERSION) && !defined(CONFIG_USER_ONLY)

    register_savevm(NULL, ""cpu"", cpu_index, CPU_SAVE_VERSION,

                    cpu_save, cpu_load, env);

    assert(cc->vmsd == NULL);

    assert(qdev_get_vmsd(DEVICE(cpu)) == NULL);

#endif

    if (cc->vmsd != NULL) {

        vmstate_register(NULL, cpu_index, cc->vmsd, cpu);

    }

}","void cpu_exec_init(CPUArchState *env) {
CPUState *cpu = ENV_GET_CPU(env); CPUClass *cc = CPU_GET_CLASS(cpu); CPUState *some_cpu; int cpu_index; #if defined(CONFIG_USER_ONLY) cpu_list_lock(); #endif cpu_index = 0; CPU_FOREACH(some_cpu) {
cpu_index++; } cpu->cpu_index = cpu_index; cpu->numa_node = 0; QTAILQ_INIT(&cpu->breakpoints); QTAILQ_INIT(&cpu->watchpoints); #ifndef CONFIG_USER_ONLY cpu->as = &address_space_memory; cpu->thread_id = qemu_get_thread_id(); #endif QTAILQ_INSERT_TAIL(&cpus, cpu, node); #if defined(CONFIG_USER_ONLY) cpu_list_unlock(); #endif if (qdev_get_vmsd(DEVICE(cpu)) == NULL) {
vmstate_register(NULL, cpu_index, &vmstate_cpu_common, cpu); } #if defined(CPU_SAVE_VERSION) && !defined(CONFIG_USER_ONLY) register_savevm(NULL, ""cpu"", cpu_index, CPU_SAVE_VERSION, cpu_save, cpu_load, env); assert(cc->vmsd == NULL); assert(qdev_get_vmsd(DEVICE(cpu)) == NULL); #endif if (cc->vmsd != NULL) {
vmstate_register(NULL, cpu_index, cc->vmsd, cpu); } }",qemu,1
"static void virtio_write_config(PCIDevice *pci_dev, uint32_t address,

                                uint32_t val, int len)

{

    VirtIOPCIProxy *proxy = DO_UPCAST(VirtIOPCIProxy, pci_dev, pci_dev);



    if (PCI_COMMAND == address) {

        if (!(val & PCI_COMMAND_MASTER)) {

            proxy->vdev->status &= !VIRTIO_CONFIG_S_DRIVER_OK;

        }

    }



    pci_default_write_config(pci_dev, address, val, len);

    if(proxy->vdev->nvectors)

        msix_write_config(pci_dev, address, val, len);

}
","static void virtio_write_config(PCIDevice *pci_dev, uint32_t address, uint32_t val, int len) {
VirtIOPCIProxy *proxy = DO_UPCAST(VirtIOPCIProxy, pci_dev, pci_dev); if (PCI_COMMAND == address) {
if (!(val & PCI_COMMAND_MASTER)) {
proxy->vdev->status &= !VIRTIO_CONFIG_S_DRIVER_OK; } } pci_default_write_config(pci_dev, address, val, len); if(proxy->vdev->nvectors) msix_write_config(pci_dev, address, val, len); } ",qemu,0
"static void test_qemu_strtoul_full_max(void)

{

    const char *str = g_strdup_printf(""%lu"", ULONG_MAX);

    unsigned long res = 999;

    int err;



    err = qemu_strtoul(str, NULL, 0, &res);



    g_assert_cmpint(err, ==, 0);

    g_assert_cmpint(res, ==, ULONG_MAX);

}
","static void test_qemu_strtoul_full_max(void) {
const char *str = g_strdup_printf(""%lu"", ULONG_MAX); unsigned long res = 999; int err; err = qemu_strtoul(str, NULL, 0, &res); g_assert_cmpint(err, ==, 0); g_assert_cmpint(res, ==, ULONG_MAX); } ",qemu,1
"GuestNetworkInterfaceList *qmp_guest_network_get_interfaces(Error **errp)

{

    GuestNetworkInterfaceList *head = NULL, *cur_item = NULL;

    struct ifaddrs *ifap, *ifa;



    if (getifaddrs(&ifap) < 0) {

        error_setg_errno(errp, errno, ""getifaddrs failed"");

        goto error;

    }



    for (ifa = ifap; ifa; ifa = ifa->ifa_next) {

        GuestNetworkInterfaceList *info;

        GuestIpAddressList **address_list = NULL, *address_item = NULL;

        char addr4[INET_ADDRSTRLEN];

        char addr6[INET6_ADDRSTRLEN];

        int sock;

        struct ifreq ifr;

        unsigned char *mac_addr;

        void *p;



        g_debug(""Processing %s interface"", ifa->ifa_name);



        info = guest_find_interface(head, ifa->ifa_name);



        if (!info) {

            info = g_malloc0(sizeof(*info));

            info->value = g_malloc0(sizeof(*info->value));

            info->value->name = g_strdup(ifa->ifa_name);



            if (!cur_item) {

                head = cur_item = info;

            } else {

                cur_item->next = info;

                cur_item = info;

            }

        }



        if (!info->value->has_hardware_address &&

            ifa->ifa_flags & SIOCGIFHWADDR) {

            /* we haven't obtained HW address yet */

            sock = socket(PF_INET, SOCK_STREAM, 0);

            if (sock == -1) {

                error_setg_errno(errp, errno, ""failed to create socket"");

                goto error;

            }



            memset(&ifr, 0, sizeof(ifr));

            pstrcpy(ifr.ifr_name, IF_NAMESIZE, info->value->name);

            if (ioctl(sock, SIOCGIFHWADDR, &ifr) == -1) {

                error_setg_errno(errp, errno,

                                 ""failed to get MAC address of %s"",

                                 ifa->ifa_name);

                goto error;

            }



            mac_addr = (unsigned char *) &ifr.ifr_hwaddr.sa_data;



            info->value->hardware_address =

                g_strdup_printf(""%02x:%02x:%02x:%02x:%02x:%02x"",

                                (int) mac_addr[0], (int) mac_addr[1],

                                (int) mac_addr[2], (int) mac_addr[3],

                                (int) mac_addr[4], (int) mac_addr[5]);



            info->value->has_hardware_address = true;

            close(sock);

        }



        if (ifa->ifa_addr &&

            ifa->ifa_addr->sa_family == AF_INET) {

            /* interface with IPv4 address */

            address_item = g_malloc0(sizeof(*address_item));

            address_item->value = g_malloc0(sizeof(*address_item->value));

            p = &((struct sockaddr_in *)ifa->ifa_addr)->sin_addr;

            if (!inet_ntop(AF_INET, p, addr4, sizeof(addr4))) {

                error_setg_errno(errp, errno, ""inet_ntop failed"");

                goto error;

            }



            address_item->value->ip_address = g_strdup(addr4);

            address_item->value->ip_address_type = GUEST_IP_ADDRESS_TYPE_IPV4;



            if (ifa->ifa_netmask) {

                /* Count the number of set bits in netmask.

                 * This is safe as '1' and '0' cannot be shuffled in netmask. */

                p = &((struct sockaddr_in *)ifa->ifa_netmask)->sin_addr;

                address_item->value->prefix = ctpop32(((uint32_t *) p)[0]);

            }

        } else if (ifa->ifa_addr &&

                   ifa->ifa_addr->sa_family == AF_INET6) {

            /* interface with IPv6 address */

            address_item = g_malloc0(sizeof(*address_item));

            address_item->value = g_malloc0(sizeof(*address_item->value));

            p = &((struct sockaddr_in6 *)ifa->ifa_addr)->sin6_addr;

            if (!inet_ntop(AF_INET6, p, addr6, sizeof(addr6))) {

                error_setg_errno(errp, errno, ""inet_ntop failed"");

                goto error;

            }



            address_item->value->ip_address = g_strdup(addr6);

            address_item->value->ip_address_type = GUEST_IP_ADDRESS_TYPE_IPV6;



            if (ifa->ifa_netmask) {

                /* Count the number of set bits in netmask.

                 * This is safe as '1' and '0' cannot be shuffled in netmask. */

                p = &((struct sockaddr_in6 *)ifa->ifa_netmask)->sin6_addr;

                address_item->value->prefix =

                    ctpop32(((uint32_t *) p)[0]) +

                    ctpop32(((uint32_t *) p)[1]) +

                    ctpop32(((uint32_t *) p)[2]) +

                    ctpop32(((uint32_t *) p)[3]);

            }

        }



        if (!address_item) {

            continue;

        }



        address_list = &info->value->ip_addresses;



        while (*address_list && (*address_list)->next) {

            address_list = &(*address_list)->next;

        }



        if (!*address_list) {

            *address_list = address_item;

        } else {

            (*address_list)->next = address_item;

        }



        info->value->has_ip_addresses = true;





    }



    freeifaddrs(ifap);

    return head;



error:

    freeifaddrs(ifap);

    qapi_free_GuestNetworkInterfaceList(head);

    return NULL;

}
","GuestNetworkInterfaceList *qmp_guest_network_get_interfaces(Error **errp) {
GuestNetworkInterfaceList *head = NULL, *cur_item = NULL; struct ifaddrs *ifap, *ifa; if (getifaddrs(&ifap) < 0) {
error_setg_errno(errp, errno, ""getifaddrs failed""); goto error; } for (ifa = ifap; ifa; ifa = ifa->ifa_next) {
GuestNetworkInterfaceList *info; GuestIpAddressList **address_list = NULL, *address_item = NULL; char addr4[INET_ADDRSTRLEN]; char addr6[INET6_ADDRSTRLEN]; int sock; struct ifreq ifr; unsigned char *mac_addr; void *p; g_debug(""Processing %s interface"", ifa->ifa_name); info = guest_find_interface(head, ifa->ifa_name); if (!info) {
info = g_malloc0(sizeof(*info)); info->value = g_malloc0(sizeof(*info->value)); info->value->name = g_strdup(ifa->ifa_name); if (!cur_item) {
head = cur_item = info; } else {
cur_item->next = info; cur_item = info; } } if (!info->value->has_hardware_address && ifa->ifa_flags & SIOCGIFHWADDR) {
/* we haven't obtained HW address yet */ sock = socket(PF_INET, SOCK_STREAM, 0); if (sock == -1) {
error_setg_errno(errp, errno, ""failed to create socket""); goto error; } memset(&ifr, 0, sizeof(ifr)); pstrcpy(ifr.ifr_name, IF_NAMESIZE, info->value->name); if (ioctl(sock, SIOCGIFHWADDR, &ifr) == -1) {
error_setg_errno(errp, errno, ""failed to get MAC address of %s"", ifa->ifa_name); goto error; } mac_addr = (unsigned char *) &ifr.ifr_hwaddr.sa_data; info->value->hardware_address = g_strdup_printf(""%02x:%02x:%02x:%02x:%02x:%02x"", (int) mac_addr[0], (int) mac_addr[1], (int) mac_addr[2], (int) mac_addr[3], (int) mac_addr[4], (int) mac_addr[5]); info->value->has_hardware_address = true; close(sock); } if (ifa->ifa_addr && ifa->ifa_addr->sa_family == AF_INET) {
/* interface with IPv4 address */ address_item = g_malloc0(sizeof(*address_item)); address_item->value = g_malloc0(sizeof(*address_item->value)); p = &((struct sockaddr_in *)ifa->ifa_addr)->sin_addr; if (!inet_ntop(AF_INET, p, addr4, sizeof(addr4))) {
error_setg_errno(errp, errno, ""inet_ntop failed""); goto error; } address_item->value->ip_address = g_strdup(addr4); address_item->value->ip_address_type = GUEST_IP_ADDRESS_TYPE_IPV4; if (ifa->ifa_netmask) {
/* Count the number of set bits in netmask. * This is safe as '1' and '0' cannot be shuffled in netmask. */ p = &((struct sockaddr_in *)ifa->ifa_netmask)->sin_addr; address_item->value->prefix = ctpop32(((uint32_t *) p)[0]); } } else if (ifa->ifa_addr && ifa->ifa_addr->sa_family == AF_INET6) {
/* interface with IPv6 address */ address_item = g_malloc0(sizeof(*address_item)); address_item->value = g_malloc0(sizeof(*address_item->value)); p = &((struct sockaddr_in6 *)ifa->ifa_addr)->sin6_addr; if (!inet_ntop(AF_INET6, p, addr6, sizeof(addr6))) {
error_setg_errno(errp, errno, ""inet_ntop failed""); goto error; } address_item->value->ip_address = g_strdup(addr6); address_item->value->ip_address_type = GUEST_IP_ADDRESS_TYPE_IPV6; if (ifa->ifa_netmask) {
/* Count the number of set bits in netmask. * This is safe as '1' and '0' cannot be shuffled in netmask. */ p = &((struct sockaddr_in6 *)ifa->ifa_netmask)->sin6_addr; address_item->value->prefix = ctpop32(((uint32_t *) p)[0]) + ctpop32(((uint32_t *) p)[1]) + ctpop32(((uint32_t *) p)[2]) + ctpop32(((uint32_t *) p)[3]); } } if (!address_item) {
continue; } address_list = &info->value->ip_addresses; while (*address_list && (*address_list)->next) {
address_list = &(*address_list)->next; } if (!*address_list) {
*address_list = address_item; } else {
(*address_list)->next = address_item; } info->value->has_ip_addresses = true; } freeifaddrs(ifap); return head; error: freeifaddrs(ifap); qapi_free_GuestNetworkInterfaceList(head); return NULL; } ",qemu,1
"static void scsi_disk_unit_attention_reported(SCSIDevice *dev)

{

    SCSIDiskState *s = DO_UPCAST(SCSIDiskState, qdev, dev);

    if (s->media_changed) {

        s->media_changed = false;

        s->qdev.unit_attention = SENSE_CODE(MEDIUM_CHANGED);

    }

}
","static void scsi_disk_unit_attention_reported(SCSIDevice *dev) {
SCSIDiskState *s = DO_UPCAST(SCSIDiskState, qdev, dev); if (s->media_changed) {
s->media_changed = false; s->qdev.unit_attention = SENSE_CODE(MEDIUM_CHANGED); } } ",qemu,0
"static void gen_slbmfev(DisasContext *ctx)

{

#if defined(CONFIG_USER_ONLY)

    gen_inval_exception(ctx, POWERPC_EXCP_PRIV_REG);

#else

    if (unlikely(ctx->pr)) {

        gen_inval_exception(ctx, POWERPC_EXCP_PRIV_REG);

        return;

    }

    gen_helper_load_slb_vsid(cpu_gpr[rS(ctx->opcode)], cpu_env,

                             cpu_gpr[rB(ctx->opcode)]);

#endif

}
","static void gen_slbmfev(DisasContext *ctx) {
#if defined(CONFIG_USER_ONLY) gen_inval_exception(ctx, POWERPC_EXCP_PRIV_REG); #else if (unlikely(ctx->pr)) {
gen_inval_exception(ctx, POWERPC_EXCP_PRIV_REG); return; } gen_helper_load_slb_vsid(cpu_gpr[rS(ctx->opcode)], cpu_env, cpu_gpr[rB(ctx->opcode)]); #endif } ",qemu,1
"static void usb_net_handle_dataout(USBNetState *s, USBPacket *p)

{

    int sz = sizeof(s->out_buf) - s->out_ptr;

    struct rndis_packet_msg_type *msg =

            (struct rndis_packet_msg_type *) s->out_buf;

    uint32_t len;



#ifdef TRAFFIC_DEBUG

    fprintf(stderr, ""usbnet: data out len %zu\n"", p->iov.size);

    iov_hexdump(p->iov.iov, p->iov.niov, stderr, ""usbnet"", p->iov.size);

#endif



    if (sz > p->iov.size) {

        sz = p->iov.size;

    }

    usb_packet_copy(p, &s->out_buf[s->out_ptr], sz);

    s->out_ptr += sz;



    if (!is_rndis(s)) {

        if (p->iov.size < 64) {

            qemu_send_packet(qemu_get_queue(s->nic), s->out_buf, s->out_ptr);

            s->out_ptr = 0;

        }

        return;

    }

    len = le32_to_cpu(msg->MessageLength);

    if (s->out_ptr < 8 || s->out_ptr < len) {

        return;

    }

    if (le32_to_cpu(msg->MessageType) == RNDIS_PACKET_MSG) {

        uint32_t offs = 8 + le32_to_cpu(msg->DataOffset);

        uint32_t size = le32_to_cpu(msg->DataLength);

        if (offs + size <= len)

            qemu_send_packet(qemu_get_queue(s->nic), s->out_buf + offs, size);

    }

    s->out_ptr -= len;

    memmove(s->out_buf, &s->out_buf[len], s->out_ptr);

}
","static void usb_net_handle_dataout(USBNetState *s, USBPacket *p) {
int sz = sizeof(s->out_buf) - s->out_ptr; struct rndis_packet_msg_type *msg = (struct rndis_packet_msg_type *) s->out_buf; uint32_t len; #ifdef TRAFFIC_DEBUG fprintf(stderr, ""usbnet: data out len %zu\n"", p->iov.size); iov_hexdump(p->iov.iov, p->iov.niov, stderr, ""usbnet"", p->iov.size); #endif if (sz > p->iov.size) {
sz = p->iov.size; } usb_packet_copy(p, &s->out_buf[s->out_ptr], sz); s->out_ptr += sz; if (!is_rndis(s)) {
if (p->iov.size < 64) {
qemu_send_packet(qemu_get_queue(s->nic), s->out_buf, s->out_ptr); s->out_ptr = 0; } return; } len = le32_to_cpu(msg->MessageLength); if (s->out_ptr < 8 || s->out_ptr < len) {
return; } if (le32_to_cpu(msg->MessageType) == RNDIS_PACKET_MSG) {
uint32_t offs = 8 + le32_to_cpu(msg->DataOffset); uint32_t size = le32_to_cpu(msg->DataLength); if (offs + size <= len) qemu_send_packet(qemu_get_queue(s->nic), s->out_buf + offs, size); } s->out_ptr -= len; memmove(s->out_buf, &s->out_buf[len], s->out_ptr); } ",qemu,1
"static int print_drive(DeviceState *dev, Property *prop, char *dest, size_t len)

{

    DriveInfo **ptr = qdev_get_prop_ptr(dev, prop);

    return snprintf(dest, len, ""%s"", (*ptr)->id);

}
","static int print_drive(DeviceState *dev, Property *prop, char *dest, size_t len) {
DriveInfo **ptr = qdev_get_prop_ptr(dev, prop); return snprintf(dest, len, ""%s"", (*ptr)->id); } ",qemu,1
"static void xhci_process_commands(XHCIState *xhci)

{

    XHCITRB trb;

    TRBType type;

    XHCIEvent event = {ER_COMMAND_COMPLETE, CC_SUCCESS};

    dma_addr_t addr;

    unsigned int i, slotid = 0;



    DPRINTF(""xhci_process_commands()\n"");

    if (!xhci_running(xhci)) {

        DPRINTF(""xhci_process_commands() called while xHC stopped or paused\n"");

        return;

    }



    xhci->crcr_low |= CRCR_CRR;



    while ((type = xhci_ring_fetch(xhci, &xhci->cmd_ring, &trb, &addr))) {

        event.ptr = addr;

        switch (type) {

        case CR_ENABLE_SLOT:

            for (i = 0; i < xhci->numslots; i++) {

                if (!xhci->slots[i].enabled) {

                    break;

                }

            }

            if (i >= xhci->numslots) {

                DPRINTF(""xhci: no device slots available\n"");

                event.ccode = CC_NO_SLOTS_ERROR;

            } else {

                slotid = i+1;

                event.ccode = xhci_enable_slot(xhci, slotid);

            }

            break;

        case CR_DISABLE_SLOT:

            slotid = xhci_get_slot(xhci, &event, &trb);

            if (slotid) {

                event.ccode = xhci_disable_slot(xhci, slotid);

            }

            break;

        case CR_ADDRESS_DEVICE:

            slotid = xhci_get_slot(xhci, &event, &trb);

            if (slotid) {

                event.ccode = xhci_address_slot(xhci, slotid, trb.parameter,

                                                trb.control & TRB_CR_BSR);

            }

            break;

        case CR_CONFIGURE_ENDPOINT:

            slotid = xhci_get_slot(xhci, &event, &trb);

            if (slotid) {

                event.ccode = xhci_configure_slot(xhci, slotid, trb.parameter,

                                                  trb.control & TRB_CR_DC);

            }

            break;

        case CR_EVALUATE_CONTEXT:

            slotid = xhci_get_slot(xhci, &event, &trb);

            if (slotid) {

                event.ccode = xhci_evaluate_slot(xhci, slotid, trb.parameter);

            }

            break;

        case CR_STOP_ENDPOINT:

            slotid = xhci_get_slot(xhci, &event, &trb);

            if (slotid) {

                unsigned int epid = (trb.control >> TRB_CR_EPID_SHIFT)

                    & TRB_CR_EPID_MASK;

                event.ccode = xhci_stop_ep(xhci, slotid, epid);

            }

            break;

        case CR_RESET_ENDPOINT:

            slotid = xhci_get_slot(xhci, &event, &trb);

            if (slotid) {

                unsigned int epid = (trb.control >> TRB_CR_EPID_SHIFT)

                    & TRB_CR_EPID_MASK;

                event.ccode = xhci_reset_ep(xhci, slotid, epid);

            }

            break;

        case CR_SET_TR_DEQUEUE:

            slotid = xhci_get_slot(xhci, &event, &trb);

            if (slotid) {

                unsigned int epid = (trb.control >> TRB_CR_EPID_SHIFT)

                    & TRB_CR_EPID_MASK;

                unsigned int streamid = (trb.status >> 16) & 0xffff;

                event.ccode = xhci_set_ep_dequeue(xhci, slotid,

                                                  epid, streamid,

                                                  trb.parameter);

            }

            break;

        case CR_RESET_DEVICE:

            slotid = xhci_get_slot(xhci, &event, &trb);

            if (slotid) {

                event.ccode = xhci_reset_slot(xhci, slotid);

            }

            break;

        case CR_GET_PORT_BANDWIDTH:

            event.ccode = xhci_get_port_bandwidth(xhci, trb.parameter);

            break;

        case CR_VENDOR_VIA_CHALLENGE_RESPONSE:

            xhci_via_challenge(xhci, trb.parameter);

            break;

        case CR_VENDOR_NEC_FIRMWARE_REVISION:

            event.type = 48; /* NEC reply */

            event.length = 0x3025;

            break;

        case CR_VENDOR_NEC_CHALLENGE_RESPONSE:

        {

            uint32_t chi = trb.parameter >> 32;

            uint32_t clo = trb.parameter;

            uint32_t val = xhci_nec_challenge(chi, clo);

            event.length = val & 0xFFFF;

            event.epid = val >> 16;

            slotid = val >> 24;

            event.type = 48; /* NEC reply */

        }

        break;

        default:

            trace_usb_xhci_unimplemented(""command"", type);

            event.ccode = CC_TRB_ERROR;

            break;

        }

        event.slotid = slotid;

        xhci_event(xhci, &event, 0);

    }

}
","static void xhci_process_commands(XHCIState *xhci) {
XHCITRB trb; TRBType type; XHCIEvent event = {
ER_COMMAND_COMPLETE, CC_SUCCESS}; dma_addr_t addr; unsigned int i, slotid = 0; DPRINTF(""xhci_process_commands()\n""); if (!xhci_running(xhci)) {
DPRINTF(""xhci_process_commands() called while xHC stopped or paused\n""); return; } xhci->crcr_low |= CRCR_CRR; while ((type = xhci_ring_fetch(xhci, &xhci->cmd_ring, &trb, &addr))) {
event.ptr = addr; switch (type) {
case CR_ENABLE_SLOT: for (i = 0; i < xhci->numslots; i++) {
if (!xhci->slots[i].enabled) {
break; } } if (i >= xhci->numslots) {
DPRINTF(""xhci: no device slots available\n""); event.ccode = CC_NO_SLOTS_ERROR; } else {
slotid = i+1; event.ccode = xhci_enable_slot(xhci, slotid); } break; case CR_DISABLE_SLOT: slotid = xhci_get_slot(xhci, &event, &trb); if (slotid) {
event.ccode = xhci_disable_slot(xhci, slotid); } break; case CR_ADDRESS_DEVICE: slotid = xhci_get_slot(xhci, &event, &trb); if (slotid) {
event.ccode = xhci_address_slot(xhci, slotid, trb.parameter, trb.control & TRB_CR_BSR); } break; case CR_CONFIGURE_ENDPOINT: slotid = xhci_get_slot(xhci, &event, &trb); if (slotid) {
event.ccode = xhci_configure_slot(xhci, slotid, trb.parameter, trb.control & TRB_CR_DC); } break; case CR_EVALUATE_CONTEXT: slotid = xhci_get_slot(xhci, &event, &trb); if (slotid) {
event.ccode = xhci_evaluate_slot(xhci, slotid, trb.parameter); } break; case CR_STOP_ENDPOINT: slotid = xhci_get_slot(xhci, &event, &trb); if (slotid) {
unsigned int epid = (trb.control >> TRB_CR_EPID_SHIFT) & TRB_CR_EPID_MASK; event.ccode = xhci_stop_ep(xhci, slotid, epid); } break; case CR_RESET_ENDPOINT: slotid = xhci_get_slot(xhci, &event, &trb); if (slotid) {
unsigned int epid = (trb.control >> TRB_CR_EPID_SHIFT) & TRB_CR_EPID_MASK; event.ccode = xhci_reset_ep(xhci, slotid, epid); } break; case CR_SET_TR_DEQUEUE: slotid = xhci_get_slot(xhci, &event, &trb); if (slotid) {
unsigned int epid = (trb.control >> TRB_CR_EPID_SHIFT) & TRB_CR_EPID_MASK; unsigned int streamid = (trb.status >> 16) & 0xffff; event.ccode = xhci_set_ep_dequeue(xhci, slotid, epid, streamid, trb.parameter); } break; case CR_RESET_DEVICE: slotid = xhci_get_slot(xhci, &event, &trb); if (slotid) {
event.ccode = xhci_reset_slot(xhci, slotid); } break; case CR_GET_PORT_BANDWIDTH: event.ccode = xhci_get_port_bandwidth(xhci, trb.parameter); break; case CR_VENDOR_VIA_CHALLENGE_RESPONSE: xhci_via_challenge(xhci, trb.parameter); break; case CR_VENDOR_NEC_FIRMWARE_REVISION: event.type = 48; /* NEC reply */ event.length = 0x3025; break; case CR_VENDOR_NEC_CHALLENGE_RESPONSE: {
uint32_t chi = trb.parameter >> 32; uint32_t clo = trb.parameter; uint32_t val = xhci_nec_challenge(chi, clo); event.length = val & 0xFFFF; event.epid = val >> 16; slotid = val >> 24; event.type = 48; /* NEC reply */ } break; default: trace_usb_xhci_unimplemented(""command"", type); event.ccode = CC_TRB_ERROR; break; } event.slotid = slotid; xhci_event(xhci, &event, 0); } } ",qemu,1
"static void netfilter_finalize(Object *obj)

{

    NetFilterState *nf = NETFILTER(obj);

    NetFilterClass *nfc = NETFILTER_GET_CLASS(obj);



    if (nfc->cleanup) {

        nfc->cleanup(nf);

    }



    if (nf->netdev && !QTAILQ_EMPTY(&nf->netdev->filters) &&

        nf->next.tqe_prev) {

        QTAILQ_REMOVE(&nf->netdev->filters, nf, next);

    }

    g_free(nf->netdev_id);

}
","static void netfilter_finalize(Object *obj) {
NetFilterState *nf = NETFILTER(obj); NetFilterClass *nfc = NETFILTER_GET_CLASS(obj); if (nfc->cleanup) {
nfc->cleanup(nf); } if (nf->netdev && !QTAILQ_EMPTY(&nf->netdev->filters) && nf->next.tqe_prev) {
QTAILQ_REMOVE(&nf->netdev->filters, nf, next); } g_free(nf->netdev_id); } ",qemu,0
"int spapr_h_cas_compose_response(target_ulong addr, target_ulong size)

{

    void *fdt, *fdt_skel;

    sPAPRDeviceTreeUpdateHeader hdr = { .version_id = 1 };



    size -= sizeof(hdr);



    /* Create sceleton */

    fdt_skel = g_malloc0(size);

    _FDT((fdt_create(fdt_skel, size)));

    _FDT((fdt_begin_node(fdt_skel, """")));

    _FDT((fdt_end_node(fdt_skel)));

    _FDT((fdt_finish(fdt_skel)));

    fdt = g_malloc0(size);

    _FDT((fdt_open_into(fdt_skel, fdt, size)));

    g_free(fdt_skel);



    /* Place to make changes to the tree */



    /* Pack resulting tree */

    _FDT((fdt_pack(fdt)));



    if (fdt_totalsize(fdt) + sizeof(hdr) > size) {

        trace_spapr_cas_failed(size);

        return -1;

    }



    cpu_physical_memory_write(addr, &hdr, sizeof(hdr));

    cpu_physical_memory_write(addr + sizeof(hdr), fdt, fdt_totalsize(fdt));

    trace_spapr_cas_continue(fdt_totalsize(fdt) + sizeof(hdr));

    g_free(fdt);



    return 0;

}
","int spapr_h_cas_compose_response(target_ulong addr, target_ulong size) {
void *fdt, *fdt_skel; sPAPRDeviceTreeUpdateHeader hdr = {
.version_id = 1 }; size -= sizeof(hdr); /* Create sceleton */ fdt_skel = g_malloc0(size); _FDT((fdt_create(fdt_skel, size))); _FDT((fdt_begin_node(fdt_skel, """"))); _FDT((fdt_end_node(fdt_skel))); _FDT((fdt_finish(fdt_skel))); fdt = g_malloc0(size); _FDT((fdt_open_into(fdt_skel, fdt, size))); g_free(fdt_skel); /* Place to make changes to the tree */ /* Pack resulting tree */ _FDT((fdt_pack(fdt))); if (fdt_totalsize(fdt) + sizeof(hdr) > size) {
trace_spapr_cas_failed(size); return -1; } cpu_physical_memory_write(addr, &hdr, sizeof(hdr)); cpu_physical_memory_write(addr + sizeof(hdr), fdt, fdt_totalsize(fdt)); trace_spapr_cas_continue(fdt_totalsize(fdt) + sizeof(hdr)); g_free(fdt); return 0; } ",qemu,1
"static int vhost_set_vring(struct vhost_dev *dev,

                           unsigned long int request,

                           struct vhost_vring_state *ring)

{

    VhostUserMsg msg = {

        .request = request,

        .flags = VHOST_USER_VERSION,

        .state = *ring,

        .size = sizeof(*ring),

    };



    vhost_user_write(dev, &msg, NULL, 0);



    return 0;

}
","static int vhost_set_vring(struct vhost_dev *dev, unsigned long int request, struct vhost_vring_state *ring) {
VhostUserMsg msg = {
.request = request, .flags = VHOST_USER_VERSION, .state = *ring, .size = sizeof(*ring), }; vhost_user_write(dev, &msg, NULL, 0); return 0; } ",qemu,1
"static void usb_uas_unrealize(USBDevice *dev, Error **errp)
{
    UASDevice *uas = USB_UAS(dev);
    qemu_bh_delete(uas->status_bh);
}","static void usb_uas_unrealize(USBDevice *dev, Error **errp) {
UASDevice *uas = USB_UAS(dev); qemu_bh_delete(uas->status_bh); }",qemu,1
"static void dp8393x_realize(DeviceState *dev, Error **errp)

{

    dp8393xState *s = DP8393X(dev);

    int i, checksum;

    uint8_t *prom;



    address_space_init(&s->as, s->dma_mr, ""dp8393x"");

    memory_region_init_io(&s->mmio, OBJECT(dev), &dp8393x_ops, s,

                          ""dp8393x-regs"", 0x40 << s->it_shift);



    s->nic = qemu_new_nic(&net_dp83932_info, &s->conf,

                          object_get_typename(OBJECT(dev)), dev->id, s);

    qemu_format_nic_info_str(qemu_get_queue(s->nic), s->conf.macaddr.a);



    s->watchdog = timer_new_ns(QEMU_CLOCK_VIRTUAL, dp8393x_watchdog, s);

    s->regs[SONIC_SR] = 0x0004; /* only revision recognized by Linux */



    memory_region_init_rom_device(&s->prom, OBJECT(dev), NULL, NULL,

                                  ""dp8393x-prom"", SONIC_PROM_SIZE, NULL);

    prom = memory_region_get_ram_ptr(&s->prom);

    checksum = 0;

    for (i = 0; i < 6; i++) {

        prom[i] = s->conf.macaddr.a[i];

        checksum += prom[i];

        if (checksum > 0xff) {

            checksum = (checksum + 1) & 0xff;

        }

    }

    prom[7] = 0xff - checksum;

}
","static void dp8393x_realize(DeviceState *dev, Error **errp) {
dp8393xState *s = DP8393X(dev); int i, checksum; uint8_t *prom; address_space_init(&s->as, s->dma_mr, ""dp8393x""); memory_region_init_io(&s->mmio, OBJECT(dev), &dp8393x_ops, s, ""dp8393x-regs"", 0x40 << s->it_shift); s->nic = qemu_new_nic(&net_dp83932_info, &s->conf, object_get_typename(OBJECT(dev)), dev->id, s); qemu_format_nic_info_str(qemu_get_queue(s->nic), s->conf.macaddr.a); s->watchdog = timer_new_ns(QEMU_CLOCK_VIRTUAL, dp8393x_watchdog, s); s->regs[SONIC_SR] = 0x0004; /* only revision recognized by Linux */ memory_region_init_rom_device(&s->prom, OBJECT(dev), NULL, NULL, ""dp8393x-prom"", SONIC_PROM_SIZE, NULL); prom = memory_region_get_ram_ptr(&s->prom); checksum = 0; for (i = 0; i < 6; i++) {
prom[i] = s->conf.macaddr.a[i]; checksum += prom[i]; if (checksum > 0xff) {
checksum = (checksum + 1) & 0xff; } } prom[7] = 0xff - checksum; } ",qemu,1
"static void test_rtas_get_time_of_day(void)

{

    QOSState *qs;

    struct tm tm;

    uint32_t ns;

    uint64_t ret;

    time_t t1, t2;



    qs = qtest_spapr_boot(""-machine pseries"");

    g_assert(qs != NULL);



    t1 = time(NULL);

    ret = qrtas_get_time_of_day(qs->alloc, &tm, &ns);

    g_assert_cmpint(ret, ==, 0);

    t2 = mktimegm(&tm);

    g_assert(t2 - t1 < 5); /* 5 sec max to run the test */



    qtest_shutdown(qs);

}
","static void test_rtas_get_time_of_day(void) {
QOSState *qs; struct tm tm; uint32_t ns; uint64_t ret; time_t t1, t2; qs = qtest_spapr_boot(""-machine pseries""); g_assert(qs != NULL); t1 = time(NULL); ret = qrtas_get_time_of_day(qs->alloc, &tm, &ns); g_assert_cmpint(ret, ==, 0); t2 = mktimegm(&tm); g_assert(t2 - t1 < 5); /* 5 sec max to run the test */ qtest_shutdown(qs); } ",qemu,1
"static inline void load_seg_cache_raw_dt(SegmentCache *sc, uint32_t e1, uint32_t e2)

{

    sc->base = get_seg_base(e1, e2);

    sc->limit = get_seg_limit(e1, e2);

    sc->flags = e2;

}
","static inline void load_seg_cache_raw_dt(SegmentCache *sc, uint32_t e1, uint32_t e2) {
sc->base = get_seg_base(e1, e2); sc->limit = get_seg_limit(e1, e2); sc->flags = e2; } ",qemu,0
"int qemu_savevm_state_iterate(QEMUFile *f)

{

    SaveStateEntry *se;

    int ret = 1;



    TAILQ_FOREACH(se, &savevm_handlers, entry) {

        if (se->save_live_state == NULL)

            continue;



        /* Section type */

        qemu_put_byte(f, QEMU_VM_SECTION_PART);

        qemu_put_be32(f, se->section_id);



        ret &= !!se->save_live_state(f, QEMU_VM_SECTION_PART, se->opaque);

    }



    if (ret)

        return 1;



    if (qemu_file_has_error(f))

        return -EIO;



    return 0;

}
","int qemu_savevm_state_iterate(QEMUFile *f) {
SaveStateEntry *se; int ret = 1; TAILQ_FOREACH(se, &savevm_handlers, entry) {
if (se->save_live_state == NULL) continue; /* Section type */ qemu_put_byte(f, QEMU_VM_SECTION_PART); qemu_put_be32(f, se->section_id); ret &= !!se->save_live_state(f, QEMU_VM_SECTION_PART, se->opaque); } if (ret) return 1; if (qemu_file_has_error(f)) return -EIO; return 0; } ",qemu,0
"void ppc_store_sr (CPUPPCState *env, int srnum, target_ulong value)

{

    LOG_MMU(""%s: reg=%d "" TARGET_FMT_lx "" "" TARGET_FMT_lx ""\n"", __func__,

            srnum, value, env->sr[srnum]);

#if defined(TARGET_PPC64)

    if (env->mmu_model & POWERPC_MMU_64) {

        uint64_t rb = 0, rs = 0;



        /* ESID = srnum */

        rb |= ((uint32_t)srnum & 0xf) << 28;

        /* Set the valid bit */

        rb |= 1 << 27;

        /* Index = ESID */

        rb |= (uint32_t)srnum;



        /* VSID = VSID */

        rs |= (value & 0xfffffff) << 12;

        /* flags = flags */

        rs |= ((value >> 27) & 0xf) << 9;



        ppc_store_slb(env, rb, rs);

    } else

#endif

    if (env->sr[srnum] != value) {

        env->sr[srnum] = value;

/* Invalidating 256MB of virtual memory in 4kB pages is way longer than

   flusing the whole TLB. */

#if !defined(FLUSH_ALL_TLBS) && 0

        {

            target_ulong page, end;

            /* Invalidate 256 MB of virtual memory */

            page = (16 << 20) * srnum;

            end = page + (16 << 20);

            for (; page != end; page += TARGET_PAGE_SIZE)

                tlb_flush_page(env, page);

        }

#else

        tlb_flush(env, 1);

#endif

    }

}
","void ppc_store_sr (CPUPPCState *env, int srnum, target_ulong value) {
LOG_MMU(""%s: reg=%d "" TARGET_FMT_lx "" "" TARGET_FMT_lx ""\n"", __func__, srnum, value, env->sr[srnum]); #if defined(TARGET_PPC64) if (env->mmu_model & POWERPC_MMU_64) {
uint64_t rb = 0, rs = 0; /* ESID = srnum */ rb |= ((uint32_t)srnum & 0xf) << 28; /* Set the valid bit */ rb |= 1 << 27; /* Index = ESID */ rb |= (uint32_t)srnum; /* VSID = VSID */ rs |= (value & 0xfffffff) << 12; /* flags = flags */ rs |= ((value >> 27) & 0xf) << 9; ppc_store_slb(env, rb, rs); } else #endif if (env->sr[srnum] != value) {
env->sr[srnum] = value; /* Invalidating 256MB of virtual memory in 4kB pages is way longer than flusing the whole TLB. */ #if !defined(FLUSH_ALL_TLBS) && 0 {
target_ulong page, end; /* Invalidate 256 MB of virtual memory */ page = (16 << 20) * srnum; end = page + (16 << 20); for (; page != end; page += TARGET_PAGE_SIZE) tlb_flush_page(env, page); } #else tlb_flush(env, 1); #endif } } ",qemu,1
"static int pci_nic_uninit(PCIDevice *dev)

{

    PCIEEPRO100State *d = DO_UPCAST(PCIEEPRO100State, dev, dev);

    EEPRO100State *s = &d->eepro100;



    cpu_unregister_io_memory(s->mmio_index);



    return 0;

}
","static int pci_nic_uninit(PCIDevice *dev) {
PCIEEPRO100State *d = DO_UPCAST(PCIEEPRO100State, dev, dev); EEPRO100State *s = &d->eepro100; cpu_unregister_io_memory(s->mmio_index); return 0; } ",qemu,0
"void bdrv_invalidate_cache(BlockDriverState *bs, Error **errp)

{

    BdrvChild *child;

    Error *local_err = NULL;

    int ret;



    if (!bs->drv)  {

        return;

    }



    if (!(bs->open_flags & BDRV_O_INACTIVE)) {

        return;

    }

    bs->open_flags &= ~BDRV_O_INACTIVE;



    if (bs->drv->bdrv_invalidate_cache) {

        bs->drv->bdrv_invalidate_cache(bs, &local_err);

        if (local_err) {

            bs->open_flags |= BDRV_O_INACTIVE;

            error_propagate(errp, local_err);

            return;

        }

    }



    QLIST_FOREACH(child, &bs->children, next) {

        bdrv_invalidate_cache(child->bs, &local_err);

        if (local_err) {

            bs->open_flags |= BDRV_O_INACTIVE;

            error_propagate(errp, local_err);

            return;

        }

    }



    ret = refresh_total_sectors(bs, bs->total_sectors);

    if (ret < 0) {

        bs->open_flags |= BDRV_O_INACTIVE;

        error_setg_errno(errp, -ret, ""Could not refresh total sector count"");

        return;

    }

}
","void bdrv_invalidate_cache(BlockDriverState *bs, Error **errp) {
BdrvChild *child; Error *local_err = NULL; int ret; if (!bs->drv) {
return; } if (!(bs->open_flags & BDRV_O_INACTIVE)) {
return; } bs->open_flags &= ~BDRV_O_INACTIVE; if (bs->drv->bdrv_invalidate_cache) {
bs->drv->bdrv_invalidate_cache(bs, &local_err); if (local_err) {
bs->open_flags |= BDRV_O_INACTIVE; error_propagate(errp, local_err); return; } } QLIST_FOREACH(child, &bs->children, next) {
bdrv_invalidate_cache(child->bs, &local_err); if (local_err) {
bs->open_flags |= BDRV_O_INACTIVE; error_propagate(errp, local_err); return; } } ret = refresh_total_sectors(bs, bs->total_sectors); if (ret < 0) {
bs->open_flags |= BDRV_O_INACTIVE; error_setg_errno(errp, -ret, ""Could not refresh total sector count""); return; } } ",qemu,1
"static void omap_prcm_write(void *opaque, hwaddr addr,

                            uint64_t value, unsigned size)

{

    struct omap_prcm_s *s = (struct omap_prcm_s *) opaque;



    if (size != 4) {

        omap_badwidth_write32(opaque, addr, value);

        return;

    }



    switch (addr) {

    case 0x000:	/* PRCM_REVISION */

    case 0x054:	/* PRCM_VOLTST */

    case 0x084:	/* PRCM_CLKCFG_STATUS */

    case 0x1e4:	/* PM_PWSTST_MPU */

    case 0x220:	/* CM_IDLEST1_CORE */

    case 0x224:	/* CM_IDLEST2_CORE */

    case 0x22c:	/* CM_IDLEST4_CORE */

    case 0x2c8:	/* PM_WKDEP_CORE */

    case 0x2e4:	/* PM_PWSTST_CORE */

    case 0x320:	/* CM_IDLEST_GFX */

    case 0x3e4:	/* PM_PWSTST_GFX */

    case 0x420:	/* CM_IDLEST_WKUP */

    case 0x520:	/* CM_IDLEST_CKGEN */

    case 0x820:	/* CM_IDLEST_DSP */

    case 0x8e4:	/* PM_PWSTST_DSP */

        OMAP_RO_REG(addr);

        return;



    case 0x010:	/* PRCM_SYSCONFIG */

        s->sysconfig = value & 1;

        break;



    case 0x018:	/* PRCM_IRQSTATUS_MPU */

        s->irqst[0] &= ~value;

        omap_prcm_int_update(s, 0);

        break;

    case 0x01c:	/* PRCM_IRQENABLE_MPU */

        s->irqen[0] = value & 0x3f;

        omap_prcm_int_update(s, 0);

        break;



    case 0x050:	/* PRCM_VOLTCTRL */

        s->voltctrl = value & 0xf1c3;

        break;



    case 0x060:	/* PRCM_CLKSRC_CTRL */

        s->clksrc[0] = value & 0xdb;

        /* TODO update clocks */

        break;



    case 0x070:	/* PRCM_CLKOUT_CTRL */

        s->clkout[0] = value & 0xbbbb;

        /* TODO update clocks */

        break;



    case 0x078:	/* PRCM_CLKEMUL_CTRL */

        s->clkemul[0] = value & 1;

        /* TODO update clocks */

        break;



    case 0x080:	/* PRCM_CLKCFG_CTRL */

        break;



    case 0x090:	/* PRCM_VOLTSETUP */

        s->setuptime[0] = value & 0xffff;

        break;

    case 0x094:	/* PRCM_CLKSSETUP */

        s->setuptime[1] = value & 0xffff;

        break;



    case 0x098:	/* PRCM_POLCTRL */

        s->clkpol[0] = value & 0x701;

        break;



    case 0x0b0:	/* GENERAL_PURPOSE1 */

    case 0x0b4:	/* GENERAL_PURPOSE2 */

    case 0x0b8:	/* GENERAL_PURPOSE3 */

    case 0x0bc:	/* GENERAL_PURPOSE4 */

    case 0x0c0:	/* GENERAL_PURPOSE5 */

    case 0x0c4:	/* GENERAL_PURPOSE6 */

    case 0x0c8:	/* GENERAL_PURPOSE7 */

    case 0x0cc:	/* GENERAL_PURPOSE8 */

    case 0x0d0:	/* GENERAL_PURPOSE9 */

    case 0x0d4:	/* GENERAL_PURPOSE10 */

    case 0x0d8:	/* GENERAL_PURPOSE11 */

    case 0x0dc:	/* GENERAL_PURPOSE12 */

    case 0x0e0:	/* GENERAL_PURPOSE13 */

    case 0x0e4:	/* GENERAL_PURPOSE14 */

    case 0x0e8:	/* GENERAL_PURPOSE15 */

    case 0x0ec:	/* GENERAL_PURPOSE16 */

    case 0x0f0:	/* GENERAL_PURPOSE17 */

    case 0x0f4:	/* GENERAL_PURPOSE18 */

    case 0x0f8:	/* GENERAL_PURPOSE19 */

    case 0x0fc:	/* GENERAL_PURPOSE20 */

        s->scratch[(addr - 0xb0) >> 2] = value;

        break;



    case 0x140:	/* CM_CLKSEL_MPU */

        s->clksel[0] = value & 0x1f;

        /* TODO update clocks */

        break;

    case 0x148:	/* CM_CLKSTCTRL_MPU */

        s->clkctrl[0] = value & 0x1f;

        break;



    case 0x158:	/* RM_RSTST_MPU */

        s->rst[0] &= ~value;

        break;

    case 0x1c8:	/* PM_WKDEP_MPU */

        s->wkup[0] = value & 0x15;

        break;



    case 0x1d4:	/* PM_EVGENCTRL_MPU */

        s->ev = value & 0x1f;

        break;

    case 0x1d8:	/* PM_EVEGENONTIM_MPU */

        s->evtime[0] = value;

        break;

    case 0x1dc:	/* PM_EVEGENOFFTIM_MPU */

        s->evtime[1] = value;

        break;



    case 0x1e0:	/* PM_PWSTCTRL_MPU */

        s->power[0] = value & 0xc0f;

        break;



    case 0x200:	/* CM_FCLKEN1_CORE */

        s->clken[0] = value & 0xbfffffff;

        /* TODO update clocks */

        /* The EN_EAC bit only gets/puts func_96m_clk.  */

        break;

    case 0x204:	/* CM_FCLKEN2_CORE */

        s->clken[1] = value & 0x00000007;

        /* TODO update clocks */

        break;

    case 0x210:	/* CM_ICLKEN1_CORE */

        s->clken[2] = value & 0xfffffff9;

        /* TODO update clocks */

        /* The EN_EAC bit only gets/puts core_l4_iclk.  */

        break;

    case 0x214:	/* CM_ICLKEN2_CORE */

        s->clken[3] = value & 0x00000007;

        /* TODO update clocks */

        break;

    case 0x21c:	/* CM_ICLKEN4_CORE */

        s->clken[4] = value & 0x0000001f;

        /* TODO update clocks */

        break;



    case 0x230:	/* CM_AUTOIDLE1_CORE */

        s->clkidle[0] = value & 0xfffffff9;

        /* TODO update clocks */

        break;

    case 0x234:	/* CM_AUTOIDLE2_CORE */

        s->clkidle[1] = value & 0x00000007;

        /* TODO update clocks */

        break;

    case 0x238:	/* CM_AUTOIDLE3_CORE */

        s->clkidle[2] = value & 0x00000007;

        /* TODO update clocks */

        break;

    case 0x23c:	/* CM_AUTOIDLE4_CORE */

        s->clkidle[3] = value & 0x0000001f;

        /* TODO update clocks */

        break;



    case 0x240:	/* CM_CLKSEL1_CORE */

        s->clksel[1] = value & 0x0fffbf7f;

        /* TODO update clocks */

        break;



    case 0x244:	/* CM_CLKSEL2_CORE */

        s->clksel[2] = value & 0x00fffffc;

        /* TODO update clocks */

        break;



    case 0x248:	/* CM_CLKSTCTRL_CORE */

        s->clkctrl[1] = value & 0x7;

        break;



    case 0x2a0:	/* PM_WKEN1_CORE */

        s->wken[0] = value & 0x04667ff8;

        break;

    case 0x2a4:	/* PM_WKEN2_CORE */

        s->wken[1] = value & 0x00000005;

        break;



    case 0x2b0:	/* PM_WKST1_CORE */

        s->wkst[0] &= ~value;

        break;

    case 0x2b4:	/* PM_WKST2_CORE */

        s->wkst[1] &= ~value;

        break;



    case 0x2e0:	/* PM_PWSTCTRL_CORE */

        s->power[1] = (value & 0x00fc3f) | (1 << 2);

        break;



    case 0x300:	/* CM_FCLKEN_GFX */

        s->clken[5] = value & 6;

        /* TODO update clocks */

        break;

    case 0x310:	/* CM_ICLKEN_GFX */

        s->clken[6] = value & 1;

        /* TODO update clocks */

        break;

    case 0x340:	/* CM_CLKSEL_GFX */

        s->clksel[3] = value & 7;

        /* TODO update clocks */

        break;

    case 0x348:	/* CM_CLKSTCTRL_GFX */

        s->clkctrl[2] = value & 1;

        break;

    case 0x350:	/* RM_RSTCTRL_GFX */

        s->rstctrl[0] = value & 1;

        /* TODO: reset */

        break;

    case 0x358:	/* RM_RSTST_GFX */

        s->rst[1] &= ~value;

        break;

    case 0x3c8:	/* PM_WKDEP_GFX */

        s->wkup[1] = value & 0x13;

        break;

    case 0x3e0:	/* PM_PWSTCTRL_GFX */

        s->power[2] = (value & 0x00c0f) | (3 << 2);

        break;



    case 0x400:	/* CM_FCLKEN_WKUP */

        s->clken[7] = value & 0xd;

        /* TODO update clocks */

        break;

    case 0x410:	/* CM_ICLKEN_WKUP */

        s->clken[8] = value & 0x3f;

        /* TODO update clocks */

        break;

    case 0x430:	/* CM_AUTOIDLE_WKUP */

        s->clkidle[4] = value & 0x0000003f;

        /* TODO update clocks */

        break;

    case 0x440:	/* CM_CLKSEL_WKUP */

        s->clksel[4] = value & 3;

        /* TODO update clocks */

        break;

    case 0x450:	/* RM_RSTCTRL_WKUP */

        /* TODO: reset */

        if (value & 2)

            qemu_system_reset_request(SHUTDOWN_CAUSE_GUEST_RESET);

        break;

    case 0x454:	/* RM_RSTTIME_WKUP */

        s->rsttime_wkup = value & 0x1fff;

        break;

    case 0x458:	/* RM_RSTST_WKUP */

        s->rst[2] &= ~value;

        break;

    case 0x4a0:	/* PM_WKEN_WKUP */

        s->wken[2] = value & 0x00000005;

        break;

    case 0x4b0:	/* PM_WKST_WKUP */

        s->wkst[2] &= ~value;

        break;



    case 0x500:	/* CM_CLKEN_PLL */

        if (value & 0xffffff30)

            fprintf(stderr, ""%s: write 0s in CM_CLKEN_PLL for ""

                            ""future compatibility\n"", __FUNCTION__);

        if ((s->clken[9] ^ value) & 0xcc) {

            s->clken[9] &= ~0xcc;

            s->clken[9] |= value & 0xcc;

            omap_prcm_apll_update(s);

        }

        if ((s->clken[9] ^ value) & 3) {

            s->clken[9] &= ~3;

            s->clken[9] |= value & 3;

            omap_prcm_dpll_update(s);

        }

        break;

    case 0x530:	/* CM_AUTOIDLE_PLL */

        s->clkidle[5] = value & 0x000000cf;

        /* TODO update clocks */

        break;

    case 0x540:	/* CM_CLKSEL1_PLL */

        if (value & 0xfc4000d7)

            fprintf(stderr, ""%s: write 0s in CM_CLKSEL1_PLL for ""

                            ""future compatibility\n"", __FUNCTION__);

        if ((s->clksel[5] ^ value) & 0x003fff00) {

            s->clksel[5] = value & 0x03bfff28;

            omap_prcm_dpll_update(s);

        }

        /* TODO update the other clocks */



        s->clksel[5] = value & 0x03bfff28;

        break;

    case 0x544:	/* CM_CLKSEL2_PLL */

        if (value & ~3)

            fprintf(stderr, ""%s: write 0s in CM_CLKSEL2_PLL[31:2] for ""

                            ""future compatibility\n"", __FUNCTION__);

        if (s->clksel[6] != (value & 3)) {

            s->clksel[6] = value & 3;

            omap_prcm_dpll_update(s);

        }

        break;



    case 0x800:	/* CM_FCLKEN_DSP */

        s->clken[10] = value & 0x501;

        /* TODO update clocks */

        break;

    case 0x810:	/* CM_ICLKEN_DSP */

        s->clken[11] = value & 0x2;

        /* TODO update clocks */

        break;

    case 0x830:	/* CM_AUTOIDLE_DSP */

        s->clkidle[6] = value & 0x2;

        /* TODO update clocks */

        break;

    case 0x840:	/* CM_CLKSEL_DSP */

        s->clksel[7] = value & 0x3fff;

        /* TODO update clocks */

        break;

    case 0x848:	/* CM_CLKSTCTRL_DSP */

        s->clkctrl[3] = value & 0x101;

        break;

    case 0x850:	/* RM_RSTCTRL_DSP */

        /* TODO: reset */

        break;

    case 0x858:	/* RM_RSTST_DSP */

        s->rst[3] &= ~value;

        break;

    case 0x8c8:	/* PM_WKDEP_DSP */

        s->wkup[2] = value & 0x13;

        break;

    case 0x8e0:	/* PM_PWSTCTRL_DSP */

        s->power[3] = (value & 0x03017) | (3 << 2);

        break;



    case 0x8f0:	/* PRCM_IRQSTATUS_DSP */

        s->irqst[1] &= ~value;

        omap_prcm_int_update(s, 1);

        break;

    case 0x8f4:	/* PRCM_IRQENABLE_DSP */

        s->irqen[1] = value & 0x7;

        omap_prcm_int_update(s, 1);

        break;



    case 0x8f8:	/* PRCM_IRQSTATUS_IVA */

        s->irqst[2] &= ~value;

        omap_prcm_int_update(s, 2);

        break;

    case 0x8fc:	/* PRCM_IRQENABLE_IVA */

        s->irqen[2] = value & 0x7;

        omap_prcm_int_update(s, 2);

        break;



    default:

        OMAP_BAD_REG(addr);

        return;

    }

}
","static void omap_prcm_write(void *opaque, hwaddr addr, uint64_t value, unsigned size) {
struct omap_prcm_s *s = (struct omap_prcm_s *) opaque; if (size != 4) {
omap_badwidth_write32(opaque, addr, value); return; } switch (addr) {
case 0x000: /* PRCM_REVISION */ case 0x054: /* PRCM_VOLTST */ case 0x084: /* PRCM_CLKCFG_STATUS */ case 0x1e4: /* PM_PWSTST_MPU */ case 0x220: /* CM_IDLEST1_CORE */ case 0x224: /* CM_IDLEST2_CORE */ case 0x22c: /* CM_IDLEST4_CORE */ case 0x2c8: /* PM_WKDEP_CORE */ case 0x2e4: /* PM_PWSTST_CORE */ case 0x320: /* CM_IDLEST_GFX */ case 0x3e4: /* PM_PWSTST_GFX */ case 0x420: /* CM_IDLEST_WKUP */ case 0x520: /* CM_IDLEST_CKGEN */ case 0x820: /* CM_IDLEST_DSP */ case 0x8e4: /* PM_PWSTST_DSP */ OMAP_RO_REG(addr); return; case 0x010: /* PRCM_SYSCONFIG */ s->sysconfig = value & 1; break; case 0x018: /* PRCM_IRQSTATUS_MPU */ s->irqst[0] &= ~value; omap_prcm_int_update(s, 0); break; case 0x01c: /* PRCM_IRQENABLE_MPU */ s->irqen[0] = value & 0x3f; omap_prcm_int_update(s, 0); break; case 0x050: /* PRCM_VOLTCTRL */ s->voltctrl = value & 0xf1c3; break; case 0x060: /* PRCM_CLKSRC_CTRL */ s->clksrc[0] = value & 0xdb; /* TODO update clocks */ break; case 0x070: /* PRCM_CLKOUT_CTRL */ s->clkout[0] = value & 0xbbbb; /* TODO update clocks */ break; case 0x078: /* PRCM_CLKEMUL_CTRL */ s->clkemul[0] = value & 1; /* TODO update clocks */ break; case 0x080: /* PRCM_CLKCFG_CTRL */ break; case 0x090: /* PRCM_VOLTSETUP */ s->setuptime[0] = value & 0xffff; break; case 0x094: /* PRCM_CLKSSETUP */ s->setuptime[1] = value & 0xffff; break; case 0x098: /* PRCM_POLCTRL */ s->clkpol[0] = value & 0x701; break; case 0x0b0: /* GENERAL_PURPOSE1 */ case 0x0b4: /* GENERAL_PURPOSE2 */ case 0x0b8: /* GENERAL_PURPOSE3 */ case 0x0bc: /* GENERAL_PURPOSE4 */ case 0x0c0: /* GENERAL_PURPOSE5 */ case 0x0c4: /* GENERAL_PURPOSE6 */ case 0x0c8: /* GENERAL_PURPOSE7 */ case 0x0cc: /* GENERAL_PURPOSE8 */ case 0x0d0: /* GENERAL_PURPOSE9 */ case 0x0d4: /* GENERAL_PURPOSE10 */ case 0x0d8: /* GENERAL_PURPOSE11 */ case 0x0dc: /* GENERAL_PURPOSE12 */ case 0x0e0: /* GENERAL_PURPOSE13 */ case 0x0e4: /* GENERAL_PURPOSE14 */ case 0x0e8: /* GENERAL_PURPOSE15 */ case 0x0ec: /* GENERAL_PURPOSE16 */ case 0x0f0: /* GENERAL_PURPOSE17 */ case 0x0f4: /* GENERAL_PURPOSE18 */ case 0x0f8: /* GENERAL_PURPOSE19 */ case 0x0fc: /* GENERAL_PURPOSE20 */ s->scratch[(addr - 0xb0) >> 2] = value; break; case 0x140: /* CM_CLKSEL_MPU */ s->clksel[0] = value & 0x1f; /* TODO update clocks */ break; case 0x148: /* CM_CLKSTCTRL_MPU */ s->clkctrl[0] = value & 0x1f; break; case 0x158: /* RM_RSTST_MPU */ s->rst[0] &= ~value; break; case 0x1c8: /* PM_WKDEP_MPU */ s->wkup[0] = value & 0x15; break; case 0x1d4: /* PM_EVGENCTRL_MPU */ s->ev = value & 0x1f; break; case 0x1d8: /* PM_EVEGENONTIM_MPU */ s->evtime[0] = value; break; case 0x1dc: /* PM_EVEGENOFFTIM_MPU */ s->evtime[1] = value; break; case 0x1e0: /* PM_PWSTCTRL_MPU */ s->power[0] = value & 0xc0f; break; case 0x200: /* CM_FCLKEN1_CORE */ s->clken[0] = value & 0xbfffffff; /* TODO update clocks */ /* The EN_EAC bit only gets/puts func_96m_clk. */ break; case 0x204: /* CM_FCLKEN2_CORE */ s->clken[1] = value & 0x00000007; /* TODO update clocks */ break; case 0x210: /* CM_ICLKEN1_CORE */ s->clken[2] = value & 0xfffffff9; /* TODO update clocks */ /* The EN_EAC bit only gets/puts core_l4_iclk. */ break; case 0x214: /* CM_ICLKEN2_CORE */ s->clken[3] = value & 0x00000007; /* TODO update clocks */ break; case 0x21c: /* CM_ICLKEN4_CORE */ s->clken[4] = value & 0x0000001f; /* TODO update clocks */ break; case 0x230: /* CM_AUTOIDLE1_CORE */ s->clkidle[0] = value & 0xfffffff9; /* TODO update clocks */ break; case 0x234: /* CM_AUTOIDLE2_CORE */ s->clkidle[1] = value & 0x00000007; /* TODO update clocks */ break; case 0x238: /* CM_AUTOIDLE3_CORE */ s->clkidle[2] = value & 0x00000007; /* TODO update clocks */ break; case 0x23c: /* CM_AUTOIDLE4_CORE */ s->clkidle[3] = value & 0x0000001f; /* TODO update clocks */ break; case 0x240: /* CM_CLKSEL1_CORE */ s->clksel[1] = value & 0x0fffbf7f; /* TODO update clocks */ break; case 0x244: /* CM_CLKSEL2_CORE */ s->clksel[2] = value & 0x00fffffc; /* TODO update clocks */ break; case 0x248: /* CM_CLKSTCTRL_CORE */ s->clkctrl[1] = value & 0x7; break; case 0x2a0: /* PM_WKEN1_CORE */ s->wken[0] = value & 0x04667ff8; break; case 0x2a4: /* PM_WKEN2_CORE */ s->wken[1] = value & 0x00000005; break; case 0x2b0: /* PM_WKST1_CORE */ s->wkst[0] &= ~value; break; case 0x2b4: /* PM_WKST2_CORE */ s->wkst[1] &= ~value; break; case 0x2e0: /* PM_PWSTCTRL_CORE */ s->power[1] = (value & 0x00fc3f) | (1 << 2); break; case 0x300: /* CM_FCLKEN_GFX */ s->clken[5] = value & 6; /* TODO update clocks */ break; case 0x310: /* CM_ICLKEN_GFX */ s->clken[6] = value & 1; /* TODO update clocks */ break; case 0x340: /* CM_CLKSEL_GFX */ s->clksel[3] = value & 7; /* TODO update clocks */ break; case 0x348: /* CM_CLKSTCTRL_GFX */ s->clkctrl[2] = value & 1; break; case 0x350: /* RM_RSTCTRL_GFX */ s->rstctrl[0] = value & 1; /* TODO: reset */ break; case 0x358: /* RM_RSTST_GFX */ s->rst[1] &= ~value; break; case 0x3c8: /* PM_WKDEP_GFX */ s->wkup[1] = value & 0x13; break; case 0x3e0: /* PM_PWSTCTRL_GFX */ s->power[2] = (value & 0x00c0f) | (3 << 2); break; case 0x400: /* CM_FCLKEN_WKUP */ s->clken[7] = value & 0xd; /* TODO update clocks */ break; case 0x410: /* CM_ICLKEN_WKUP */ s->clken[8] = value & 0x3f; /* TODO update clocks */ break; case 0x430: /* CM_AUTOIDLE_WKUP */ s->clkidle[4] = value & 0x0000003f; /* TODO update clocks */ break; case 0x440: /* CM_CLKSEL_WKUP */ s->clksel[4] = value & 3; /* TODO update clocks */ break; case 0x450: /* RM_RSTCTRL_WKUP */ /* TODO: reset */ if (value & 2) qemu_system_reset_request(SHUTDOWN_CAUSE_GUEST_RESET); break; case 0x454: /* RM_RSTTIME_WKUP */ s->rsttime_wkup = value & 0x1fff; break; case 0x458: /* RM_RSTST_WKUP */ s->rst[2] &= ~value; break; case 0x4a0: /* PM_WKEN_WKUP */ s->wken[2] = value & 0x00000005; break; case 0x4b0: /* PM_WKST_WKUP */ s->wkst[2] &= ~value; break; case 0x500: /* CM_CLKEN_PLL */ if (value & 0xffffff30) fprintf(stderr, ""%s: write 0s in CM_CLKEN_PLL for "" ""future compatibility\n"", __FUNCTION__); if ((s->clken[9] ^ value) & 0xcc) {
s->clken[9] &= ~0xcc; s->clken[9] |= value & 0xcc; omap_prcm_apll_update(s); } if ((s->clken[9] ^ value) & 3) {
s->clken[9] &= ~3; s->clken[9] |= value & 3; omap_prcm_dpll_update(s); } break; case 0x530: /* CM_AUTOIDLE_PLL */ s->clkidle[5] = value & 0x000000cf; /* TODO update clocks */ break; case 0x540: /* CM_CLKSEL1_PLL */ if (value & 0xfc4000d7) fprintf(stderr, ""%s: write 0s in CM_CLKSEL1_PLL for "" ""future compatibility\n"", __FUNCTION__); if ((s->clksel[5] ^ value) & 0x003fff00) {
s->clksel[5] = value & 0x03bfff28; omap_prcm_dpll_update(s); } /* TODO update the other clocks */ s->clksel[5] = value & 0x03bfff28; break; case 0x544: /* CM_CLKSEL2_PLL */ if (value & ~3) fprintf(stderr, ""%s: write 0s in CM_CLKSEL2_PLL[31:2] for "" ""future compatibility\n"", __FUNCTION__); if (s->clksel[6] != (value & 3)) {
s->clksel[6] = value & 3; omap_prcm_dpll_update(s); } break; case 0x800: /* CM_FCLKEN_DSP */ s->clken[10] = value & 0x501; /* TODO update clocks */ break; case 0x810: /* CM_ICLKEN_DSP */ s->clken[11] = value & 0x2; /* TODO update clocks */ break; case 0x830: /* CM_AUTOIDLE_DSP */ s->clkidle[6] = value & 0x2; /* TODO update clocks */ break; case 0x840: /* CM_CLKSEL_DSP */ s->clksel[7] = value & 0x3fff; /* TODO update clocks */ break; case 0x848: /* CM_CLKSTCTRL_DSP */ s->clkctrl[3] = value & 0x101; break; case 0x850: /* RM_RSTCTRL_DSP */ /* TODO: reset */ break; case 0x858: /* RM_RSTST_DSP */ s->rst[3] &= ~value; break; case 0x8c8: /* PM_WKDEP_DSP */ s->wkup[2] = value & 0x13; break; case 0x8e0: /* PM_PWSTCTRL_DSP */ s->power[3] = (value & 0x03017) | (3 << 2); break; case 0x8f0: /* PRCM_IRQSTATUS_DSP */ s->irqst[1] &= ~value; omap_prcm_int_update(s, 1); break; case 0x8f4: /* PRCM_IRQENABLE_DSP */ s->irqen[1] = value & 0x7; omap_prcm_int_update(s, 1); break; case 0x8f8: /* PRCM_IRQSTATUS_IVA */ s->irqst[2] &= ~value; omap_prcm_int_update(s, 2); break; case 0x8fc: /* PRCM_IRQENABLE_IVA */ s->irqen[2] = value & 0x7; omap_prcm_int_update(s, 2); break; default: OMAP_BAD_REG(addr); return; } } ",qemu,0
"int index_from_key(const char *key)

{

    int i;



    for (i = 0; QKeyCode_lookup[i] != NULL; i++) {

        if (!strcmp(key, QKeyCode_lookup[i])) {

            break;

        }

    }



    /* Return Q_KEY_CODE__MAX if the key is invalid */

    return i;

}
","int index_from_key(const char *key) {
int i; for (i = 0; QKeyCode_lookup[i] != NULL; i++) {
if (!strcmp(key, QKeyCode_lookup[i])) {
break; } } /* Return Q_KEY_CODE__MAX if the key is invalid */ return i; } ",qemu,1
"static int64_t coroutine_fn iscsi_co_get_block_status(BlockDriverState *bs,

                                                  int64_t sector_num,

                                                  int nb_sectors, int *pnum,

                                                  BlockDriverState **file)

{

    IscsiLun *iscsilun = bs->opaque;

    struct scsi_get_lba_status *lbas = NULL;

    struct scsi_lba_status_descriptor *lbasd = NULL;

    struct IscsiTask iTask;

    uint64_t lba;

    int64_t ret;



    if (!is_sector_request_lun_aligned(sector_num, nb_sectors, iscsilun)) {

        ret = -EINVAL;

        goto out;

    }



    /* default to all sectors allocated */

    ret = BDRV_BLOCK_DATA;

    ret |= (sector_num << BDRV_SECTOR_BITS) | BDRV_BLOCK_OFFSET_VALID;

    *pnum = nb_sectors;



    /* LUN does not support logical block provisioning */

    if (!iscsilun->lbpme) {

        goto out;

    }



    lba = sector_qemu2lun(sector_num, iscsilun);



    iscsi_co_init_iscsitask(iscsilun, &iTask);

    qemu_mutex_lock(&iscsilun->mutex);

retry:

    if (iscsi_get_lba_status_task(iscsilun->iscsi, iscsilun->lun,

                                  lba, 8 + 16, iscsi_co_generic_cb,

                                  &iTask) == NULL) {

        ret = -ENOMEM;

        goto out_unlock;

    }



    while (!iTask.complete) {

        iscsi_set_events(iscsilun);

        qemu_mutex_unlock(&iscsilun->mutex);

        qemu_coroutine_yield();

        qemu_mutex_lock(&iscsilun->mutex);

    }



    if (iTask.do_retry) {

        if (iTask.task != NULL) {

            scsi_free_scsi_task(iTask.task);

            iTask.task = NULL;

        }

        iTask.complete = 0;

        goto retry;

    }



    if (iTask.status != SCSI_STATUS_GOOD) {

        /* in case the get_lba_status_callout fails (i.e.

         * because the device is busy or the cmd is not

         * supported) we pretend all blocks are allocated

         * for backwards compatibility */

        error_report(""iSCSI GET_LBA_STATUS failed at lba %"" PRIu64 "": %s"",

                     lba, iTask.err_str);

        goto out_unlock;

    }



    lbas = scsi_datain_unmarshall(iTask.task);

    if (lbas == NULL) {

        ret = -EIO;

        goto out_unlock;

    }



    lbasd = &lbas->descriptors[0];



    if (sector_qemu2lun(sector_num, iscsilun) != lbasd->lba) {

        ret = -EIO;

        goto out_unlock;

    }



    *pnum = sector_lun2qemu(lbasd->num_blocks, iscsilun);



    if (lbasd->provisioning == SCSI_PROVISIONING_TYPE_DEALLOCATED ||

        lbasd->provisioning == SCSI_PROVISIONING_TYPE_ANCHORED) {

        ret &= ~BDRV_BLOCK_DATA;

        if (iscsilun->lbprz) {

            ret |= BDRV_BLOCK_ZERO;

        }

    }



    if (ret & BDRV_BLOCK_ZERO) {

        iscsi_allocmap_set_unallocated(iscsilun, sector_num, *pnum);

    } else {

        iscsi_allocmap_set_allocated(iscsilun, sector_num, *pnum);

    }



    if (*pnum > nb_sectors) {

        *pnum = nb_sectors;

    }

out_unlock:

    qemu_mutex_unlock(&iscsilun->mutex);

    g_free(iTask.err_str);

out:

    if (iTask.task != NULL) {

        scsi_free_scsi_task(iTask.task);

    }

    if (ret > 0 && ret & BDRV_BLOCK_OFFSET_VALID) {

        *file = bs;

    }

    return ret;

}
","static int64_t coroutine_fn iscsi_co_get_block_status(BlockDriverState *bs, int64_t sector_num, int nb_sectors, int *pnum, BlockDriverState **file) {
IscsiLun *iscsilun = bs->opaque; struct scsi_get_lba_status *lbas = NULL; struct scsi_lba_status_descriptor *lbasd = NULL; struct IscsiTask iTask; uint64_t lba; int64_t ret; if (!is_sector_request_lun_aligned(sector_num, nb_sectors, iscsilun)) {
ret = -EINVAL; goto out; } /* default to all sectors allocated */ ret = BDRV_BLOCK_DATA; ret |= (sector_num << BDRV_SECTOR_BITS) | BDRV_BLOCK_OFFSET_VALID; *pnum = nb_sectors; /* LUN does not support logical block provisioning */ if (!iscsilun->lbpme) {
goto out; } lba = sector_qemu2lun(sector_num, iscsilun); iscsi_co_init_iscsitask(iscsilun, &iTask); qemu_mutex_lock(&iscsilun->mutex); retry: if (iscsi_get_lba_status_task(iscsilun->iscsi, iscsilun->lun, lba, 8 + 16, iscsi_co_generic_cb, &iTask) == NULL) {
ret = -ENOMEM; goto out_unlock; } while (!iTask.complete) {
iscsi_set_events(iscsilun); qemu_mutex_unlock(&iscsilun->mutex); qemu_coroutine_yield(); qemu_mutex_lock(&iscsilun->mutex); } if (iTask.do_retry) {
if (iTask.task != NULL) {
scsi_free_scsi_task(iTask.task); iTask.task = NULL; } iTask.complete = 0; goto retry; } if (iTask.status != SCSI_STATUS_GOOD) {
/* in case the get_lba_status_callout fails (i.e. * because the device is busy or the cmd is not * supported) we pretend all blocks are allocated * for backwards compatibility */ error_report(""iSCSI GET_LBA_STATUS failed at lba %"" PRIu64 "": %s"", lba, iTask.err_str); goto out_unlock; } lbas = scsi_datain_unmarshall(iTask.task); if (lbas == NULL) {
ret = -EIO; goto out_unlock; } lbasd = &lbas->descriptors[0]; if (sector_qemu2lun(sector_num, iscsilun) != lbasd->lba) {
ret = -EIO; goto out_unlock; } *pnum = sector_lun2qemu(lbasd->num_blocks, iscsilun); if (lbasd->provisioning == SCSI_PROVISIONING_TYPE_DEALLOCATED || lbasd->provisioning == SCSI_PROVISIONING_TYPE_ANCHORED) {
ret &= ~BDRV_BLOCK_DATA; if (iscsilun->lbprz) {
ret |= BDRV_BLOCK_ZERO; } } if (ret & BDRV_BLOCK_ZERO) {
iscsi_allocmap_set_unallocated(iscsilun, sector_num, *pnum); } else {
iscsi_allocmap_set_allocated(iscsilun, sector_num, *pnum); } if (*pnum > nb_sectors) {
*pnum = nb_sectors; } out_unlock: qemu_mutex_unlock(&iscsilun->mutex); g_free(iTask.err_str); out: if (iTask.task != NULL) {
scsi_free_scsi_task(iTask.task); } if (ret > 0 && ret & BDRV_BLOCK_OFFSET_VALID) {
*file = bs; } return ret; } ",qemu,1
"static int protocol_client_auth_vnc(VncState *vs, uint8_t *data, size_t len)

{

    unsigned char response[VNC_AUTH_CHALLENGE_SIZE];

    size_t i, pwlen;

    unsigned char key[8];

    time_t now = time(NULL);

    QCryptoCipher *cipher = NULL;

    Error *err = NULL;



    if (!vs->vd->password) {

        VNC_DEBUG(""No password configured on server"");

        goto reject;

    }

    if (vs->vd->expires < now) {

        VNC_DEBUG(""Password is expired"");

        goto reject;

    }



    memcpy(response, vs->challenge, VNC_AUTH_CHALLENGE_SIZE);



    /* Calculate the expected challenge response */

    pwlen = strlen(vs->vd->password);

    for (i=0; i<sizeof(key); i++)

        key[i] = i<pwlen ? vs->vd->password[i] : 0;



    cipher = qcrypto_cipher_new(

        QCRYPTO_CIPHER_ALG_DES_RFB,

        QCRYPTO_CIPHER_MODE_ECB,

        key, G_N_ELEMENTS(key),

        &err);

    if (!cipher) {

        VNC_DEBUG(""Cannot initialize cipher %s"",

                  error_get_pretty(err));

        error_free(err);

        goto reject;

    }



    if (qcrypto_cipher_encrypt(cipher,

                               vs->challenge,

                               response,

                               VNC_AUTH_CHALLENGE_SIZE,

                               &err) < 0) {

        VNC_DEBUG(""Cannot encrypt challenge %s"",

                  error_get_pretty(err));

        error_free(err);

        goto reject;

    }



    /* Compare expected vs actual challenge response */

    if (memcmp(response, data, VNC_AUTH_CHALLENGE_SIZE) != 0) {

        VNC_DEBUG(""Client challenge response did not match\n"");

        goto reject;

    } else {

        VNC_DEBUG(""Accepting VNC challenge response\n"");

        vnc_write_u32(vs, 0); /* Accept auth */

        vnc_flush(vs);



        start_client_init(vs);

    }



    qcrypto_cipher_free(cipher);

    return 0;



reject:

    vnc_write_u32(vs, 1); /* Reject auth */

    if (vs->minor >= 8) {

        static const char err[] = ""Authentication failed"";

        vnc_write_u32(vs, sizeof(err));

        vnc_write(vs, err, sizeof(err));

    }

    vnc_flush(vs);

    vnc_client_error(vs);

    qcrypto_cipher_free(cipher);

    return 0;

}
","static int protocol_client_auth_vnc(VncState *vs, uint8_t *data, size_t len) {
unsigned char response[VNC_AUTH_CHALLENGE_SIZE]; size_t i, pwlen; unsigned char key[8]; time_t now = time(NULL); QCryptoCipher *cipher = NULL; Error *err = NULL; if (!vs->vd->password) {
VNC_DEBUG(""No password configured on server""); goto reject; } if (vs->vd->expires < now) {
VNC_DEBUG(""Password is expired""); goto reject; } memcpy(response, vs->challenge, VNC_AUTH_CHALLENGE_SIZE); /* Calculate the expected challenge response */ pwlen = strlen(vs->vd->password); for (i=0; i<sizeof(key); i++) key[i] = i<pwlen ? vs->vd->password[i] : 0; cipher = qcrypto_cipher_new( QCRYPTO_CIPHER_ALG_DES_RFB, QCRYPTO_CIPHER_MODE_ECB, key, G_N_ELEMENTS(key), &err); if (!cipher) {
VNC_DEBUG(""Cannot initialize cipher %s"", error_get_pretty(err)); error_free(err); goto reject; } if (qcrypto_cipher_encrypt(cipher, vs->challenge, response, VNC_AUTH_CHALLENGE_SIZE, &err) < 0) {
VNC_DEBUG(""Cannot encrypt challenge %s"", error_get_pretty(err)); error_free(err); goto reject; } /* Compare expected vs actual challenge response */ if (memcmp(response, data, VNC_AUTH_CHALLENGE_SIZE) != 0) {
VNC_DEBUG(""Client challenge response did not match\n""); goto reject; } else {
VNC_DEBUG(""Accepting VNC challenge response\n""); vnc_write_u32(vs, 0); /* Accept auth */ vnc_flush(vs); start_client_init(vs); } qcrypto_cipher_free(cipher); return 0; reject: vnc_write_u32(vs, 1); /* Reject auth */ if (vs->minor >= 8) {
static const char err[] = ""Authentication failed""; vnc_write_u32(vs, sizeof(err)); vnc_write(vs, err, sizeof(err)); } vnc_flush(vs); vnc_client_error(vs); qcrypto_cipher_free(cipher); return 0; } ",qemu,1
"static VirtIOBlockReq *virtio_blk_get_request(VirtIOBlock *s)

{

    VirtIOBlockReq *req = virtio_blk_alloc_request(s);



    if (!virtqueue_pop(s->vq, req->elem)) {

        virtio_blk_free_request(req);

        return NULL;

    }



    return req;

}
","static VirtIOBlockReq *virtio_blk_get_request(VirtIOBlock *s) {
VirtIOBlockReq *req = virtio_blk_alloc_request(s); if (!virtqueue_pop(s->vq, req->elem)) {
virtio_blk_free_request(req); return NULL; } return req; } ",qemu,1
"static int virtio_net_handle_offloads(VirtIONet *n, uint8_t cmd,

                                     struct iovec *iov, unsigned int iov_cnt)

{

    VirtIODevice *vdev = VIRTIO_DEVICE(n);

    uint64_t offloads;

    size_t s;



    if (!((1 << VIRTIO_NET_F_CTRL_GUEST_OFFLOADS) & vdev->guest_features)) {

        return VIRTIO_NET_ERR;

    }



    s = iov_to_buf(iov, iov_cnt, 0, &offloads, sizeof(offloads));

    if (s != sizeof(offloads)) {

        return VIRTIO_NET_ERR;

    }



    if (cmd == VIRTIO_NET_CTRL_GUEST_OFFLOADS_SET) {

        uint64_t supported_offloads;



        if (!n->has_vnet_hdr) {

            return VIRTIO_NET_ERR;

        }



        supported_offloads = virtio_net_supported_guest_offloads(n);

        if (offloads & ~supported_offloads) {

            return VIRTIO_NET_ERR;

        }



        n->curr_guest_offloads = offloads;

        virtio_net_apply_guest_offloads(n);



        return VIRTIO_NET_OK;

    } else {

        return VIRTIO_NET_ERR;

    }

}
","static int virtio_net_handle_offloads(VirtIONet *n, uint8_t cmd, struct iovec *iov, unsigned int iov_cnt) {
VirtIODevice *vdev = VIRTIO_DEVICE(n); uint64_t offloads; size_t s; if (!((1 << VIRTIO_NET_F_CTRL_GUEST_OFFLOADS) & vdev->guest_features)) {
return VIRTIO_NET_ERR; } s = iov_to_buf(iov, iov_cnt, 0, &offloads, sizeof(offloads)); if (s != sizeof(offloads)) {
return VIRTIO_NET_ERR; } if (cmd == VIRTIO_NET_CTRL_GUEST_OFFLOADS_SET) {
uint64_t supported_offloads; if (!n->has_vnet_hdr) {
return VIRTIO_NET_ERR; } supported_offloads = virtio_net_supported_guest_offloads(n); if (offloads & ~supported_offloads) {
return VIRTIO_NET_ERR; } n->curr_guest_offloads = offloads; virtio_net_apply_guest_offloads(n); return VIRTIO_NET_OK; } else {
return VIRTIO_NET_ERR; } } ",qemu,0
"static inline void tcg_out_addi(TCGContext *s, int reg, tcg_target_long val)

{

    if (val != 0) {

        if (val == (val & 0xfff))

            tcg_out_arithi(s, reg, reg, val, ARITH_ADD);

        else

            fprintf(stderr, ""unimplemented addi %ld\n"", (long)val);

    }

}
","static inline void tcg_out_addi(TCGContext *s, int reg, tcg_target_long val) {
if (val != 0) {
if (val == (val & 0xfff)) tcg_out_arithi(s, reg, reg, val, ARITH_ADD); else fprintf(stderr, ""unimplemented addi %ld\n"", (long)val); } } ",qemu,0
"static void vbe_update_vgaregs(VGACommonState *s)

{

    int h, shift_control;



    if (!vbe_enabled(s)) {

        /* vbe is turned off -- nothing to do */

        return;

    }



    /* graphic mode + memory map 1 */

    s->gr[VGA_GFX_MISC] = (s->gr[VGA_GFX_MISC] & ~0x0c) | 0x04 |

        VGA_GR06_GRAPHICS_MODE;

    s->cr[VGA_CRTC_MODE] |= 3; /* no CGA modes */

    s->cr[VGA_CRTC_OFFSET] = s->vbe_line_offset >> 3;

    /* width */

    s->cr[VGA_CRTC_H_DISP] =

        (s->vbe_regs[VBE_DISPI_INDEX_XRES] >> 3) - 1;

    /* height (only meaningful if < 1024) */

    h = s->vbe_regs[VBE_DISPI_INDEX_YRES] - 1;

    s->cr[VGA_CRTC_V_DISP_END] = h;

    s->cr[VGA_CRTC_OVERFLOW] = (s->cr[VGA_CRTC_OVERFLOW] & ~0x42) |

        ((h >> 7) & 0x02) | ((h >> 3) & 0x40);

    /* line compare to 1023 */

    s->cr[VGA_CRTC_LINE_COMPARE] = 0xff;

    s->cr[VGA_CRTC_OVERFLOW] |= 0x10;

    s->cr[VGA_CRTC_MAX_SCAN] |= 0x40;



    if (s->vbe_regs[VBE_DISPI_INDEX_BPP] == 4) {

        shift_control = 0;

        s->sr[VGA_SEQ_CLOCK_MODE] &= ~8; /* no double line */

    } else {

        shift_control = 2;

        /* set chain 4 mode */

        s->sr[VGA_SEQ_MEMORY_MODE] |= VGA_SR04_CHN_4M;

        /* activate all planes */

        s->sr[VGA_SEQ_PLANE_WRITE] |= VGA_SR02_ALL_PLANES;

    }

    s->gr[VGA_GFX_MODE] = (s->gr[VGA_GFX_MODE] & ~0x60) |

        (shift_control << 5);

    s->cr[VGA_CRTC_MAX_SCAN] &= ~0x9f; /* no double scan */

}
","static void vbe_update_vgaregs(VGACommonState *s) {
int h, shift_control; if (!vbe_enabled(s)) {
/* vbe is turned off -- nothing to do */ return; } /* graphic mode + memory map 1 */ s->gr[VGA_GFX_MISC] = (s->gr[VGA_GFX_MISC] & ~0x0c) | 0x04 | VGA_GR06_GRAPHICS_MODE; s->cr[VGA_CRTC_MODE] |= 3; /* no CGA modes */ s->cr[VGA_CRTC_OFFSET] = s->vbe_line_offset >> 3; /* width */ s->cr[VGA_CRTC_H_DISP] = (s->vbe_regs[VBE_DISPI_INDEX_XRES] >> 3) - 1; /* height (only meaningful if < 1024) */ h = s->vbe_regs[VBE_DISPI_INDEX_YRES] - 1; s->cr[VGA_CRTC_V_DISP_END] = h; s->cr[VGA_CRTC_OVERFLOW] = (s->cr[VGA_CRTC_OVERFLOW] & ~0x42) | ((h >> 7) & 0x02) | ((h >> 3) & 0x40); /* line compare to 1023 */ s->cr[VGA_CRTC_LINE_COMPARE] = 0xff; s->cr[VGA_CRTC_OVERFLOW] |= 0x10; s->cr[VGA_CRTC_MAX_SCAN] |= 0x40; if (s->vbe_regs[VBE_DISPI_INDEX_BPP] == 4) {
shift_control = 0; s->sr[VGA_SEQ_CLOCK_MODE] &= ~8; /* no double line */ } else {
shift_control = 2; /* set chain 4 mode */ s->sr[VGA_SEQ_MEMORY_MODE] |= VGA_SR04_CHN_4M; /* activate all planes */ s->sr[VGA_SEQ_PLANE_WRITE] |= VGA_SR02_ALL_PLANES; } s->gr[VGA_GFX_MODE] = (s->gr[VGA_GFX_MODE] & ~0x60) | (shift_control << 5); s->cr[VGA_CRTC_MAX_SCAN] &= ~0x9f; /* no double scan */ } ",qemu,1
"void OPPROTO op_idivw_AX_T0(void)

{

    int num, den, q, r;



    num = (EAX & 0xffff) | ((EDX & 0xffff) << 16);

    den = (int16_t)T0;

    if (den == 0) {

        raise_exception(EXCP00_DIVZ);

    }

    q = (num / den) & 0xffff;

    r = (num % den) & 0xffff;

    EAX = (EAX & ~0xffff) | q;

    EDX = (EDX & ~0xffff) | r;

}
","void OPPROTO op_idivw_AX_T0(void) {
int num, den, q, r; num = (EAX & 0xffff) | ((EDX & 0xffff) << 16); den = (int16_t)T0; if (den == 0) {
raise_exception(EXCP00_DIVZ); } q = (num / den) & 0xffff; r = (num % den) & 0xffff; EAX = (EAX & ~0xffff) | q; EDX = (EDX & ~0xffff) | r; } ",qemu,1
"void dp83932_init(NICInfo *nd, target_phys_addr_t base, int it_shift,

                  qemu_irq irq, void* mem_opaque,

                  void (*memory_rw)(void *opaque, target_phys_addr_t addr, uint8_t *buf, int len, int is_write))

{

    dp8393xState *s;

    int io;



    qemu_check_nic_model(nd, ""dp83932"");



    s = qemu_mallocz(sizeof(dp8393xState));



    s->mem_opaque = mem_opaque;

    s->memory_rw = memory_rw;

    s->it_shift = it_shift;

    s->irq = irq;

    s->watchdog = qemu_new_timer(vm_clock, dp8393x_watchdog, s);

    s->regs[SONIC_SR] = 0x0004; /* only revision recognized by Linux */



    s->vc = qemu_new_vlan_client(nd->vlan, nd->model, nd->name,

                                 nic_receive, nic_can_receive, s);



    qemu_format_nic_info_str(s->vc, nd->macaddr);

    qemu_register_reset(nic_reset, s);

    nic_reset(s);



    io = cpu_register_io_memory(0, dp8393x_read, dp8393x_write, s);

    cpu_register_physical_memory(base, 0x40 << it_shift, io);

}
","void dp83932_init(NICInfo *nd, target_phys_addr_t base, int it_shift, qemu_irq irq, void* mem_opaque, void (*memory_rw)(void *opaque, target_phys_addr_t addr, uint8_t *buf, int len, int is_write)) {
dp8393xState *s; int io; qemu_check_nic_model(nd, ""dp83932""); s = qemu_mallocz(sizeof(dp8393xState)); s->mem_opaque = mem_opaque; s->memory_rw = memory_rw; s->it_shift = it_shift; s->irq = irq; s->watchdog = qemu_new_timer(vm_clock, dp8393x_watchdog, s); s->regs[SONIC_SR] = 0x0004; /* only revision recognized by Linux */ s->vc = qemu_new_vlan_client(nd->vlan, nd->model, nd->name, nic_receive, nic_can_receive, s); qemu_format_nic_info_str(s->vc, nd->macaddr); qemu_register_reset(nic_reset, s); nic_reset(s); io = cpu_register_io_memory(0, dp8393x_read, dp8393x_write, s); cpu_register_physical_memory(base, 0x40 << it_shift, io); } ",qemu,1
"static void lm32_uclinux_init(MachineState *machine)

{

    const char *cpu_model = machine->cpu_model;

    const char *kernel_filename = machine->kernel_filename;

    const char *kernel_cmdline = machine->kernel_cmdline;

    const char *initrd_filename = machine->initrd_filename;

    LM32CPU *cpu;

    CPULM32State *env;

    DriveInfo *dinfo;

    MemoryRegion *address_space_mem =  get_system_memory();

    MemoryRegion *phys_ram = g_new(MemoryRegion, 1);

    qemu_irq *cpu_irq, irq[32];

    HWSetup *hw;

    ResetInfo *reset_info;

    int i;



    /* memory map */

    hwaddr flash_base   = 0x04000000;

    size_t flash_sector_size        = 256 * 1024;

    size_t flash_size               = 32 * 1024 * 1024;

    hwaddr ram_base     = 0x08000000;

    size_t ram_size                 = 64 * 1024 * 1024;

    hwaddr uart0_base   = 0x80000000;

    hwaddr timer0_base  = 0x80002000;

    hwaddr timer1_base  = 0x80010000;

    hwaddr timer2_base  = 0x80012000;

    int uart0_irq                   = 0;

    int timer0_irq                  = 1;

    int timer1_irq                  = 20;

    int timer2_irq                  = 21;

    hwaddr hwsetup_base = 0x0bffe000;

    hwaddr cmdline_base = 0x0bfff000;

    hwaddr initrd_base  = 0x08400000;

    size_t initrd_max               = 0x01000000;



    reset_info = g_malloc0(sizeof(ResetInfo));



    if (cpu_model == NULL) {

        cpu_model = ""lm32-full"";

    }

    cpu = cpu_lm32_init(cpu_model);

    if (cpu == NULL) {

        fprintf(stderr, ""qemu: unable to find CPU '%s'\n"", cpu_model);

        exit(1);

    }



    env = &cpu->env;

    reset_info->cpu = cpu;



    reset_info->flash_base = flash_base;



    memory_region_init_ram(phys_ram, NULL, ""lm32_uclinux.sdram"", ram_size,

                           &error_abort);

    vmstate_register_ram_global(phys_ram);

    memory_region_add_subregion(address_space_mem, ram_base, phys_ram);



    dinfo = drive_get(IF_PFLASH, 0, 0);

    /* Spansion S29NS128P */

    pflash_cfi02_register(flash_base, NULL, ""lm32_uclinux.flash"", flash_size,

                          dinfo ? blk_bs(blk_by_legacy_dinfo(dinfo)) : NULL,

                          flash_sector_size, flash_size / flash_sector_size,

                          1, 2, 0x01, 0x7e, 0x43, 0x00, 0x555, 0x2aa, 1);



    /* create irq lines */

    cpu_irq = qemu_allocate_irqs(cpu_irq_handler, env, 1);

    env->pic_state = lm32_pic_init(*cpu_irq);

    for (i = 0; i < 32; i++) {

        irq[i] = qdev_get_gpio_in(env->pic_state, i);

    }



    sysbus_create_simple(""lm32-uart"", uart0_base, irq[uart0_irq]);

    sysbus_create_simple(""lm32-timer"", timer0_base, irq[timer0_irq]);

    sysbus_create_simple(""lm32-timer"", timer1_base, irq[timer1_irq]);

    sysbus_create_simple(""lm32-timer"", timer2_base, irq[timer2_irq]);



    /* make sure juart isn't the first chardev */

    env->juart_state = lm32_juart_init();



    reset_info->bootstrap_pc = flash_base;



    if (kernel_filename) {

        uint64_t entry;

        int kernel_size;



        kernel_size = load_elf(kernel_filename, NULL, NULL, &entry, NULL, NULL,

                               1, ELF_MACHINE, 0);

        reset_info->bootstrap_pc = entry;



        if (kernel_size < 0) {

            kernel_size = load_image_targphys(kernel_filename, ram_base,

                                              ram_size);

            reset_info->bootstrap_pc = ram_base;

        }



        if (kernel_size < 0) {

            fprintf(stderr, ""qemu: could not load kernel '%s'\n"",

                    kernel_filename);

            exit(1);

        }

    }



    /* generate a rom with the hardware description */

    hw = hwsetup_init();

    hwsetup_add_cpu(hw, ""LM32"", 75000000);

    hwsetup_add_flash(hw, ""flash"", flash_base, flash_size);

    hwsetup_add_ddr_sdram(hw, ""ddr_sdram"", ram_base, ram_size);

    hwsetup_add_timer(hw, ""timer0"", timer0_base, timer0_irq);

    hwsetup_add_timer(hw, ""timer1_dev_only"", timer1_base, timer1_irq);

    hwsetup_add_timer(hw, ""timer2_dev_only"", timer2_base, timer2_irq);

    hwsetup_add_uart(hw, ""uart"", uart0_base, uart0_irq);

    hwsetup_add_trailer(hw);

    hwsetup_create_rom(hw, hwsetup_base);

    hwsetup_free(hw);



    reset_info->hwsetup_base = hwsetup_base;



    if (kernel_cmdline && strlen(kernel_cmdline)) {

        pstrcpy_targphys(""cmdline"", cmdline_base, TARGET_PAGE_SIZE,

                kernel_cmdline);

        reset_info->cmdline_base = cmdline_base;

    }



    if (initrd_filename) {

        size_t initrd_size;

        initrd_size = load_image_targphys(initrd_filename, initrd_base,

                initrd_max);

        reset_info->initrd_base = initrd_base;

        reset_info->initrd_size = initrd_size;

    }



    qemu_register_reset(main_cpu_reset, reset_info);

}
","static void lm32_uclinux_init(MachineState *machine) {
const char *cpu_model = machine->cpu_model; const char *kernel_filename = machine->kernel_filename; const char *kernel_cmdline = machine->kernel_cmdline; const char *initrd_filename = machine->initrd_filename; LM32CPU *cpu; CPULM32State *env; DriveInfo *dinfo; MemoryRegion *address_space_mem = get_system_memory(); MemoryRegion *phys_ram = g_new(MemoryRegion, 1); qemu_irq *cpu_irq, irq[32]; HWSetup *hw; ResetInfo *reset_info; int i; /* memory map */ hwaddr flash_base = 0x04000000; size_t flash_sector_size = 256 * 1024; size_t flash_size = 32 * 1024 * 1024; hwaddr ram_base = 0x08000000; size_t ram_size = 64 * 1024 * 1024; hwaddr uart0_base = 0x80000000; hwaddr timer0_base = 0x80002000; hwaddr timer1_base = 0x80010000; hwaddr timer2_base = 0x80012000; int uart0_irq = 0; int timer0_irq = 1; int timer1_irq = 20; int timer2_irq = 21; hwaddr hwsetup_base = 0x0bffe000; hwaddr cmdline_base = 0x0bfff000; hwaddr initrd_base = 0x08400000; size_t initrd_max = 0x01000000; reset_info = g_malloc0(sizeof(ResetInfo)); if (cpu_model == NULL) {
cpu_model = ""lm32-full""; } cpu = cpu_lm32_init(cpu_model); if (cpu == NULL) {
fprintf(stderr, ""qemu: unable to find CPU '%s'\n"", cpu_model); exit(1); } env = &cpu->env; reset_info->cpu = cpu; reset_info->flash_base = flash_base; memory_region_init_ram(phys_ram, NULL, ""lm32_uclinux.sdram"", ram_size, &error_abort); vmstate_register_ram_global(phys_ram); memory_region_add_subregion(address_space_mem, ram_base, phys_ram); dinfo = drive_get(IF_PFLASH, 0, 0); /* Spansion S29NS128P */ pflash_cfi02_register(flash_base, NULL, ""lm32_uclinux.flash"", flash_size, dinfo ? blk_bs(blk_by_legacy_dinfo(dinfo)) : NULL, flash_sector_size, flash_size / flash_sector_size, 1, 2, 0x01, 0x7e, 0x43, 0x00, 0x555, 0x2aa, 1); /* create irq lines */ cpu_irq = qemu_allocate_irqs(cpu_irq_handler, env, 1); env->pic_state = lm32_pic_init(*cpu_irq); for (i = 0; i < 32; i++) {
irq[i] = qdev_get_gpio_in(env->pic_state, i); } sysbus_create_simple(""lm32-uart"", uart0_base, irq[uart0_irq]); sysbus_create_simple(""lm32-timer"", timer0_base, irq[timer0_irq]); sysbus_create_simple(""lm32-timer"", timer1_base, irq[timer1_irq]); sysbus_create_simple(""lm32-timer"", timer2_base, irq[timer2_irq]); /* make sure juart isn't the first chardev */ env->juart_state = lm32_juart_init(); reset_info->bootstrap_pc = flash_base; if (kernel_filename) {
uint64_t entry; int kernel_size; kernel_size = load_elf(kernel_filename, NULL, NULL, &entry, NULL, NULL, 1, ELF_MACHINE, 0); reset_info->bootstrap_pc = entry; if (kernel_size < 0) {
kernel_size = load_image_targphys(kernel_filename, ram_base, ram_size); reset_info->bootstrap_pc = ram_base; } if (kernel_size < 0) {
fprintf(stderr, ""qemu: could not load kernel '%s'\n"", kernel_filename); exit(1); } } /* generate a rom with the hardware description */ hw = hwsetup_init(); hwsetup_add_cpu(hw, ""LM32"", 75000000); hwsetup_add_flash(hw, ""flash"", flash_base, flash_size); hwsetup_add_ddr_sdram(hw, ""ddr_sdram"", ram_base, ram_size); hwsetup_add_timer(hw, ""timer0"", timer0_base, timer0_irq); hwsetup_add_timer(hw, ""timer1_dev_only"", timer1_base, timer1_irq); hwsetup_add_timer(hw, ""timer2_dev_only"", timer2_base, timer2_irq); hwsetup_add_uart(hw, ""uart"", uart0_base, uart0_irq); hwsetup_add_trailer(hw); hwsetup_create_rom(hw, hwsetup_base); hwsetup_free(hw); reset_info->hwsetup_base = hwsetup_base; if (kernel_cmdline && strlen(kernel_cmdline)) {
pstrcpy_targphys(""cmdline"", cmdline_base, TARGET_PAGE_SIZE, kernel_cmdline); reset_info->cmdline_base = cmdline_base; } if (initrd_filename) {
size_t initrd_size; initrd_size = load_image_targphys(initrd_filename, initrd_base, initrd_max); reset_info->initrd_base = initrd_base; reset_info->initrd_size = initrd_size; } qemu_register_reset(main_cpu_reset, reset_info); } ",qemu,0
"static void smbios_build_type_1_fields(const char *t)

{

    char buf[1024];



    if (get_param_value(buf, sizeof(buf), ""manufacturer"", t))

        smbios_add_field(1, offsetof(struct smbios_type_1, manufacturer_str),

                         strlen(buf) + 1, buf);

    if (get_param_value(buf, sizeof(buf), ""product"", t))

        smbios_add_field(1, offsetof(struct smbios_type_1, product_name_str),

                         strlen(buf) + 1, buf);

    if (get_param_value(buf, sizeof(buf), ""version"", t))

        smbios_add_field(1, offsetof(struct smbios_type_1, version_str),

                         strlen(buf) + 1, buf);

    if (get_param_value(buf, sizeof(buf), ""serial"", t))

        smbios_add_field(1, offsetof(struct smbios_type_1, serial_number_str),

                         strlen(buf) + 1, buf);

    if (get_param_value(buf, sizeof(buf), ""uuid"", t)) {

        if (qemu_uuid_parse(buf, qemu_uuid) != 0) {

            fprintf(stderr, ""Invalid SMBIOS UUID string\n"");

            exit(1);

        }

    }

    if (get_param_value(buf, sizeof(buf), ""sku"", t))

        smbios_add_field(1, offsetof(struct smbios_type_1, sku_number_str),

                         strlen(buf) + 1, buf);

    if (get_param_value(buf, sizeof(buf), ""family"", t))

        smbios_add_field(1, offsetof(struct smbios_type_1, family_str),

                         strlen(buf) + 1, buf);

}
","static void smbios_build_type_1_fields(const char *t) {
char buf[1024]; if (get_param_value(buf, sizeof(buf), ""manufacturer"", t)) smbios_add_field(1, offsetof(struct smbios_type_1, manufacturer_str), strlen(buf) + 1, buf); if (get_param_value(buf, sizeof(buf), ""product"", t)) smbios_add_field(1, offsetof(struct smbios_type_1, product_name_str), strlen(buf) + 1, buf); if (get_param_value(buf, sizeof(buf), ""version"", t)) smbios_add_field(1, offsetof(struct smbios_type_1, version_str), strlen(buf) + 1, buf); if (get_param_value(buf, sizeof(buf), ""serial"", t)) smbios_add_field(1, offsetof(struct smbios_type_1, serial_number_str), strlen(buf) + 1, buf); if (get_param_value(buf, sizeof(buf), ""uuid"", t)) {
if (qemu_uuid_parse(buf, qemu_uuid) != 0) {
fprintf(stderr, ""Invalid SMBIOS UUID string\n""); exit(1); } } if (get_param_value(buf, sizeof(buf), ""sku"", t)) smbios_add_field(1, offsetof(struct smbios_type_1, sku_number_str), strlen(buf) + 1, buf); if (get_param_value(buf, sizeof(buf), ""family"", t)) smbios_add_field(1, offsetof(struct smbios_type_1, family_str), strlen(buf) + 1, buf); } ",qemu,0
"static void qemu_chr_parse_spice_vmc(QemuOpts *opts, ChardevBackend *backend,

                                     Error **errp)

{

    const char *name = qemu_opt_get(opts, ""name"");



    if (name == NULL) {

        error_setg(errp, ""chardev: spice channel: no name given"");

        return;

    }

    backend->u.spicevmc = g_new0(ChardevSpiceChannel, 1);

    backend->u.spicevmc->type = g_strdup(name);

}
","static void qemu_chr_parse_spice_vmc(QemuOpts *opts, ChardevBackend *backend, Error **errp) {
const char *name = qemu_opt_get(opts, ""name""); if (name == NULL) {
error_setg(errp, ""chardev: spice channel: no name given""); return; } backend->u.spicevmc = g_new0(ChardevSpiceChannel, 1); backend->u.spicevmc->type = g_strdup(name); } ",qemu,0
"void qemu_system_powerdown(void)

{

    if(pm_state->pmen & PWRBTN_EN) {

        pm_state->pmsts |= PWRBTN_EN;

	pm_update_sci(pm_state);

    }

}
","void qemu_system_powerdown(void) {
if(pm_state->pmen & PWRBTN_EN) {
pm_state->pmsts |= PWRBTN_EN; pm_update_sci(pm_state); } } ",qemu,1
"static inline void gen_evsel(DisasContext *ctx)

{

    int l1 = gen_new_label();

    int l2 = gen_new_label();

    int l3 = gen_new_label();

    int l4 = gen_new_label();

    TCGv_i32 t0 = tcg_temp_local_new_i32();

    tcg_gen_andi_i32(t0, cpu_crf[ctx->opcode & 0x07], 1 << 3);

    tcg_gen_brcondi_i32(TCG_COND_EQ, t0, 0, l1);

    tcg_gen_mov_tl(cpu_gprh[rD(ctx->opcode)], cpu_gprh[rA(ctx->opcode)]);

    tcg_gen_br(l2);

    gen_set_label(l1);

    tcg_gen_mov_tl(cpu_gprh[rD(ctx->opcode)], cpu_gprh[rB(ctx->opcode)]);

    gen_set_label(l2);

    tcg_gen_andi_i32(t0, cpu_crf[ctx->opcode & 0x07], 1 << 2);

    tcg_gen_brcondi_i32(TCG_COND_EQ, t0, 0, l3);

    tcg_gen_mov_tl(cpu_gpr[rD(ctx->opcode)], cpu_gpr[rA(ctx->opcode)]);

    tcg_gen_br(l4);

    gen_set_label(l3);

    tcg_gen_mov_tl(cpu_gpr[rD(ctx->opcode)], cpu_gpr[rB(ctx->opcode)]);

    gen_set_label(l4);

    tcg_temp_free_i32(t0);

}
","static inline void gen_evsel(DisasContext *ctx) {
int l1 = gen_new_label(); int l2 = gen_new_label(); int l3 = gen_new_label(); int l4 = gen_new_label(); TCGv_i32 t0 = tcg_temp_local_new_i32(); tcg_gen_andi_i32(t0, cpu_crf[ctx->opcode & 0x07], 1 << 3); tcg_gen_brcondi_i32(TCG_COND_EQ, t0, 0, l1); tcg_gen_mov_tl(cpu_gprh[rD(ctx->opcode)], cpu_gprh[rA(ctx->opcode)]); tcg_gen_br(l2); gen_set_label(l1); tcg_gen_mov_tl(cpu_gprh[rD(ctx->opcode)], cpu_gprh[rB(ctx->opcode)]); gen_set_label(l2); tcg_gen_andi_i32(t0, cpu_crf[ctx->opcode & 0x07], 1 << 2); tcg_gen_brcondi_i32(TCG_COND_EQ, t0, 0, l3); tcg_gen_mov_tl(cpu_gpr[rD(ctx->opcode)], cpu_gpr[rA(ctx->opcode)]); tcg_gen_br(l4); gen_set_label(l3); tcg_gen_mov_tl(cpu_gpr[rD(ctx->opcode)], cpu_gpr[rB(ctx->opcode)]); gen_set_label(l4); tcg_temp_free_i32(t0); } ",qemu,0
"static void uhci_queue_free(UHCIQueue *queue)

{

    UHCIState *s = queue->uhci;

    UHCIAsync *async;



    while (!QTAILQ_EMPTY(&queue->asyncs)) {

        async = QTAILQ_FIRST(&queue->asyncs);

        uhci_async_cancel(async);

    }



    trace_usb_uhci_queue_del(queue->token);

    QTAILQ_REMOVE(&s->queues, queue, next);

    g_free(queue);

}
","static void uhci_queue_free(UHCIQueue *queue) {
UHCIState *s = queue->uhci; UHCIAsync *async; while (!QTAILQ_EMPTY(&queue->asyncs)) {
async = QTAILQ_FIRST(&queue->asyncs); uhci_async_cancel(async); } trace_usb_uhci_queue_del(queue->token); QTAILQ_REMOVE(&s->queues, queue, next); g_free(queue); } ",qemu,1
"static void spapr_add_lmbs(DeviceState *dev, uint64_t addr_start, uint64_t size,

                           uint32_t node, bool dedicated_hp_event_source,

                           Error **errp)

{

    sPAPRDRConnector *drc;

    sPAPRDRConnectorClass *drck;

    uint32_t nr_lmbs = size/SPAPR_MEMORY_BLOCK_SIZE;

    int i, fdt_offset, fdt_size;

    void *fdt;

    uint64_t addr = addr_start;



    for (i = 0; i < nr_lmbs; i++) {

        drc = spapr_dr_connector_by_id(SPAPR_DR_CONNECTOR_TYPE_LMB,

                addr/SPAPR_MEMORY_BLOCK_SIZE);

        g_assert(drc);



        fdt = create_device_tree(&fdt_size);

        fdt_offset = spapr_populate_memory_node(fdt, node, addr,

                                                SPAPR_MEMORY_BLOCK_SIZE);



        drck = SPAPR_DR_CONNECTOR_GET_CLASS(drc);

        drck->attach(drc, dev, fdt, fdt_offset, !dev->hotplugged, errp);

        addr += SPAPR_MEMORY_BLOCK_SIZE;







    /* send hotplug notification to the

     * guest only in case of hotplugged memory

     */

    if (dev->hotplugged) {

        if (dedicated_hp_event_source) {

            drc = spapr_dr_connector_by_id(SPAPR_DR_CONNECTOR_TYPE_LMB,

                    addr_start / SPAPR_MEMORY_BLOCK_SIZE);

            drck = SPAPR_DR_CONNECTOR_GET_CLASS(drc);

            spapr_hotplug_req_add_by_count_indexed(SPAPR_DR_CONNECTOR_TYPE_LMB,

                                                   nr_lmbs,

                                                   drck->get_index(drc));

        } else {

            spapr_hotplug_req_add_by_count(SPAPR_DR_CONNECTOR_TYPE_LMB,

                                           nr_lmbs);


","static void spapr_add_lmbs(DeviceState *dev, uint64_t addr_start, uint64_t size, uint32_t node, bool dedicated_hp_event_source, Error **errp) {
sPAPRDRConnector *drc; sPAPRDRConnectorClass *drck; uint32_t nr_lmbs = size/SPAPR_MEMORY_BLOCK_SIZE; int i, fdt_offset, fdt_size; void *fdt; uint64_t addr = addr_start; for (i = 0; i < nr_lmbs; i++) {
drc = spapr_dr_connector_by_id(SPAPR_DR_CONNECTOR_TYPE_LMB, addr/SPAPR_MEMORY_BLOCK_SIZE); g_assert(drc); fdt = create_device_tree(&fdt_size); fdt_offset = spapr_populate_memory_node(fdt, node, addr, SPAPR_MEMORY_BLOCK_SIZE); drck = SPAPR_DR_CONNECTOR_GET_CLASS(drc); drck->attach(drc, dev, fdt, fdt_offset, !dev->hotplugged, errp); addr += SPAPR_MEMORY_BLOCK_SIZE; /* send hotplug notification to the * guest only in case of hotplugged memory */ if (dev->hotplugged) {
if (dedicated_hp_event_source) {
drc = spapr_dr_connector_by_id(SPAPR_DR_CONNECTOR_TYPE_LMB, addr_start / SPAPR_MEMORY_BLOCK_SIZE); drck = SPAPR_DR_CONNECTOR_GET_CLASS(drc); spapr_hotplug_req_add_by_count_indexed(SPAPR_DR_CONNECTOR_TYPE_LMB, nr_lmbs, drck->get_index(drc)); } else {
spapr_hotplug_req_add_by_count(SPAPR_DR_CONNECTOR_TYPE_LMB, nr_lmbs); ",qemu,1
"static void print_net_client(Monitor *mon, VLANClientState *vc)

{

    monitor_printf(mon, ""%s: type=%s,%s\n"", vc->name,

                   net_client_types[vc->info->type].type, vc->info_str);

}
","static void print_net_client(Monitor *mon, VLANClientState *vc) {
monitor_printf(mon, ""%s: type=%s,%s\n"", vc->name, net_client_types[vc->info->type].type, vc->info_str); } ",qemu,1
"static PCIINTxRoute gpex_route_intx_pin_to_irq(void *opaque, int pin)

{

    PCIINTxRoute route;

    GPEXHost *s = opaque;



    route.mode = PCI_INTX_ENABLED;

    route.irq = s->irq_num[pin];



    return route;

}
","static PCIINTxRoute gpex_route_intx_pin_to_irq(void *opaque, int pin) {
PCIINTxRoute route; GPEXHost *s = opaque; route.mode = PCI_INTX_ENABLED; route.irq = s->irq_num[pin]; return route; } ",qemu,0
"uint64_t ldq_be_phys(target_phys_addr_t addr)

{

    return ldq_phys_internal(addr, DEVICE_BIG_ENDIAN);

}
","uint64_t ldq_be_phys(target_phys_addr_t addr) {
return ldq_phys_internal(addr, DEVICE_BIG_ENDIAN); } ",qemu,0
"void acpi_pm1_cnt_init(ACPIREGS *ar, MemoryRegion *parent, uint8_t s4_val)

{

    ar->pm1.cnt.s4_val = s4_val;

    ar->wakeup.notify = acpi_notify_wakeup;

    qemu_register_wakeup_notifier(&ar->wakeup);

    memory_region_init_io(&ar->pm1.cnt.io, memory_region_owner(parent),

                          &acpi_pm_cnt_ops, ar, ""acpi-cnt"", 2);

    memory_region_add_subregion(parent, 4, &ar->pm1.cnt.io);

}
","void acpi_pm1_cnt_init(ACPIREGS *ar, MemoryRegion *parent, uint8_t s4_val) {
ar->pm1.cnt.s4_val = s4_val; ar->wakeup.notify = acpi_notify_wakeup; qemu_register_wakeup_notifier(&ar->wakeup); memory_region_init_io(&ar->pm1.cnt.io, memory_region_owner(parent), &acpi_pm_cnt_ops, ar, ""acpi-cnt"", 2); memory_region_add_subregion(parent, 4, &ar->pm1.cnt.io); } ",qemu,0
"static void handle_hmp_command(Monitor *mon, const char *cmdline)

{

    QDict *qdict;

    const mon_cmd_t *cmd;



    qdict = qdict_new();



    cmd = monitor_parse_command(mon, cmdline, 0, mon->cmd_table, qdict);

    if (cmd) {

        cmd->mhandler.cmd(mon, qdict);

    }



    QDECREF(qdict);

}
","static void handle_hmp_command(Monitor *mon, const char *cmdline) {
QDict *qdict; const mon_cmd_t *cmd; qdict = qdict_new(); cmd = monitor_parse_command(mon, cmdline, 0, mon->cmd_table, qdict); if (cmd) {
cmd->mhandler.cmd(mon, qdict); } QDECREF(qdict); } ",qemu,0
"static void virtio_balloon_save(QEMUFile *f, void *opaque)

{

    VirtIOBalloon *s = opaque;



    virtio_save(&s->vdev, f);



    qemu_put_be32(f, s->num_pages);

    qemu_put_be32(f, s->actual);

    qemu_put_buffer(f, (uint8_t *)&s->stats_vq_elem, sizeof(VirtQueueElement));

    qemu_put_buffer(f, (uint8_t *)&s->stats_vq_offset, sizeof(size_t));

    qemu_put_buffer(f, (uint8_t *)&s->stats_callback, sizeof(MonitorCompletion));

    qemu_put_buffer(f, (uint8_t *)&s->stats_opaque_callback_data, sizeof(void));

}
","static void virtio_balloon_save(QEMUFile *f, void *opaque) {
VirtIOBalloon *s = opaque; virtio_save(&s->vdev, f); qemu_put_be32(f, s->num_pages); qemu_put_be32(f, s->actual); qemu_put_buffer(f, (uint8_t *)&s->stats_vq_elem, sizeof(VirtQueueElement)); qemu_put_buffer(f, (uint8_t *)&s->stats_vq_offset, sizeof(size_t)); qemu_put_buffer(f, (uint8_t *)&s->stats_callback, sizeof(MonitorCompletion)); qemu_put_buffer(f, (uint8_t *)&s->stats_opaque_callback_data, sizeof(void)); } ",qemu,1
"static void acpi_build_update(void *build_opaque, uint32_t offset)

{

    AcpiBuildState *build_state = build_opaque;

    AcpiBuildTables tables;



    /* No state to update or already patched? Nothing to do. */

    if (!build_state || build_state->patched) {

        return;

    }

    build_state->patched = 1;



    acpi_build_tables_init(&tables);



    acpi_build(build_state->guest_info, &tables);



    assert(acpi_data_len(tables.table_data) == build_state->table_size);



    /* Make sure RAM size is correct - in case it got changed by migration */

    qemu_ram_resize(build_state->table_ram, build_state->table_size,

                    &error_abort);



    memcpy(qemu_get_ram_ptr(build_state->table_ram), tables.table_data->data,

           build_state->table_size);

    memcpy(build_state->rsdp, tables.rsdp->data, acpi_data_len(tables.rsdp));

    memcpy(qemu_get_ram_ptr(build_state->linker_ram), tables.linker->data,

           build_state->linker_size);



    cpu_physical_memory_set_dirty_range_nocode(build_state->table_ram,

                                               build_state->table_size);



    acpi_build_tables_cleanup(&tables, true);

}
","static void acpi_build_update(void *build_opaque, uint32_t offset) {
AcpiBuildState *build_state = build_opaque; AcpiBuildTables tables; /* No state to update or already patched? Nothing to do. */ if (!build_state || build_state->patched) {
return; } build_state->patched = 1; acpi_build_tables_init(&tables); acpi_build(build_state->guest_info, &tables); assert(acpi_data_len(tables.table_data) == build_state->table_size); /* Make sure RAM size is correct - in case it got changed by migration */ qemu_ram_resize(build_state->table_ram, build_state->table_size, &error_abort); memcpy(qemu_get_ram_ptr(build_state->table_ram), tables.table_data->data, build_state->table_size); memcpy(build_state->rsdp, tables.rsdp->data, acpi_data_len(tables.rsdp)); memcpy(qemu_get_ram_ptr(build_state->linker_ram), tables.linker->data, build_state->linker_size); cpu_physical_memory_set_dirty_range_nocode(build_state->table_ram, build_state->table_size); acpi_build_tables_cleanup(&tables, true); } ",qemu,0
"static void tcg_out_qemu_st(TCGContext *s, const TCGArg *args, bool is_64)

{

    TCGReg datalo, datahi, addrlo, rbase;

    TCGReg addrhi __attribute__((unused));

    TCGMemOpIdx oi;

    TCGMemOp opc, s_bits;

#ifdef CONFIG_SOFTMMU

    int mem_index;

    tcg_insn_unit *label_ptr;

#endif



    datalo = *args++;

    datahi = (TCG_TARGET_REG_BITS == 32 && is_64 ? *args++ : 0);

    addrlo = *args++;

    addrhi = (TCG_TARGET_REG_BITS < TARGET_LONG_BITS ? *args++ : 0);

    oi = *args++;

    opc = get_memop(oi);

    s_bits = opc & MO_SIZE;



#ifdef CONFIG_SOFTMMU

    mem_index = get_mmuidx(oi);

    addrlo = tcg_out_tlb_read(s, s_bits, addrlo, addrhi, mem_index, false);



    /* Load a pointer into the current opcode w/conditional branch-link. */

    label_ptr = s->code_ptr;

    tcg_out_bc_noaddr(s, BC | BI(7, CR_EQ) | BO_COND_FALSE | LK);



    rbase = TCG_REG_R3;

#else  /* !CONFIG_SOFTMMU */

    rbase = GUEST_BASE ? TCG_GUEST_BASE_REG : 0;

    if (TCG_TARGET_REG_BITS > TARGET_LONG_BITS) {

        tcg_out_ext32u(s, TCG_REG_TMP1, addrlo);

        addrlo = TCG_REG_TMP1;

    }

#endif



    if (TCG_TARGET_REG_BITS == 32 && s_bits == MO_64) {

        if (opc & MO_BSWAP) {

            tcg_out32(s, ADDI | TAI(TCG_REG_R0, addrlo, 4));

            tcg_out32(s, STWBRX | SAB(datalo, rbase, addrlo));

            tcg_out32(s, STWBRX | SAB(datahi, rbase, TCG_REG_R0));

        } else if (rbase != 0) {

            tcg_out32(s, ADDI | TAI(TCG_REG_R0, addrlo, 4));

            tcg_out32(s, STWX | SAB(datahi, rbase, addrlo));

            tcg_out32(s, STWX | SAB(datalo, rbase, TCG_REG_R0));

        } else {

            tcg_out32(s, STW | TAI(datahi, addrlo, 0));

            tcg_out32(s, STW | TAI(datalo, addrlo, 4));

        }

    } else {

        uint32_t insn = qemu_stx_opc[opc & (MO_BSWAP | MO_SIZE)];

        if (!HAVE_ISA_2_06 && insn == STDBRX) {

            tcg_out32(s, STWBRX | SAB(datalo, rbase, addrlo));

            tcg_out32(s, ADDI | TAI(TCG_REG_TMP1, addrlo, 4));

            tcg_out_shri64(s, TCG_REG_R0, datalo, 32);

            tcg_out32(s, STWBRX | SAB(TCG_REG_R0, rbase, TCG_REG_TMP1));

        } else {

            tcg_out32(s, insn | SAB(datalo, rbase, addrlo));

        }

    }



#ifdef CONFIG_SOFTMMU

    add_qemu_ldst_label(s, false, oi, datalo, datahi, addrlo, addrhi,

                        s->code_ptr, label_ptr);

#endif

}
","static void tcg_out_qemu_st(TCGContext *s, const TCGArg *args, bool is_64) {
TCGReg datalo, datahi, addrlo, rbase; TCGReg addrhi __attribute__((unused)); TCGMemOpIdx oi; TCGMemOp opc, s_bits; #ifdef CONFIG_SOFTMMU int mem_index; tcg_insn_unit *label_ptr; #endif datalo = *args++; datahi = (TCG_TARGET_REG_BITS == 32 && is_64 ? *args++ : 0); addrlo = *args++; addrhi = (TCG_TARGET_REG_BITS < TARGET_LONG_BITS ? *args++ : 0); oi = *args++; opc = get_memop(oi); s_bits = opc & MO_SIZE; #ifdef CONFIG_SOFTMMU mem_index = get_mmuidx(oi); addrlo = tcg_out_tlb_read(s, s_bits, addrlo, addrhi, mem_index, false); /* Load a pointer into the current opcode w/conditional branch-link. */ label_ptr = s->code_ptr; tcg_out_bc_noaddr(s, BC | BI(7, CR_EQ) | BO_COND_FALSE | LK); rbase = TCG_REG_R3; #else /* !CONFIG_SOFTMMU */ rbase = GUEST_BASE ? TCG_GUEST_BASE_REG : 0; if (TCG_TARGET_REG_BITS > TARGET_LONG_BITS) {
tcg_out_ext32u(s, TCG_REG_TMP1, addrlo); addrlo = TCG_REG_TMP1; } #endif if (TCG_TARGET_REG_BITS == 32 && s_bits == MO_64) {
if (opc & MO_BSWAP) {
tcg_out32(s, ADDI | TAI(TCG_REG_R0, addrlo, 4)); tcg_out32(s, STWBRX | SAB(datalo, rbase, addrlo)); tcg_out32(s, STWBRX | SAB(datahi, rbase, TCG_REG_R0)); } else if (rbase != 0) {
tcg_out32(s, ADDI | TAI(TCG_REG_R0, addrlo, 4)); tcg_out32(s, STWX | SAB(datahi, rbase, addrlo)); tcg_out32(s, STWX | SAB(datalo, rbase, TCG_REG_R0)); } else {
tcg_out32(s, STW | TAI(datahi, addrlo, 0)); tcg_out32(s, STW | TAI(datalo, addrlo, 4)); } } else {
uint32_t insn = qemu_stx_opc[opc & (MO_BSWAP | MO_SIZE)]; if (!HAVE_ISA_2_06 && insn == STDBRX) {
tcg_out32(s, STWBRX | SAB(datalo, rbase, addrlo)); tcg_out32(s, ADDI | TAI(TCG_REG_TMP1, addrlo, 4)); tcg_out_shri64(s, TCG_REG_R0, datalo, 32); tcg_out32(s, STWBRX | SAB(TCG_REG_R0, rbase, TCG_REG_TMP1)); } else {
tcg_out32(s, insn | SAB(datalo, rbase, addrlo)); } } #ifdef CONFIG_SOFTMMU add_qemu_ldst_label(s, false, oi, datalo, datahi, addrlo, addrhi, s->code_ptr, label_ptr); #endif } ",qemu,1
"static BlockAIOCB *blk_aio_prwv(BlockBackend *blk, int64_t offset, int bytes,

                                QEMUIOVector *qiov, CoroutineEntry co_entry,

                                BdrvRequestFlags flags,

                                BlockCompletionFunc *cb, void *opaque)

{

    BlkAioEmAIOCB *acb;

    Coroutine *co;



    bdrv_inc_in_flight(blk_bs(blk));

    acb = blk_aio_get(&blk_aio_em_aiocb_info, blk, cb, opaque);

    acb->rwco = (BlkRwCo) {

        .blk    = blk,

        .offset = offset,

        .qiov   = qiov,

        .flags  = flags,

        .ret    = NOT_DONE,

    };

    acb->bytes = bytes;

    acb->has_returned = false;



    co = qemu_coroutine_create(co_entry, acb);

    qemu_coroutine_enter(co);



    acb->has_returned = true;

    if (acb->rwco.ret != NOT_DONE) {

        aio_bh_schedule_oneshot(blk_get_aio_context(blk),

                                blk_aio_complete_bh, acb);

    }



    return &acb->common;

}
","static BlockAIOCB *blk_aio_prwv(BlockBackend *blk, int64_t offset, int bytes, QEMUIOVector *qiov, CoroutineEntry co_entry, BdrvRequestFlags flags, BlockCompletionFunc *cb, void *opaque) {
BlkAioEmAIOCB *acb; Coroutine *co; bdrv_inc_in_flight(blk_bs(blk)); acb = blk_aio_get(&blk_aio_em_aiocb_info, blk, cb, opaque); acb->rwco = (BlkRwCo) {
.blk = blk, .offset = offset, .qiov = qiov, .flags = flags, .ret = NOT_DONE, }; acb->bytes = bytes; acb->has_returned = false; co = qemu_coroutine_create(co_entry, acb); qemu_coroutine_enter(co); acb->has_returned = true; if (acb->rwco.ret != NOT_DONE) {
aio_bh_schedule_oneshot(blk_get_aio_context(blk), blk_aio_complete_bh, acb); } return &acb->common; } ",qemu,1
"static int zero_single_l2(BlockDriverState *bs, uint64_t offset,

                          uint64_t nb_clusters, int flags)

{

    BDRVQcow2State *s = bs->opaque;

    uint64_t *l2_table;

    int l2_index;

    int ret;

    int i;



    ret = get_cluster_table(bs, offset, &l2_table, &l2_index);

    if (ret < 0) {

        return ret;

    }



    /* Limit nb_clusters to one L2 table */

    nb_clusters = MIN(nb_clusters, s->l2_size - l2_index);

    assert(nb_clusters <= INT_MAX);



    for (i = 0; i < nb_clusters; i++) {

        uint64_t old_offset;



        old_offset = be64_to_cpu(l2_table[l2_index + i]);



        /* Update L2 entries */

        qcow2_cache_entry_mark_dirty(bs, s->l2_table_cache, l2_table);

        if (old_offset & QCOW_OFLAG_COMPRESSED || flags & BDRV_REQ_MAY_UNMAP) {

            l2_table[l2_index + i] = cpu_to_be64(QCOW_OFLAG_ZERO);

            qcow2_free_any_clusters(bs, old_offset, 1, QCOW2_DISCARD_REQUEST);

        } else {

            l2_table[l2_index + i] |= cpu_to_be64(QCOW_OFLAG_ZERO);

        }

    }



    qcow2_cache_put(bs, s->l2_table_cache, (void **) &l2_table);



    return nb_clusters;

}
","static int zero_single_l2(BlockDriverState *bs, uint64_t offset, uint64_t nb_clusters, int flags) {
BDRVQcow2State *s = bs->opaque; uint64_t *l2_table; int l2_index; int ret; int i; ret = get_cluster_table(bs, offset, &l2_table, &l2_index); if (ret < 0) {
return ret; } /* Limit nb_clusters to one L2 table */ nb_clusters = MIN(nb_clusters, s->l2_size - l2_index); assert(nb_clusters <= INT_MAX); for (i = 0; i < nb_clusters; i++) {
uint64_t old_offset; old_offset = be64_to_cpu(l2_table[l2_index + i]); /* Update L2 entries */ qcow2_cache_entry_mark_dirty(bs, s->l2_table_cache, l2_table); if (old_offset & QCOW_OFLAG_COMPRESSED || flags & BDRV_REQ_MAY_UNMAP) {
l2_table[l2_index + i] = cpu_to_be64(QCOW_OFLAG_ZERO); qcow2_free_any_clusters(bs, old_offset, 1, QCOW2_DISCARD_REQUEST); } else {
l2_table[l2_index + i] |= cpu_to_be64(QCOW_OFLAG_ZERO); } } qcow2_cache_put(bs, s->l2_table_cache, (void **) &l2_table); return nb_clusters; } ",qemu,0
"int main_loop(void *opaque)

{

    struct pollfd ufds[3], *pf, *serial_ufd, *net_ufd, *gdb_ufd;

    int ret, n, timeout, serial_ok;

    uint8_t ch;

    CPUState *env = global_env;



    if (!term_inited) {

        /* initialize terminal only there so that the user has a

           chance to stop QEMU with Ctrl-C before the gdb connection

           is launched */

        term_inited = 1;

        term_init();

    }



    serial_ok = 1;

    cpu_enable_ticks();

    for(;;) {

        ret = cpu_x86_exec(env);

        if (reset_requested) {

            ret = EXCP_INTERRUPT; 

            break;

        }

        if (ret == EXCP_DEBUG) {

            ret = EXCP_DEBUG;

            break;

        }

        /* if hlt instruction, we wait until the next IRQ */

        if (ret == EXCP_HLT) 

            timeout = 10;

        else

            timeout = 0;

        /* poll any events */

        serial_ufd = NULL;

        pf = ufds;

        if (serial_ok && !(serial_ports[0].lsr & UART_LSR_DR)) {

            serial_ufd = pf;

            pf->fd = 0;

            pf->events = POLLIN;

            pf++;

        }

        net_ufd = NULL;

        if (net_fd > 0 && ne2000_can_receive(&ne2000_state)) {

            net_ufd = pf;

            pf->fd = net_fd;

            pf->events = POLLIN;

            pf++;

        }

        gdb_ufd = NULL;

        if (gdbstub_fd > 0) {

            gdb_ufd = pf;

            pf->fd = gdbstub_fd;

            pf->events = POLLIN;

            pf++;

        }



        ret = poll(ufds, pf - ufds, timeout);

        if (ret > 0) {

            if (serial_ufd && (serial_ufd->revents & POLLIN)) {

                n = read(0, &ch, 1);

                if (n == 1) {

                    serial_received_byte(&serial_ports[0], ch);

                } else {

		    /* Closed, stop polling. */

                    serial_ok = 0;

                }

            }

            if (net_ufd && (net_ufd->revents & POLLIN)) {

                uint8_t buf[MAX_ETH_FRAME_SIZE];



                n = read(net_fd, buf, MAX_ETH_FRAME_SIZE);

                if (n > 0) {

                    if (n < 60) {

                        memset(buf + n, 0, 60 - n);

                        n = 60;

                    }

                    ne2000_receive(&ne2000_state, buf, n);

                }

            }

            if (gdb_ufd && (gdb_ufd->revents & POLLIN)) {

                uint8_t buf[1];

                /* stop emulation if requested by gdb */

                n = read(gdbstub_fd, buf, 1);

                if (n == 1) {

                    ret = EXCP_INTERRUPT; 

                    break;

                }

            }

        }



        /* timer IRQ */

        if (timer_irq_pending) {

            pic_set_irq(0, 1);

            pic_set_irq(0, 0);

            timer_irq_pending = 0;

            /* XXX: RTC test */

            if (cmos_data[RTC_REG_B] & 0x40) {

                pic_set_irq(8, 1);

            }

        }



        /* VGA */

        if (gui_refresh_pending) {

            display_state.dpy_refresh(&display_state);

            gui_refresh_pending = 0;

        }

    }

    cpu_disable_ticks();

    return ret;

}
","int main_loop(void *opaque) {
struct pollfd ufds[3], *pf, *serial_ufd, *net_ufd, *gdb_ufd; int ret, n, timeout, serial_ok; uint8_t ch; CPUState *env = global_env; if (!term_inited) {
/* initialize terminal only there so that the user has a chance to stop QEMU with Ctrl-C before the gdb connection is launched */ term_inited = 1; term_init(); } serial_ok = 1; cpu_enable_ticks(); for(;;) {
ret = cpu_x86_exec(env); if (reset_requested) {
ret = EXCP_INTERRUPT; break; } if (ret == EXCP_DEBUG) {
ret = EXCP_DEBUG; break; } /* if hlt instruction, we wait until the next IRQ */ if (ret == EXCP_HLT) timeout = 10; else timeout = 0; /* poll any events */ serial_ufd = NULL; pf = ufds; if (serial_ok && !(serial_ports[0].lsr & UART_LSR_DR)) {
serial_ufd = pf; pf->fd = 0; pf->events = POLLIN; pf++; } net_ufd = NULL; if (net_fd > 0 && ne2000_can_receive(&ne2000_state)) {
net_ufd = pf; pf->fd = net_fd; pf->events = POLLIN; pf++; } gdb_ufd = NULL; if (gdbstub_fd > 0) {
gdb_ufd = pf; pf->fd = gdbstub_fd; pf->events = POLLIN; pf++; } ret = poll(ufds, pf - ufds, timeout); if (ret > 0) {
if (serial_ufd && (serial_ufd->revents & POLLIN)) {
n = read(0, &ch, 1); if (n == 1) {
serial_received_byte(&serial_ports[0], ch); } else {
/* Closed, stop polling. */ serial_ok = 0; } } if (net_ufd && (net_ufd->revents & POLLIN)) {
uint8_t buf[MAX_ETH_FRAME_SIZE]; n = read(net_fd, buf, MAX_ETH_FRAME_SIZE); if (n > 0) {
if (n < 60) {
memset(buf + n, 0, 60 - n); n = 60; } ne2000_receive(&ne2000_state, buf, n); } } if (gdb_ufd && (gdb_ufd->revents & POLLIN)) {
uint8_t buf[1]; /* stop emulation if requested by gdb */ n = read(gdbstub_fd, buf, 1); if (n == 1) {
ret = EXCP_INTERRUPT; break; } } } /* timer IRQ */ if (timer_irq_pending) {
pic_set_irq(0, 1); pic_set_irq(0, 0); timer_irq_pending = 0; /* XXX: RTC test */ if (cmos_data[RTC_REG_B] & 0x40) {
pic_set_irq(8, 1); } } /* VGA */ if (gui_refresh_pending) {
display_state.dpy_refresh(&display_state); gui_refresh_pending = 0; } } cpu_disable_ticks(); return ret; } ",qemu,0
"static void *atomic_mmu_lookup(CPUArchState *env, target_ulong addr,

                               TCGMemOpIdx oi, uintptr_t retaddr)

{

    size_t mmu_idx = get_mmuidx(oi);

    size_t index = (addr >> TARGET_PAGE_BITS) & (CPU_TLB_SIZE - 1);

    CPUTLBEntry *tlbe = &env->tlb_table[mmu_idx][index];

    target_ulong tlb_addr = tlbe->addr_write;

    TCGMemOp mop = get_memop(oi);

    int a_bits = get_alignment_bits(mop);

    int s_bits = mop & MO_SIZE;



    /* Adjust the given return address.  */

    retaddr -= GETPC_ADJ;



    /* Enforce guest required alignment.  */

    if (unlikely(a_bits > 0 && (addr & ((1 << a_bits) - 1)))) {

        /* ??? Maybe indicate atomic op to cpu_unaligned_access */

        cpu_unaligned_access(ENV_GET_CPU(env), addr, MMU_DATA_STORE,

                             mmu_idx, retaddr);

    }



    /* Enforce qemu required alignment.  */

    if (unlikely(addr & ((1 << s_bits) - 1))) {

        /* We get here if guest alignment was not requested,

           or was not enforced by cpu_unaligned_access above.

           We might widen the access and emulate, but for now

           mark an exception and exit the cpu loop.  */

        goto stop_the_world;

    }



    /* Check TLB entry and enforce page permissions.  */

    if ((addr & TARGET_PAGE_MASK)

        != (tlb_addr & (TARGET_PAGE_MASK | TLB_INVALID_MASK))) {

        if (!VICTIM_TLB_HIT(addr_write, addr)) {

            tlb_fill(ENV_GET_CPU(env), addr, MMU_DATA_STORE, mmu_idx, retaddr);

        }

        tlb_addr = tlbe->addr_write;

    }



    /* Check notdirty */

    if (unlikely(tlb_addr & TLB_NOTDIRTY)) {

        tlb_set_dirty(ENV_GET_CPU(env), addr);

        tlb_addr = tlb_addr & ~TLB_NOTDIRTY;

    }



    /* Notice an IO access  */

    if (unlikely(tlb_addr & ~TARGET_PAGE_MASK)) {

        /* There's really nothing that can be done to

           support this apart from stop-the-world.  */

        goto stop_the_world;

    }



    /* Let the guest notice RMW on a write-only page.  */

    if (unlikely(tlbe->addr_read != tlb_addr)) {

        tlb_fill(ENV_GET_CPU(env), addr, MMU_DATA_LOAD, mmu_idx, retaddr);

        /* Since we don't support reads and writes to different addresses,

           and we do have the proper page loaded for write, this shouldn't

           ever return.  But just in case, handle via stop-the-world.  */

        goto stop_the_world;

    }



    return (void *)((uintptr_t)addr + tlbe->addend);



 stop_the_world:

    cpu_loop_exit_atomic(ENV_GET_CPU(env), retaddr);

}
","static void *atomic_mmu_lookup(CPUArchState *env, target_ulong addr, TCGMemOpIdx oi, uintptr_t retaddr) {
size_t mmu_idx = get_mmuidx(oi); size_t index = (addr >> TARGET_PAGE_BITS) & (CPU_TLB_SIZE - 1); CPUTLBEntry *tlbe = &env->tlb_table[mmu_idx][index]; target_ulong tlb_addr = tlbe->addr_write; TCGMemOp mop = get_memop(oi); int a_bits = get_alignment_bits(mop); int s_bits = mop & MO_SIZE; /* Adjust the given return address. */ retaddr -= GETPC_ADJ; /* Enforce guest required alignment. */ if (unlikely(a_bits > 0 && (addr & ((1 << a_bits) - 1)))) {
/* ??? Maybe indicate atomic op to cpu_unaligned_access */ cpu_unaligned_access(ENV_GET_CPU(env), addr, MMU_DATA_STORE, mmu_idx, retaddr); } /* Enforce qemu required alignment. */ if (unlikely(addr & ((1 << s_bits) - 1))) {
/* We get here if guest alignment was not requested, or was not enforced by cpu_unaligned_access above. We might widen the access and emulate, but for now mark an exception and exit the cpu loop. */ goto stop_the_world; } /* Check TLB entry and enforce page permissions. */ if ((addr & TARGET_PAGE_MASK) != (tlb_addr & (TARGET_PAGE_MASK | TLB_INVALID_MASK))) {
if (!VICTIM_TLB_HIT(addr_write, addr)) {
tlb_fill(ENV_GET_CPU(env), addr, MMU_DATA_STORE, mmu_idx, retaddr); } tlb_addr = tlbe->addr_write; } /* Check notdirty */ if (unlikely(tlb_addr & TLB_NOTDIRTY)) {
tlb_set_dirty(ENV_GET_CPU(env), addr); tlb_addr = tlb_addr & ~TLB_NOTDIRTY; } /* Notice an IO access */ if (unlikely(tlb_addr & ~TARGET_PAGE_MASK)) {
/* There's really nothing that can be done to support this apart from stop-the-world. */ goto stop_the_world; } /* Let the guest notice RMW on a write-only page. */ if (unlikely(tlbe->addr_read != tlb_addr)) {
tlb_fill(ENV_GET_CPU(env), addr, MMU_DATA_LOAD, mmu_idx, retaddr); /* Since we don't support reads and writes to different addresses, and we do have the proper page loaded for write, this shouldn't ever return. But just in case, handle via stop-the-world. */ goto stop_the_world; } return (void *)((uintptr_t)addr + tlbe->addend); stop_the_world: cpu_loop_exit_atomic(ENV_GET_CPU(env), retaddr); } ",qemu,0
"static void monitor_event(void *opaque, int event)

{

    Monitor *mon = opaque;



    switch (event) {

    case CHR_EVENT_MUX_IN:

        qemu_mutex_lock(&mon->out_lock);

        mon->mux_out = 0;

        qemu_mutex_unlock(&mon->out_lock);

        if (mon->reset_seen) {


            monitor_resume(mon);

            monitor_flush(mon);

        } else {

            mon->suspend_cnt = 0;

        }

        break;



    case CHR_EVENT_MUX_OUT:

        if (mon->reset_seen) {

            if (mon->suspend_cnt == 0) {

                monitor_printf(mon, ""\n"");

            }

            monitor_flush(mon);

            monitor_suspend(mon);

        } else {

            mon->suspend_cnt++;

        }

        qemu_mutex_lock(&mon->out_lock);

        mon->mux_out = 1;

        qemu_mutex_unlock(&mon->out_lock);

        break;



    case CHR_EVENT_OPENED:

        monitor_printf(mon, ""QEMU %s monitor - type 'help' for more ""

                       ""information\n"", QEMU_VERSION);

        if (!mon->mux_out) {


            readline_show_prompt(mon->rs);

        }

        mon->reset_seen = 1;

        mon_refcount++;

        break;



    case CHR_EVENT_CLOSED:

        mon_refcount--;

        monitor_fdsets_cleanup();

        break;

    }

}","static void monitor_event(void *opaque, int event) {
Monitor *mon = opaque; switch (event) {
case CHR_EVENT_MUX_IN: qemu_mutex_lock(&mon->out_lock); mon->mux_out = 0; qemu_mutex_unlock(&mon->out_lock); if (mon->reset_seen) {
monitor_resume(mon); monitor_flush(mon); } else {
mon->suspend_cnt = 0; } break; case CHR_EVENT_MUX_OUT: if (mon->reset_seen) {
if (mon->suspend_cnt == 0) {
monitor_printf(mon, ""\n""); } monitor_flush(mon); monitor_suspend(mon); } else {
mon->suspend_cnt++; } qemu_mutex_lock(&mon->out_lock); mon->mux_out = 1; qemu_mutex_unlock(&mon->out_lock); break; case CHR_EVENT_OPENED: monitor_printf(mon, ""QEMU %s monitor - type 'help' for more "" ""information\n"", QEMU_VERSION); if (!mon->mux_out) {
readline_show_prompt(mon->rs); } mon->reset_seen = 1; mon_refcount++; break; case CHR_EVENT_CLOSED: mon_refcount--; monitor_fdsets_cleanup(); break; } }",qemu,1
"static uint64_t fw_cfg_data_mem_read(void *opaque, hwaddr addr,

                                     unsigned size)

{

    return fw_cfg_read(opaque);

}
","static uint64_t fw_cfg_data_mem_read(void *opaque, hwaddr addr, unsigned size) {
return fw_cfg_read(opaque); } ",qemu,0
"static uint64_t pic_read(void *opaque, target_phys_addr_t addr,

                         unsigned size)

{

    HeathrowPICS *s = opaque;

    HeathrowPIC *pic;

    unsigned int n;

    uint32_t value;



    n = ((addr & 0xfff) - 0x10) >> 4;

    if (n >= 2) {

        value = 0;

    } else {

        pic = &s->pics[n];

        switch(addr & 0xf) {

        case 0x0:

            value = pic->events;

            break;

        case 0x4:

            value = pic->mask;

            break;

        case 0xc:

            value = pic->levels;

            break;

        default:

            value = 0;

            break;

        }

    }

    PIC_DPRINTF(""readl: "" TARGET_FMT_plx "" %u: %08x\n"", addr, n, value);

    return value;

}
","static uint64_t pic_read(void *opaque, target_phys_addr_t addr, unsigned size) {
HeathrowPICS *s = opaque; HeathrowPIC *pic; unsigned int n; uint32_t value; n = ((addr & 0xfff) - 0x10) >> 4; if (n >= 2) {
value = 0; } else {
pic = &s->pics[n]; switch(addr & 0xf) {
case 0x0: value = pic->events; break; case 0x4: value = pic->mask; break; case 0xc: value = pic->levels; break; default: value = 0; break; } } PIC_DPRINTF(""readl: "" TARGET_FMT_plx "" %u: %08x\n"", addr, n, value); return value; } ",qemu,0
"static int ram_save_setup(QEMUFile *f, void *opaque)

{

    RAMBlock *block;

    int64_t ram_pages = last_ram_offset() >> TARGET_PAGE_BITS;



    migration_bitmap = bitmap_new(ram_pages);

    bitmap_set(migration_bitmap, 0, ram_pages);

    migration_dirty_pages = ram_pages;

    mig_throttle_on = false;

    dirty_rate_high_cnt = 0;



    if (migrate_use_xbzrle()) {

        qemu_mutex_lock_iothread();

        XBZRLE.cache = cache_init(migrate_xbzrle_cache_size() /

                                  TARGET_PAGE_SIZE,

                                  TARGET_PAGE_SIZE);

        if (!XBZRLE.cache) {

            qemu_mutex_unlock_iothread();

            DPRINTF(""Error creating cache\n"");

            return -1;

        }

        qemu_mutex_init(&XBZRLE.lock);

        qemu_mutex_unlock_iothread();



        /* We prefer not to abort if there is no memory */

        XBZRLE.encoded_buf = g_try_malloc0(TARGET_PAGE_SIZE);

        if (!XBZRLE.encoded_buf) {

            DPRINTF(""Error allocating encoded_buf\n"");

            return -1;

        }



        XBZRLE.current_buf = g_try_malloc(TARGET_PAGE_SIZE);

        if (!XBZRLE.current_buf) {

            DPRINTF(""Error allocating current_buf\n"");

            g_free(XBZRLE.encoded_buf);

            XBZRLE.encoded_buf = NULL;

            return -1;

        }



        acct_clear();

    }



    qemu_mutex_lock_iothread();

    qemu_mutex_lock_ramlist();

    bytes_transferred = 0;

    reset_ram_globals();



    memory_global_dirty_log_start();

    migration_bitmap_sync();

    qemu_mutex_unlock_iothread();



    qemu_put_be64(f, ram_bytes_total() | RAM_SAVE_FLAG_MEM_SIZE);



    QTAILQ_FOREACH(block, &ram_list.blocks, next) {

        qemu_put_byte(f, strlen(block->idstr));

        qemu_put_buffer(f, (uint8_t *)block->idstr, strlen(block->idstr));

        qemu_put_be64(f, block->length);

    }



    qemu_mutex_unlock_ramlist();



    ram_control_before_iterate(f, RAM_CONTROL_SETUP);

    ram_control_after_iterate(f, RAM_CONTROL_SETUP);



    qemu_put_be64(f, RAM_SAVE_FLAG_EOS);



    return 0;

}
","static int ram_save_setup(QEMUFile *f, void *opaque) {
RAMBlock *block; int64_t ram_pages = last_ram_offset() >> TARGET_PAGE_BITS; migration_bitmap = bitmap_new(ram_pages); bitmap_set(migration_bitmap, 0, ram_pages); migration_dirty_pages = ram_pages; mig_throttle_on = false; dirty_rate_high_cnt = 0; if (migrate_use_xbzrle()) {
qemu_mutex_lock_iothread(); XBZRLE.cache = cache_init(migrate_xbzrle_cache_size() / TARGET_PAGE_SIZE, TARGET_PAGE_SIZE); if (!XBZRLE.cache) {
qemu_mutex_unlock_iothread(); DPRINTF(""Error creating cache\n""); return -1; } qemu_mutex_init(&XBZRLE.lock); qemu_mutex_unlock_iothread(); /* We prefer not to abort if there is no memory */ XBZRLE.encoded_buf = g_try_malloc0(TARGET_PAGE_SIZE); if (!XBZRLE.encoded_buf) {
DPRINTF(""Error allocating encoded_buf\n""); return -1; } XBZRLE.current_buf = g_try_malloc(TARGET_PAGE_SIZE); if (!XBZRLE.current_buf) {
DPRINTF(""Error allocating current_buf\n""); g_free(XBZRLE.encoded_buf); XBZRLE.encoded_buf = NULL; return -1; } acct_clear(); } qemu_mutex_lock_iothread(); qemu_mutex_lock_ramlist(); bytes_transferred = 0; reset_ram_globals(); memory_global_dirty_log_start(); migration_bitmap_sync(); qemu_mutex_unlock_iothread(); qemu_put_be64(f, ram_bytes_total() | RAM_SAVE_FLAG_MEM_SIZE); QTAILQ_FOREACH(block, &ram_list.blocks, next) {
qemu_put_byte(f, strlen(block->idstr)); qemu_put_buffer(f, (uint8_t *)block->idstr, strlen(block->idstr)); qemu_put_be64(f, block->length); } qemu_mutex_unlock_ramlist(); ram_control_before_iterate(f, RAM_CONTROL_SETUP); ram_control_after_iterate(f, RAM_CONTROL_SETUP); qemu_put_be64(f, RAM_SAVE_FLAG_EOS); return 0; } ",qemu,1
"static int pci_bridge_dev_initfn(PCIDevice *dev)

{

    PCIBridge *br = DO_UPCAST(PCIBridge, dev, dev);

    PCIBridgeDev *bridge_dev = DO_UPCAST(PCIBridgeDev, bridge, br);

    int err;

    pci_bridge_map_irq(br, NULL, pci_bridge_dev_map_irq_fn);

    err = pci_bridge_initfn(dev);

    if (err) {

        goto bridge_error;

    }

    memory_region_init(&bridge_dev->bar, ""shpc-bar"", shpc_bar_size(dev));

    err = shpc_init(dev, &br->sec_bus, &bridge_dev->bar, 0);

    if (err) {

        goto shpc_error;

    }

    err = slotid_cap_init(dev, 0, bridge_dev->chassis_nr, 0);

    if (err) {

        goto slotid_error;

    }

    if ((bridge_dev->flags & (1 << PCI_BRIDGE_DEV_F_MSI_REQ)) &&

        msi_supported) {

        err = msi_init(dev, 0, 1, true, true);

        if (err < 0) {

            goto msi_error;

        }

    }

    /* TODO: spec recommends using 64 bit prefetcheable BAR.

     * Check whether that works well. */

    pci_register_bar(dev, 0, PCI_BASE_ADDRESS_SPACE_MEMORY |

		     PCI_BASE_ADDRESS_MEM_TYPE_64, &bridge_dev->bar);

    dev->config[PCI_INTERRUPT_PIN] = 0x1;

    return 0;

msi_error:

    slotid_cap_cleanup(dev);

slotid_error:

    shpc_cleanup(dev, &bridge_dev->bar);

shpc_error:

    memory_region_destroy(&bridge_dev->bar);

bridge_error:

    return err;

}
","static int pci_bridge_dev_initfn(PCIDevice *dev) {
PCIBridge *br = DO_UPCAST(PCIBridge, dev, dev); PCIBridgeDev *bridge_dev = DO_UPCAST(PCIBridgeDev, bridge, br); int err; pci_bridge_map_irq(br, NULL, pci_bridge_dev_map_irq_fn); err = pci_bridge_initfn(dev); if (err) {
goto bridge_error; } memory_region_init(&bridge_dev->bar, ""shpc-bar"", shpc_bar_size(dev)); err = shpc_init(dev, &br->sec_bus, &bridge_dev->bar, 0); if (err) {
goto shpc_error; } err = slotid_cap_init(dev, 0, bridge_dev->chassis_nr, 0); if (err) {
goto slotid_error; } if ((bridge_dev->flags & (1 << PCI_BRIDGE_DEV_F_MSI_REQ)) && msi_supported) {
err = msi_init(dev, 0, 1, true, true); if (err < 0) {
goto msi_error; } } /* TODO: spec recommends using 64 bit prefetcheable BAR. * Check whether that works well. */ pci_register_bar(dev, 0, PCI_BASE_ADDRESS_SPACE_MEMORY | PCI_BASE_ADDRESS_MEM_TYPE_64, &bridge_dev->bar); dev->config[PCI_INTERRUPT_PIN] = 0x1; return 0; msi_error: slotid_cap_cleanup(dev); slotid_error: shpc_cleanup(dev, &bridge_dev->bar); shpc_error: memory_region_destroy(&bridge_dev->bar); bridge_error: return err; } ",qemu,1
"BlockAIOCB *ide_issue_trim(

        int64_t offset, QEMUIOVector *qiov,

        BlockCompletionFunc *cb, void *cb_opaque, void *opaque)

{

    BlockBackend *blk = opaque;

    TrimAIOCB *iocb;



    iocb = blk_aio_get(&trim_aiocb_info, blk, cb, cb_opaque);

    iocb->blk = blk;

    iocb->bh = qemu_bh_new(ide_trim_bh_cb, iocb);

    iocb->ret = 0;

    iocb->qiov = qiov;

    iocb->i = -1;

    iocb->j = 0;

    ide_issue_trim_cb(iocb, 0);

    return &iocb->common;

}
","BlockAIOCB *ide_issue_trim( int64_t offset, QEMUIOVector *qiov, BlockCompletionFunc *cb, void *cb_opaque, void *opaque) {
BlockBackend *blk = opaque; TrimAIOCB *iocb; iocb = blk_aio_get(&trim_aiocb_info, blk, cb, cb_opaque); iocb->blk = blk; iocb->bh = qemu_bh_new(ide_trim_bh_cb, iocb); iocb->ret = 0; iocb->qiov = qiov; iocb->i = -1; iocb->j = 0; ide_issue_trim_cb(iocb, 0); return &iocb->common; } ",qemu,0
"static void report_unavailable_features(FeatureWord w, uint32_t mask)

{

    FeatureWordInfo *f = &feature_word_info[w];

    int i;



    for (i = 0; i < 32; ++i) {

        if ((1UL << i) & mask) {

            const char *reg = get_register_name_32(f->cpuid_reg);

            assert(reg);

            fprintf(stderr, ""warning: %s doesn't support requested feature: ""

                ""CPUID.%02XH:%s%s%s [bit %d]\n"",

                kvm_enabled() ? ""host"" : ""TCG"",

                f->cpuid_eax, reg,

                f->feat_names[i] ? ""."" : """",

                f->feat_names[i] ? f->feat_names[i] : """", i);

        }

    }

}
","static void report_unavailable_features(FeatureWord w, uint32_t mask) {
FeatureWordInfo *f = &feature_word_info[w]; int i; for (i = 0; i < 32; ++i) {
if ((1UL << i) & mask) {
const char *reg = get_register_name_32(f->cpuid_reg); assert(reg); fprintf(stderr, ""warning: %s doesn't support requested feature: "" ""CPUID.%02XH:%s%s%s [bit %d]\n"", kvm_enabled() ? ""host"" : ""TCG"", f->cpuid_eax, reg, f->feat_names[i] ? ""."" : """", f->feat_names[i] ? f->feat_names[i] : """", i); } } } ",qemu,0
"void monitor_flush(Monitor *mon)

{

    int rc;

    size_t len;

    const char *buf;



    if (mon->skip_flush) {

        return;

    }



    buf = qstring_get_str(mon->outbuf);

    len = qstring_get_length(mon->outbuf);



    if (len && !mon->mux_out) {

        rc = qemu_chr_fe_write(mon->chr, (const uint8_t *) buf, len);

        if (rc == len) {

            /* all flushed */

            QDECREF(mon->outbuf);

            mon->outbuf = qstring_new();

            return;

        }

        if (rc > 0) {

            /* partinal write */

            QString *tmp = qstring_from_str(buf + rc);

            QDECREF(mon->outbuf);

            mon->outbuf = tmp;

        }

        if (mon->watch == 0) {

            mon->watch = qemu_chr_fe_add_watch(mon->chr, G_IO_OUT,

                                               monitor_unblocked, mon);

        }

    }

}
","void monitor_flush(Monitor *mon) {
int rc; size_t len; const char *buf; if (mon->skip_flush) {
return; } buf = qstring_get_str(mon->outbuf); len = qstring_get_length(mon->outbuf); if (len && !mon->mux_out) {
rc = qemu_chr_fe_write(mon->chr, (const uint8_t *) buf, len); if (rc == len) {
/* all flushed */ QDECREF(mon->outbuf); mon->outbuf = qstring_new(); return; } if (rc > 0) {
/* partinal write */ QString *tmp = qstring_from_str(buf + rc); QDECREF(mon->outbuf); mon->outbuf = tmp; } if (mon->watch == 0) {
mon->watch = qemu_chr_fe_add_watch(mon->chr, G_IO_OUT, monitor_unblocked, mon); } } } ",qemu,1
"static uint32_t gic_dist_readb(void *opaque, target_phys_addr_t offset)

{

    GICState *s = (GICState *)opaque;

    uint32_t res;

    int irq;

    int i;

    int cpu;

    int cm;

    int mask;



    cpu = gic_get_current_cpu(s);

    cm = 1 << cpu;

    if (offset < 0x100) {

        if (offset == 0)

            return s->enabled;

        if (offset == 4)

            return ((s->num_irq / 32) - 1) | ((NUM_CPU(s) - 1) << 5);

        if (offset < 0x08)

            return 0;

        if (offset >= 0x80) {

            /* Interrupt Security , RAZ/WI */

            return 0;

        }

        goto bad_reg;

    } else if (offset < 0x200) {

        /* Interrupt Set/Clear Enable.  */

        if (offset < 0x180)

            irq = (offset - 0x100) * 8;

        else

            irq = (offset - 0x180) * 8;

        irq += GIC_BASE_IRQ;

        if (irq >= s->num_irq)

            goto bad_reg;

        res = 0;

        for (i = 0; i < 8; i++) {

            if (GIC_TEST_ENABLED(irq + i, cm)) {

                res |= (1 << i);

            }

        }

    } else if (offset < 0x300) {

        /* Interrupt Set/Clear Pending.  */

        if (offset < 0x280)

            irq = (offset - 0x200) * 8;

        else

            irq = (offset - 0x280) * 8;

        irq += GIC_BASE_IRQ;

        if (irq >= s->num_irq)

            goto bad_reg;

        res = 0;

        mask = (irq < GIC_INTERNAL) ?  cm : ALL_CPU_MASK;

        for (i = 0; i < 8; i++) {

            if (GIC_TEST_PENDING(irq + i, mask)) {

                res |= (1 << i);

            }

        }

    } else if (offset < 0x400) {

        /* Interrupt Active.  */

        irq = (offset - 0x300) * 8 + GIC_BASE_IRQ;

        if (irq >= s->num_irq)

            goto bad_reg;

        res = 0;

        mask = (irq < GIC_INTERNAL) ?  cm : ALL_CPU_MASK;

        for (i = 0; i < 8; i++) {

            if (GIC_TEST_ACTIVE(irq + i, mask)) {

                res |= (1 << i);

            }

        }

    } else if (offset < 0x800) {

        /* Interrupt Priority.  */

        irq = (offset - 0x400) + GIC_BASE_IRQ;

        if (irq >= s->num_irq)

            goto bad_reg;

        res = GIC_GET_PRIORITY(irq, cpu);

    } else if (offset < 0xc00) {

        /* Interrupt CPU Target.  */

        if (s->num_cpu == 1 && s->revision != REV_11MPCORE) {

            /* For uniprocessor GICs these RAZ/WI */

            res = 0;

        } else {

            irq = (offset - 0x800) + GIC_BASE_IRQ;

            if (irq >= s->num_irq) {

                goto bad_reg;

            }

            if (irq >= 29 && irq <= 31) {

                res = cm;

            } else {

                res = GIC_TARGET(irq);

            }

        }

    } else if (offset < 0xf00) {

        /* Interrupt Configuration.  */

        irq = (offset - 0xc00) * 2 + GIC_BASE_IRQ;

        if (irq >= s->num_irq)

            goto bad_reg;

        res = 0;

        for (i = 0; i < 4; i++) {

            if (GIC_TEST_MODEL(irq + i))

                res |= (1 << (i * 2));

            if (GIC_TEST_TRIGGER(irq + i))

                res |= (2 << (i * 2));

        }

    } else if (offset < 0xfe0) {

        goto bad_reg;

    } else /* offset >= 0xfe0 */ {

        if (offset & 3) {

            res = 0;

        } else {

            res = gic_id[(offset - 0xfe0) >> 2];

        }

    }

    return res;

bad_reg:

    hw_error(""gic_dist_readb: Bad offset %x\n"", (int)offset);

    return 0;

}
","static uint32_t gic_dist_readb(void *opaque, target_phys_addr_t offset) {
GICState *s = (GICState *)opaque; uint32_t res; int irq; int i; int cpu; int cm; int mask; cpu = gic_get_current_cpu(s); cm = 1 << cpu; if (offset < 0x100) {
if (offset == 0) return s->enabled; if (offset == 4) return ((s->num_irq / 32) - 1) | ((NUM_CPU(s) - 1) << 5); if (offset < 0x08) return 0; if (offset >= 0x80) {
/* Interrupt Security , RAZ/WI */ return 0; } goto bad_reg; } else if (offset < 0x200) {
/* Interrupt Set/Clear Enable. */ if (offset < 0x180) irq = (offset - 0x100) * 8; else irq = (offset - 0x180) * 8; irq += GIC_BASE_IRQ; if (irq >= s->num_irq) goto bad_reg; res = 0; for (i = 0; i < 8; i++) {
if (GIC_TEST_ENABLED(irq + i, cm)) {
res |= (1 << i); } } } else if (offset < 0x300) {
/* Interrupt Set/Clear Pending. */ if (offset < 0x280) irq = (offset - 0x200) * 8; else irq = (offset - 0x280) * 8; irq += GIC_BASE_IRQ; if (irq >= s->num_irq) goto bad_reg; res = 0; mask = (irq < GIC_INTERNAL) ? cm : ALL_CPU_MASK; for (i = 0; i < 8; i++) {
if (GIC_TEST_PENDING(irq + i, mask)) {
res |= (1 << i); } } } else if (offset < 0x400) {
/* Interrupt Active. */ irq = (offset - 0x300) * 8 + GIC_BASE_IRQ; if (irq >= s->num_irq) goto bad_reg; res = 0; mask = (irq < GIC_INTERNAL) ? cm : ALL_CPU_MASK; for (i = 0; i < 8; i++) {
if (GIC_TEST_ACTIVE(irq + i, mask)) {
res |= (1 << i); } } } else if (offset < 0x800) {
/* Interrupt Priority. */ irq = (offset - 0x400) + GIC_BASE_IRQ; if (irq >= s->num_irq) goto bad_reg; res = GIC_GET_PRIORITY(irq, cpu); } else if (offset < 0xc00) {
/* Interrupt CPU Target. */ if (s->num_cpu == 1 && s->revision != REV_11MPCORE) {
/* For uniprocessor GICs these RAZ/WI */ res = 0; } else {
irq = (offset - 0x800) + GIC_BASE_IRQ; if (irq >= s->num_irq) {
goto bad_reg; } if (irq >= 29 && irq <= 31) {
res = cm; } else {
res = GIC_TARGET(irq); } } } else if (offset < 0xf00) {
/* Interrupt Configuration. */ irq = (offset - 0xc00) * 2 + GIC_BASE_IRQ; if (irq >= s->num_irq) goto bad_reg; res = 0; for (i = 0; i < 4; i++) {
if (GIC_TEST_MODEL(irq + i)) res |= (1 << (i * 2)); if (GIC_TEST_TRIGGER(irq + i)) res |= (2 << (i * 2)); } } else if (offset < 0xfe0) {
goto bad_reg; } else /* offset >= 0xfe0 */ {
if (offset & 3) {
res = 0; } else {
res = gic_id[(offset - 0xfe0) >> 2]; } } return res; bad_reg: hw_error(""gic_dist_readb: Bad offset %x\n"", (int)offset); return 0; } ",qemu,0
"static void create_gic(const VirtBoardInfo *vbi, qemu_irq *pic)

{

    /* We create a standalone GIC v2 */

    DeviceState *gicdev;

    SysBusDevice *gicbusdev;

    const char *gictype = ""arm_gic"";

    int i;



    if (kvm_irqchip_in_kernel()) {

        gictype = ""kvm-arm-gic"";

    }



    gicdev = qdev_create(NULL, gictype);

    qdev_prop_set_uint32(gicdev, ""revision"", 2);

    qdev_prop_set_uint32(gicdev, ""num-cpu"", smp_cpus);

    /* Note that the num-irq property counts both internal and external

     * interrupts; there are always 32 of the former (mandated by GIC spec).

     */

    qdev_prop_set_uint32(gicdev, ""num-irq"", NUM_IRQS + 32);

    qdev_init_nofail(gicdev);

    gicbusdev = SYS_BUS_DEVICE(gicdev);

    sysbus_mmio_map(gicbusdev, 0, vbi->memmap[VIRT_GIC_DIST].base);

    sysbus_mmio_map(gicbusdev, 1, vbi->memmap[VIRT_GIC_CPU].base);



    /* Wire the outputs from each CPU's generic timer to the

     * appropriate GIC PPI inputs, and the GIC's IRQ output to

     * the CPU's IRQ input.

     */

    for (i = 0; i < smp_cpus; i++) {

        DeviceState *cpudev = DEVICE(qemu_get_cpu(i));

        int ppibase = NUM_IRQS + i * 32;

        /* physical timer; we wire it up to the non-secure timer's ID,

         * since a real A15 always has TrustZone but QEMU doesn't.

         */

        qdev_connect_gpio_out(cpudev, 0,

                              qdev_get_gpio_in(gicdev, ppibase + 30));

        /* virtual timer */

        qdev_connect_gpio_out(cpudev, 1,

                              qdev_get_gpio_in(gicdev, ppibase + 27));



        sysbus_connect_irq(gicbusdev, i, qdev_get_gpio_in(cpudev, ARM_CPU_IRQ));

    }



    for (i = 0; i < NUM_IRQS; i++) {

        pic[i] = qdev_get_gpio_in(gicdev, i);

    }



    fdt_add_gic_node(vbi);

}
","static void create_gic(const VirtBoardInfo *vbi, qemu_irq *pic) {
/* We create a standalone GIC v2 */ DeviceState *gicdev; SysBusDevice *gicbusdev; const char *gictype = ""arm_gic""; int i; if (kvm_irqchip_in_kernel()) {
gictype = ""kvm-arm-gic""; } gicdev = qdev_create(NULL, gictype); qdev_prop_set_uint32(gicdev, ""revision"", 2); qdev_prop_set_uint32(gicdev, ""num-cpu"", smp_cpus); /* Note that the num-irq property counts both internal and external * interrupts; there are always 32 of the former (mandated by GIC spec). */ qdev_prop_set_uint32(gicdev, ""num-irq"", NUM_IRQS + 32); qdev_init_nofail(gicdev); gicbusdev = SYS_BUS_DEVICE(gicdev); sysbus_mmio_map(gicbusdev, 0, vbi->memmap[VIRT_GIC_DIST].base); sysbus_mmio_map(gicbusdev, 1, vbi->memmap[VIRT_GIC_CPU].base); /* Wire the outputs from each CPU's generic timer to the * appropriate GIC PPI inputs, and the GIC's IRQ output to * the CPU's IRQ input. */ for (i = 0; i < smp_cpus; i++) {
DeviceState *cpudev = DEVICE(qemu_get_cpu(i)); int ppibase = NUM_IRQS + i * 32; /* physical timer; we wire it up to the non-secure timer's ID, * since a real A15 always has TrustZone but QEMU doesn't. */ qdev_connect_gpio_out(cpudev, 0, qdev_get_gpio_in(gicdev, ppibase + 30)); /* virtual timer */ qdev_connect_gpio_out(cpudev, 1, qdev_get_gpio_in(gicdev, ppibase + 27)); sysbus_connect_irq(gicbusdev, i, qdev_get_gpio_in(cpudev, ARM_CPU_IRQ)); } for (i = 0; i < NUM_IRQS; i++) {
pic[i] = qdev_get_gpio_in(gicdev, i); } fdt_add_gic_node(vbi); } ",qemu,1
"static void migration_end(void)

{

    if (migration_bitmap) {

        memory_global_dirty_log_stop();

        g_free(migration_bitmap);

        migration_bitmap = NULL;

    }



    XBZRLE_cache_lock();

    if (XBZRLE.cache) {

        cache_fini(XBZRLE.cache);

        g_free(XBZRLE.encoded_buf);

        g_free(XBZRLE.current_buf);

        XBZRLE.cache = NULL;

        XBZRLE.encoded_buf = NULL;

        XBZRLE.current_buf = NULL;

    }

    XBZRLE_cache_unlock();

}
","static void migration_end(void) {
if (migration_bitmap) {
memory_global_dirty_log_stop(); g_free(migration_bitmap); migration_bitmap = NULL; } XBZRLE_cache_lock(); if (XBZRLE.cache) {
cache_fini(XBZRLE.cache); g_free(XBZRLE.encoded_buf); g_free(XBZRLE.current_buf); XBZRLE.cache = NULL; XBZRLE.encoded_buf = NULL; XBZRLE.current_buf = NULL; } XBZRLE_cache_unlock(); } ",qemu,0
"int qemu_can_send_packet(VLANClientState *sender)

{

    VLANState *vlan = sender->vlan;

    VLANClientState *vc;



    for (vc = vlan->first_client; vc != NULL; vc = vc->next) {

        if (vc == sender) {

            continue;

        }



        /* no can_receive() handler, they can always receive */

        if (!vc->can_receive || vc->can_receive(vc->opaque)) {

            return 1;

        }

    }

    return 0;

}
","int qemu_can_send_packet(VLANClientState *sender) {
VLANState *vlan = sender->vlan; VLANClientState *vc; for (vc = vlan->first_client; vc != NULL; vc = vc->next) {
if (vc == sender) {
continue; } /* no can_receive() handler, they can always receive */ if (!vc->can_receive || vc->can_receive(vc->opaque)) {
return 1; } } return 0; } ",qemu,0
"static long do_sigreturn_v2(CPUARMState *env)

{

    abi_ulong frame_addr;

    struct sigframe_v2 *frame = NULL;



    /*

     * Since we stacked the signal on a 64-bit boundary,

     * then 'sp' should be word aligned here.  If it's

     * not, then the user is trying to mess with us.

     */

    frame_addr = env->regs[13];

    trace_user_do_sigreturn(env, frame_addr);

    if (frame_addr & 7) {

        goto badframe;

    }



    if (!lock_user_struct(VERIFY_READ, frame, frame_addr, 1)) {

        goto badframe;

    }



    if (do_sigframe_return_v2(env, frame_addr, &frame->uc)) {

        goto badframe;

    }



    unlock_user_struct(frame, frame_addr, 0);

    return env->regs[0];



badframe:

    unlock_user_struct(frame, frame_addr, 0);

    force_sig(TARGET_SIGSEGV /* , current */);

    return 0;

}
","static long do_sigreturn_v2(CPUARMState *env) {
abi_ulong frame_addr; struct sigframe_v2 *frame = NULL; /* * Since we stacked the signal on a 64-bit boundary, * then 'sp' should be word aligned here. If it's * not, then the user is trying to mess with us. */ frame_addr = env->regs[13]; trace_user_do_sigreturn(env, frame_addr); if (frame_addr & 7) {
goto badframe; } if (!lock_user_struct(VERIFY_READ, frame, frame_addr, 1)) {
goto badframe; } if (do_sigframe_return_v2(env, frame_addr, &frame->uc)) {
goto badframe; } unlock_user_struct(frame, frame_addr, 0); return env->regs[0]; badframe: unlock_user_struct(frame, frame_addr, 0); force_sig(TARGET_SIGSEGV /* , current */); return 0; } ",qemu,0
"static uint64_t hb_regs_read(void *opaque, target_phys_addr_t offset,

                             unsigned size)

{

    uint32_t *regs = opaque;

    uint32_t value = regs[offset/4];



    if ((offset == 0x100) || (offset == 0x108) || (offset == 0x10C)) {

        value |= 0x30000000;

    }



    return value;

}
","static uint64_t hb_regs_read(void *opaque, target_phys_addr_t offset, unsigned size) {
uint32_t *regs = opaque; uint32_t value = regs[offset/4]; if ((offset == 0x100) || (offset == 0x108) || (offset == 0x10C)) {
value |= 0x30000000; } return value; } ",qemu,0
"PPC_OP(test_ctr)

{

    T0 = regs->ctr;

    RETURN();

}
","PPC_OP(test_ctr) {
T0 = regs->ctr; RETURN(); } ",qemu,1
"AlphaCPU *cpu_alpha_init(const char *cpu_model)

{

    AlphaCPU *cpu;

    ObjectClass *cpu_class;



    cpu_class = alpha_cpu_class_by_name(cpu_model);

    if (cpu_class == NULL) {

        /* Default to ev67; no reason not to emulate insns by default.  */

        cpu_class = object_class_by_name(TYPE(""ev67""));

    }

    cpu = ALPHA_CPU(object_new(object_class_get_name(cpu_class)));



    object_property_set_bool(OBJECT(cpu), true, ""realized"", NULL);



    return cpu;

}
","AlphaCPU *cpu_alpha_init(const char *cpu_model) {
AlphaCPU *cpu; ObjectClass *cpu_class; cpu_class = alpha_cpu_class_by_name(cpu_model); if (cpu_class == NULL) {
/* Default to ev67; no reason not to emulate insns by default. */ cpu_class = object_class_by_name(TYPE(""ev67"")); } cpu = ALPHA_CPU(object_new(object_class_get_name(cpu_class))); object_property_set_bool(OBJECT(cpu), true, ""realized"", NULL); return cpu; } ",qemu,0
"void qht_statistics_init(struct qht *ht, struct qht_stats *stats)

{

    struct qht_map *map;

    int i;



    map = atomic_rcu_read(&ht->map);



    stats->head_buckets = map->n_buckets;

    stats->used_head_buckets = 0;

    stats->entries = 0;

    qdist_init(&stats->chain);

    qdist_init(&stats->occupancy);



    for (i = 0; i < map->n_buckets; i++) {

        struct qht_bucket *head = &map->buckets[i];

        struct qht_bucket *b;

        unsigned int version;

        size_t buckets;

        size_t entries;

        int j;



        do {

            version = seqlock_read_begin(&head->sequence);

            buckets = 0;

            entries = 0;

            b = head;

            do {

                for (j = 0; j < QHT_BUCKET_ENTRIES; j++) {

                    if (atomic_read(&b->pointers[j]) == NULL) {

                        break;

                    }

                    entries++;

                }

                buckets++;

                b = atomic_rcu_read(&b->next);

            } while (b);

        } while (seqlock_read_retry(&head->sequence, version));



        if (entries) {

            qdist_inc(&stats->chain, buckets);

            qdist_inc(&stats->occupancy,

                      (double)entries / QHT_BUCKET_ENTRIES / buckets);

            stats->used_head_buckets++;

            stats->entries += entries;

        } else {

            qdist_inc(&stats->occupancy, 0);

        }

    }

}
","void qht_statistics_init(struct qht *ht, struct qht_stats *stats) {
struct qht_map *map; int i; map = atomic_rcu_read(&ht->map); stats->head_buckets = map->n_buckets; stats->used_head_buckets = 0; stats->entries = 0; qdist_init(&stats->chain); qdist_init(&stats->occupancy); for (i = 0; i < map->n_buckets; i++) {
struct qht_bucket *head = &map->buckets[i]; struct qht_bucket *b; unsigned int version; size_t buckets; size_t entries; int j; do {
version = seqlock_read_begin(&head->sequence); buckets = 0; entries = 0; b = head; do {
for (j = 0; j < QHT_BUCKET_ENTRIES; j++) {
if (atomic_read(&b->pointers[j]) == NULL) {
break; } entries++; } buckets++; b = atomic_rcu_read(&b->next); } while (b); } while (seqlock_read_retry(&head->sequence, version)); if (entries) {
qdist_inc(&stats->chain, buckets); qdist_inc(&stats->occupancy, (double)entries / QHT_BUCKET_ENTRIES / buckets); stats->used_head_buckets++; stats->entries += entries; } else {
qdist_inc(&stats->occupancy, 0); } } } ",qemu,1
"static SocketAddress *nbd_config(BDRVNBDState *s, QDict *options, Error **errp)

{

    SocketAddress *saddr = NULL;

    QDict *addr = NULL;

    QObject *crumpled_addr = NULL;

    Visitor *iv = NULL;

    Error *local_err = NULL;



    qdict_extract_subqdict(options, &addr, ""server."");

    if (!qdict_size(addr)) {

        error_setg(errp, ""NBD server address missing"");

        goto done;

    }



    crumpled_addr = qdict_crumple(addr, errp);

    if (!crumpled_addr) {

        goto done;

    }











    iv = qobject_input_visitor_new(crumpled_addr);

    visit_type_SocketAddress(iv, NULL, &saddr, &local_err);

    if (local_err) {

        error_propagate(errp, local_err);

        goto done;

    }



done:

    QDECREF(addr);

    qobject_decref(crumpled_addr);

    visit_free(iv);

    return saddr;

}","static SocketAddress *nbd_config(BDRVNBDState *s, QDict *options, Error **errp) {
SocketAddress *saddr = NULL; QDict *addr = NULL; QObject *crumpled_addr = NULL; Visitor *iv = NULL; Error *local_err = NULL; qdict_extract_subqdict(options, &addr, ""server.""); if (!qdict_size(addr)) {
error_setg(errp, ""NBD server address missing""); goto done; } crumpled_addr = qdict_crumple(addr, errp); if (!crumpled_addr) {
goto done; } iv = qobject_input_visitor_new(crumpled_addr); visit_type_SocketAddress(iv, NULL, &saddr, &local_err); if (local_err) {
error_propagate(errp, local_err); goto done; } done: QDECREF(addr); qobject_decref(crumpled_addr); visit_free(iv); return saddr; }",qemu,1
"int qemu_fsdev_add(QemuOpts *opts)

{

    int i;

    struct FsDriverListEntry *fsle;

    const char *fsdev_id = qemu_opts_id(opts);

    const char *fsdriver = qemu_opt_get(opts, ""fsdriver"");

    const char *writeout = qemu_opt_get(opts, ""writeout"");

    bool ro = qemu_opt_get_bool(opts, ""readonly"", 0);



    if (!fsdev_id) {

        fprintf(stderr, ""fsdev: No id specified\n"");

        return -1;

    }



    if (fsdriver) {

        for (i = 0; i < ARRAY_SIZE(FsDrivers); i++) {

            if (strcmp(FsDrivers[i].name, fsdriver) == 0) {

                break;

            }

        }



        if (i == ARRAY_SIZE(FsDrivers)) {

            fprintf(stderr, ""fsdev: fsdriver %s not found\n"", fsdriver);

            return -1;

        }

    } else {

        fprintf(stderr, ""fsdev: No fsdriver specified\n"");

        return -1;

    }



    fsle = g_malloc0(sizeof(*fsle));

    fsle->fse.fsdev_id = g_strdup(fsdev_id);

    fsle->fse.ops = FsDrivers[i].ops;

    if (writeout) {

        if (!strcmp(writeout, ""immediate"")) {

            fsle->fse.export_flags |= V9FS_IMMEDIATE_WRITEOUT;

        }

    }

    if (ro) {

        fsle->fse.export_flags |= V9FS_RDONLY;

    } else {

        fsle->fse.export_flags &= ~V9FS_RDONLY;

    }



    if (fsle->fse.ops->parse_opts) {

        if (fsle->fse.ops->parse_opts(opts, &fsle->fse)) {



            return -1;

        }

    }



    QTAILQ_INSERT_TAIL(&fsdriver_entries, fsle, next);

    return 0;

}","int qemu_fsdev_add(QemuOpts *opts) {
int i; struct FsDriverListEntry *fsle; const char *fsdev_id = qemu_opts_id(opts); const char *fsdriver = qemu_opt_get(opts, ""fsdriver""); const char *writeout = qemu_opt_get(opts, ""writeout""); bool ro = qemu_opt_get_bool(opts, ""readonly"", 0); if (!fsdev_id) {
fprintf(stderr, ""fsdev: No id specified\n""); return -1; } if (fsdriver) {
for (i = 0; i < ARRAY_SIZE(FsDrivers); i++) {
if (strcmp(FsDrivers[i].name, fsdriver) == 0) {
break; } } if (i == ARRAY_SIZE(FsDrivers)) {
fprintf(stderr, ""fsdev: fsdriver %s not found\n"", fsdriver); return -1; } } else {
fprintf(stderr, ""fsdev: No fsdriver specified\n""); return -1; } fsle = g_malloc0(sizeof(*fsle)); fsle->fse.fsdev_id = g_strdup(fsdev_id); fsle->fse.ops = FsDrivers[i].ops; if (writeout) {
if (!strcmp(writeout, ""immediate"")) {
fsle->fse.export_flags |= V9FS_IMMEDIATE_WRITEOUT; } } if (ro) {
fsle->fse.export_flags |= V9FS_RDONLY; } else {
fsle->fse.export_flags &= ~V9FS_RDONLY; } if (fsle->fse.ops->parse_opts) {
if (fsle->fse.ops->parse_opts(opts, &fsle->fse)) {
return -1; } } QTAILQ_INSERT_TAIL(&fsdriver_entries, fsle, next); return 0; }",qemu,1
"static int ram_save_page(RAMState *rs, PageSearchStatus *pss, bool last_stage)

{

    int pages = -1;

    uint64_t bytes_xmit;

    ram_addr_t current_addr;

    uint8_t *p;

    int ret;

    bool send_async = true;

    RAMBlock *block = pss->block;

    ram_addr_t offset = pss->page << TARGET_PAGE_BITS;



    p = block->host + offset;

    trace_ram_save_page(block->idstr, (uint64_t)offset, p);



    /* In doubt sent page as normal */

    bytes_xmit = 0;

    ret = ram_control_save_page(rs->f, block->offset,

                           offset, TARGET_PAGE_SIZE, &bytes_xmit);

    if (bytes_xmit) {

        rs->bytes_transferred += bytes_xmit;

        pages = 1;

    }



    XBZRLE_cache_lock();



    current_addr = block->offset + offset;



    if (ret != RAM_SAVE_CONTROL_NOT_SUPP) {

        if (ret != RAM_SAVE_CONTROL_DELAYED) {

            if (bytes_xmit > 0) {

                rs->norm_pages++;

            } else if (bytes_xmit == 0) {

                rs->zero_pages++;

            }

        }

    } else {

        pages = save_zero_page(rs, block, offset, p);

        if (pages > 0) {

            /* Must let xbzrle know, otherwise a previous (now 0'd) cached

             * page would be stale

             */

            xbzrle_cache_zero_page(rs, current_addr);

            ram_release_pages(block->idstr, offset, pages);

        } else if (!rs->ram_bulk_stage &&

                   !migration_in_postcopy() && migrate_use_xbzrle()) {

            pages = save_xbzrle_page(rs, &p, current_addr, block,

                                     offset, last_stage);

            if (!last_stage) {

                /* Can't send this cached data async, since the cache page

                 * might get updated before it gets to the wire

                 */

                send_async = false;

            }

        }

    }



    /* XBZRLE overflow or normal page */

    if (pages == -1) {

        rs->bytes_transferred += save_page_header(rs, block,

                                                  offset | RAM_SAVE_FLAG_PAGE);

        if (send_async) {

            qemu_put_buffer_async(rs->f, p, TARGET_PAGE_SIZE,

                                  migrate_release_ram() &

                                  migration_in_postcopy());

        } else {

            qemu_put_buffer(rs->f, p, TARGET_PAGE_SIZE);

        }

        rs->bytes_transferred += TARGET_PAGE_SIZE;

        pages = 1;

        rs->norm_pages++;

    }



    XBZRLE_cache_unlock();



    return pages;

}
","static int ram_save_page(RAMState *rs, PageSearchStatus *pss, bool last_stage) {
int pages = -1; uint64_t bytes_xmit; ram_addr_t current_addr; uint8_t *p; int ret; bool send_async = true; RAMBlock *block = pss->block; ram_addr_t offset = pss->page << TARGET_PAGE_BITS; p = block->host + offset; trace_ram_save_page(block->idstr, (uint64_t)offset, p); /* In doubt sent page as normal */ bytes_xmit = 0; ret = ram_control_save_page(rs->f, block->offset, offset, TARGET_PAGE_SIZE, &bytes_xmit); if (bytes_xmit) {
rs->bytes_transferred += bytes_xmit; pages = 1; } XBZRLE_cache_lock(); current_addr = block->offset + offset; if (ret != RAM_SAVE_CONTROL_NOT_SUPP) {
if (ret != RAM_SAVE_CONTROL_DELAYED) {
if (bytes_xmit > 0) {
rs->norm_pages++; } else if (bytes_xmit == 0) {
rs->zero_pages++; } } } else {
pages = save_zero_page(rs, block, offset, p); if (pages > 0) {
/* Must let xbzrle know, otherwise a previous (now 0'd) cached * page would be stale */ xbzrle_cache_zero_page(rs, current_addr); ram_release_pages(block->idstr, offset, pages); } else if (!rs->ram_bulk_stage && !migration_in_postcopy() && migrate_use_xbzrle()) {
pages = save_xbzrle_page(rs, &p, current_addr, block, offset, last_stage); if (!last_stage) {
/* Can't send this cached data async, since the cache page * might get updated before it gets to the wire */ send_async = false; } } } /* XBZRLE overflow or normal page */ if (pages == -1) {
rs->bytes_transferred += save_page_header(rs, block, offset | RAM_SAVE_FLAG_PAGE); if (send_async) {
qemu_put_buffer_async(rs->f, p, TARGET_PAGE_SIZE, migrate_release_ram() & migration_in_postcopy()); } else {
qemu_put_buffer(rs->f, p, TARGET_PAGE_SIZE); } rs->bytes_transferred += TARGET_PAGE_SIZE; pages = 1; rs->norm_pages++; } XBZRLE_cache_unlock(); return pages; } ",qemu,1
"NEON_TYPE4(s8, int8_t)
NEON_TYPE4(u8, uint8_t)
NEON_TYPE2(s16, int16_t)
NEON_TYPE2(u16, uint16_t)
NEON_TYPE1(s32, int32_t)
NEON_TYPE1(u32, uint32_t)
#undef NEON_TYPE4
#undef NEON_TYPE2
#undef NEON_TYPE1
/* Copy from a uint32_t to a vector structure type.  */
#define NEON_UNPACK(vtype, dest, val) do { \
    union { \
        vtype v; \
        uint32_t i; \
    } conv_u; \
    conv_u.i = (val); \
    dest = conv_u.v; \
    } while(0)
/* Copy from a vector structure type to a uint32_t.  */
#define NEON_PACK(vtype, dest, val) do { \
    union { \
        vtype v; \
        uint32_t i; \
    } conv_u; \
    conv_u.v = (val); \
    dest = conv_u.i; \
    } while(0)
#define NEON_DO1 \
    NEON_FN(vdest.v1, vsrc1.v1, vsrc2.v1);
#define NEON_DO2 \
    NEON_FN(vdest.v1, vsrc1.v1, vsrc2.v1); \
    NEON_FN(vdest.v2, vsrc1.v2, vsrc2.v2);
#define NEON_DO4 \
    NEON_FN(vdest.v1, vsrc1.v1, vsrc2.v1); \
    NEON_FN(vdest.v2, vsrc1.v2, vsrc2.v2); \
    NEON_FN(vdest.v3, vsrc1.v3, vsrc2.v3); \
    NEON_FN(vdest.v4, vsrc1.v4, vsrc2.v4);
#define NEON_VOP_BODY(vtype, n) \
{ \
    uint32_t res; \
    vtype vsrc1; \
    vtype vsrc2; \
    vtype vdest; \
    NEON_UNPACK(vtype, vsrc1, arg1); \
    NEON_UNPACK(vtype, vsrc2, arg2); \
    NEON_DO##n; \
    NEON_PACK(vtype, res, vdest); \
    return res; \
#define NEON_VOP(name, vtype, n) \
uint32_t HELPER(glue(neon_,name))(uint32_t arg1, uint32_t arg2) \
NEON_VOP_BODY(vtype, n)
#define NEON_VOP_ENV(name, vtype, n) \
uint32_t HELPER(glue(neon_,name))(CPUState *env, uint32_t arg1, uint32_t arg2) \
NEON_VOP_BODY(vtype, n)
/* Pairwise operations.  */
/* For 32-bit elements each segment only contains a single element, so
   the elementwise and pairwise operations are the same.  */
#define NEON_PDO2 \
    NEON_FN(vdest.v1, vsrc1.v1, vsrc1.v2); \
    NEON_FN(vdest.v2, vsrc2.v1, vsrc2.v2);
#define NEON_PDO4 \
    NEON_FN(vdest.v1, vsrc1.v1, vsrc1.v2); \
    NEON_FN(vdest.v2, vsrc1.v3, vsrc1.v4); \
    NEON_FN(vdest.v3, vsrc2.v1, vsrc2.v2); \
    NEON_FN(vdest.v4, vsrc2.v3, vsrc2.v4); \
#define NEON_POP(name, vtype, n) \
uint32_t HELPER(glue(neon_,name))(uint32_t arg1, uint32_t arg2) \
{ \
    uint32_t res; \
    vtype vsrc1; \
    vtype vsrc2; \
    vtype vdest; \
    NEON_UNPACK(vtype, vsrc1, arg1); \
    NEON_UNPACK(vtype, vsrc2, arg2); \
    NEON_PDO##n; \
    NEON_PACK(vtype, res, vdest); \
    return res; \
/* Unary operators.  */
#define NEON_VOP1(name, vtype, n) \
uint32_t HELPER(glue(neon_,name))(uint32_t arg) \
{ \
    vtype vsrc1; \
    vtype vdest; \
    NEON_UNPACK(vtype, vsrc1, arg); \
    NEON_DO##n; \
    NEON_PACK(vtype, arg, vdest); \
    return arg; \
#define NEON_USAT(dest, src1, src2, type) do { \
    uint32_t tmp = (uint32_t)src1 + (uint32_t)src2; \
    if (tmp != (type)tmp) { \
        SET_QC(); \
        dest = ~0; \
    } else { \
        dest = tmp; \
    }} while(0)
#define NEON_FN(dest, src1, src2) NEON_USAT(dest, src1, src2, uint8_t)
NEON_VOP_ENV(qadd_u8, neon_u8, 4)
#undef NEON_FN
#define NEON_FN(dest, src1, src2) NEON_USAT(dest, src1, src2, uint16_t)
NEON_VOP_ENV(qadd_u16, neon_u16, 2)
#undef NEON_FN
#undef NEON_USAT","NEON_TYPE4(s8, int8_t) NEON_TYPE4(u8, uint8_t) NEON_TYPE2(s16, int16_t) NEON_TYPE2(u16, uint16_t) NEON_TYPE1(s32, int32_t) NEON_TYPE1(u32, uint32_t) #undef NEON_TYPE4 #undef NEON_TYPE2 #undef NEON_TYPE1 /* Copy from a uint32_t to a vector structure type. */ #define NEON_UNPACK(vtype, dest, val) do {
\ union {
\ vtype v; \ uint32_t i; \ } conv_u; \ conv_u.i = (val); \ dest = conv_u.v; \ } while(0) /* Copy from a vector structure type to a uint32_t. */ #define NEON_PACK(vtype, dest, val) do {
\ union {
\ vtype v; \ uint32_t i; \ } conv_u; \ conv_u.v = (val); \ dest = conv_u.i; \ } while(0) #define NEON_DO1 \ NEON_FN(vdest.v1, vsrc1.v1, vsrc2.v1); #define NEON_DO2 \ NEON_FN(vdest.v1, vsrc1.v1, vsrc2.v1); \ NEON_FN(vdest.v2, vsrc1.v2, vsrc2.v2); #define NEON_DO4 \ NEON_FN(vdest.v1, vsrc1.v1, vsrc2.v1); \ NEON_FN(vdest.v2, vsrc1.v2, vsrc2.v2); \ NEON_FN(vdest.v3, vsrc1.v3, vsrc2.v3); \ NEON_FN(vdest.v4, vsrc1.v4, vsrc2.v4); #define NEON_VOP_BODY(vtype, n) \ {
\ uint32_t res; \ vtype vsrc1; \ vtype vsrc2; \ vtype vdest; \ NEON_UNPACK(vtype, vsrc1, arg1); \ NEON_UNPACK(vtype, vsrc2, arg2); \ NEON_DO##n; \ NEON_PACK(vtype, res, vdest); \ return res; \ #define NEON_VOP(name, vtype, n) \ uint32_t HELPER(glue(neon_,name))(uint32_t arg1, uint32_t arg2) \ NEON_VOP_BODY(vtype, n) #define NEON_VOP_ENV(name, vtype, n) \ uint32_t HELPER(glue(neon_,name))(CPUState *env, uint32_t arg1, uint32_t arg2) \ NEON_VOP_BODY(vtype, n) /* Pairwise operations. */ /* For 32-bit elements each segment only contains a single element, so the elementwise and pairwise operations are the same. */ #define NEON_PDO2 \ NEON_FN(vdest.v1, vsrc1.v1, vsrc1.v2); \ NEON_FN(vdest.v2, vsrc2.v1, vsrc2.v2); #define NEON_PDO4 \ NEON_FN(vdest.v1, vsrc1.v1, vsrc1.v2); \ NEON_FN(vdest.v2, vsrc1.v3, vsrc1.v4); \ NEON_FN(vdest.v3, vsrc2.v1, vsrc2.v2); \ NEON_FN(vdest.v4, vsrc2.v3, vsrc2.v4); \ #define NEON_POP(name, vtype, n) \ uint32_t HELPER(glue(neon_,name))(uint32_t arg1, uint32_t arg2) \ {
\ uint32_t res; \ vtype vsrc1; \ vtype vsrc2; \ vtype vdest; \ NEON_UNPACK(vtype, vsrc1, arg1); \ NEON_UNPACK(vtype, vsrc2, arg2); \ NEON_PDO##n; \ NEON_PACK(vtype, res, vdest); \ return res; \ /* Unary operators. */ #define NEON_VOP1(name, vtype, n) \ uint32_t HELPER(glue(neon_,name))(uint32_t arg) \ {
\ vtype vsrc1; \ vtype vdest; \ NEON_UNPACK(vtype, vsrc1, arg); \ NEON_DO##n; \ NEON_PACK(vtype, arg, vdest); \ return arg; \ #define NEON_USAT(dest, src1, src2, type) do {
\ uint32_t tmp = (uint32_t)src1 + (uint32_t)src2; \ if (tmp != (type)tmp) {
\ SET_QC(); \ dest = ~0; \ } else {
\ dest = tmp; \ }} while(0) #define NEON_FN(dest, src1, src2) NEON_USAT(dest, src1, src2, uint8_t) NEON_VOP_ENV(qadd_u8, neon_u8, 4) #undef NEON_FN #define NEON_FN(dest, src1, src2) NEON_USAT(dest, src1, src2, uint16_t) NEON_VOP_ENV(qadd_u16, neon_u16, 2) #undef NEON_FN #undef NEON_USAT",qemu,1
"void qemu_cpu_kick(CPUState *cpu)

{

    qemu_cond_broadcast(cpu->halt_cond);

    if (tcg_enabled()) {

        cpu_exit(cpu);

        /* Also ensure current RR cpu is kicked */

        qemu_cpu_kick_rr_cpu();

    } else {

        if (hax_enabled()) {

            /*

             * FIXME: race condition with the exit_request check in

             * hax_vcpu_hax_exec

             */

            cpu->exit_request = 1;

        }

        qemu_cpu_kick_thread(cpu);

    }

}
","void qemu_cpu_kick(CPUState *cpu) {
qemu_cond_broadcast(cpu->halt_cond); if (tcg_enabled()) {
cpu_exit(cpu); /* Also ensure current RR cpu is kicked */ qemu_cpu_kick_rr_cpu(); } else {
if (hax_enabled()) {
/* * FIXME: race condition with the exit_request check in * hax_vcpu_hax_exec */ cpu->exit_request = 1; } qemu_cpu_kick_thread(cpu); } } ",qemu,1
"static inline int usb_bt_fifo_dequeue(struct usb_hci_in_fifo_s *fifo,

                USBPacket *p)

{

    int len;



    if (likely(!fifo->len))

        return USB_RET_STALL;



    len = MIN(p->len, fifo->fifo[fifo->start].len);

    memcpy(p->data, fifo->fifo[fifo->start].data, len);

    if (len == p->len) {

        fifo->fifo[fifo->start].len -= len;

        fifo->fifo[fifo->start].data += len;

    } else {

        fifo->start ++;

        fifo->start &= CFIFO_LEN_MASK;

        fifo->len --;

    }



    fifo->dstart += len;

    fifo->dlen -= len;

    if (fifo->dstart >= fifo->dsize) {

        fifo->dstart = 0;

        fifo->dsize = DFIFO_LEN_MASK + 1;

    }



    return len;

}
","static inline int usb_bt_fifo_dequeue(struct usb_hci_in_fifo_s *fifo, USBPacket *p) {
int len; if (likely(!fifo->len)) return USB_RET_STALL; len = MIN(p->len, fifo->fifo[fifo->start].len); memcpy(p->data, fifo->fifo[fifo->start].data, len); if (len == p->len) {
fifo->fifo[fifo->start].len -= len; fifo->fifo[fifo->start].data += len; } else {
fifo->start ++; fifo->start &= CFIFO_LEN_MASK; fifo->len --; } fifo->dstart += len; fifo->dlen -= len; if (fifo->dstart >= fifo->dsize) {
fifo->dstart = 0; fifo->dsize = DFIFO_LEN_MASK + 1; } return len; } ",qemu,1
"static int get_pci_config_device(QEMUFile *f, void *pv, size_t size)

{

    PCIDevice *s = container_of(pv, PCIDevice, config);

    uint8_t *config;

    int i;



    assert(size == pci_config_size(s));

    config = qemu_malloc(size);



    qemu_get_buffer(f, config, size);

    for (i = 0; i < size; ++i) {

        if ((config[i] ^ s->config[i]) & s->cmask[i] & ~s->wmask[i]) {

            qemu_free(config);

            return -EINVAL;

        }

    }

    memcpy(s->config, config, size);



    pci_update_mappings(s);



    qemu_free(config);

    return 0;

}
","static int get_pci_config_device(QEMUFile *f, void *pv, size_t size) {
PCIDevice *s = container_of(pv, PCIDevice, config); uint8_t *config; int i; assert(size == pci_config_size(s)); config = qemu_malloc(size); qemu_get_buffer(f, config, size); for (i = 0; i < size; ++i) {
if ((config[i] ^ s->config[i]) & s->cmask[i] & ~s->wmask[i]) {
qemu_free(config); return -EINVAL; } } memcpy(s->config, config, size); pci_update_mappings(s); qemu_free(config); return 0; } ",qemu,0
"void tcg_prologue_init(TCGContext *s)

{

    size_t prologue_size, total_size;

    void *buf0, *buf1;



    /* Put the prologue at the beginning of code_gen_buffer.  */

    buf0 = s->code_gen_buffer;

    s->code_ptr = buf0;

    s->code_buf = buf0;

    s->code_gen_prologue = buf0;



    /* Generate the prologue.  */

    tcg_target_qemu_prologue(s);

    buf1 = s->code_ptr;

    flush_icache_range((uintptr_t)buf0, (uintptr_t)buf1);



    /* Deduct the prologue from the buffer.  */

    prologue_size = tcg_current_code_size(s);

    s->code_gen_ptr = buf1;

    s->code_gen_buffer = buf1;

    s->code_buf = buf1;

    total_size = s->code_gen_buffer_size - prologue_size;

    s->code_gen_buffer_size = total_size;



    /* Compute a high-water mark, at which we voluntarily flush the buffer

       and start over.  The size here is arbitrary, significantly larger

       than we expect the code generation for any one opcode to require.  */

    /* ??? We currently have no good estimate for, or checks in,

       tcg_out_tb_finalize.  If there are quite a lot of guest memory ops,

       the number of out-of-line fragments could be quite high.  In the

       short-term, increase the highwater buffer.  */

    s->code_gen_highwater = s->code_gen_buffer + (total_size - 64*1024);



    tcg_register_jit(s->code_gen_buffer, total_size);



#ifdef DEBUG_DISAS

    if (qemu_loglevel_mask(CPU_LOG_TB_OUT_ASM)) {

        qemu_log(""PROLOGUE: [size=%zu]\n"", prologue_size);

        log_disas(buf0, prologue_size);

        qemu_log(""\n"");

        qemu_log_flush();

    }

#endif

}
","void tcg_prologue_init(TCGContext *s) {
size_t prologue_size, total_size; void *buf0, *buf1; /* Put the prologue at the beginning of code_gen_buffer. */ buf0 = s->code_gen_buffer; s->code_ptr = buf0; s->code_buf = buf0; s->code_gen_prologue = buf0; /* Generate the prologue. */ tcg_target_qemu_prologue(s); buf1 = s->code_ptr; flush_icache_range((uintptr_t)buf0, (uintptr_t)buf1); /* Deduct the prologue from the buffer. */ prologue_size = tcg_current_code_size(s); s->code_gen_ptr = buf1; s->code_gen_buffer = buf1; s->code_buf = buf1; total_size = s->code_gen_buffer_size - prologue_size; s->code_gen_buffer_size = total_size; /* Compute a high-water mark, at which we voluntarily flush the buffer and start over. The size here is arbitrary, significantly larger than we expect the code generation for any one opcode to require. */ /* ??? We currently have no good estimate for, or checks in, tcg_out_tb_finalize. If there are quite a lot of guest memory ops, the number of out-of-line fragments could be quite high. In the short-term, increase the highwater buffer. */ s->code_gen_highwater = s->code_gen_buffer + (total_size - 64*1024); tcg_register_jit(s->code_gen_buffer, total_size); #ifdef DEBUG_DISAS if (qemu_loglevel_mask(CPU_LOG_TB_OUT_ASM)) {
qemu_log(""PROLOGUE: [size=%zu]\n"", prologue_size); log_disas(buf0, prologue_size); qemu_log(""\n""); qemu_log_flush(); } #endif } ",qemu,0
"static void qobject_input_type_number(Visitor *v, const char *name, double *obj,

                                      Error **errp)

{

    QObjectInputVisitor *qiv = to_qiv(v);

    QObject *qobj = qobject_input_get_object(qiv, name, true, errp);

    QInt *qint;

    QFloat *qfloat;



    if (!qobj) {

        return;

    }

    qint = qobject_to_qint(qobj);

    if (qint) {

        *obj = qint_get_int(qobject_to_qint(qobj));

        return;

    }



    qfloat = qobject_to_qfloat(qobj);

    if (qfloat) {

        *obj = qfloat_get_double(qobject_to_qfloat(qobj));

        return;

    }



    error_setg(errp, QERR_INVALID_PARAMETER_TYPE, name ? name : ""null"",

               ""number"");

}
","static void qobject_input_type_number(Visitor *v, const char *name, double *obj, Error **errp) {
QObjectInputVisitor *qiv = to_qiv(v); QObject *qobj = qobject_input_get_object(qiv, name, true, errp); QInt *qint; QFloat *qfloat; if (!qobj) {
return; } qint = qobject_to_qint(qobj); if (qint) {
*obj = qint_get_int(qobject_to_qint(qobj)); return; } qfloat = qobject_to_qfloat(qobj); if (qfloat) {
*obj = qfloat_get_double(qobject_to_qfloat(qobj)); return; } error_setg(errp, QERR_INVALID_PARAMETER_TYPE, name ? name : ""null"", ""number""); } ",qemu,1
"int kvm_arch_post_run(CPUState *env, struct kvm_run *run)

{

    return 0;

}
","int kvm_arch_post_run(CPUState *env, struct kvm_run *run) {
return 0; } ",qemu,0
"static void gen_spr_405 (CPUPPCState *env)

{

    spr_register(env, SPR_4xx_CCR0, ""CCR0"",

                 SPR_NOACCESS, SPR_NOACCESS,

                 &spr_read_generic, &spr_write_generic,

                 0x00700000);

    /* Debug */

    /* XXX : not implemented */

    spr_register(env, SPR_405_DBCR1, ""DBCR1"",

                 SPR_NOACCESS, SPR_NOACCESS,

                 &spr_read_generic, &spr_write_generic,

                 0x00000000);

    /* XXX : not implemented */

    spr_register(env, SPR_405_DVC1, ""DVC1"",

                 SPR_NOACCESS, SPR_NOACCESS,

                 &spr_read_generic, &spr_write_generic,

                 0x00000000);

    /* XXX : not implemented */

    spr_register(env, SPR_405_DVC2, ""DVC2"",

                 SPR_NOACCESS, SPR_NOACCESS,

                 &spr_read_generic, &spr_write_generic,

                 0x00000000);

    /* XXX : not implemented */

    spr_register(env, SPR_405_IAC3, ""IAC3"",

                 SPR_NOACCESS, SPR_NOACCESS,

                 &spr_read_generic, &spr_write_generic,

                 0x00000000);

    /* XXX : not implemented */

    spr_register(env, SPR_405_IAC4, ""IAC4"",

                 SPR_NOACCESS, SPR_NOACCESS,

                 &spr_read_generic, &spr_write_generic,

                 0x00000000);

    /* Storage control */

    spr_register(env, SPR_405_SLER, ""SLER"",

                 SPR_NOACCESS, SPR_NOACCESS,

                 &spr_read_generic, &spr_write_40x_sler,

                 0x00000000);

    /* XXX : not implemented */

    spr_register(env, SPR_405_SU0R, ""SU0R"",

                 SPR_NOACCESS, SPR_NOACCESS,

                 &spr_read_generic, &spr_write_generic,

                 0x00000000);

    /* SPRG */

    spr_register(env, SPR_USPRG0, ""USPRG0"",

                 &spr_read_ureg, SPR_NOACCESS,

                 &spr_read_ureg, SPR_NOACCESS,

                 0x00000000);

    spr_register(env, SPR_SPRG4, ""SPRG4"",

                 SPR_NOACCESS, SPR_NOACCESS,

                 &spr_read_generic, &spr_write_generic,

                 0x00000000);

    spr_register(env, SPR_USPRG4, ""USPRG4"",

                 &spr_read_ureg, SPR_NOACCESS,

                 &spr_read_ureg, SPR_NOACCESS,

                 0x00000000);

    spr_register(env, SPR_SPRG5, ""SPRG5"",

                 SPR_NOACCESS, SPR_NOACCESS,

                 spr_read_generic, &spr_write_generic,

                 0x00000000);

    spr_register(env, SPR_USPRG5, ""USPRG5"",

                 &spr_read_ureg, SPR_NOACCESS,

                 &spr_read_ureg, SPR_NOACCESS,

                 0x00000000);

    spr_register(env, SPR_SPRG6, ""SPRG6"",

                 SPR_NOACCESS, SPR_NOACCESS,

                 spr_read_generic, &spr_write_generic,

                 0x00000000);

    spr_register(env, SPR_USPRG6, ""USPRG6"",

                 &spr_read_ureg, SPR_NOACCESS,

                 &spr_read_ureg, SPR_NOACCESS,

                 0x00000000);

    spr_register(env, SPR_SPRG7, ""SPRG7"",

                 SPR_NOACCESS, SPR_NOACCESS,

                 spr_read_generic, &spr_write_generic,

                 0x00000000);

    spr_register(env, SPR_USPRG7, ""USPRG7"",

                 &spr_read_ureg, SPR_NOACCESS,

                 &spr_read_ureg, SPR_NOACCESS,

                 0x00000000);

}
","static void gen_spr_405 (CPUPPCState *env) {
spr_register(env, SPR_4xx_CCR0, ""CCR0"", SPR_NOACCESS, SPR_NOACCESS, &spr_read_generic, &spr_write_generic, 0x00700000); /* Debug */ /* XXX : not implemented */ spr_register(env, SPR_405_DBCR1, ""DBCR1"", SPR_NOACCESS, SPR_NOACCESS, &spr_read_generic, &spr_write_generic, 0x00000000); /* XXX : not implemented */ spr_register(env, SPR_405_DVC1, ""DVC1"", SPR_NOACCESS, SPR_NOACCESS, &spr_read_generic, &spr_write_generic, 0x00000000); /* XXX : not implemented */ spr_register(env, SPR_405_DVC2, ""DVC2"", SPR_NOACCESS, SPR_NOACCESS, &spr_read_generic, &spr_write_generic, 0x00000000); /* XXX : not implemented */ spr_register(env, SPR_405_IAC3, ""IAC3"", SPR_NOACCESS, SPR_NOACCESS, &spr_read_generic, &spr_write_generic, 0x00000000); /* XXX : not implemented */ spr_register(env, SPR_405_IAC4, ""IAC4"", SPR_NOACCESS, SPR_NOACCESS, &spr_read_generic, &spr_write_generic, 0x00000000); /* Storage control */ spr_register(env, SPR_405_SLER, ""SLER"", SPR_NOACCESS, SPR_NOACCESS, &spr_read_generic, &spr_write_40x_sler, 0x00000000); /* XXX : not implemented */ spr_register(env, SPR_405_SU0R, ""SU0R"", SPR_NOACCESS, SPR_NOACCESS, &spr_read_generic, &spr_write_generic, 0x00000000); /* SPRG */ spr_register(env, SPR_USPRG0, ""USPRG0"", &spr_read_ureg, SPR_NOACCESS, &spr_read_ureg, SPR_NOACCESS, 0x00000000); spr_register(env, SPR_SPRG4, ""SPRG4"", SPR_NOACCESS, SPR_NOACCESS, &spr_read_generic, &spr_write_generic, 0x00000000); spr_register(env, SPR_USPRG4, ""USPRG4"", &spr_read_ureg, SPR_NOACCESS, &spr_read_ureg, SPR_NOACCESS, 0x00000000); spr_register(env, SPR_SPRG5, ""SPRG5"", SPR_NOACCESS, SPR_NOACCESS, spr_read_generic, &spr_write_generic, 0x00000000); spr_register(env, SPR_USPRG5, ""USPRG5"", &spr_read_ureg, SPR_NOACCESS, &spr_read_ureg, SPR_NOACCESS, 0x00000000); spr_register(env, SPR_SPRG6, ""SPRG6"", SPR_NOACCESS, SPR_NOACCESS, spr_read_generic, &spr_write_generic, 0x00000000); spr_register(env, SPR_USPRG6, ""USPRG6"", &spr_read_ureg, SPR_NOACCESS, &spr_read_ureg, SPR_NOACCESS, 0x00000000); spr_register(env, SPR_SPRG7, ""SPRG7"", SPR_NOACCESS, SPR_NOACCESS, spr_read_generic, &spr_write_generic, 0x00000000); spr_register(env, SPR_USPRG7, ""USPRG7"", &spr_read_ureg, SPR_NOACCESS, &spr_read_ureg, SPR_NOACCESS, 0x00000000); } ",qemu,0
"static int v9fs_do_symlink(V9fsState *s, V9fsString *oldpath,

                            V9fsString *newpath)

{

    return s->ops->symlink(&s->ctx, oldpath->data, newpath->data);

}
","static int v9fs_do_symlink(V9fsState *s, V9fsString *oldpath, V9fsString *newpath) {
return s->ops->symlink(&s->ctx, oldpath->data, newpath->data); } ",qemu,0
"unsigned int EmulateAll(unsigned int opcode, FPA11* qfpa, CPUARMState* qregs)

{

  unsigned int nRc = 0;

//  unsigned long flags;

  FPA11 *fpa11;

//  save_flags(flags); sti();



  qemufpa=qfpa;

  user_registers=qregs;



#if 0

  fprintf(stderr,""emulating FP insn 0x%08x, PC=0x%08x\n"",

          opcode, qregs[REG_PC]);

#endif

  fpa11 = GET_FPA11();



  if (fpa11->initflag == 0)		/* good place for __builtin_expect */

  {

    resetFPA11();

    SetRoundingMode(ROUND_TO_NEAREST);

    SetRoundingPrecision(ROUND_EXTENDED);

    fpa11->initflag = 1;

  }



  set_float_exception_flags(0, &fpa11->fp_status);



  if (TEST_OPCODE(opcode,MASK_CPRT))

  {

    //fprintf(stderr,""emulating CPRT\n"");

    /* Emulate conversion opcodes. */

    /* Emulate register transfer opcodes. */

    /* Emulate comparison opcodes. */

    nRc = EmulateCPRT(opcode);

  }

  else if (TEST_OPCODE(opcode,MASK_CPDO))

  {

    //fprintf(stderr,""emulating CPDO\n"");

    /* Emulate monadic arithmetic opcodes. */

    /* Emulate dyadic arithmetic opcodes. */

    nRc = EmulateCPDO(opcode);

  }

  else if (TEST_OPCODE(opcode,MASK_CPDT))

  {

    //fprintf(stderr,""emulating CPDT\n"");

    /* Emulate load/store opcodes. */

    /* Emulate load/store multiple opcodes. */

    nRc = EmulateCPDT(opcode);

  }

  else

  {

    /* Invalid instruction detected.  Return FALSE. */

    nRc = 0;

  }



//  restore_flags(flags);

  if(nRc == 1 && get_float_exception_flags(&fpa11->fp_status))

  {

    //printf(""fef 0x%x\n"",float_exception_flags);

    nRc -= get_float_exception_flags(&fpa11->fp_status);

  }



  //printf(""returning %d\n"",nRc);

  return(nRc);

}
","unsigned int EmulateAll(unsigned int opcode, FPA11* qfpa, CPUARMState* qregs) {
unsigned int nRc = 0; // unsigned long flags; FPA11 *fpa11; // save_flags(flags); sti(); qemufpa=qfpa; user_registers=qregs; #if 0 fprintf(stderr,""emulating FP insn 0x%08x, PC=0x%08x\n"", opcode, qregs[REG_PC]); #endif fpa11 = GET_FPA11(); if (fpa11->initflag == 0) /* good place for __builtin_expect */ {
resetFPA11(); SetRoundingMode(ROUND_TO_NEAREST); SetRoundingPrecision(ROUND_EXTENDED); fpa11->initflag = 1; } set_float_exception_flags(0, &fpa11->fp_status); if (TEST_OPCODE(opcode,MASK_CPRT)) {
//fprintf(stderr,""emulating CPRT\n""); /* Emulate conversion opcodes. */ /* Emulate register transfer opcodes. */ /* Emulate comparison opcodes. */ nRc = EmulateCPRT(opcode); } else if (TEST_OPCODE(opcode,MASK_CPDO)) {
//fprintf(stderr,""emulating CPDO\n""); /* Emulate monadic arithmetic opcodes. */ /* Emulate dyadic arithmetic opcodes. */ nRc = EmulateCPDO(opcode); } else if (TEST_OPCODE(opcode,MASK_CPDT)) {
//fprintf(stderr,""emulating CPDT\n""); /* Emulate load/store opcodes. */ /* Emulate load/store multiple opcodes. */ nRc = EmulateCPDT(opcode); } else {
/* Invalid instruction detected. Return FALSE. */ nRc = 0; } // restore_flags(flags); if(nRc == 1 && get_float_exception_flags(&fpa11->fp_status)) {
//printf(""fef 0x%x\n"",float_exception_flags); nRc -= get_float_exception_flags(&fpa11->fp_status); } //printf(""returning %d\n"",nRc); return(nRc); } ",qemu,0
"static void qpa_fini_in (HWVoiceIn *hw)

{

    void *ret;

    PAVoiceIn *pa = (PAVoiceIn *) hw;



    audio_pt_lock (&pa->pt, AUDIO_FUNC);

    pa->done = 1;

    audio_pt_unlock_and_signal (&pa->pt, AUDIO_FUNC);

    audio_pt_join (&pa->pt, &ret, AUDIO_FUNC);



    if (pa->s) {

        pa_simple_free (pa->s);

        pa->s = NULL;

    }



    audio_pt_fini (&pa->pt, AUDIO_FUNC);

    g_free (pa->pcm_buf);

    pa->pcm_buf = NULL;

}
","static void qpa_fini_in (HWVoiceIn *hw) {
void *ret; PAVoiceIn *pa = (PAVoiceIn *) hw; audio_pt_lock (&pa->pt, AUDIO_FUNC); pa->done = 1; audio_pt_unlock_and_signal (&pa->pt, AUDIO_FUNC); audio_pt_join (&pa->pt, &ret, AUDIO_FUNC); if (pa->s) {
pa_simple_free (pa->s); pa->s = NULL; } audio_pt_fini (&pa->pt, AUDIO_FUNC); g_free (pa->pcm_buf); pa->pcm_buf = NULL; } ",qemu,1
"void ppc_store_sdr1(CPUPPCState *env, target_ulong value)

{

    LOG_MMU(""%s: "" TARGET_FMT_lx ""\n"", __func__, value);

    if (env->spr[SPR_SDR1] != value) {

        env->spr[SPR_SDR1] = value;

#if defined(TARGET_PPC64)

        if (env->mmu_model & POWERPC_MMU_64) {

            target_ulong htabsize = value & SDR_64_HTABSIZE;



            if (htabsize > 28) {

                fprintf(stderr, ""Invalid HTABSIZE 0x"" TARGET_FMT_lx

                        "" stored in SDR1\n"", htabsize);

                htabsize = 28;

            }

            env->htab_mask = (1ULL << (htabsize + 18)) - 1;

            env->htab_base = value & SDR_64_HTABORG;

        } else

#endif /* defined(TARGET_PPC64) */

        {

            /* FIXME: Should check for valid HTABMASK values */

            env->htab_mask = ((value & SDR_32_HTABMASK) << 16) | 0xFFFF;

            env->htab_base = value & SDR_32_HTABORG;

        }

        tlb_flush(env, 1);

    }

}
","void ppc_store_sdr1(CPUPPCState *env, target_ulong value) {
LOG_MMU(""%s: "" TARGET_FMT_lx ""\n"", __func__, value); if (env->spr[SPR_SDR1] != value) {
env->spr[SPR_SDR1] = value; #if defined(TARGET_PPC64) if (env->mmu_model & POWERPC_MMU_64) {
target_ulong htabsize = value & SDR_64_HTABSIZE; if (htabsize > 28) {
fprintf(stderr, ""Invalid HTABSIZE 0x"" TARGET_FMT_lx "" stored in SDR1\n"", htabsize); htabsize = 28; } env->htab_mask = (1ULL << (htabsize + 18)) - 1; env->htab_base = value & SDR_64_HTABORG; } else #endif /* defined(TARGET_PPC64) */ {
/* FIXME: Should check for valid HTABMASK values */ env->htab_mask = ((value & SDR_32_HTABMASK) << 16) | 0xFFFF; env->htab_base = value & SDR_32_HTABORG; } tlb_flush(env, 1); } } ",qemu,1
"uint64_t float64_to_uint64_round_to_zero (float64 a STATUS_PARAM)

{

    int64_t v;



    v = int64_to_float64(INT64_MIN STATUS_VAR);

    v = float64_to_int64_round_to_zero((a + v) STATUS_VAR);



    return v - INT64_MIN;

}
","uint64_t float64_to_uint64_round_to_zero (float64 a STATUS_PARAM) {
int64_t v; v = int64_to_float64(INT64_MIN STATUS_VAR); v = float64_to_int64_round_to_zero((a + v) STATUS_VAR); return v - INT64_MIN; } ",qemu,0
"static void minimac2_cleanup(NetClientState *nc)

{

    MilkymistMinimac2State *s = qemu_get_nic_opaque(nc);



    s->nic = NULL;

}
","static void minimac2_cleanup(NetClientState *nc) {
MilkymistMinimac2State *s = qemu_get_nic_opaque(nc); s->nic = NULL; } ",qemu,0
"uint32_t helper_efdctui (uint64_t val)

{

    CPU_DoubleU u;



    u.ll = val;

    /* NaN are not treated the same way IEEE 754 does */

    if (unlikely(float64_is_nan(u.d)))

        return 0;



    return float64_to_uint32(u.d, &env->vec_status);

}
","uint32_t helper_efdctui (uint64_t val) {
CPU_DoubleU u; u.ll = val; /* NaN are not treated the same way IEEE 754 does */ if (unlikely(float64_is_nan(u.d))) return 0; return float64_to_uint32(u.d, &env->vec_status); } ",qemu,0
"void virtqueue_discard(VirtQueue *vq, const VirtQueueElement *elem,

                       unsigned int len)

{

    vq->last_avail_idx--;


    virtqueue_unmap_sg(vq, elem, len);

}","void virtqueue_discard(VirtQueue *vq, const VirtQueueElement *elem, unsigned int len) {
vq->last_avail_idx--; virtqueue_unmap_sg(vq, elem, len); }",qemu,1
"static MemTxResult memory_region_oldmmio_write_accessor(MemoryRegion *mr,

                                                        hwaddr addr,

                                                        uint64_t *value,

                                                        unsigned size,

                                                        unsigned shift,

                                                        uint64_t mask,

                                                        MemTxAttrs attrs)

{

    uint64_t tmp;



    tmp = (*value >> shift) & mask;

    if (mr->subpage) {

        trace_memory_region_subpage_write(get_cpu_index(), mr, addr, tmp, size);






    } else if (TRACE_MEMORY_REGION_OPS_WRITE_ENABLED) {

        hwaddr abs_addr = memory_region_to_absolute_addr(mr, addr);

        trace_memory_region_ops_write(get_cpu_index(), mr, abs_addr, tmp, size);

    }

    mr->ops->old_mmio.write[ctz32(size)](mr->opaque, addr, tmp);

    return MEMTX_OK;

}","static MemTxResult memory_region_oldmmio_write_accessor(MemoryRegion *mr, hwaddr addr, uint64_t *value, unsigned size, unsigned shift, uint64_t mask, MemTxAttrs attrs) {
uint64_t tmp; tmp = (*value >> shift) & mask; if (mr->subpage) {
trace_memory_region_subpage_write(get_cpu_index(), mr, addr, tmp, size); } else if (TRACE_MEMORY_REGION_OPS_WRITE_ENABLED) {
hwaddr abs_addr = memory_region_to_absolute_addr(mr, addr); trace_memory_region_ops_write(get_cpu_index(), mr, abs_addr, tmp, size); } mr->ops->old_mmio.write[ctz32(size)](mr->opaque, addr, tmp); return MEMTX_OK; }",qemu,1
"static int parse_uint64(DeviceState *dev, Property *prop, const char *str)

{

    uint64_t *ptr = qdev_get_prop_ptr(dev, prop);

    char *end;



    /* accept both hex and decimal */

    *ptr = strtoull(str, &end, 0);

    if ((*end != '\0') || (end == str)) {

        return -EINVAL;

    }



    return 0;

}
","static int parse_uint64(DeviceState *dev, Property *prop, const char *str) {
uint64_t *ptr = qdev_get_prop_ptr(dev, prop); char *end; /* accept both hex and decimal */ *ptr = strtoull(str, &end, 0); if ((*end != '\0') || (end == str)) {
return -EINVAL; } return 0; } ",qemu,1
"petalogix_ml605_init(MachineState *machine)

{

    ram_addr_t ram_size = machine->ram_size;

    MemoryRegion *address_space_mem = get_system_memory();

    DeviceState *dev, *dma, *eth0;

    Object *ds, *cs;

    MicroBlazeCPU *cpu;

    SysBusDevice *busdev;

    DriveInfo *dinfo;

    int i;

    MemoryRegion *phys_lmb_bram = g_new(MemoryRegion, 1);

    MemoryRegion *phys_ram = g_new(MemoryRegion, 1);

    qemu_irq irq[32];



    /* init CPUs */

    cpu = MICROBLAZE_CPU(object_new(TYPE_MICROBLAZE_CPU));

    object_property_set_bool(OBJECT(cpu), true, ""realized"", &error_abort);



    /* Attach emulated BRAM through the LMB.  */

    memory_region_init_ram(phys_lmb_bram, NULL, ""petalogix_ml605.lmb_bram"",

                           LMB_BRAM_SIZE, &error_abort);

    vmstate_register_ram_global(phys_lmb_bram);

    memory_region_add_subregion(address_space_mem, 0x00000000, phys_lmb_bram);



    memory_region_init_ram(phys_ram, NULL, ""petalogix_ml605.ram"", ram_size,

                           &error_abort);

    vmstate_register_ram_global(phys_ram);

    memory_region_add_subregion(address_space_mem, MEMORY_BASEADDR, phys_ram);



    dinfo = drive_get(IF_PFLASH, 0, 0);

    /* 5th parameter 2 means bank-width

     * 10th paremeter 0 means little-endian */

    pflash_cfi01_register(FLASH_BASEADDR,

                          NULL, ""petalogix_ml605.flash"", FLASH_SIZE,

                          dinfo ? blk_bs(blk_by_legacy_dinfo(dinfo)) : NULL,

                          (64 * 1024), FLASH_SIZE >> 16,

                          2, 0x89, 0x18, 0x0000, 0x0, 0);





    dev = qdev_create(NULL, ""xlnx.xps-intc"");

    qdev_prop_set_uint32(dev, ""kind-of-intr"", 1 << TIMER_IRQ);

    qdev_init_nofail(dev);

    sysbus_mmio_map(SYS_BUS_DEVICE(dev), 0, INTC_BASEADDR);

    sysbus_connect_irq(SYS_BUS_DEVICE(dev), 0,

                       qdev_get_gpio_in(DEVICE(cpu), MB_CPU_IRQ));

    for (i = 0; i < 32; i++) {

        irq[i] = qdev_get_gpio_in(dev, i);

    }



    serial_mm_init(address_space_mem, UART16550_BASEADDR + 0x1000, 2,

                   irq[UART16550_IRQ], 115200, serial_hds[0],

                   DEVICE_LITTLE_ENDIAN);



    /* 2 timers at irq 2 @ 100 Mhz.  */

    dev = qdev_create(NULL, ""xlnx.xps-timer"");

    qdev_prop_set_uint32(dev, ""one-timer-only"", 0);

    qdev_prop_set_uint32(dev, ""clock-frequency"", 100 * 1000000);

    qdev_init_nofail(dev);

    sysbus_mmio_map(SYS_BUS_DEVICE(dev), 0, TIMER_BASEADDR);

    sysbus_connect_irq(SYS_BUS_DEVICE(dev), 0, irq[TIMER_IRQ]);



    /* axi ethernet and dma initialization. */

    qemu_check_nic_model(&nd_table[0], ""xlnx.axi-ethernet"");

    eth0 = qdev_create(NULL, ""xlnx.axi-ethernet"");

    dma = qdev_create(NULL, ""xlnx.axi-dma"");



    /* FIXME: attach to the sysbus instead */

    object_property_add_child(qdev_get_machine(), ""xilinx-eth"", OBJECT(eth0),

                              NULL);

    object_property_add_child(qdev_get_machine(), ""xilinx-dma"", OBJECT(dma),

                              NULL);



    ds = object_property_get_link(OBJECT(dma),

                                  ""axistream-connected-target"", NULL);

    cs = object_property_get_link(OBJECT(dma),

                                  ""axistream-control-connected-target"", NULL);

    qdev_set_nic_properties(eth0, &nd_table[0]);

    qdev_prop_set_uint32(eth0, ""rxmem"", 0x1000);

    qdev_prop_set_uint32(eth0, ""txmem"", 0x1000);

    object_property_set_link(OBJECT(eth0), OBJECT(ds),

                             ""axistream-connected"", &error_abort);

    object_property_set_link(OBJECT(eth0), OBJECT(cs),

                             ""axistream-control-connected"", &error_abort);

    qdev_init_nofail(eth0);

    sysbus_mmio_map(SYS_BUS_DEVICE(eth0), 0, AXIENET_BASEADDR);

    sysbus_connect_irq(SYS_BUS_DEVICE(eth0), 0, irq[AXIENET_IRQ]);



    ds = object_property_get_link(OBJECT(eth0),

                                  ""axistream-connected-target"", NULL);

    cs = object_property_get_link(OBJECT(eth0),

                                  ""axistream-control-connected-target"", NULL);

    qdev_prop_set_uint32(dma, ""freqhz"", 100 * 1000000);

    object_property_set_link(OBJECT(dma), OBJECT(ds),

                             ""axistream-connected"", &error_abort);

    object_property_set_link(OBJECT(dma), OBJECT(cs),

                             ""axistream-control-connected"", &error_abort);

    qdev_init_nofail(dma);

    sysbus_mmio_map(SYS_BUS_DEVICE(dma), 0, AXIDMA_BASEADDR);

    sysbus_connect_irq(SYS_BUS_DEVICE(dma), 0, irq[AXIDMA_IRQ0]);

    sysbus_connect_irq(SYS_BUS_DEVICE(dma), 1, irq[AXIDMA_IRQ1]);



    {

        SSIBus *spi;



        dev = qdev_create(NULL, ""xlnx.xps-spi"");

        qdev_prop_set_uint8(dev, ""num-ss-bits"", NUM_SPI_FLASHES);

        qdev_init_nofail(dev);

        busdev = SYS_BUS_DEVICE(dev);

        sysbus_mmio_map(busdev, 0, SPI_BASEADDR);

        sysbus_connect_irq(busdev, 0, irq[SPI_IRQ]);



        spi = (SSIBus *)qdev_get_child_bus(dev, ""spi"");



        for (i = 0; i < NUM_SPI_FLASHES; i++) {

            qemu_irq cs_line;



            dev = ssi_create_slave(spi, ""n25q128"");

            cs_line = qdev_get_gpio_in_named(dev, SSI_GPIO_CS, 0);

            sysbus_connect_irq(busdev, i+1, cs_line);

        }

    }



    microblaze_load_kernel(cpu, MEMORY_BASEADDR, ram_size,

                           machine->initrd_filename,

                           BINARY_DEVICE_TREE_FILE,

                           machine_cpu_reset);



}
","petalogix_ml605_init(MachineState *machine) {
ram_addr_t ram_size = machine->ram_size; MemoryRegion *address_space_mem = get_system_memory(); DeviceState *dev, *dma, *eth0; Object *ds, *cs; MicroBlazeCPU *cpu; SysBusDevice *busdev; DriveInfo *dinfo; int i; MemoryRegion *phys_lmb_bram = g_new(MemoryRegion, 1); MemoryRegion *phys_ram = g_new(MemoryRegion, 1); qemu_irq irq[32]; /* init CPUs */ cpu = MICROBLAZE_CPU(object_new(TYPE_MICROBLAZE_CPU)); object_property_set_bool(OBJECT(cpu), true, ""realized"", &error_abort); /* Attach emulated BRAM through the LMB. */ memory_region_init_ram(phys_lmb_bram, NULL, ""petalogix_ml605.lmb_bram"", LMB_BRAM_SIZE, &error_abort); vmstate_register_ram_global(phys_lmb_bram); memory_region_add_subregion(address_space_mem, 0x00000000, phys_lmb_bram); memory_region_init_ram(phys_ram, NULL, ""petalogix_ml605.ram"", ram_size, &error_abort); vmstate_register_ram_global(phys_ram); memory_region_add_subregion(address_space_mem, MEMORY_BASEADDR, phys_ram); dinfo = drive_get(IF_PFLASH, 0, 0); /* 5th parameter 2 means bank-width * 10th paremeter 0 means little-endian */ pflash_cfi01_register(FLASH_BASEADDR, NULL, ""petalogix_ml605.flash"", FLASH_SIZE, dinfo ? blk_bs(blk_by_legacy_dinfo(dinfo)) : NULL, (64 * 1024), FLASH_SIZE >> 16, 2, 0x89, 0x18, 0x0000, 0x0, 0); dev = qdev_create(NULL, ""xlnx.xps-intc""); qdev_prop_set_uint32(dev, ""kind-of-intr"", 1 << TIMER_IRQ); qdev_init_nofail(dev); sysbus_mmio_map(SYS_BUS_DEVICE(dev), 0, INTC_BASEADDR); sysbus_connect_irq(SYS_BUS_DEVICE(dev), 0, qdev_get_gpio_in(DEVICE(cpu), MB_CPU_IRQ)); for (i = 0; i < 32; i++) {
irq[i] = qdev_get_gpio_in(dev, i); } serial_mm_init(address_space_mem, UART16550_BASEADDR + 0x1000, 2, irq[UART16550_IRQ], 115200, serial_hds[0], DEVICE_LITTLE_ENDIAN); /* 2 timers at irq 2 @ 100 Mhz. */ dev = qdev_create(NULL, ""xlnx.xps-timer""); qdev_prop_set_uint32(dev, ""one-timer-only"", 0); qdev_prop_set_uint32(dev, ""clock-frequency"", 100 * 1000000); qdev_init_nofail(dev); sysbus_mmio_map(SYS_BUS_DEVICE(dev), 0, TIMER_BASEADDR); sysbus_connect_irq(SYS_BUS_DEVICE(dev), 0, irq[TIMER_IRQ]); /* axi ethernet and dma initialization. */ qemu_check_nic_model(&nd_table[0], ""xlnx.axi-ethernet""); eth0 = qdev_create(NULL, ""xlnx.axi-ethernet""); dma = qdev_create(NULL, ""xlnx.axi-dma""); /* FIXME: attach to the sysbus instead */ object_property_add_child(qdev_get_machine(), ""xilinx-eth"", OBJECT(eth0), NULL); object_property_add_child(qdev_get_machine(), ""xilinx-dma"", OBJECT(dma), NULL); ds = object_property_get_link(OBJECT(dma), ""axistream-connected-target"", NULL); cs = object_property_get_link(OBJECT(dma), ""axistream-control-connected-target"", NULL); qdev_set_nic_properties(eth0, &nd_table[0]); qdev_prop_set_uint32(eth0, ""rxmem"", 0x1000); qdev_prop_set_uint32(eth0, ""txmem"", 0x1000); object_property_set_link(OBJECT(eth0), OBJECT(ds), ""axistream-connected"", &error_abort); object_property_set_link(OBJECT(eth0), OBJECT(cs), ""axistream-control-connected"", &error_abort); qdev_init_nofail(eth0); sysbus_mmio_map(SYS_BUS_DEVICE(eth0), 0, AXIENET_BASEADDR); sysbus_connect_irq(SYS_BUS_DEVICE(eth0), 0, irq[AXIENET_IRQ]); ds = object_property_get_link(OBJECT(eth0), ""axistream-connected-target"", NULL); cs = object_property_get_link(OBJECT(eth0), ""axistream-control-connected-target"", NULL); qdev_prop_set_uint32(dma, ""freqhz"", 100 * 1000000); object_property_set_link(OBJECT(dma), OBJECT(ds), ""axistream-connected"", &error_abort); object_property_set_link(OBJECT(dma), OBJECT(cs), ""axistream-control-connected"", &error_abort); qdev_init_nofail(dma); sysbus_mmio_map(SYS_BUS_DEVICE(dma), 0, AXIDMA_BASEADDR); sysbus_connect_irq(SYS_BUS_DEVICE(dma), 0, irq[AXIDMA_IRQ0]); sysbus_connect_irq(SYS_BUS_DEVICE(dma), 1, irq[AXIDMA_IRQ1]); {
SSIBus *spi; dev = qdev_create(NULL, ""xlnx.xps-spi""); qdev_prop_set_uint8(dev, ""num-ss-bits"", NUM_SPI_FLASHES); qdev_init_nofail(dev); busdev = SYS_BUS_DEVICE(dev); sysbus_mmio_map(busdev, 0, SPI_BASEADDR); sysbus_connect_irq(busdev, 0, irq[SPI_IRQ]); spi = (SSIBus *)qdev_get_child_bus(dev, ""spi""); for (i = 0; i < NUM_SPI_FLASHES; i++) {
qemu_irq cs_line; dev = ssi_create_slave(spi, ""n25q128""); cs_line = qdev_get_gpio_in_named(dev, SSI_GPIO_CS, 0); sysbus_connect_irq(busdev, i+1, cs_line); } } microblaze_load_kernel(cpu, MEMORY_BASEADDR, ram_size, machine->initrd_filename, BINARY_DEVICE_TREE_FILE, machine_cpu_reset); } ",qemu,0
"void replay_bh_schedule_event(QEMUBH *bh)

{

    if (replay_mode != REPLAY_MODE_NONE) {

        uint64_t id = replay_get_current_step();

        replay_add_event(REPLAY_ASYNC_EVENT_BH, bh, NULL, id);

    } else {

        qemu_bh_schedule(bh);

    }

}
","void replay_bh_schedule_event(QEMUBH *bh) {
if (replay_mode != REPLAY_MODE_NONE) {
uint64_t id = replay_get_current_step(); replay_add_event(REPLAY_ASYNC_EVENT_BH, bh, NULL, id); } else {
qemu_bh_schedule(bh); } } ",qemu,1
"void HELPER(divs)(CPUM68KState *env, uint32_t word)

{

    int32_t num;

    int32_t den;

    int32_t quot;

    int32_t rem;



    num = env->div1;

    den = env->div2;

    if (den == 0) {

        raise_exception(env, EXCP_DIV0);

    }

    quot = num / den;

    rem = num % den;



    env->cc_v = (word && quot != (int16_t)quot ? -1 : 0);

    env->cc_z = quot;

    env->cc_n = quot;

    env->cc_c = 0;



    env->div1 = quot;

    env->div2 = rem;

}
","void HELPER(divs)(CPUM68KState *env, uint32_t word) {
int32_t num; int32_t den; int32_t quot; int32_t rem; num = env->div1; den = env->div2; if (den == 0) {
raise_exception(env, EXCP_DIV0); } quot = num / den; rem = num % den; env->cc_v = (word && quot != (int16_t)quot ? -1 : 0); env->cc_z = quot; env->cc_n = quot; env->cc_c = 0; env->div1 = quot; env->div2 = rem; } ",qemu,1
"void css_conditional_io_interrupt(SubchDev *sch)

{

    /*

     * If the subchannel is not currently status pending, make it pending

     * with alert status.

     */

    if (!(sch->curr_status.scsw.ctrl & SCSW_STCTL_STATUS_PEND)) {

        S390CPU *cpu = s390_cpu_addr2state(0);

        uint8_t isc = (sch->curr_status.pmcw.flags & PMCW_FLAGS_MASK_ISC) >> 11;



        trace_css_io_interrupt(sch->cssid, sch->ssid, sch->schid,

                               sch->curr_status.pmcw.intparm, isc,

                               ""(unsolicited)"");

        sch->curr_status.scsw.ctrl &= ~SCSW_CTRL_MASK_STCTL;

        sch->curr_status.scsw.ctrl |=

            SCSW_STCTL_ALERT | SCSW_STCTL_STATUS_PEND;

        /* Inject an I/O interrupt. */

        s390_io_interrupt(cpu,

                          css_build_subchannel_id(sch),

                          sch->schid,

                          sch->curr_status.pmcw.intparm,

                          (0x80 >> isc) << 24);

    }

}
","void css_conditional_io_interrupt(SubchDev *sch) {
/* * If the subchannel is not currently status pending, make it pending * with alert status. */ if (!(sch->curr_status.scsw.ctrl & SCSW_STCTL_STATUS_PEND)) {
S390CPU *cpu = s390_cpu_addr2state(0); uint8_t isc = (sch->curr_status.pmcw.flags & PMCW_FLAGS_MASK_ISC) >> 11; trace_css_io_interrupt(sch->cssid, sch->ssid, sch->schid, sch->curr_status.pmcw.intparm, isc, ""(unsolicited)""); sch->curr_status.scsw.ctrl &= ~SCSW_CTRL_MASK_STCTL; sch->curr_status.scsw.ctrl |= SCSW_STCTL_ALERT | SCSW_STCTL_STATUS_PEND; /* Inject an I/O interrupt. */ s390_io_interrupt(cpu, css_build_subchannel_id(sch), sch->schid, sch->curr_status.pmcw.intparm, (0x80 >> isc) << 24); } } ",qemu,0
"static int ehci_process_itd(EHCIState *ehci,

                            EHCIitd *itd,

                            uint32_t addr)

{

    USBDevice *dev;

    USBEndpoint *ep;

    uint32_t i, len, pid, dir, devaddr, endp;

    uint32_t pg, off, ptr1, ptr2, max, mult;



    ehci->periodic_sched_active = PERIODIC_ACTIVE;



    dir =(itd->bufptr[1] & ITD_BUFPTR_DIRECTION);

    devaddr = get_field(itd->bufptr[0], ITD_BUFPTR_DEVADDR);

    endp = get_field(itd->bufptr[0], ITD_BUFPTR_EP);

    max = get_field(itd->bufptr[1], ITD_BUFPTR_MAXPKT);

    mult = get_field(itd->bufptr[2], ITD_BUFPTR_MULT);



    for(i = 0; i < 8; i++) {

        if (itd->transact[i] & ITD_XACT_ACTIVE) {

            pg   = get_field(itd->transact[i], ITD_XACT_PGSEL);

            off  = itd->transact[i] & ITD_XACT_OFFSET_MASK;

            ptr1 = (itd->bufptr[pg] & ITD_BUFPTR_MASK);

            ptr2 = (itd->bufptr[pg+1] & ITD_BUFPTR_MASK);

            len  = get_field(itd->transact[i], ITD_XACT_LENGTH);



            if (len > max * mult) {

                len = max * mult;

            }



            if (len > BUFF_SIZE) {

                return -1;

            }



            qemu_sglist_init(&ehci->isgl, DEVICE(ehci), 2, ehci->as);

            if (off + len > 4096) {

                /* transfer crosses page border */

                uint32_t len2 = off + len - 4096;

                uint32_t len1 = len - len2;

                qemu_sglist_add(&ehci->isgl, ptr1 + off, len1);

                qemu_sglist_add(&ehci->isgl, ptr2, len2);

            } else {

                qemu_sglist_add(&ehci->isgl, ptr1 + off, len);

            }



            pid = dir ? USB_TOKEN_IN : USB_TOKEN_OUT;



            dev = ehci_find_device(ehci, devaddr);

            ep = usb_ep_get(dev, pid, endp);

            if (ep && ep->type == USB_ENDPOINT_XFER_ISOC) {

                usb_packet_setup(&ehci->ipacket, pid, ep, 0, addr, false,

                                 (itd->transact[i] & ITD_XACT_IOC) != 0);

                usb_packet_map(&ehci->ipacket, &ehci->isgl);

                usb_handle_packet(dev, &ehci->ipacket);

                usb_packet_unmap(&ehci->ipacket, &ehci->isgl);

            } else {

                DPRINTF(""ISOCH: attempt to addess non-iso endpoint\n"");

                ehci->ipacket.status = USB_RET_NAK;

                ehci->ipacket.actual_length = 0;

            }

            qemu_sglist_destroy(&ehci->isgl);



            switch (ehci->ipacket.status) {

            case USB_RET_SUCCESS:

                break;

            default:

                fprintf(stderr, ""Unexpected iso usb result: %d\n"",

                        ehci->ipacket.status);

                /* Fall through */

            case USB_RET_IOERROR:

            case USB_RET_NODEV:

                /* 3.3.2: XACTERR is only allowed on IN transactions */

                if (dir) {

                    itd->transact[i] |= ITD_XACT_XACTERR;

                    ehci_raise_irq(ehci, USBSTS_ERRINT);

                }

                break;

            case USB_RET_BABBLE:

                itd->transact[i] |= ITD_XACT_BABBLE;

                ehci_raise_irq(ehci, USBSTS_ERRINT);

                break;

            case USB_RET_NAK:

                /* no data for us, so do a zero-length transfer */

                ehci->ipacket.actual_length = 0;

                break;

            }

            if (!dir) {

                set_field(&itd->transact[i], len - ehci->ipacket.actual_length,

                          ITD_XACT_LENGTH); /* OUT */

            } else {

                set_field(&itd->transact[i], ehci->ipacket.actual_length,

                          ITD_XACT_LENGTH); /* IN */

            }

            if (itd->transact[i] & ITD_XACT_IOC) {

                ehci_raise_irq(ehci, USBSTS_INT);

            }

            itd->transact[i] &= ~ITD_XACT_ACTIVE;

        }

    }

    return 0;

}
","static int ehci_process_itd(EHCIState *ehci, EHCIitd *itd, uint32_t addr) {
USBDevice *dev; USBEndpoint *ep; uint32_t i, len, pid, dir, devaddr, endp; uint32_t pg, off, ptr1, ptr2, max, mult; ehci->periodic_sched_active = PERIODIC_ACTIVE; dir =(itd->bufptr[1] & ITD_BUFPTR_DIRECTION); devaddr = get_field(itd->bufptr[0], ITD_BUFPTR_DEVADDR); endp = get_field(itd->bufptr[0], ITD_BUFPTR_EP); max = get_field(itd->bufptr[1], ITD_BUFPTR_MAXPKT); mult = get_field(itd->bufptr[2], ITD_BUFPTR_MULT); for(i = 0; i < 8; i++) {
if (itd->transact[i] & ITD_XACT_ACTIVE) {
pg = get_field(itd->transact[i], ITD_XACT_PGSEL); off = itd->transact[i] & ITD_XACT_OFFSET_MASK; ptr1 = (itd->bufptr[pg] & ITD_BUFPTR_MASK); ptr2 = (itd->bufptr[pg+1] & ITD_BUFPTR_MASK); len = get_field(itd->transact[i], ITD_XACT_LENGTH); if (len > max * mult) {
len = max * mult; } if (len > BUFF_SIZE) {
return -1; } qemu_sglist_init(&ehci->isgl, DEVICE(ehci), 2, ehci->as); if (off + len > 4096) {
/* transfer crosses page border */ uint32_t len2 = off + len - 4096; uint32_t len1 = len - len2; qemu_sglist_add(&ehci->isgl, ptr1 + off, len1); qemu_sglist_add(&ehci->isgl, ptr2, len2); } else {
qemu_sglist_add(&ehci->isgl, ptr1 + off, len); } pid = dir ? USB_TOKEN_IN : USB_TOKEN_OUT; dev = ehci_find_device(ehci, devaddr); ep = usb_ep_get(dev, pid, endp); if (ep && ep->type == USB_ENDPOINT_XFER_ISOC) {
usb_packet_setup(&ehci->ipacket, pid, ep, 0, addr, false, (itd->transact[i] & ITD_XACT_IOC) != 0); usb_packet_map(&ehci->ipacket, &ehci->isgl); usb_handle_packet(dev, &ehci->ipacket); usb_packet_unmap(&ehci->ipacket, &ehci->isgl); } else {
DPRINTF(""ISOCH: attempt to addess non-iso endpoint\n""); ehci->ipacket.status = USB_RET_NAK; ehci->ipacket.actual_length = 0; } qemu_sglist_destroy(&ehci->isgl); switch (ehci->ipacket.status) {
case USB_RET_SUCCESS: break; default: fprintf(stderr, ""Unexpected iso usb result: %d\n"", ehci->ipacket.status); /* Fall through */ case USB_RET_IOERROR: case USB_RET_NODEV: /* 3.3.2: XACTERR is only allowed on IN transactions */ if (dir) {
itd->transact[i] |= ITD_XACT_XACTERR; ehci_raise_irq(ehci, USBSTS_ERRINT); } break; case USB_RET_BABBLE: itd->transact[i] |= ITD_XACT_BABBLE; ehci_raise_irq(ehci, USBSTS_ERRINT); break; case USB_RET_NAK: /* no data for us, so do a zero-length transfer */ ehci->ipacket.actual_length = 0; break; } if (!dir) {
set_field(&itd->transact[i], len - ehci->ipacket.actual_length, ITD_XACT_LENGTH); /* OUT */ } else {
set_field(&itd->transact[i], ehci->ipacket.actual_length, ITD_XACT_LENGTH); /* IN */ } if (itd->transact[i] & ITD_XACT_IOC) {
ehci_raise_irq(ehci, USBSTS_INT); } itd->transact[i] &= ~ITD_XACT_ACTIVE; } } return 0; } ",qemu,0
"static void audio_init (void)

{

    size_t i;

    int done = 0;

    const char *drvname;

    VMChangeStateEntry *e;

    AudioState *s = &glob_audio_state;



    if (s->drv) {

        return;

    }



    QLIST_INIT (&s->hw_head_out);

    QLIST_INIT (&s->hw_head_in);

    QLIST_INIT (&s->cap_head);

    atexit (audio_atexit);



    s->ts = timer_new_ns(QEMU_CLOCK_VIRTUAL, audio_timer, s);

    if (!s->ts) {

        hw_error(""Could not create audio timer\n"");

    }



    audio_process_options (""AUDIO"", audio_options);



    s->nb_hw_voices_out = conf.fixed_out.nb_voices;

    s->nb_hw_voices_in = conf.fixed_in.nb_voices;



    if (s->nb_hw_voices_out <= 0) {

        dolog (""Bogus number of playback voices %d, setting to 1\n"",

               s->nb_hw_voices_out);

        s->nb_hw_voices_out = 1;

    }



    if (s->nb_hw_voices_in <= 0) {

        dolog (""Bogus number of capture voices %d, setting to 0\n"",

               s->nb_hw_voices_in);

        s->nb_hw_voices_in = 0;

    }



    {

        int def;

        drvname = audio_get_conf_str (""QEMU_AUDIO_DRV"", NULL, &def);

    }



    if (drvname) {

        int found = 0;



        for (i = 0; i < ARRAY_SIZE (drvtab); i++) {

            if (!strcmp (drvname, drvtab[i]->name)) {

                done = !audio_driver_init (s, drvtab[i]);

                found = 1;

                break;

            }

        }



        if (!found) {

            dolog (""Unknown audio driver `%s'\n"", drvname);

            dolog (""Run with -audio-help to list available drivers\n"");

        }

    }



    if (!done) {

        for (i = 0; !done && i < ARRAY_SIZE (drvtab); i++) {

            if (drvtab[i]->can_be_default) {

                done = !audio_driver_init (s, drvtab[i]);

            }

        }

    }



    if (!done) {

        done = !audio_driver_init (s, &no_audio_driver);

        if (!done) {

            hw_error(""Could not initialize audio subsystem\n"");

        }

        else {

            dolog (""warning: Using timer based audio emulation\n"");

        }

    }



    if (conf.period.hertz <= 0) {

        if (conf.period.hertz < 0) {

            dolog (""warning: Timer period is negative - %d ""

                   ""treating as zero\n"",

                   conf.period.hertz);

        }

        conf.period.ticks = 1;

    } else {

        conf.period.ticks =

            muldiv64 (1, get_ticks_per_sec (), conf.period.hertz);

    }



    e = qemu_add_vm_change_state_handler (audio_vm_change_state_handler, s);

    if (!e) {

        dolog (""warning: Could not register change state handler\n""

               ""(Audio can continue looping even after stopping the VM)\n"");

    }



    QLIST_INIT (&s->card_head);

    vmstate_register (NULL, 0, &vmstate_audio, s);

}
","static void audio_init (void) {
size_t i; int done = 0; const char *drvname; VMChangeStateEntry *e; AudioState *s = &glob_audio_state; if (s->drv) {
return; } QLIST_INIT (&s->hw_head_out); QLIST_INIT (&s->hw_head_in); QLIST_INIT (&s->cap_head); atexit (audio_atexit); s->ts = timer_new_ns(QEMU_CLOCK_VIRTUAL, audio_timer, s); if (!s->ts) {
hw_error(""Could not create audio timer\n""); } audio_process_options (""AUDIO"", audio_options); s->nb_hw_voices_out = conf.fixed_out.nb_voices; s->nb_hw_voices_in = conf.fixed_in.nb_voices; if (s->nb_hw_voices_out <= 0) {
dolog (""Bogus number of playback voices %d, setting to 1\n"", s->nb_hw_voices_out); s->nb_hw_voices_out = 1; } if (s->nb_hw_voices_in <= 0) {
dolog (""Bogus number of capture voices %d, setting to 0\n"", s->nb_hw_voices_in); s->nb_hw_voices_in = 0; } {
int def; drvname = audio_get_conf_str (""QEMU_AUDIO_DRV"", NULL, &def); } if (drvname) {
int found = 0; for (i = 0; i < ARRAY_SIZE (drvtab); i++) {
if (!strcmp (drvname, drvtab[i]->name)) {
done = !audio_driver_init (s, drvtab[i]); found = 1; break; } } if (!found) {
dolog (""Unknown audio driver `%s'\n"", drvname); dolog (""Run with -audio-help to list available drivers\n""); } } if (!done) {
for (i = 0; !done && i < ARRAY_SIZE (drvtab); i++) {
if (drvtab[i]->can_be_default) {
done = !audio_driver_init (s, drvtab[i]); } } } if (!done) {
done = !audio_driver_init (s, &no_audio_driver); if (!done) {
hw_error(""Could not initialize audio subsystem\n""); } else {
dolog (""warning: Using timer based audio emulation\n""); } } if (conf.period.hertz <= 0) {
if (conf.period.hertz < 0) {
dolog (""warning: Timer period is negative - %d "" ""treating as zero\n"", conf.period.hertz); } conf.period.ticks = 1; } else {
conf.period.ticks = muldiv64 (1, get_ticks_per_sec (), conf.period.hertz); } e = qemu_add_vm_change_state_handler (audio_vm_change_state_handler, s); if (!e) {
dolog (""warning: Could not register change state handler\n"" ""(Audio can continue looping even after stopping the VM)\n""); } QLIST_INIT (&s->card_head); vmstate_register (NULL, 0, &vmstate_audio, s); } ",qemu,1
"static void pxb_dev_exitfn(PCIDevice *pci_dev)

{

    PXBDev *pxb = PXB_DEV(pci_dev);



    pxb_dev_list = g_list_remove(pxb_dev_list, pxb);

}
","static void pxb_dev_exitfn(PCIDevice *pci_dev) {
PXBDev *pxb = PXB_DEV(pci_dev); pxb_dev_list = g_list_remove(pxb_dev_list, pxb); } ",qemu,0
"static inline void gen_op_eval_ble(TCGv dst, TCGv_i32 src)

{

    gen_mov_reg_N(cpu_tmp0, src);

    gen_mov_reg_V(dst, src);

    tcg_gen_xor_tl(dst, dst, cpu_tmp0);

    gen_mov_reg_Z(cpu_tmp0, src);

    tcg_gen_or_tl(dst, dst, cpu_tmp0);

}
","static inline void gen_op_eval_ble(TCGv dst, TCGv_i32 src) {
gen_mov_reg_N(cpu_tmp0, src); gen_mov_reg_V(dst, src); tcg_gen_xor_tl(dst, dst, cpu_tmp0); gen_mov_reg_Z(cpu_tmp0, src); tcg_gen_or_tl(dst, dst, cpu_tmp0); } ",qemu,0
"static int bdrv_wr_badreq_bytes(BlockDriverState *bs,

                                int64_t offset, int count)

{

    int64_t size = bs->total_sectors << SECTOR_BITS;

    if (count < 0 ||

        offset < 0)

        return 1;



    if (offset > size - count) {

        if (bs->autogrow)

            bs->total_sectors = (offset + count + SECTOR_SIZE - 1) >> SECTOR_BITS;

        else

            return 1;

    }

    return 0;

}
","static int bdrv_wr_badreq_bytes(BlockDriverState *bs, int64_t offset, int count) {
int64_t size = bs->total_sectors << SECTOR_BITS; if (count < 0 || offset < 0) return 1; if (offset > size - count) {
if (bs->autogrow) bs->total_sectors = (offset + count + SECTOR_SIZE - 1) >> SECTOR_BITS; else return 1; } return 0; } ",qemu,1
"static V9fsSynthNode *v9fs_add_dir_node(V9fsSynthNode *parent, int mode,

                                        const char *name,

                                        V9fsSynthNodeAttr *attr, int inode)

{

    V9fsSynthNode *node;



    /* Add directory type and remove write bits */

    mode = ((mode & 0777) | S_IFDIR) & ~(S_IWUSR | S_IWGRP | S_IWOTH);

    node = g_malloc0(sizeof(V9fsSynthNode));

    if (attr) {

        /* We are adding .. or . entries */

        node->attr = attr;

        node->attr->nlink++;

    } else {

        node->attr = &node->actual_attr;

        node->attr->inode = inode;

        node->attr->nlink = 1;

        /* We don't allow write to directories */

        node->attr->mode   = mode;

        node->attr->write = NULL;

        node->attr->read  = NULL;

    }

    node->private = node;

    pstrcpy(node->name, sizeof(node->name), name);

    QLIST_INSERT_HEAD_RCU(&parent->child, node, sibling);

    return node;

}
","static V9fsSynthNode *v9fs_add_dir_node(V9fsSynthNode *parent, int mode, const char *name, V9fsSynthNodeAttr *attr, int inode) {
V9fsSynthNode *node; /* Add directory type and remove write bits */ mode = ((mode & 0777) | S_IFDIR) & ~(S_IWUSR | S_IWGRP | S_IWOTH); node = g_malloc0(sizeof(V9fsSynthNode)); if (attr) {
/* We are adding .. or . entries */ node->attr = attr; node->attr->nlink++; } else {
node->attr = &node->actual_attr; node->attr->inode = inode; node->attr->nlink = 1; /* We don't allow write to directories */ node->attr->mode = mode; node->attr->write = NULL; node->attr->read = NULL; } node->private = node; pstrcpy(node->name, sizeof(node->name), name); QLIST_INSERT_HEAD_RCU(&parent->child, node, sibling); return node; } ",qemu,0
"static void virtio_gpu_resource_create_2d(VirtIOGPU *g,

                                          struct virtio_gpu_ctrl_command *cmd)

{

    pixman_format_code_t pformat;

    struct virtio_gpu_simple_resource *res;

    struct virtio_gpu_resource_create_2d c2d;



    VIRTIO_GPU_FILL_CMD(c2d);

    trace_virtio_gpu_cmd_res_create_2d(c2d.resource_id, c2d.format,

                                       c2d.width, c2d.height);



    if (c2d.resource_id == 0) {

        qemu_log_mask(LOG_GUEST_ERROR, ""%s: resource id 0 is not allowed\n"",

                      __func__);

        cmd->error = VIRTIO_GPU_RESP_ERR_INVALID_RESOURCE_ID;

        return;

    }



    res = virtio_gpu_find_resource(g, c2d.resource_id);

    if (res) {

        qemu_log_mask(LOG_GUEST_ERROR, ""%s: resource already exists %d\n"",

                      __func__, c2d.resource_id);

        cmd->error = VIRTIO_GPU_RESP_ERR_INVALID_RESOURCE_ID;

        return;

    }



    res = g_new0(struct virtio_gpu_simple_resource, 1);



    res->width = c2d.width;

    res->height = c2d.height;

    res->format = c2d.format;

    res->resource_id = c2d.resource_id;



    pformat = get_pixman_format(c2d.format);

    if (!pformat) {

        qemu_log_mask(LOG_GUEST_ERROR,

                      ""%s: host couldn't handle guest format %d\n"",

                      __func__, c2d.format);

        g_free(res);

        cmd->error = VIRTIO_GPU_RESP_ERR_INVALID_PARAMETER;

        return;

    }

    res->image = pixman_image_create_bits(pformat,

                                          c2d.width,

                                          c2d.height,

                                          NULL, 0);



    if (!res->image) {

        qemu_log_mask(LOG_GUEST_ERROR,

                      ""%s: resource creation failed %d %d %d\n"",

                      __func__, c2d.resource_id, c2d.width, c2d.height);

        g_free(res);

        cmd->error = VIRTIO_GPU_RESP_ERR_OUT_OF_MEMORY;

        return;

    }



    QTAILQ_INSERT_HEAD(&g->reslist, res, next);

}
","static void virtio_gpu_resource_create_2d(VirtIOGPU *g, struct virtio_gpu_ctrl_command *cmd) {
pixman_format_code_t pformat; struct virtio_gpu_simple_resource *res; struct virtio_gpu_resource_create_2d c2d; VIRTIO_GPU_FILL_CMD(c2d); trace_virtio_gpu_cmd_res_create_2d(c2d.resource_id, c2d.format, c2d.width, c2d.height); if (c2d.resource_id == 0) {
qemu_log_mask(LOG_GUEST_ERROR, ""%s: resource id 0 is not allowed\n"", __func__); cmd->error = VIRTIO_GPU_RESP_ERR_INVALID_RESOURCE_ID; return; } res = virtio_gpu_find_resource(g, c2d.resource_id); if (res) {
qemu_log_mask(LOG_GUEST_ERROR, ""%s: resource already exists %d\n"", __func__, c2d.resource_id); cmd->error = VIRTIO_GPU_RESP_ERR_INVALID_RESOURCE_ID; return; } res = g_new0(struct virtio_gpu_simple_resource, 1); res->width = c2d.width; res->height = c2d.height; res->format = c2d.format; res->resource_id = c2d.resource_id; pformat = get_pixman_format(c2d.format); if (!pformat) {
qemu_log_mask(LOG_GUEST_ERROR, ""%s: host couldn't handle guest format %d\n"", __func__, c2d.format); g_free(res); cmd->error = VIRTIO_GPU_RESP_ERR_INVALID_PARAMETER; return; } res->image = pixman_image_create_bits(pformat, c2d.width, c2d.height, NULL, 0); if (!res->image) {
qemu_log_mask(LOG_GUEST_ERROR, ""%s: resource creation failed %d %d %d\n"", __func__, c2d.resource_id, c2d.width, c2d.height); g_free(res); cmd->error = VIRTIO_GPU_RESP_ERR_OUT_OF_MEMORY; return; } QTAILQ_INSERT_HEAD(&g->reslist, res, next); } ",qemu,1
"static int socket_accept(int sock)

{

    struct sockaddr_un addr;

    socklen_t addrlen;

    int ret;



    addrlen = sizeof(addr);

    do {

        ret = accept(sock, (struct sockaddr *)&addr, &addrlen);

    } while (ret == -1 && errno == EINTR);

    g_assert_no_errno(ret);

    close(sock);



    return ret;

}
","static int socket_accept(int sock) {
struct sockaddr_un addr; socklen_t addrlen; int ret; addrlen = sizeof(addr); do {
ret = accept(sock, (struct sockaddr *)&addr, &addrlen); } while (ret == -1 && errno == EINTR); g_assert_no_errno(ret); close(sock); return ret; } ",qemu,1
"static int htab_load(QEMUFile *f, void *opaque, int version_id)

{

    sPAPRMachineState *spapr = opaque;

    uint32_t section_hdr;

    int fd = -1;



    if (version_id < 1 || version_id > 1) {

        error_report(""htab_load() bad version"");

        return -EINVAL;

    }



    section_hdr = qemu_get_be32(f);



    if (section_hdr) {

        Error *local_err;



        /* First section gives the htab size */

        spapr_reallocate_hpt(spapr, section_hdr, &local_err);

        if (local_err) {

            error_report_err(local_err);

            return -EINVAL;

        }

        return 0;

    }



    if (!spapr->htab) {

        assert(kvm_enabled());



        fd = kvmppc_get_htab_fd(true);

        if (fd < 0) {

            error_report(""Unable to open fd to restore KVM hash table: %s"",

                         strerror(errno));

        }

    }



    while (true) {

        uint32_t index;

        uint16_t n_valid, n_invalid;



        index = qemu_get_be32(f);

        n_valid = qemu_get_be16(f);

        n_invalid = qemu_get_be16(f);



        if ((index == 0) && (n_valid == 0) && (n_invalid == 0)) {

            /* End of Stream */

            break;

        }



        if ((index + n_valid + n_invalid) >

            (HTAB_SIZE(spapr) / HASH_PTE_SIZE_64)) {

            /* Bad index in stream */

            error_report(

                ""htab_load() bad index %d (%hd+%hd entries) in htab stream (htab_shift=%d)"",

                index, n_valid, n_invalid, spapr->htab_shift);

            return -EINVAL;

        }



        if (spapr->htab) {

            if (n_valid) {

                qemu_get_buffer(f, HPTE(spapr->htab, index),

                                HASH_PTE_SIZE_64 * n_valid);

            }

            if (n_invalid) {

                memset(HPTE(spapr->htab, index + n_valid), 0,

                       HASH_PTE_SIZE_64 * n_invalid);

            }

        } else {

            int rc;



            assert(fd >= 0);



            rc = kvmppc_load_htab_chunk(f, fd, index, n_valid, n_invalid);

            if (rc < 0) {

                return rc;

            }

        }

    }



    if (!spapr->htab) {

        assert(fd >= 0);

        close(fd);

    }



    return 0;

}
","static int htab_load(QEMUFile *f, void *opaque, int version_id) {
sPAPRMachineState *spapr = opaque; uint32_t section_hdr; int fd = -1; if (version_id < 1 || version_id > 1) {
error_report(""htab_load() bad version""); return -EINVAL; } section_hdr = qemu_get_be32(f); if (section_hdr) {
Error *local_err; /* First section gives the htab size */ spapr_reallocate_hpt(spapr, section_hdr, &local_err); if (local_err) {
error_report_err(local_err); return -EINVAL; } return 0; } if (!spapr->htab) {
assert(kvm_enabled()); fd = kvmppc_get_htab_fd(true); if (fd < 0) {
error_report(""Unable to open fd to restore KVM hash table: %s"", strerror(errno)); } } while (true) {
uint32_t index; uint16_t n_valid, n_invalid; index = qemu_get_be32(f); n_valid = qemu_get_be16(f); n_invalid = qemu_get_be16(f); if ((index == 0) && (n_valid == 0) && (n_invalid == 0)) {
/* End of Stream */ break; } if ((index + n_valid + n_invalid) > (HTAB_SIZE(spapr) / HASH_PTE_SIZE_64)) {
/* Bad index in stream */ error_report( ""htab_load() bad index %d (%hd+%hd entries) in htab stream (htab_shift=%d)"", index, n_valid, n_invalid, spapr->htab_shift); return -EINVAL; } if (spapr->htab) {
if (n_valid) {
qemu_get_buffer(f, HPTE(spapr->htab, index), HASH_PTE_SIZE_64 * n_valid); } if (n_invalid) {
memset(HPTE(spapr->htab, index + n_valid), 0, HASH_PTE_SIZE_64 * n_invalid); } } else {
int rc; assert(fd >= 0); rc = kvmppc_load_htab_chunk(f, fd, index, n_valid, n_invalid); if (rc < 0) {
return rc; } } } if (!spapr->htab) {
assert(fd >= 0); close(fd); } return 0; } ",qemu,1
"static void onenand_command(OneNANDState *s)

{

    int b;

    int sec;

    void *buf;

#define SETADDR(block, page)			\

    sec = (s->addr[page] & 3) +			\

            ((((s->addr[page] >> 2) & 0x3f) +	\

              (((s->addr[block] & 0xfff) |	\

                (s->addr[block] >> 15 ?		\

                 s->density_mask : 0)) << 6)) << (PAGE_SHIFT - 9));

#define SETBUF_M()				\

    buf = (s->bufaddr & 8) ?			\

            s->data[(s->bufaddr >> 2) & 1][0] : s->boot[0];	\

    buf += (s->bufaddr & 3) << 9;

#define SETBUF_S()				\

    buf = (s->bufaddr & 8) ?			\

            s->data[(s->bufaddr >> 2) & 1][1] : s->boot[1];	\

    buf += (s->bufaddr & 3) << 4;



    switch (s->command) {

    case 0x00:	/* Load single/multiple sector data unit into buffer */

        SETADDR(ONEN_BUF_BLOCK, ONEN_BUF_PAGE)



        SETBUF_M()

        if (onenand_load_main(s, sec, s->count, buf))

            s->status |= ONEN_ERR_CMD | ONEN_ERR_LOAD;



#if 0

        SETBUF_S()

        if (onenand_load_spare(s, sec, s->count, buf))

            s->status |= ONEN_ERR_CMD | ONEN_ERR_LOAD;

#endif



        /* TODO: if (s->bufaddr & 3) + s->count was > 4 (2k-pages)

         * or    if (s->bufaddr & 1) + s->count was > 2 (1k-pages)

         * then we need two split the read/write into two chunks.

         */

        s->intstatus |= ONEN_INT | ONEN_INT_LOAD;

        break;

    case 0x13:	/* Load single/multiple spare sector into buffer */

        SETADDR(ONEN_BUF_BLOCK, ONEN_BUF_PAGE)



        SETBUF_S()

        if (onenand_load_spare(s, sec, s->count, buf))

            s->status |= ONEN_ERR_CMD | ONEN_ERR_LOAD;



        /* TODO: if (s->bufaddr & 3) + s->count was > 4 (2k-pages)

         * or    if (s->bufaddr & 1) + s->count was > 2 (1k-pages)

         * then we need two split the read/write into two chunks.

         */

        s->intstatus |= ONEN_INT | ONEN_INT_LOAD;

        break;

    case 0x80:	/* Program single/multiple sector data unit from buffer */

        SETADDR(ONEN_BUF_BLOCK, ONEN_BUF_PAGE)



        SETBUF_M()

        if (onenand_prog_main(s, sec, s->count, buf))

            s->status |= ONEN_ERR_CMD | ONEN_ERR_PROG;



#if 0

        SETBUF_S()

        if (onenand_prog_spare(s, sec, s->count, buf))

            s->status |= ONEN_ERR_CMD | ONEN_ERR_PROG;

#endif



        /* TODO: if (s->bufaddr & 3) + s->count was > 4 (2k-pages)

         * or    if (s->bufaddr & 1) + s->count was > 2 (1k-pages)

         * then we need two split the read/write into two chunks.

         */

        s->intstatus |= ONEN_INT | ONEN_INT_PROG;

        break;

    case 0x1a:	/* Program single/multiple spare area sector from buffer */

        SETADDR(ONEN_BUF_BLOCK, ONEN_BUF_PAGE)



        SETBUF_S()

        if (onenand_prog_spare(s, sec, s->count, buf))

            s->status |= ONEN_ERR_CMD | ONEN_ERR_PROG;



        /* TODO: if (s->bufaddr & 3) + s->count was > 4 (2k-pages)

         * or    if (s->bufaddr & 1) + s->count was > 2 (1k-pages)

         * then we need two split the read/write into two chunks.

         */

        s->intstatus |= ONEN_INT | ONEN_INT_PROG;

        break;

    case 0x1b:	/* Copy-back program */

        SETBUF_S()



        SETADDR(ONEN_BUF_BLOCK, ONEN_BUF_PAGE)

        if (onenand_load_main(s, sec, s->count, buf))

            s->status |= ONEN_ERR_CMD | ONEN_ERR_PROG;



        SETADDR(ONEN_BUF_DEST_BLOCK, ONEN_BUF_DEST_PAGE)

        if (onenand_prog_main(s, sec, s->count, buf))

            s->status |= ONEN_ERR_CMD | ONEN_ERR_PROG;



        /* TODO: spare areas */



        s->intstatus |= ONEN_INT | ONEN_INT_PROG;

        break;



    case 0x23:	/* Unlock NAND array block(s) */

        s->intstatus |= ONEN_INT;



        /* XXX the previous (?) area should be locked automatically */

        for (b = s->unladdr[0]; b <= s->unladdr[1]; b ++) {

            if (b >= s->blocks) {

                s->status |= ONEN_ERR_CMD;

                break;

            }

            if (s->blockwp[b] == ONEN_LOCK_LOCKTIGHTEN)

                break;



            s->wpstatus = s->blockwp[b] = ONEN_LOCK_UNLOCKED;

        }

        break;

    case 0x27:	/* Unlock All NAND array blocks */

        s->intstatus |= ONEN_INT;



        for (b = 0; b < s->blocks; b ++) {

            if (b >= s->blocks) {

                s->status |= ONEN_ERR_CMD;

                break;

            }

            if (s->blockwp[b] == ONEN_LOCK_LOCKTIGHTEN)

                break;



            s->wpstatus = s->blockwp[b] = ONEN_LOCK_UNLOCKED;

        }

        break;



    case 0x2a:	/* Lock NAND array block(s) */

        s->intstatus |= ONEN_INT;



        for (b = s->unladdr[0]; b <= s->unladdr[1]; b ++) {

            if (b >= s->blocks) {

                s->status |= ONEN_ERR_CMD;

                break;

            }

            if (s->blockwp[b] == ONEN_LOCK_LOCKTIGHTEN)

                break;



            s->wpstatus = s->blockwp[b] = ONEN_LOCK_LOCKED;

        }

        break;

    case 0x2c:	/* Lock-tight NAND array block(s) */

        s->intstatus |= ONEN_INT;



        for (b = s->unladdr[0]; b <= s->unladdr[1]; b ++) {

            if (b >= s->blocks) {

                s->status |= ONEN_ERR_CMD;

                break;

            }

            if (s->blockwp[b] == ONEN_LOCK_UNLOCKED)

                continue;



            s->wpstatus = s->blockwp[b] = ONEN_LOCK_LOCKTIGHTEN;

        }

        break;



    case 0x71:	/* Erase-Verify-Read */

        s->intstatus |= ONEN_INT;

        break;

    case 0x95:	/* Multi-block erase */

        qemu_irq_pulse(s->intr);

        /* Fall through.  */

    case 0x94:	/* Block erase */

        sec = ((s->addr[ONEN_BUF_BLOCK] & 0xfff) |

                        (s->addr[ONEN_BUF_BLOCK] >> 15 ? s->density_mask : 0))

                << (BLOCK_SHIFT - 9);

        if (onenand_erase(s, sec, 1 << (BLOCK_SHIFT - 9)))

            s->status |= ONEN_ERR_CMD | ONEN_ERR_ERASE;



        s->intstatus |= ONEN_INT | ONEN_INT_ERASE;

        break;

    case 0xb0:	/* Erase suspend */

        break;

    case 0x30:	/* Erase resume */

        s->intstatus |= ONEN_INT | ONEN_INT_ERASE;

        break;



    case 0xf0:	/* Reset NAND Flash core */

        onenand_reset(s, 0);

        break;

    case 0xf3:	/* Reset OneNAND */

        onenand_reset(s, 0);

        break;



    case 0x65:	/* OTP Access */

        s->intstatus |= ONEN_INT;

        s->bdrv_cur = NULL;

        s->current = s->otp;

        s->secs_cur = 1 << (BLOCK_SHIFT - 9);

        s->addr[ONEN_BUF_BLOCK] = 0;

        s->otpmode = 1;

        break;



    default:

        s->status |= ONEN_ERR_CMD;

        s->intstatus |= ONEN_INT;

        fprintf(stderr, ""%s: unknown OneNAND command %x\n"",

                        __func__, s->command);

    }



    onenand_intr_update(s);

}
","static void onenand_command(OneNANDState *s) {
int b; int sec; void *buf; #define SETADDR(block, page) \ sec = (s->addr[page] & 3) + \ ((((s->addr[page] >> 2) & 0x3f) + \ (((s->addr[block] & 0xfff) | \ (s->addr[block] >> 15 ? \ s->density_mask : 0)) << 6)) << (PAGE_SHIFT - 9)); #define SETBUF_M() \ buf = (s->bufaddr & 8) ? \ s->data[(s->bufaddr >> 2) & 1][0] : s->boot[0]; \ buf += (s->bufaddr & 3) << 9; #define SETBUF_S() \ buf = (s->bufaddr & 8) ? \ s->data[(s->bufaddr >> 2) & 1][1] : s->boot[1]; \ buf += (s->bufaddr & 3) << 4; switch (s->command) {
case 0x00: /* Load single/multiple sector data unit into buffer */ SETADDR(ONEN_BUF_BLOCK, ONEN_BUF_PAGE) SETBUF_M() if (onenand_load_main(s, sec, s->count, buf)) s->status |= ONEN_ERR_CMD | ONEN_ERR_LOAD; #if 0 SETBUF_S() if (onenand_load_spare(s, sec, s->count, buf)) s->status |= ONEN_ERR_CMD | ONEN_ERR_LOAD; #endif /* TODO: if (s->bufaddr & 3) + s->count was > 4 (2k-pages) * or if (s->bufaddr & 1) + s->count was > 2 (1k-pages) * then we need two split the read/write into two chunks. */ s->intstatus |= ONEN_INT | ONEN_INT_LOAD; break; case 0x13: /* Load single/multiple spare sector into buffer */ SETADDR(ONEN_BUF_BLOCK, ONEN_BUF_PAGE) SETBUF_S() if (onenand_load_spare(s, sec, s->count, buf)) s->status |= ONEN_ERR_CMD | ONEN_ERR_LOAD; /* TODO: if (s->bufaddr & 3) + s->count was > 4 (2k-pages) * or if (s->bufaddr & 1) + s->count was > 2 (1k-pages) * then we need two split the read/write into two chunks. */ s->intstatus |= ONEN_INT | ONEN_INT_LOAD; break; case 0x80: /* Program single/multiple sector data unit from buffer */ SETADDR(ONEN_BUF_BLOCK, ONEN_BUF_PAGE) SETBUF_M() if (onenand_prog_main(s, sec, s->count, buf)) s->status |= ONEN_ERR_CMD | ONEN_ERR_PROG; #if 0 SETBUF_S() if (onenand_prog_spare(s, sec, s->count, buf)) s->status |= ONEN_ERR_CMD | ONEN_ERR_PROG; #endif /* TODO: if (s->bufaddr & 3) + s->count was > 4 (2k-pages) * or if (s->bufaddr & 1) + s->count was > 2 (1k-pages) * then we need two split the read/write into two chunks. */ s->intstatus |= ONEN_INT | ONEN_INT_PROG; break; case 0x1a: /* Program single/multiple spare area sector from buffer */ SETADDR(ONEN_BUF_BLOCK, ONEN_BUF_PAGE) SETBUF_S() if (onenand_prog_spare(s, sec, s->count, buf)) s->status |= ONEN_ERR_CMD | ONEN_ERR_PROG; /* TODO: if (s->bufaddr & 3) + s->count was > 4 (2k-pages) * or if (s->bufaddr & 1) + s->count was > 2 (1k-pages) * then we need two split the read/write into two chunks. */ s->intstatus |= ONEN_INT | ONEN_INT_PROG; break; case 0x1b: /* Copy-back program */ SETBUF_S() SETADDR(ONEN_BUF_BLOCK, ONEN_BUF_PAGE) if (onenand_load_main(s, sec, s->count, buf)) s->status |= ONEN_ERR_CMD | ONEN_ERR_PROG; SETADDR(ONEN_BUF_DEST_BLOCK, ONEN_BUF_DEST_PAGE) if (onenand_prog_main(s, sec, s->count, buf)) s->status |= ONEN_ERR_CMD | ONEN_ERR_PROG; /* TODO: spare areas */ s->intstatus |= ONEN_INT | ONEN_INT_PROG; break; case 0x23: /* Unlock NAND array block(s) */ s->intstatus |= ONEN_INT; /* XXX the previous (?) area should be locked automatically */ for (b = s->unladdr[0]; b <= s->unladdr[1]; b ++) {
if (b >= s->blocks) {
s->status |= ONEN_ERR_CMD; break; } if (s->blockwp[b] == ONEN_LOCK_LOCKTIGHTEN) break; s->wpstatus = s->blockwp[b] = ONEN_LOCK_UNLOCKED; } break; case 0x27: /* Unlock All NAND array blocks */ s->intstatus |= ONEN_INT; for (b = 0; b < s->blocks; b ++) {
if (b >= s->blocks) {
s->status |= ONEN_ERR_CMD; break; } if (s->blockwp[b] == ONEN_LOCK_LOCKTIGHTEN) break; s->wpstatus = s->blockwp[b] = ONEN_LOCK_UNLOCKED; } break; case 0x2a: /* Lock NAND array block(s) */ s->intstatus |= ONEN_INT; for (b = s->unladdr[0]; b <= s->unladdr[1]; b ++) {
if (b >= s->blocks) {
s->status |= ONEN_ERR_CMD; break; } if (s->blockwp[b] == ONEN_LOCK_LOCKTIGHTEN) break; s->wpstatus = s->blockwp[b] = ONEN_LOCK_LOCKED; } break; case 0x2c: /* Lock-tight NAND array block(s) */ s->intstatus |= ONEN_INT; for (b = s->unladdr[0]; b <= s->unladdr[1]; b ++) {
if (b >= s->blocks) {
s->status |= ONEN_ERR_CMD; break; } if (s->blockwp[b] == ONEN_LOCK_UNLOCKED) continue; s->wpstatus = s->blockwp[b] = ONEN_LOCK_LOCKTIGHTEN; } break; case 0x71: /* Erase-Verify-Read */ s->intstatus |= ONEN_INT; break; case 0x95: /* Multi-block erase */ qemu_irq_pulse(s->intr); /* Fall through. */ case 0x94: /* Block erase */ sec = ((s->addr[ONEN_BUF_BLOCK] & 0xfff) | (s->addr[ONEN_BUF_BLOCK] >> 15 ? s->density_mask : 0)) << (BLOCK_SHIFT - 9); if (onenand_erase(s, sec, 1 << (BLOCK_SHIFT - 9))) s->status |= ONEN_ERR_CMD | ONEN_ERR_ERASE; s->intstatus |= ONEN_INT | ONEN_INT_ERASE; break; case 0xb0: /* Erase suspend */ break; case 0x30: /* Erase resume */ s->intstatus |= ONEN_INT | ONEN_INT_ERASE; break; case 0xf0: /* Reset NAND Flash core */ onenand_reset(s, 0); break; case 0xf3: /* Reset OneNAND */ onenand_reset(s, 0); break; case 0x65: /* OTP Access */ s->intstatus |= ONEN_INT; s->bdrv_cur = NULL; s->current = s->otp; s->secs_cur = 1 << (BLOCK_SHIFT - 9); s->addr[ONEN_BUF_BLOCK] = 0; s->otpmode = 1; break; default: s->status |= ONEN_ERR_CMD; s->intstatus |= ONEN_INT; fprintf(stderr, ""%s: unknown OneNAND command %x\n"", __func__, s->command); } onenand_intr_update(s); } ",qemu,0
"static uint32_t lan9118_readw(void *opaque, target_phys_addr_t offset)

{

    lan9118_state *s = (lan9118_state *)opaque;

    uint32_t val;



    if (s->read_word_prev_offset != (offset & ~0x3)) {

        /* New offset, reset word counter */

        s->read_word_n = 0;

        s->read_word_prev_offset = offset & ~0x3;

    }



    s->read_word_n++;

    if (s->read_word_n == 1) {

        s->read_long = lan9118_readl(s, offset & ~3, 4);

    } else {

        s->read_word_n = 0;

    }



    if (offset & 2) {

        val = s->read_long >> 16;

    } else {

        val = s->read_long & 0xFFFF;

    }



    //DPRINTF(""Readw reg 0x%02x, val 0x%x\n"", (int)offset, val);

    return val;

}
","static uint32_t lan9118_readw(void *opaque, target_phys_addr_t offset) {
lan9118_state *s = (lan9118_state *)opaque; uint32_t val; if (s->read_word_prev_offset != (offset & ~0x3)) {
/* New offset, reset word counter */ s->read_word_n = 0; s->read_word_prev_offset = offset & ~0x3; } s->read_word_n++; if (s->read_word_n == 1) {
s->read_long = lan9118_readl(s, offset & ~3, 4); } else {
s->read_word_n = 0; } if (offset & 2) {
val = s->read_long >> 16; } else {
val = s->read_long & 0xFFFF; } //DPRINTF(""Readw reg 0x%02x, val 0x%x\n"", (int)offset, val); return val; } ",qemu,0
"static void x86_cpu_expand_features(X86CPU *cpu, Error **errp)

{

    CPUX86State *env = &cpu->env;

    FeatureWord w;

    GList *l;

    Error *local_err = NULL;



    /*TODO: cpu->max_features incorrectly overwrites features

     * set using ""feat=on|off"". Once we fix this, we can convert

     * plus_features & minus_features to global properties

     * inside x86_cpu_parse_featurestr() too.

     */

    if (cpu->max_features) {

        for (w = 0; w < FEATURE_WORDS; w++) {

            env->features[w] =

                x86_cpu_get_supported_feature_word(w, cpu->migratable);

        }

    }



    for (l = plus_features; l; l = l->next) {

        const char *prop = l->data;

        object_property_set_bool(OBJECT(cpu), true, prop, &local_err);

        if (local_err) {

            goto out;

        }

    }



    for (l = minus_features; l; l = l->next) {

        const char *prop = l->data;

        object_property_set_bool(OBJECT(cpu), false, prop, &local_err);

        if (local_err) {

            goto out;

        }

    }



    if (!kvm_enabled() || !cpu->expose_kvm) {

        env->features[FEAT_KVM] = 0;

    }



    x86_cpu_enable_xsave_components(cpu);



    /* CPUID[EAX=7,ECX=0].EBX always increased level automatically: */

    x86_cpu_adjust_feat_level(cpu, FEAT_7_0_EBX);

    if (cpu->full_cpuid_auto_level) {

        x86_cpu_adjust_feat_level(cpu, FEAT_1_EDX);

        x86_cpu_adjust_feat_level(cpu, FEAT_1_ECX);

        x86_cpu_adjust_feat_level(cpu, FEAT_6_EAX);

        x86_cpu_adjust_feat_level(cpu, FEAT_7_0_ECX);

        x86_cpu_adjust_feat_level(cpu, FEAT_8000_0001_EDX);

        x86_cpu_adjust_feat_level(cpu, FEAT_8000_0001_ECX);

        x86_cpu_adjust_feat_level(cpu, FEAT_8000_0007_EDX);

        x86_cpu_adjust_feat_level(cpu, FEAT_C000_0001_EDX);

        x86_cpu_adjust_feat_level(cpu, FEAT_SVM);

        x86_cpu_adjust_feat_level(cpu, FEAT_XSAVE);

        /* SVM requires CPUID[0x8000000A] */

        if (env->features[FEAT_8000_0001_ECX] & CPUID_EXT3_SVM) {

            x86_cpu_adjust_level(cpu, &env->cpuid_min_xlevel, 0x8000000A);

        }

    }



    /* Set cpuid_*level* based on cpuid_min_*level, if not explicitly set */

    if (env->cpuid_level == UINT32_MAX) {

        env->cpuid_level = env->cpuid_min_level;

    }

    if (env->cpuid_xlevel == UINT32_MAX) {

        env->cpuid_xlevel = env->cpuid_min_xlevel;

    }

    if (env->cpuid_xlevel2 == UINT32_MAX) {

        env->cpuid_xlevel2 = env->cpuid_min_xlevel2;

    }



out:

    if (local_err != NULL) {

        error_propagate(errp, local_err);

    }

}
","static void x86_cpu_expand_features(X86CPU *cpu, Error **errp) {
CPUX86State *env = &cpu->env; FeatureWord w; GList *l; Error *local_err = NULL; /*TODO: cpu->max_features incorrectly overwrites features * set using ""feat=on|off"". Once we fix this, we can convert * plus_features & minus_features to global properties * inside x86_cpu_parse_featurestr() too. */ if (cpu->max_features) {
for (w = 0; w < FEATURE_WORDS; w++) {
env->features[w] = x86_cpu_get_supported_feature_word(w, cpu->migratable); } } for (l = plus_features; l; l = l->next) {
const char *prop = l->data; object_property_set_bool(OBJECT(cpu), true, prop, &local_err); if (local_err) {
goto out; } } for (l = minus_features; l; l = l->next) {
const char *prop = l->data; object_property_set_bool(OBJECT(cpu), false, prop, &local_err); if (local_err) {
goto out; } } if (!kvm_enabled() || !cpu->expose_kvm) {
env->features[FEAT_KVM] = 0; } x86_cpu_enable_xsave_components(cpu); /* CPUID[EAX=7,ECX=0].EBX always increased level automatically: */ x86_cpu_adjust_feat_level(cpu, FEAT_7_0_EBX); if (cpu->full_cpuid_auto_level) {
x86_cpu_adjust_feat_level(cpu, FEAT_1_EDX); x86_cpu_adjust_feat_level(cpu, FEAT_1_ECX); x86_cpu_adjust_feat_level(cpu, FEAT_6_EAX); x86_cpu_adjust_feat_level(cpu, FEAT_7_0_ECX); x86_cpu_adjust_feat_level(cpu, FEAT_8000_0001_EDX); x86_cpu_adjust_feat_level(cpu, FEAT_8000_0001_ECX); x86_cpu_adjust_feat_level(cpu, FEAT_8000_0007_EDX); x86_cpu_adjust_feat_level(cpu, FEAT_C000_0001_EDX); x86_cpu_adjust_feat_level(cpu, FEAT_SVM); x86_cpu_adjust_feat_level(cpu, FEAT_XSAVE); /* SVM requires CPUID[0x8000000A] */ if (env->features[FEAT_8000_0001_ECX] & CPUID_EXT3_SVM) {
x86_cpu_adjust_level(cpu, &env->cpuid_min_xlevel, 0x8000000A); } } /* Set cpuid_*level* based on cpuid_min_*level, if not explicitly set */ if (env->cpuid_level == UINT32_MAX) {
env->cpuid_level = env->cpuid_min_level; } if (env->cpuid_xlevel == UINT32_MAX) {
env->cpuid_xlevel = env->cpuid_min_xlevel; } if (env->cpuid_xlevel2 == UINT32_MAX) {
env->cpuid_xlevel2 = env->cpuid_min_xlevel2; } out: if (local_err != NULL) {
error_propagate(errp, local_err); } } ",qemu,0
"int bdrv_has_zero_init_1(BlockDriverState *bs)

{

    return 1;

}
","int bdrv_has_zero_init_1(BlockDriverState *bs) {
return 1; } ",qemu,0
"static void intel_hda_update_int_sts(IntelHDAState *d)

{

    uint32_t sts = 0;

    uint32_t i;



    /* update controller status */

    if (d->rirb_sts & ICH6_RBSTS_IRQ) {

        sts |= (1 << 30);

    }

    if (d->rirb_sts & ICH6_RBSTS_OVERRUN) {

        sts |= (1 << 30);

    }

    if (d->state_sts & d->wake_en) {

        sts |= (1 << 30);

    }



    /* update stream status */

    for (i = 0; i < 8; i++) {

        /* buffer completion interrupt */

        if (d->st[i].ctl & (1 << 26)) {

            sts |= (1 << i);

        }

    }



    /* update global status */

    if (sts & d->int_ctl) {

        sts |= (1 << 31);

    }



    d->int_sts = sts;

}
","static void intel_hda_update_int_sts(IntelHDAState *d) {
uint32_t sts = 0; uint32_t i; /* update controller status */ if (d->rirb_sts & ICH6_RBSTS_IRQ) {
sts |= (1 << 30); } if (d->rirb_sts & ICH6_RBSTS_OVERRUN) {
sts |= (1 << 30); } if (d->state_sts & d->wake_en) {
sts |= (1 << 30); } /* update stream status */ for (i = 0; i < 8; i++) {
/* buffer completion interrupt */ if (d->st[i].ctl & (1 << 26)) {
sts |= (1 << i); } } /* update global status */ if (sts & d->int_ctl) {
sts |= (1 << 31); } d->int_sts = sts; } ",qemu,1
"static uint64_t imx_serial_read(void *opaque, hwaddr offset,

                                unsigned size)

{

    IMXSerialState *s = (IMXSerialState *)opaque;

    uint32_t c;



    DPRINTF(""read(offset=%x)\n"", offset >> 2);

    switch (offset >> 2) {

    case 0x0: /* URXD */

        c = s->readbuff;

        if (!(s->uts1 & UTS1_RXEMPTY)) {

            /* Character is valid */

            c |= URXD_CHARRDY;

            s->usr1 &= ~USR1_RRDY;

            s->usr2 &= ~USR2_RDR;

            s->uts1 |= UTS1_RXEMPTY;

            imx_update(s);

            qemu_chr_accept_input(s->chr);

        }

        return c;



    case 0x20: /* UCR1 */

        return s->ucr1;



    case 0x21: /* UCR2 */

        return s->ucr2;



    case 0x25: /* USR1 */

        return s->usr1;



    case 0x26: /* USR2 */

        return s->usr2;



    case 0x2A: /* BRM Modulator */

        return s->ubmr;



    case 0x2B: /* Baud Rate Count */

        return s->ubrc;



    case 0x2d: /* Test register */

        return s->uts1;



    case 0x24: /* UFCR */

        return s->ufcr;



    case 0x2c:

        return s->onems;



    case 0x22: /* UCR3 */

        return s->ucr3;



    case 0x23: /* UCR4 */

    case 0x29: /* BRM Incremental */

        return 0x0; /* TODO */



    default:

        IPRINTF(""%s: bad offset: 0x%x\n"", __func__, (int)offset);

        return 0;

    }

}
","static uint64_t imx_serial_read(void *opaque, hwaddr offset, unsigned size) {
IMXSerialState *s = (IMXSerialState *)opaque; uint32_t c; DPRINTF(""read(offset=%x)\n"", offset >> 2); switch (offset >> 2) {
case 0x0: /* URXD */ c = s->readbuff; if (!(s->uts1 & UTS1_RXEMPTY)) {
/* Character is valid */ c |= URXD_CHARRDY; s->usr1 &= ~USR1_RRDY; s->usr2 &= ~USR2_RDR; s->uts1 |= UTS1_RXEMPTY; imx_update(s); qemu_chr_accept_input(s->chr); } return c; case 0x20: /* UCR1 */ return s->ucr1; case 0x21: /* UCR2 */ return s->ucr2; case 0x25: /* USR1 */ return s->usr1; case 0x26: /* USR2 */ return s->usr2; case 0x2A: /* BRM Modulator */ return s->ubmr; case 0x2B: /* Baud Rate Count */ return s->ubrc; case 0x2d: /* Test register */ return s->uts1; case 0x24: /* UFCR */ return s->ufcr; case 0x2c: return s->onems; case 0x22: /* UCR3 */ return s->ucr3; case 0x23: /* UCR4 */ case 0x29: /* BRM Incremental */ return 0x0; /* TODO */ default: IPRINTF(""%s: bad offset: 0x%x\n"", __func__, (int)offset); return 0; } } ",qemu,1
"int coroutine_fn bdrv_co_write_zeroes(BlockDriverState *bs,

                                      int64_t sector_num, int nb_sectors,

                                      BdrvRequestFlags flags)

{

    trace_bdrv_co_write_zeroes(bs, sector_num, nb_sectors, flags);



    if (!(bs->open_flags & BDRV_O_UNMAP)) {

        flags &= ~BDRV_REQ_MAY_UNMAP;

    }



    return bdrv_co_do_writev(bs, sector_num, nb_sectors, NULL,

                             BDRV_REQ_ZERO_WRITE | flags);

}
","int coroutine_fn bdrv_co_write_zeroes(BlockDriverState *bs, int64_t sector_num, int nb_sectors, BdrvRequestFlags flags) {
trace_bdrv_co_write_zeroes(bs, sector_num, nb_sectors, flags); if (!(bs->open_flags & BDRV_O_UNMAP)) {
flags &= ~BDRV_REQ_MAY_UNMAP; } return bdrv_co_do_writev(bs, sector_num, nb_sectors, NULL, BDRV_REQ_ZERO_WRITE | flags); } ",qemu,0
"static void gen_mtspr(DisasContext *ctx)

{

    void (*write_cb)(DisasContext *ctx, int sprn, int gprn);

    uint32_t sprn = SPR(ctx->opcode);



#if defined(CONFIG_USER_ONLY)

    write_cb = ctx->spr_cb[sprn].uea_write;

#else

    if (ctx->pr) {

        write_cb = ctx->spr_cb[sprn].uea_write;

    } else if (ctx->hv) {

        write_cb = ctx->spr_cb[sprn].hea_write;

    } else {

        write_cb = ctx->spr_cb[sprn].oea_write;

    }

#endif

    if (likely(write_cb != NULL)) {

        if (likely(write_cb != SPR_NOACCESS)) {

            (*write_cb)(ctx, sprn, rS(ctx->opcode));

        } else {

            /* Privilege exception */

            fprintf(stderr, ""Trying to write privileged spr %d (0x%03x) at ""

                    TARGET_FMT_lx ""\n"", sprn, sprn, ctx->nip - 4);

            if (qemu_log_separate()) {

                qemu_log(""Trying to write privileged spr %d (0x%03x) at ""

                         TARGET_FMT_lx ""\n"", sprn, sprn, ctx->nip - 4);

            }

            gen_inval_exception(ctx, POWERPC_EXCP_PRIV_REG);

        }

    } else {

        /* Not defined */

        if (qemu_log_separate()) {

            qemu_log(""Trying to write invalid spr %d (0x%03x) at ""

                     TARGET_FMT_lx ""\n"", sprn, sprn, ctx->nip - 4);

        }

        fprintf(stderr, ""Trying to write invalid spr %d (0x%03x) at ""

                TARGET_FMT_lx ""\n"", sprn, sprn, ctx->nip - 4);

        gen_inval_exception(ctx, POWERPC_EXCP_INVAL_SPR);

    }

}
","static void gen_mtspr(DisasContext *ctx) {
void (*write_cb)(DisasContext *ctx, int sprn, int gprn); uint32_t sprn = SPR(ctx->opcode); #if defined(CONFIG_USER_ONLY) write_cb = ctx->spr_cb[sprn].uea_write; #else if (ctx->pr) {
write_cb = ctx->spr_cb[sprn].uea_write; } else if (ctx->hv) {
write_cb = ctx->spr_cb[sprn].hea_write; } else {
write_cb = ctx->spr_cb[sprn].oea_write; } #endif if (likely(write_cb != NULL)) {
if (likely(write_cb != SPR_NOACCESS)) {
(*write_cb)(ctx, sprn, rS(ctx->opcode)); } else {
/* Privilege exception */ fprintf(stderr, ""Trying to write privileged spr %d (0x%03x) at "" TARGET_FMT_lx ""\n"", sprn, sprn, ctx->nip - 4); if (qemu_log_separate()) {
qemu_log(""Trying to write privileged spr %d (0x%03x) at "" TARGET_FMT_lx ""\n"", sprn, sprn, ctx->nip - 4); } gen_inval_exception(ctx, POWERPC_EXCP_PRIV_REG); } } else {
/* Not defined */ if (qemu_log_separate()) {
qemu_log(""Trying to write invalid spr %d (0x%03x) at "" TARGET_FMT_lx ""\n"", sprn, sprn, ctx->nip - 4); } fprintf(stderr, ""Trying to write invalid spr %d (0x%03x) at "" TARGET_FMT_lx ""\n"", sprn, sprn, ctx->nip - 4); gen_inval_exception(ctx, POWERPC_EXCP_INVAL_SPR); } } ",qemu,1
"static int coroutine_fn bdrv_co_do_copy_on_readv(BlockDriverState *bs,

        int64_t sector_num, int nb_sectors, QEMUIOVector *qiov)

{

    /* Perform I/O through a temporary buffer so that users who scribble over

     * their read buffer while the operation is in progress do not end up

     * modifying the image file.  This is critical for zero-copy guest I/O

     * where anything might happen inside guest memory.

     */

    void *bounce_buffer;



    BlockDriver *drv = bs->drv;

    struct iovec iov;

    QEMUIOVector bounce_qiov;

    int64_t cluster_sector_num;

    int cluster_nb_sectors;

    size_t skip_bytes;

    int ret;



    /* Cover entire cluster so no additional backing file I/O is required when

     * allocating cluster in the image file.

     */

    bdrv_round_to_clusters(bs, sector_num, nb_sectors,

                           &cluster_sector_num, &cluster_nb_sectors);



    trace_bdrv_co_do_copy_on_readv(bs, sector_num, nb_sectors,

                                   cluster_sector_num, cluster_nb_sectors);



    iov.iov_len = cluster_nb_sectors * BDRV_SECTOR_SIZE;

    iov.iov_base = bounce_buffer = qemu_blockalign(bs, iov.iov_len);

    qemu_iovec_init_external(&bounce_qiov, &iov, 1);



    ret = drv->bdrv_co_readv(bs, cluster_sector_num, cluster_nb_sectors,

                             &bounce_qiov);

    if (ret < 0) {

        goto err;

    }



    if (drv->bdrv_co_write_zeroes &&

        buffer_is_zero(bounce_buffer, iov.iov_len)) {

        ret = bdrv_co_do_write_zeroes(bs, cluster_sector_num,

                                      cluster_nb_sectors, 0);

    } else {

        /* This does not change the data on the disk, it is not necessary

         * to flush even in cache=writethrough mode.

         */

        ret = drv->bdrv_co_writev(bs, cluster_sector_num, cluster_nb_sectors,

                                  &bounce_qiov);

    }



    if (ret < 0) {

        /* It might be okay to ignore write errors for guest requests.  If this

         * is a deliberate copy-on-read then we don't want to ignore the error.

         * Simply report it in all cases.

         */

        goto err;

    }



    skip_bytes = (sector_num - cluster_sector_num) * BDRV_SECTOR_SIZE;

    qemu_iovec_from_buf(qiov, 0, bounce_buffer + skip_bytes,

                        nb_sectors * BDRV_SECTOR_SIZE);



err:

    qemu_vfree(bounce_buffer);

    return ret;

}
","static int coroutine_fn bdrv_co_do_copy_on_readv(BlockDriverState *bs, int64_t sector_num, int nb_sectors, QEMUIOVector *qiov) {
/* Perform I/O through a temporary buffer so that users who scribble over * their read buffer while the operation is in progress do not end up * modifying the image file. This is critical for zero-copy guest I/O * where anything might happen inside guest memory. */ void *bounce_buffer; BlockDriver *drv = bs->drv; struct iovec iov; QEMUIOVector bounce_qiov; int64_t cluster_sector_num; int cluster_nb_sectors; size_t skip_bytes; int ret; /* Cover entire cluster so no additional backing file I/O is required when * allocating cluster in the image file. */ bdrv_round_to_clusters(bs, sector_num, nb_sectors, &cluster_sector_num, &cluster_nb_sectors); trace_bdrv_co_do_copy_on_readv(bs, sector_num, nb_sectors, cluster_sector_num, cluster_nb_sectors); iov.iov_len = cluster_nb_sectors * BDRV_SECTOR_SIZE; iov.iov_base = bounce_buffer = qemu_blockalign(bs, iov.iov_len); qemu_iovec_init_external(&bounce_qiov, &iov, 1); ret = drv->bdrv_co_readv(bs, cluster_sector_num, cluster_nb_sectors, &bounce_qiov); if (ret < 0) {
goto err; } if (drv->bdrv_co_write_zeroes && buffer_is_zero(bounce_buffer, iov.iov_len)) {
ret = bdrv_co_do_write_zeroes(bs, cluster_sector_num, cluster_nb_sectors, 0); } else {
/* This does not change the data on the disk, it is not necessary * to flush even in cache=writethrough mode. */ ret = drv->bdrv_co_writev(bs, cluster_sector_num, cluster_nb_sectors, &bounce_qiov); } if (ret < 0) {
/* It might be okay to ignore write errors for guest requests. If this * is a deliberate copy-on-read then we don't want to ignore the error. * Simply report it in all cases. */ goto err; } skip_bytes = (sector_num - cluster_sector_num) * BDRV_SECTOR_SIZE; qemu_iovec_from_buf(qiov, 0, bounce_buffer + skip_bytes, nb_sectors * BDRV_SECTOR_SIZE); err: qemu_vfree(bounce_buffer); return ret; } ",qemu,1
"static ram_addr_t ram_save_remaining(void)

{

    return ram_list.dirty_pages;

}
","static ram_addr_t ram_save_remaining(void) {
return ram_list.dirty_pages; } ",qemu,0
"static void tswap_siginfo(target_siginfo_t *tinfo,

                          const target_siginfo_t *info)

{

    int sig = info->si_signo;

    tinfo->si_signo = tswap32(sig);

    tinfo->si_errno = tswap32(info->si_errno);

    tinfo->si_code = tswap32(info->si_code);



    if (sig == TARGET_SIGILL || sig == TARGET_SIGFPE || sig == TARGET_SIGSEGV

        || sig == TARGET_SIGBUS || sig == TARGET_SIGTRAP) {

        tinfo->_sifields._sigfault._addr

            = tswapal(info->_sifields._sigfault._addr);

    } else if (sig == TARGET_SIGIO) {

        tinfo->_sifields._sigpoll._band

            = tswap32(info->_sifields._sigpoll._band);

        tinfo->_sifields._sigpoll._fd = tswap32(info->_sifields._sigpoll._fd);

    } else if (sig == TARGET_SIGCHLD) {

        tinfo->_sifields._sigchld._pid

            = tswap32(info->_sifields._sigchld._pid);

        tinfo->_sifields._sigchld._uid

            = tswap32(info->_sifields._sigchld._uid);

        tinfo->_sifields._sigchld._status

            = tswap32(info->_sifields._sigchld._status);

        tinfo->_sifields._sigchld._utime

            = tswapal(info->_sifields._sigchld._utime);

        tinfo->_sifields._sigchld._stime

            = tswapal(info->_sifields._sigchld._stime);

    } else if (sig >= TARGET_SIGRTMIN) {

        tinfo->_sifields._rt._pid = tswap32(info->_sifields._rt._pid);

        tinfo->_sifields._rt._uid = tswap32(info->_sifields._rt._uid);

        tinfo->_sifields._rt._sigval.sival_ptr

            = tswapal(info->_sifields._rt._sigval.sival_ptr);

    }

}
","static void tswap_siginfo(target_siginfo_t *tinfo, const target_siginfo_t *info) {
int sig = info->si_signo; tinfo->si_signo = tswap32(sig); tinfo->si_errno = tswap32(info->si_errno); tinfo->si_code = tswap32(info->si_code); if (sig == TARGET_SIGILL || sig == TARGET_SIGFPE || sig == TARGET_SIGSEGV || sig == TARGET_SIGBUS || sig == TARGET_SIGTRAP) {
tinfo->_sifields._sigfault._addr = tswapal(info->_sifields._sigfault._addr); } else if (sig == TARGET_SIGIO) {
tinfo->_sifields._sigpoll._band = tswap32(info->_sifields._sigpoll._band); tinfo->_sifields._sigpoll._fd = tswap32(info->_sifields._sigpoll._fd); } else if (sig == TARGET_SIGCHLD) {
tinfo->_sifields._sigchld._pid = tswap32(info->_sifields._sigchld._pid); tinfo->_sifields._sigchld._uid = tswap32(info->_sifields._sigchld._uid); tinfo->_sifields._sigchld._status = tswap32(info->_sifields._sigchld._status); tinfo->_sifields._sigchld._utime = tswapal(info->_sifields._sigchld._utime); tinfo->_sifields._sigchld._stime = tswapal(info->_sifields._sigchld._stime); } else if (sig >= TARGET_SIGRTMIN) {
tinfo->_sifields._rt._pid = tswap32(info->_sifields._rt._pid); tinfo->_sifields._rt._uid = tswap32(info->_sifields._rt._uid); tinfo->_sifields._rt._sigval.sival_ptr = tswapal(info->_sifields._rt._sigval.sival_ptr); } } ",qemu,1
"void do_tw (int flags)

{

    if (!likely(!((Ts0 < Ts1 && (flags & 0x10)) ||

                  (Ts0 > Ts1 && (flags & 0x08)) ||

                  (Ts0 == Ts1 && (flags & 0x04)) ||

                  (T0 < T1 && (flags & 0x02)) ||

                  (T0 > T1 && (flags & 0x01)))))

        do_raise_exception_err(EXCP_PROGRAM, EXCP_TRAP);

}
","void do_tw (int flags) {
if (!likely(!((Ts0 < Ts1 && (flags & 0x10)) || (Ts0 > Ts1 && (flags & 0x08)) || (Ts0 == Ts1 && (flags & 0x04)) || (T0 < T1 && (flags & 0x02)) || (T0 > T1 && (flags & 0x01))))) do_raise_exception_err(EXCP_PROGRAM, EXCP_TRAP); } ",qemu,1
"static void vga_draw_text(VGAState *s, int full_update)

{

    int cx, cy, cheight, cw, ch, cattr, height, width, ch_attr;

    int cx_min, cx_max, linesize, x_incr;

    uint32_t offset, fgcol, bgcol, v, cursor_offset;

    uint8_t *d1, *d, *src, *s1, *dest, *cursor_ptr;

    const uint8_t *font_ptr, *font_base[2];

    int dup9, line_offset, depth_index;

    uint32_t *palette;

    uint32_t *ch_attr_ptr;

    vga_draw_glyph8_func *vga_draw_glyph8;

    vga_draw_glyph9_func *vga_draw_glyph9;



    vga_dirty_log_stop(s);



    /* compute font data address (in plane 2) */

    v = s->sr[3];

    offset = (((v >> 4) & 1) | ((v << 1) & 6)) * 8192 * 4 + 2;

    if (offset != s->font_offsets[0]) {

        s->font_offsets[0] = offset;

        full_update = 1;

    }

    font_base[0] = s->vram_ptr + offset;



    offset = (((v >> 5) & 1) | ((v >> 1) & 6)) * 8192 * 4 + 2;

    font_base[1] = s->vram_ptr + offset;

    if (offset != s->font_offsets[1]) {

        s->font_offsets[1] = offset;

        full_update = 1;

    }

    if (s->plane_updated & (1 << 2)) {

        /* if the plane 2 was modified since the last display, it

           indicates the font may have been modified */

        s->plane_updated = 0;

        full_update = 1;

    }

    full_update |= update_basic_params(s);



    line_offset = s->line_offset;

    s1 = s->vram_ptr + (s->start_addr * 4);



    vga_get_text_resolution(s, &width, &height, &cw, &cheight);

    x_incr = cw * ((ds_get_bits_per_pixel(s->ds) + 7) >> 3);

    if ((height * width) > CH_ATTR_SIZE) {

        /* better than nothing: exit if transient size is too big */

        return;

    }



    if (width != s->last_width || height != s->last_height ||

        cw != s->last_cw || cheight != s->last_ch || s->last_depth) {

        s->last_scr_width = width * cw;

        s->last_scr_height = height * cheight;

        qemu_console_resize(s->ds, s->last_scr_width, s->last_scr_height);

        s->last_depth = 0;

        s->last_width = width;

        s->last_height = height;

        s->last_ch = cheight;

        s->last_cw = cw;

        full_update = 1;

    }

    s->rgb_to_pixel =

        rgb_to_pixel_dup_table[get_depth_index(s->ds)];

    full_update |= update_palette16(s);

    palette = s->last_palette;

    x_incr = cw * ((ds_get_bits_per_pixel(s->ds) + 7) >> 3);



    cursor_offset = ((s->cr[0x0e] << 8) | s->cr[0x0f]) - s->start_addr;

    if (cursor_offset != s->cursor_offset ||

        s->cr[0xa] != s->cursor_start ||

        s->cr[0xb] != s->cursor_end) {

      /* if the cursor position changed, we update the old and new

         chars */

        if (s->cursor_offset < CH_ATTR_SIZE)

            s->last_ch_attr[s->cursor_offset] = -1;

        if (cursor_offset < CH_ATTR_SIZE)

            s->last_ch_attr[cursor_offset] = -1;

        s->cursor_offset = cursor_offset;

        s->cursor_start = s->cr[0xa];

        s->cursor_end = s->cr[0xb];

    }

    cursor_ptr = s->vram_ptr + (s->start_addr + cursor_offset) * 4;



    depth_index = get_depth_index(s->ds);

    if (cw == 16)

        vga_draw_glyph8 = vga_draw_glyph16_table[depth_index];

    else

        vga_draw_glyph8 = vga_draw_glyph8_table[depth_index];

    vga_draw_glyph9 = vga_draw_glyph9_table[depth_index];



    dest = ds_get_data(s->ds);

    linesize = ds_get_linesize(s->ds);

    ch_attr_ptr = s->last_ch_attr;

    for(cy = 0; cy < height; cy++) {

        d1 = dest;

        src = s1;

        cx_min = width;

        cx_max = -1;

        for(cx = 0; cx < width; cx++) {

            ch_attr = *(uint16_t *)src;

            if (full_update || ch_attr != *ch_attr_ptr) {

                if (cx < cx_min)

                    cx_min = cx;

                if (cx > cx_max)

                    cx_max = cx;

                *ch_attr_ptr = ch_attr;

#ifdef WORDS_BIGENDIAN

                ch = ch_attr >> 8;

                cattr = ch_attr & 0xff;

#else

                ch = ch_attr & 0xff;

                cattr = ch_attr >> 8;

#endif

                font_ptr = font_base[(cattr >> 3) & 1];

                font_ptr += 32 * 4 * ch;

                bgcol = palette[cattr >> 4];

                fgcol = palette[cattr & 0x0f];

                if (cw != 9) {

                    vga_draw_glyph8(d1, linesize,

                                    font_ptr, cheight, fgcol, bgcol);

                } else {

                    dup9 = 0;

                    if (ch >= 0xb0 && ch <= 0xdf && (s->ar[0x10] & 0x04))

                        dup9 = 1;

                    vga_draw_glyph9(d1, linesize,

                                    font_ptr, cheight, fgcol, bgcol, dup9);

                }

                if (src == cursor_ptr &&

                    !(s->cr[0x0a] & 0x20)) {

                    int line_start, line_last, h;

                    /* draw the cursor */

                    line_start = s->cr[0x0a] & 0x1f;

                    line_last = s->cr[0x0b] & 0x1f;

                    /* XXX: check that */

                    if (line_last > cheight - 1)

                        line_last = cheight - 1;

                    if (line_last >= line_start && line_start < cheight) {

                        h = line_last - line_start + 1;

                        d = d1 + linesize * line_start;

                        if (cw != 9) {

                            vga_draw_glyph8(d, linesize,

                                            cursor_glyph, h, fgcol, bgcol);

                        } else {

                            vga_draw_glyph9(d, linesize,

                                            cursor_glyph, h, fgcol, bgcol, 1);

                        }

                    }

                }

            }

            d1 += x_incr;

            src += 4;

            ch_attr_ptr++;

        }

        if (cx_max != -1) {

            dpy_update(s->ds, cx_min * cw, cy * cheight,

                       (cx_max - cx_min + 1) * cw, cheight);

        }

        dest += linesize * cheight;

        s1 += line_offset;

    }

}
","static void vga_draw_text(VGAState *s, int full_update) {
int cx, cy, cheight, cw, ch, cattr, height, width, ch_attr; int cx_min, cx_max, linesize, x_incr; uint32_t offset, fgcol, bgcol, v, cursor_offset; uint8_t *d1, *d, *src, *s1, *dest, *cursor_ptr; const uint8_t *font_ptr, *font_base[2]; int dup9, line_offset, depth_index; uint32_t *palette; uint32_t *ch_attr_ptr; vga_draw_glyph8_func *vga_draw_glyph8; vga_draw_glyph9_func *vga_draw_glyph9; vga_dirty_log_stop(s); /* compute font data address (in plane 2) */ v = s->sr[3]; offset = (((v >> 4) & 1) | ((v << 1) & 6)) * 8192 * 4 + 2; if (offset != s->font_offsets[0]) {
s->font_offsets[0] = offset; full_update = 1; } font_base[0] = s->vram_ptr + offset; offset = (((v >> 5) & 1) | ((v >> 1) & 6)) * 8192 * 4 + 2; font_base[1] = s->vram_ptr + offset; if (offset != s->font_offsets[1]) {
s->font_offsets[1] = offset; full_update = 1; } if (s->plane_updated & (1 << 2)) {
/* if the plane 2 was modified since the last display, it indicates the font may have been modified */ s->plane_updated = 0; full_update = 1; } full_update |= update_basic_params(s); line_offset = s->line_offset; s1 = s->vram_ptr + (s->start_addr * 4); vga_get_text_resolution(s, &width, &height, &cw, &cheight); x_incr = cw * ((ds_get_bits_per_pixel(s->ds) + 7) >> 3); if ((height * width) > CH_ATTR_SIZE) {
/* better than nothing: exit if transient size is too big */ return; } if (width != s->last_width || height != s->last_height || cw != s->last_cw || cheight != s->last_ch || s->last_depth) {
s->last_scr_width = width * cw; s->last_scr_height = height * cheight; qemu_console_resize(s->ds, s->last_scr_width, s->last_scr_height); s->last_depth = 0; s->last_width = width; s->last_height = height; s->last_ch = cheight; s->last_cw = cw; full_update = 1; } s->rgb_to_pixel = rgb_to_pixel_dup_table[get_depth_index(s->ds)]; full_update |= update_palette16(s); palette = s->last_palette; x_incr = cw * ((ds_get_bits_per_pixel(s->ds) + 7) >> 3); cursor_offset = ((s->cr[0x0e] << 8) | s->cr[0x0f]) - s->start_addr; if (cursor_offset != s->cursor_offset || s->cr[0xa] != s->cursor_start || s->cr[0xb] != s->cursor_end) {
/* if the cursor position changed, we update the old and new chars */ if (s->cursor_offset < CH_ATTR_SIZE) s->last_ch_attr[s->cursor_offset] = -1; if (cursor_offset < CH_ATTR_SIZE) s->last_ch_attr[cursor_offset] = -1; s->cursor_offset = cursor_offset; s->cursor_start = s->cr[0xa]; s->cursor_end = s->cr[0xb]; } cursor_ptr = s->vram_ptr + (s->start_addr + cursor_offset) * 4; depth_index = get_depth_index(s->ds); if (cw == 16) vga_draw_glyph8 = vga_draw_glyph16_table[depth_index]; else vga_draw_glyph8 = vga_draw_glyph8_table[depth_index]; vga_draw_glyph9 = vga_draw_glyph9_table[depth_index]; dest = ds_get_data(s->ds); linesize = ds_get_linesize(s->ds); ch_attr_ptr = s->last_ch_attr; for(cy = 0; cy < height; cy++) {
d1 = dest; src = s1; cx_min = width; cx_max = -1; for(cx = 0; cx < width; cx++) {
ch_attr = *(uint16_t *)src; if (full_update || ch_attr != *ch_attr_ptr) {
if (cx < cx_min) cx_min = cx; if (cx > cx_max) cx_max = cx; *ch_attr_ptr = ch_attr; #ifdef WORDS_BIGENDIAN ch = ch_attr >> 8; cattr = ch_attr & 0xff; #else ch = ch_attr & 0xff; cattr = ch_attr >> 8; #endif font_ptr = font_base[(cattr >> 3) & 1]; font_ptr += 32 * 4 * ch; bgcol = palette[cattr >> 4]; fgcol = palette[cattr & 0x0f]; if (cw != 9) {
vga_draw_glyph8(d1, linesize, font_ptr, cheight, fgcol, bgcol); } else {
dup9 = 0; if (ch >= 0xb0 && ch <= 0xdf && (s->ar[0x10] & 0x04)) dup9 = 1; vga_draw_glyph9(d1, linesize, font_ptr, cheight, fgcol, bgcol, dup9); } if (src == cursor_ptr && !(s->cr[0x0a] & 0x20)) {
int line_start, line_last, h; /* draw the cursor */ line_start = s->cr[0x0a] & 0x1f; line_last = s->cr[0x0b] & 0x1f; /* XXX: check that */ if (line_last > cheight - 1) line_last = cheight - 1; if (line_last >= line_start && line_start < cheight) {
h = line_last - line_start + 1; d = d1 + linesize * line_start; if (cw != 9) {
vga_draw_glyph8(d, linesize, cursor_glyph, h, fgcol, bgcol); } else {
vga_draw_glyph9(d, linesize, cursor_glyph, h, fgcol, bgcol, 1); } } } } d1 += x_incr; src += 4; ch_attr_ptr++; } if (cx_max != -1) {
dpy_update(s->ds, cx_min * cw, cy * cheight, (cx_max - cx_min + 1) * cw, cheight); } dest += linesize * cheight; s1 += line_offset; } } ",qemu,1
"void memory_region_init_io(MemoryRegion *mr,

                           const MemoryRegionOps *ops,

                           void *opaque,

                           const char *name,

                           uint64_t size)

{

    memory_region_init(mr, name, size);

    mr->ops = ops;

    mr->opaque = opaque;

    mr->terminates = true;

    mr->backend_registered = false;

}
","void memory_region_init_io(MemoryRegion *mr, const MemoryRegionOps *ops, void *opaque, const char *name, uint64_t size) {
memory_region_init(mr, name, size); mr->ops = ops; mr->opaque = opaque; mr->terminates = true; mr->backend_registered = false; } ",qemu,0
"void qtest_add_func(const char *str, void (*fn))

{

    gchar *path = g_strdup_printf(""/%s/%s"", qtest_get_arch(), str);

    g_test_add_func(path, fn);


}","void qtest_add_func(const char *str, void (*fn)) {
gchar *path = g_strdup_printf(""/%s/%s"", qtest_get_arch(), str); g_test_add_func(path, fn); }",qemu,1
"static void test_validate_fail_list(TestInputVisitorData *data,

                                     const void *unused)

{

    UserDefOneList *head = NULL;

    Error *err = NULL;

    Visitor *v;



    v = validate_test_init(data, ""[ { 'string': 'string0', 'integer': 42 }, { 'string': 'string1', 'integer': 43 }, { 'string': 'string2', 'integer': 44, 'extra': 'ggg' } ]"");



    visit_type_UserDefOneList(v, NULL, &head, &err);

    error_free_or_abort(&err);

    g_assert(!head);

}
","static void test_validate_fail_list(TestInputVisitorData *data, const void *unused) {
UserDefOneList *head = NULL; Error *err = NULL; Visitor *v; v = validate_test_init(data, ""[ {
'string': 'string0', 'integer': 42 }, {
'string': 'string1', 'integer': 43 }, {
'string': 'string2', 'integer': 44, 'extra': 'ggg' } ]""); visit_type_UserDefOneList(v, NULL, &head, &err); error_free_or_abort(&err); g_assert(!head); } ",qemu,0
"static int vdi_check(BlockDriverState *bs, BdrvCheckResult *res)

{

    /* TODO: additional checks possible. */

    BDRVVdiState *s = (BDRVVdiState *)bs->opaque;

    uint32_t blocks_allocated = 0;

    uint32_t block;

    uint32_t *bmap;

    logout(""\n"");



    bmap = g_malloc(s->header.blocks_in_image * sizeof(uint32_t));

    memset(bmap, 0xff, s->header.blocks_in_image * sizeof(uint32_t));



    /* Check block map and value of blocks_allocated. */

    for (block = 0; block < s->header.blocks_in_image; block++) {

        uint32_t bmap_entry = le32_to_cpu(s->bmap[block]);

        if (VDI_IS_ALLOCATED(bmap_entry)) {

            if (bmap_entry < s->header.blocks_in_image) {

                blocks_allocated++;

                if (!VDI_IS_ALLOCATED(bmap[bmap_entry])) {

                    bmap[bmap_entry] = bmap_entry;

                } else {

                    fprintf(stderr, ""ERROR: block index %"" PRIu32

                            "" also used by %"" PRIu32 ""\n"", bmap[bmap_entry], bmap_entry);

                    res->corruptions++;

                }

            } else {

                fprintf(stderr, ""ERROR: block index %"" PRIu32

                        "" too large, is %"" PRIu32 ""\n"", block, bmap_entry);

                res->corruptions++;

            }

        }

    }

    if (blocks_allocated != s->header.blocks_allocated) {

        fprintf(stderr, ""ERROR: allocated blocks mismatch, is %"" PRIu32

               "", should be %"" PRIu32 ""\n"",

               blocks_allocated, s->header.blocks_allocated);

        res->corruptions++;

    }



    g_free(bmap);



    return 0;

}
","static int vdi_check(BlockDriverState *bs, BdrvCheckResult *res) {
/* TODO: additional checks possible. */ BDRVVdiState *s = (BDRVVdiState *)bs->opaque; uint32_t blocks_allocated = 0; uint32_t block; uint32_t *bmap; logout(""\n""); bmap = g_malloc(s->header.blocks_in_image * sizeof(uint32_t)); memset(bmap, 0xff, s->header.blocks_in_image * sizeof(uint32_t)); /* Check block map and value of blocks_allocated. */ for (block = 0; block < s->header.blocks_in_image; block++) {
uint32_t bmap_entry = le32_to_cpu(s->bmap[block]); if (VDI_IS_ALLOCATED(bmap_entry)) {
if (bmap_entry < s->header.blocks_in_image) {
blocks_allocated++; if (!VDI_IS_ALLOCATED(bmap[bmap_entry])) {
bmap[bmap_entry] = bmap_entry; } else {
fprintf(stderr, ""ERROR: block index %"" PRIu32 "" also used by %"" PRIu32 ""\n"", bmap[bmap_entry], bmap_entry); res->corruptions++; } } else {
fprintf(stderr, ""ERROR: block index %"" PRIu32 "" too large, is %"" PRIu32 ""\n"", block, bmap_entry); res->corruptions++; } } } if (blocks_allocated != s->header.blocks_allocated) {
fprintf(stderr, ""ERROR: allocated blocks mismatch, is %"" PRIu32 "", should be %"" PRIu32 ""\n"", blocks_allocated, s->header.blocks_allocated); res->corruptions++; } g_free(bmap); return 0; } ",qemu,0
"static int blk_send_response_one(struct ioreq *ioreq)

{

    struct XenBlkDev  *blkdev = ioreq->blkdev;

    int               send_notify   = 0;

    int               have_requests = 0;

    blkif_response_t  resp;

    void              *dst;



    resp.id        = ioreq->req.id;

    resp.operation = ioreq->req.operation;

    resp.status    = ioreq->status;



    /* Place on the response ring for the relevant domain. */

    switch (blkdev->protocol) {

    case BLKIF_PROTOCOL_NATIVE:

        dst = RING_GET_RESPONSE(&blkdev->rings.native, blkdev->rings.native.rsp_prod_pvt);

        break;

    case BLKIF_PROTOCOL_X86_32:

        dst = RING_GET_RESPONSE(&blkdev->rings.x86_32_part,

                                blkdev->rings.x86_32_part.rsp_prod_pvt);

        break;

    case BLKIF_PROTOCOL_X86_64:

        dst = RING_GET_RESPONSE(&blkdev->rings.x86_64_part,

                                blkdev->rings.x86_64_part.rsp_prod_pvt);

        break;

    default:

        dst = NULL;


    }

    memcpy(dst, &resp, sizeof(resp));

    blkdev->rings.common.rsp_prod_pvt++;



    RING_PUSH_RESPONSES_AND_CHECK_NOTIFY(&blkdev->rings.common, send_notify);

    if (blkdev->rings.common.rsp_prod_pvt == blkdev->rings.common.req_cons) {

        /*

         * Tail check for pending requests. Allows frontend to avoid

         * notifications if requests are already in flight (lower

         * overheads and promotes batching).

         */

        RING_FINAL_CHECK_FOR_REQUESTS(&blkdev->rings.common, have_requests);

    } else if (RING_HAS_UNCONSUMED_REQUESTS(&blkdev->rings.common)) {

        have_requests = 1;

    }



    if (have_requests) {

        blkdev->more_work++;

    }

    return send_notify;

}","static int blk_send_response_one(struct ioreq *ioreq) {
struct XenBlkDev *blkdev = ioreq->blkdev; int send_notify = 0; int have_requests = 0; blkif_response_t resp; void *dst; resp.id = ioreq->req.id; resp.operation = ioreq->req.operation; resp.status = ioreq->status; /* Place on the response ring for the relevant domain. */ switch (blkdev->protocol) {
case BLKIF_PROTOCOL_NATIVE: dst = RING_GET_RESPONSE(&blkdev->rings.native, blkdev->rings.native.rsp_prod_pvt); break; case BLKIF_PROTOCOL_X86_32: dst = RING_GET_RESPONSE(&blkdev->rings.x86_32_part, blkdev->rings.x86_32_part.rsp_prod_pvt); break; case BLKIF_PROTOCOL_X86_64: dst = RING_GET_RESPONSE(&blkdev->rings.x86_64_part, blkdev->rings.x86_64_part.rsp_prod_pvt); break; default: dst = NULL; } memcpy(dst, &resp, sizeof(resp)); blkdev->rings.common.rsp_prod_pvt++; RING_PUSH_RESPONSES_AND_CHECK_NOTIFY(&blkdev->rings.common, send_notify); if (blkdev->rings.common.rsp_prod_pvt == blkdev->rings.common.req_cons) {
/* * Tail check for pending requests. Allows frontend to avoid * notifications if requests are already in flight (lower * overheads and promotes batching). */ RING_FINAL_CHECK_FOR_REQUESTS(&blkdev->rings.common, have_requests); } else if (RING_HAS_UNCONSUMED_REQUESTS(&blkdev->rings.common)) {
have_requests = 1; } if (have_requests) {
blkdev->more_work++; } return send_notify; }",qemu,1
"static void versatile_init(ram_addr_t ram_size,

                     const char *boot_device,

                     const char *kernel_filename, const char *kernel_cmdline,

                     const char *initrd_filename, const char *cpu_model,

                     int board_id)

{

    CPUState *env;

    ram_addr_t ram_offset;

    qemu_irq *cpu_pic;

    qemu_irq pic[32];

    qemu_irq sic[32];

    DeviceState *dev;

    PCIBus *pci_bus;

    NICInfo *nd;

    int n;

    int done_smc = 0;



    if (!cpu_model)

        cpu_model = ""arm926"";

    env = cpu_init(cpu_model);

    if (!env) {

        fprintf(stderr, ""Unable to find CPU definition\n"");

        exit(1);

    }

    ram_offset = qemu_ram_alloc(NULL, ""versatile.ram"", ram_size);

    /* ??? RAM should repeat to fill physical memory space.  */

    /* SDRAM at address zero.  */

    cpu_register_physical_memory(0, ram_size, ram_offset | IO_MEM_RAM);



    arm_sysctl_init(0x10000000, 0x41007004, 0x02000000);

    cpu_pic = arm_pic_init_cpu(env);

    dev = sysbus_create_varargs(""pl190"", 0x10140000,

                                cpu_pic[0], cpu_pic[1], NULL);

    for (n = 0; n < 32; n++) {

        pic[n] = qdev_get_gpio_in(dev, n);

    }

    dev = sysbus_create_simple(""versatilepb_sic"", 0x10003000, NULL);

    for (n = 0; n < 32; n++) {

        sysbus_connect_irq(sysbus_from_qdev(dev), n, pic[n]);

        sic[n] = qdev_get_gpio_in(dev, n);

    }



    sysbus_create_simple(""pl050_keyboard"", 0x10006000, sic[3]);

    sysbus_create_simple(""pl050_mouse"", 0x10007000, sic[4]);



    dev = sysbus_create_varargs(""versatile_pci"", 0x40000000,

                                sic[27], sic[28], sic[29], sic[30], NULL);

    pci_bus = (PCIBus *)qdev_get_child_bus(dev, ""pci"");



    /* The Versatile PCI bridge does not provide access to PCI IO space,

       so many of the qemu PCI devices are not useable.  */

    for(n = 0; n < nb_nics; n++) {

        nd = &nd_table[n];



        if ((!nd->model && !done_smc) || strcmp(nd->model, ""smc91c111"") == 0) {

            smc91c111_init(nd, 0x10010000, sic[25]);

            done_smc = 1;

        } else {

            pci_nic_init_nofail(nd, ""rtl8139"", NULL);

        }

    }

    if (usb_enabled) {

        usb_ohci_init_pci(pci_bus, -1);

    }

    n = drive_get_max_bus(IF_SCSI);

    while (n >= 0) {

        pci_create_simple(pci_bus, -1, ""lsi53c895a"");

        n--;

    }



    sysbus_create_simple(""pl011"", 0x101f1000, pic[12]);

    sysbus_create_simple(""pl011"", 0x101f2000, pic[13]);

    sysbus_create_simple(""pl011"", 0x101f3000, pic[14]);

    sysbus_create_simple(""pl011"", 0x10009000, sic[6]);



    sysbus_create_simple(""pl080"", 0x10130000, pic[17]);

    sysbus_create_simple(""sp804"", 0x101e2000, pic[4]);

    sysbus_create_simple(""sp804"", 0x101e3000, pic[5]);



    /* The versatile/PB actually has a modified Color LCD controller

       that includes hardware cursor support from the PL111.  */

    sysbus_create_simple(""pl110_versatile"", 0x10120000, pic[16]);



    sysbus_create_varargs(""pl181"", 0x10005000, sic[22], sic[1], NULL);

    sysbus_create_varargs(""pl181"", 0x1000b000, sic[23], sic[2], NULL);



    /* Add PL031 Real Time Clock. */

    sysbus_create_simple(""pl031"", 0x101e8000, pic[10]);



    /* Memory map for Versatile/PB:  */

    /* 0x10000000 System registers.  */

    /* 0x10001000 PCI controller config registers.  */

    /* 0x10002000 Serial bus interface.  */

    /*  0x10003000 Secondary interrupt controller.  */

    /* 0x10004000 AACI (audio).  */

    /*  0x10005000 MMCI0.  */

    /*  0x10006000 KMI0 (keyboard).  */

    /*  0x10007000 KMI1 (mouse).  */

    /* 0x10008000 Character LCD Interface.  */

    /*  0x10009000 UART3.  */

    /* 0x1000a000 Smart card 1.  */

    /*  0x1000b000 MMCI1.  */

    /*  0x10010000 Ethernet.  */

    /* 0x10020000 USB.  */

    /* 0x10100000 SSMC.  */

    /* 0x10110000 MPMC.  */

    /*  0x10120000 CLCD Controller.  */

    /*  0x10130000 DMA Controller.  */

    /*  0x10140000 Vectored interrupt controller.  */

    /* 0x101d0000 AHB Monitor Interface.  */

    /* 0x101e0000 System Controller.  */

    /* 0x101e1000 Watchdog Interface.  */

    /* 0x101e2000 Timer 0/1.  */

    /* 0x101e3000 Timer 2/3.  */

    /* 0x101e4000 GPIO port 0.  */

    /* 0x101e5000 GPIO port 1.  */

    /* 0x101e6000 GPIO port 2.  */

    /* 0x101e7000 GPIO port 3.  */

    /* 0x101e8000 RTC.  */

    /* 0x101f0000 Smart card 0.  */

    /*  0x101f1000 UART0.  */

    /*  0x101f2000 UART1.  */

    /*  0x101f3000 UART2.  */

    /* 0x101f4000 SSPI.  */



    versatile_binfo.ram_size = ram_size;

    versatile_binfo.kernel_filename = kernel_filename;

    versatile_binfo.kernel_cmdline = kernel_cmdline;

    versatile_binfo.initrd_filename = initrd_filename;

    versatile_binfo.board_id = board_id;

    arm_load_kernel(env, &versatile_binfo);

}
","static void versatile_init(ram_addr_t ram_size, const char *boot_device, const char *kernel_filename, const char *kernel_cmdline, const char *initrd_filename, const char *cpu_model, int board_id) {
CPUState *env; ram_addr_t ram_offset; qemu_irq *cpu_pic; qemu_irq pic[32]; qemu_irq sic[32]; DeviceState *dev; PCIBus *pci_bus; NICInfo *nd; int n; int done_smc = 0; if (!cpu_model) cpu_model = ""arm926""; env = cpu_init(cpu_model); if (!env) {
fprintf(stderr, ""Unable to find CPU definition\n""); exit(1); } ram_offset = qemu_ram_alloc(NULL, ""versatile.ram"", ram_size); /* ??? RAM should repeat to fill physical memory space. */ /* SDRAM at address zero. */ cpu_register_physical_memory(0, ram_size, ram_offset | IO_MEM_RAM); arm_sysctl_init(0x10000000, 0x41007004, 0x02000000); cpu_pic = arm_pic_init_cpu(env); dev = sysbus_create_varargs(""pl190"", 0x10140000, cpu_pic[0], cpu_pic[1], NULL); for (n = 0; n < 32; n++) {
pic[n] = qdev_get_gpio_in(dev, n); } dev = sysbus_create_simple(""versatilepb_sic"", 0x10003000, NULL); for (n = 0; n < 32; n++) {
sysbus_connect_irq(sysbus_from_qdev(dev), n, pic[n]); sic[n] = qdev_get_gpio_in(dev, n); } sysbus_create_simple(""pl050_keyboard"", 0x10006000, sic[3]); sysbus_create_simple(""pl050_mouse"", 0x10007000, sic[4]); dev = sysbus_create_varargs(""versatile_pci"", 0x40000000, sic[27], sic[28], sic[29], sic[30], NULL); pci_bus = (PCIBus *)qdev_get_child_bus(dev, ""pci""); /* The Versatile PCI bridge does not provide access to PCI IO space, so many of the qemu PCI devices are not useable. */ for(n = 0; n < nb_nics; n++) {
nd = &nd_table[n]; if ((!nd->model && !done_smc) || strcmp(nd->model, ""smc91c111"") == 0) {
smc91c111_init(nd, 0x10010000, sic[25]); done_smc = 1; } else {
pci_nic_init_nofail(nd, ""rtl8139"", NULL); } } if (usb_enabled) {
usb_ohci_init_pci(pci_bus, -1); } n = drive_get_max_bus(IF_SCSI); while (n >= 0) {
pci_create_simple(pci_bus, -1, ""lsi53c895a""); n--; } sysbus_create_simple(""pl011"", 0x101f1000, pic[12]); sysbus_create_simple(""pl011"", 0x101f2000, pic[13]); sysbus_create_simple(""pl011"", 0x101f3000, pic[14]); sysbus_create_simple(""pl011"", 0x10009000, sic[6]); sysbus_create_simple(""pl080"", 0x10130000, pic[17]); sysbus_create_simple(""sp804"", 0x101e2000, pic[4]); sysbus_create_simple(""sp804"", 0x101e3000, pic[5]); /* The versatile/PB actually has a modified Color LCD controller that includes hardware cursor support from the PL111. */ sysbus_create_simple(""pl110_versatile"", 0x10120000, pic[16]); sysbus_create_varargs(""pl181"", 0x10005000, sic[22], sic[1], NULL); sysbus_create_varargs(""pl181"", 0x1000b000, sic[23], sic[2], NULL); /* Add PL031 Real Time Clock. */ sysbus_create_simple(""pl031"", 0x101e8000, pic[10]); /* Memory map for Versatile/PB: */ /* 0x10000000 System registers. */ /* 0x10001000 PCI controller config registers. */ /* 0x10002000 Serial bus interface. */ /* 0x10003000 Secondary interrupt controller. */ /* 0x10004000 AACI (audio). */ /* 0x10005000 MMCI0. */ /* 0x10006000 KMI0 (keyboard). */ /* 0x10007000 KMI1 (mouse). */ /* 0x10008000 Character LCD Interface. */ /* 0x10009000 UART3. */ /* 0x1000a000 Smart card 1. */ /* 0x1000b000 MMCI1. */ /* 0x10010000 Ethernet. */ /* 0x10020000 USB. */ /* 0x10100000 SSMC. */ /* 0x10110000 MPMC. */ /* 0x10120000 CLCD Controller. */ /* 0x10130000 DMA Controller. */ /* 0x10140000 Vectored interrupt controller. */ /* 0x101d0000 AHB Monitor Interface. */ /* 0x101e0000 System Controller. */ /* 0x101e1000 Watchdog Interface. */ /* 0x101e2000 Timer 0/1. */ /* 0x101e3000 Timer 2/3. */ /* 0x101e4000 GPIO port 0. */ /* 0x101e5000 GPIO port 1. */ /* 0x101e6000 GPIO port 2. */ /* 0x101e7000 GPIO port 3. */ /* 0x101e8000 RTC. */ /* 0x101f0000 Smart card 0. */ /* 0x101f1000 UART0. */ /* 0x101f2000 UART1. */ /* 0x101f3000 UART2. */ /* 0x101f4000 SSPI. */ versatile_binfo.ram_size = ram_size; versatile_binfo.kernel_filename = kernel_filename; versatile_binfo.kernel_cmdline = kernel_cmdline; versatile_binfo.initrd_filename = initrd_filename; versatile_binfo.board_id = board_id; arm_load_kernel(env, &versatile_binfo); } ",qemu,1
"void qdev_prop_register_global(GlobalProperty *prop)

{

    QTAILQ_INSERT_TAIL(&global_props, prop, next);

}
","void qdev_prop_register_global(GlobalProperty *prop) {
QTAILQ_INSERT_TAIL(&global_props, prop, next); } ",qemu,1
"void error_setg_file_open(Error **errp, int os_errno, const char *filename)

{

    error_setg_errno(errp, os_errno, ""Could not open '%s'"", filename);

}
","void error_setg_file_open(Error **errp, int os_errno, const char *filename) {
error_setg_errno(errp, os_errno, ""Could not open '%s'"", filename); } ",qemu,1
"void commit_start(BlockDriverState *bs, BlockDriverState *base,

                  BlockDriverState *top, int64_t speed,

                  BlockdevOnError on_error, BlockCompletionFunc *cb,

                  void *opaque, const char *backing_file_str, Error **errp)

{

    CommitBlockJob *s;

    BlockReopenQueue *reopen_queue = NULL;

    int orig_overlay_flags;

    int orig_base_flags;

    BlockDriverState *overlay_bs;

    Error *local_err = NULL;



    assert(top != bs);

    if (top == base) {

        error_setg(errp, ""Invalid files for merge: top and base are the same"");

        return;

    }



    overlay_bs = bdrv_find_overlay(bs, top);



    if (overlay_bs == NULL) {

        error_setg(errp, ""Could not find overlay image for %s:"", top->filename);

        return;

    }



    s = block_job_create(&commit_job_driver, bs, speed, cb, opaque, errp);

    if (!s) {

        return;

    }



    orig_base_flags    = bdrv_get_flags(base);

    orig_overlay_flags = bdrv_get_flags(overlay_bs);



    /* convert base & overlay_bs to r/w, if necessary */

    if (!(orig_overlay_flags & BDRV_O_RDWR)) {

        reopen_queue = bdrv_reopen_queue(reopen_queue, overlay_bs, NULL,

                                         orig_overlay_flags | BDRV_O_RDWR);

    }

    if (!(orig_base_flags & BDRV_O_RDWR)) {

        reopen_queue = bdrv_reopen_queue(reopen_queue, base, NULL,

                                         orig_base_flags | BDRV_O_RDWR);

    }

    if (reopen_queue) {

        bdrv_reopen_multiple(reopen_queue, &local_err);

        if (local_err != NULL) {

            error_propagate(errp, local_err);

            block_job_unref(&s->common);

            return;

        }

    }





    s->base = blk_new();

    blk_insert_bs(s->base, base);



    s->top = blk_new();

    blk_insert_bs(s->top, top);



    s->active = bs;



    s->base_flags          = orig_base_flags;

    s->orig_overlay_flags  = orig_overlay_flags;



    s->backing_file_str = g_strdup(backing_file_str);



    s->on_error = on_error;

    s->common.co = qemu_coroutine_create(commit_run);



    trace_commit_start(bs, base, top, s, s->common.co, opaque);

    qemu_coroutine_enter(s->common.co, s);

}
","void commit_start(BlockDriverState *bs, BlockDriverState *base, BlockDriverState *top, int64_t speed, BlockdevOnError on_error, BlockCompletionFunc *cb, void *opaque, const char *backing_file_str, Error **errp) {
CommitBlockJob *s; BlockReopenQueue *reopen_queue = NULL; int orig_overlay_flags; int orig_base_flags; BlockDriverState *overlay_bs; Error *local_err = NULL; assert(top != bs); if (top == base) {
error_setg(errp, ""Invalid files for merge: top and base are the same""); return; } overlay_bs = bdrv_find_overlay(bs, top); if (overlay_bs == NULL) {
error_setg(errp, ""Could not find overlay image for %s:"", top->filename); return; } s = block_job_create(&commit_job_driver, bs, speed, cb, opaque, errp); if (!s) {
return; } orig_base_flags = bdrv_get_flags(base); orig_overlay_flags = bdrv_get_flags(overlay_bs); /* convert base & overlay_bs to r/w, if necessary */ if (!(orig_overlay_flags & BDRV_O_RDWR)) {
reopen_queue = bdrv_reopen_queue(reopen_queue, overlay_bs, NULL, orig_overlay_flags | BDRV_O_RDWR); } if (!(orig_base_flags & BDRV_O_RDWR)) {
reopen_queue = bdrv_reopen_queue(reopen_queue, base, NULL, orig_base_flags | BDRV_O_RDWR); } if (reopen_queue) {
bdrv_reopen_multiple(reopen_queue, &local_err); if (local_err != NULL) {
error_propagate(errp, local_err); block_job_unref(&s->common); return; } } s->base = blk_new(); blk_insert_bs(s->base, base); s->top = blk_new(); blk_insert_bs(s->top, top); s->active = bs; s->base_flags = orig_base_flags; s->orig_overlay_flags = orig_overlay_flags; s->backing_file_str = g_strdup(backing_file_str); s->on_error = on_error; s->common.co = qemu_coroutine_create(commit_run); trace_commit_start(bs, base, top, s, s->common.co, opaque); qemu_coroutine_enter(s->common.co, s); } ",qemu,1
"static int proxy_chmod(FsContext *fs_ctx, V9fsPath *fs_path, FsCred *credp)

{

    int retval;

    retval = v9fs_request(fs_ctx->private, T_CHMOD, NULL, ""sd"",

                          fs_path, credp->fc_mode);

    if (retval < 0) {

        errno = -retval;

    }

    return retval;

}
","static int proxy_chmod(FsContext *fs_ctx, V9fsPath *fs_path, FsCred *credp) {
int retval; retval = v9fs_request(fs_ctx->private, T_CHMOD, NULL, ""sd"", fs_path, credp->fc_mode); if (retval < 0) {
errno = -retval; } return retval; } ",qemu,0
"static inline uint32_t vmsvga_fifo_read_raw(struct vmsvga_state_s *s)

{

    uint32_t cmd = s->fifo[CMD(stop) >> 2];

    s->cmd->stop = cpu_to_le32(CMD(stop) + 4);

    if (CMD(stop) >= CMD(max))

        s->cmd->stop = s->cmd->min;

    return cmd;

}
","static inline uint32_t vmsvga_fifo_read_raw(struct vmsvga_state_s *s) {
uint32_t cmd = s->fifo[CMD(stop) >> 2]; s->cmd->stop = cpu_to_le32(CMD(stop) + 4); if (CMD(stop) >= CMD(max)) s->cmd->stop = s->cmd->min; return cmd; } ",qemu,0
"PCIBus *i440fx_init(PCII440FXState **pi440fx_state,

                    int *piix3_devfn,

                    ISABus **isa_bus, qemu_irq *pic,

                    MemoryRegion *address_space_mem,

                    MemoryRegion *address_space_io,

                    ram_addr_t ram_size,

                    hwaddr pci_hole_start,

                    hwaddr pci_hole_size,

                    ram_addr_t above_4g_mem_size,

                    MemoryRegion *pci_address_space,

                    MemoryRegion *ram_memory)

{

    DeviceState *dev;

    PCIBus *b;

    PCIDevice *d;

    PCIHostState *s;

    PIIX3State *piix3;

    PCII440FXState *f;

    unsigned i;

    I440FXState *i440fx;



    dev = qdev_create(NULL, TYPE_I440FX_PCI_HOST_BRIDGE);

    s = PCI_HOST_BRIDGE(dev);

    b = pci_bus_new(dev, NULL, pci_address_space,

                    address_space_io, 0, TYPE_PCI_BUS);

    s->bus = b;

    object_property_add_child(qdev_get_machine(), ""i440fx"", OBJECT(dev), NULL);

    qdev_init_nofail(dev);



    d = pci_create_simple(b, 0, TYPE_I440FX_PCI_DEVICE);

    *pi440fx_state = I440FX_PCI_DEVICE(d);

    f = *pi440fx_state;

    f->system_memory = address_space_mem;

    f->pci_address_space = pci_address_space;

    f->ram_memory = ram_memory;



    i440fx = I440FX_PCI_HOST_BRIDGE(dev);

    /* Set PCI window size the way seabios has always done it. */

    /* Power of 2 so bios can cover it with a single MTRR */

    if (ram_size <= 0x80000000) {

        i440fx->pci_info.w32.begin = 0x80000000;

    } else if (ram_size <= 0xc0000000) {

        i440fx->pci_info.w32.begin = 0xc0000000;

    } else {

        i440fx->pci_info.w32.begin = 0xe0000000;

    }



    memory_region_init_alias(&f->pci_hole, OBJECT(d), ""pci-hole"", f->pci_address_space,

                             pci_hole_start, pci_hole_size);

    memory_region_add_subregion(f->system_memory, pci_hole_start, &f->pci_hole);



    pc_init_pci64_hole(&i440fx->pci_info, 0x100000000ULL + above_4g_mem_size,

                       i440fx->pci_hole64_size);

    memory_region_init_alias(&f->pci_hole_64bit, OBJECT(d), ""pci-hole64"",

                             f->pci_address_space,

                             i440fx->pci_info.w64.begin,

                             i440fx->pci_hole64_size);

    if (i440fx->pci_hole64_size) {

        memory_region_add_subregion(f->system_memory,

                                    i440fx->pci_info.w64.begin,

                                    &f->pci_hole_64bit);

    }

    memory_region_init_alias(&f->smram_region, OBJECT(d), ""smram-region"",

                             f->pci_address_space, 0xa0000, 0x20000);

    memory_region_add_subregion_overlap(f->system_memory, 0xa0000,

                                        &f->smram_region, 1);

    memory_region_set_enabled(&f->smram_region, false);

    init_pam(dev, f->ram_memory, f->system_memory, f->pci_address_space,

             &f->pam_regions[0], PAM_BIOS_BASE, PAM_BIOS_SIZE);

    for (i = 0; i < 12; ++i) {

        init_pam(dev, f->ram_memory, f->system_memory, f->pci_address_space,

                 &f->pam_regions[i+1], PAM_EXPAN_BASE + i * PAM_EXPAN_SIZE,

                 PAM_EXPAN_SIZE);

    }



    /* Xen supports additional interrupt routes from the PCI devices to

     * the IOAPIC: the four pins of each PCI device on the bus are also

     * connected to the IOAPIC directly.

     * These additional routes can be discovered through ACPI. */

    if (xen_enabled()) {

        piix3 = DO_UPCAST(PIIX3State, dev,

                pci_create_simple_multifunction(b, -1, true, ""PIIX3-xen""));

        pci_bus_irqs(b, xen_piix3_set_irq, xen_pci_slot_get_pirq,

                piix3, XEN_PIIX_NUM_PIRQS);

    } else {

        piix3 = DO_UPCAST(PIIX3State, dev,

                pci_create_simple_multifunction(b, -1, true, ""PIIX3""));

        pci_bus_irqs(b, piix3_set_irq, pci_slot_get_pirq, piix3,

                PIIX_NUM_PIRQS);

        pci_bus_set_route_irq_fn(b, piix3_route_intx_pin_to_irq);

    }

    piix3->pic = pic;

    *isa_bus = ISA_BUS(qdev_get_child_bus(DEVICE(piix3), ""isa.0""));



    *piix3_devfn = piix3->dev.devfn;



    ram_size = ram_size / 8 / 1024 / 1024;

    if (ram_size > 255) {

        ram_size = 255;

    }

    d->config[0x57] = ram_size;



    i440fx_update_memory_mappings(f);



    return b;

}
","PCIBus *i440fx_init(PCII440FXState **pi440fx_state, int *piix3_devfn, ISABus **isa_bus, qemu_irq *pic, MemoryRegion *address_space_mem, MemoryRegion *address_space_io, ram_addr_t ram_size, hwaddr pci_hole_start, hwaddr pci_hole_size, ram_addr_t above_4g_mem_size, MemoryRegion *pci_address_space, MemoryRegion *ram_memory) {
DeviceState *dev; PCIBus *b; PCIDevice *d; PCIHostState *s; PIIX3State *piix3; PCII440FXState *f; unsigned i; I440FXState *i440fx; dev = qdev_create(NULL, TYPE_I440FX_PCI_HOST_BRIDGE); s = PCI_HOST_BRIDGE(dev); b = pci_bus_new(dev, NULL, pci_address_space, address_space_io, 0, TYPE_PCI_BUS); s->bus = b; object_property_add_child(qdev_get_machine(), ""i440fx"", OBJECT(dev), NULL); qdev_init_nofail(dev); d = pci_create_simple(b, 0, TYPE_I440FX_PCI_DEVICE); *pi440fx_state = I440FX_PCI_DEVICE(d); f = *pi440fx_state; f->system_memory = address_space_mem; f->pci_address_space = pci_address_space; f->ram_memory = ram_memory; i440fx = I440FX_PCI_HOST_BRIDGE(dev); /* Set PCI window size the way seabios has always done it. */ /* Power of 2 so bios can cover it with a single MTRR */ if (ram_size <= 0x80000000) {
i440fx->pci_info.w32.begin = 0x80000000; } else if (ram_size <= 0xc0000000) {
i440fx->pci_info.w32.begin = 0xc0000000; } else {
i440fx->pci_info.w32.begin = 0xe0000000; } memory_region_init_alias(&f->pci_hole, OBJECT(d), ""pci-hole"", f->pci_address_space, pci_hole_start, pci_hole_size); memory_region_add_subregion(f->system_memory, pci_hole_start, &f->pci_hole); pc_init_pci64_hole(&i440fx->pci_info, 0x100000000ULL + above_4g_mem_size, i440fx->pci_hole64_size); memory_region_init_alias(&f->pci_hole_64bit, OBJECT(d), ""pci-hole64"", f->pci_address_space, i440fx->pci_info.w64.begin, i440fx->pci_hole64_size); if (i440fx->pci_hole64_size) {
memory_region_add_subregion(f->system_memory, i440fx->pci_info.w64.begin, &f->pci_hole_64bit); } memory_region_init_alias(&f->smram_region, OBJECT(d), ""smram-region"", f->pci_address_space, 0xa0000, 0x20000); memory_region_add_subregion_overlap(f->system_memory, 0xa0000, &f->smram_region, 1); memory_region_set_enabled(&f->smram_region, false); init_pam(dev, f->ram_memory, f->system_memory, f->pci_address_space, &f->pam_regions[0], PAM_BIOS_BASE, PAM_BIOS_SIZE); for (i = 0; i < 12; ++i) {
init_pam(dev, f->ram_memory, f->system_memory, f->pci_address_space, &f->pam_regions[i+1], PAM_EXPAN_BASE + i * PAM_EXPAN_SIZE, PAM_EXPAN_SIZE); } /* Xen supports additional interrupt routes from the PCI devices to * the IOAPIC: the four pins of each PCI device on the bus are also * connected to the IOAPIC directly. * These additional routes can be discovered through ACPI. */ if (xen_enabled()) {
piix3 = DO_UPCAST(PIIX3State, dev, pci_create_simple_multifunction(b, -1, true, ""PIIX3-xen"")); pci_bus_irqs(b, xen_piix3_set_irq, xen_pci_slot_get_pirq, piix3, XEN_PIIX_NUM_PIRQS); } else {
piix3 = DO_UPCAST(PIIX3State, dev, pci_create_simple_multifunction(b, -1, true, ""PIIX3"")); pci_bus_irqs(b, piix3_set_irq, pci_slot_get_pirq, piix3, PIIX_NUM_PIRQS); pci_bus_set_route_irq_fn(b, piix3_route_intx_pin_to_irq); } piix3->pic = pic; *isa_bus = ISA_BUS(qdev_get_child_bus(DEVICE(piix3), ""isa.0"")); *piix3_devfn = piix3->dev.devfn; ram_size = ram_size / 8 / 1024 / 1024; if (ram_size > 255) {
ram_size = 255; } d->config[0x57] = ram_size; i440fx_update_memory_mappings(f); return b; } ",qemu,0
"static coroutine_fn int qcow2_co_readv(BlockDriverState *bs, int64_t sector_num,

                          int remaining_sectors, QEMUIOVector *qiov)

{

    BDRVQcowState *s = bs->opaque;

    int index_in_cluster, n1;

    int ret;

    int cur_nr_sectors; /* number of sectors in current iteration */

    uint64_t cluster_offset = 0;

    uint64_t bytes_done = 0;

    QEMUIOVector hd_qiov;

    uint8_t *cluster_data = NULL;



    qemu_iovec_init(&hd_qiov, qiov->niov);



    qemu_co_mutex_lock(&s->lock);



    while (remaining_sectors != 0) {



        /* prepare next request */

        cur_nr_sectors = remaining_sectors;

        if (s->crypt_method) {

            cur_nr_sectors = MIN(cur_nr_sectors,

                QCOW_MAX_CRYPT_CLUSTERS * s->cluster_sectors);

        }



        ret = qcow2_get_cluster_offset(bs, sector_num << 9,

            &cur_nr_sectors, &cluster_offset);

        if (ret < 0) {

            goto fail;

        }



        index_in_cluster = sector_num & (s->cluster_sectors - 1);



        qemu_iovec_reset(&hd_qiov);

        qemu_iovec_concat(&hd_qiov, qiov, bytes_done,

            cur_nr_sectors * 512);



        switch (ret) {

        case QCOW2_CLUSTER_UNALLOCATED:



            if (bs->backing_hd) {

                /* read from the base image */

                n1 = qcow2_backing_read1(bs->backing_hd, &hd_qiov,

                    sector_num, cur_nr_sectors);

                if (n1 > 0) {

                    BLKDBG_EVENT(bs->file, BLKDBG_READ_BACKING_AIO);

                    qemu_co_mutex_unlock(&s->lock);

                    ret = bdrv_co_readv(bs->backing_hd, sector_num,

                                        n1, &hd_qiov);

                    qemu_co_mutex_lock(&s->lock);

                    if (ret < 0) {

                        goto fail;

                    }

                }

            } else {

                /* Note: in this case, no need to wait */

                qemu_iovec_memset(&hd_qiov, 0, 0, 512 * cur_nr_sectors);

            }

            break;



        case QCOW2_CLUSTER_ZERO:

            if (s->qcow_version < 3) {

                ret = -EIO;

                goto fail;

            }

            qemu_iovec_memset(&hd_qiov, 0, 0, 512 * cur_nr_sectors);

            break;



        case QCOW2_CLUSTER_COMPRESSED:

            /* add AIO support for compressed blocks ? */

            ret = qcow2_decompress_cluster(bs, cluster_offset);

            if (ret < 0) {

                goto fail;

            }



            qemu_iovec_from_buf(&hd_qiov, 0,

                s->cluster_cache + index_in_cluster * 512,

                512 * cur_nr_sectors);

            break;



        case QCOW2_CLUSTER_NORMAL:

            if ((cluster_offset & 511) != 0) {

                ret = -EIO;

                goto fail;

            }



            if (s->crypt_method) {

                /*

                 * For encrypted images, read everything into a temporary

                 * contiguous buffer on which the AES functions can work.

                 */

                if (!cluster_data) {

                    cluster_data =

                        qemu_blockalign(bs, QCOW_MAX_CRYPT_CLUSTERS * s->cluster_size);

                }



                assert(cur_nr_sectors <=

                    QCOW_MAX_CRYPT_CLUSTERS * s->cluster_sectors);

                qemu_iovec_reset(&hd_qiov);

                qemu_iovec_add(&hd_qiov, cluster_data,

                    512 * cur_nr_sectors);

            }



            BLKDBG_EVENT(bs->file, BLKDBG_READ_AIO);

            qemu_co_mutex_unlock(&s->lock);

            ret = bdrv_co_readv(bs->file,

                                (cluster_offset >> 9) + index_in_cluster,

                                cur_nr_sectors, &hd_qiov);

            qemu_co_mutex_lock(&s->lock);

            if (ret < 0) {

                goto fail;

            }

            if (s->crypt_method) {

                qcow2_encrypt_sectors(s, sector_num,  cluster_data,

                    cluster_data, cur_nr_sectors, 0, &s->aes_decrypt_key);

                qemu_iovec_from_buf(qiov, bytes_done,

                    cluster_data, 512 * cur_nr_sectors);

            }

            break;



        default:

            g_assert_not_reached();

            ret = -EIO;

            goto fail;

        }



        remaining_sectors -= cur_nr_sectors;

        sector_num += cur_nr_sectors;

        bytes_done += cur_nr_sectors * 512;

    }

    ret = 0;



fail:

    qemu_co_mutex_unlock(&s->lock);



    qemu_iovec_destroy(&hd_qiov);

    qemu_vfree(cluster_data);



    return ret;

}
","static coroutine_fn int qcow2_co_readv(BlockDriverState *bs, int64_t sector_num, int remaining_sectors, QEMUIOVector *qiov) {
BDRVQcowState *s = bs->opaque; int index_in_cluster, n1; int ret; int cur_nr_sectors; /* number of sectors in current iteration */ uint64_t cluster_offset = 0; uint64_t bytes_done = 0; QEMUIOVector hd_qiov; uint8_t *cluster_data = NULL; qemu_iovec_init(&hd_qiov, qiov->niov); qemu_co_mutex_lock(&s->lock); while (remaining_sectors != 0) {
/* prepare next request */ cur_nr_sectors = remaining_sectors; if (s->crypt_method) {
cur_nr_sectors = MIN(cur_nr_sectors, QCOW_MAX_CRYPT_CLUSTERS * s->cluster_sectors); } ret = qcow2_get_cluster_offset(bs, sector_num << 9, &cur_nr_sectors, &cluster_offset); if (ret < 0) {
goto fail; } index_in_cluster = sector_num & (s->cluster_sectors - 1); qemu_iovec_reset(&hd_qiov); qemu_iovec_concat(&hd_qiov, qiov, bytes_done, cur_nr_sectors * 512); switch (ret) {
case QCOW2_CLUSTER_UNALLOCATED: if (bs->backing_hd) {
/* read from the base image */ n1 = qcow2_backing_read1(bs->backing_hd, &hd_qiov, sector_num, cur_nr_sectors); if (n1 > 0) {
BLKDBG_EVENT(bs->file, BLKDBG_READ_BACKING_AIO); qemu_co_mutex_unlock(&s->lock); ret = bdrv_co_readv(bs->backing_hd, sector_num, n1, &hd_qiov); qemu_co_mutex_lock(&s->lock); if (ret < 0) {
goto fail; } } } else {
/* Note: in this case, no need to wait */ qemu_iovec_memset(&hd_qiov, 0, 0, 512 * cur_nr_sectors); } break; case QCOW2_CLUSTER_ZERO: if (s->qcow_version < 3) {
ret = -EIO; goto fail; } qemu_iovec_memset(&hd_qiov, 0, 0, 512 * cur_nr_sectors); break; case QCOW2_CLUSTER_COMPRESSED: /* add AIO support for compressed blocks ? */ ret = qcow2_decompress_cluster(bs, cluster_offset); if (ret < 0) {
goto fail; } qemu_iovec_from_buf(&hd_qiov, 0, s->cluster_cache + index_in_cluster * 512, 512 * cur_nr_sectors); break; case QCOW2_CLUSTER_NORMAL: if ((cluster_offset & 511) != 0) {
ret = -EIO; goto fail; } if (s->crypt_method) {
/* * For encrypted images, read everything into a temporary * contiguous buffer on which the AES functions can work. */ if (!cluster_data) {
cluster_data = qemu_blockalign(bs, QCOW_MAX_CRYPT_CLUSTERS * s->cluster_size); } assert(cur_nr_sectors <= QCOW_MAX_CRYPT_CLUSTERS * s->cluster_sectors); qemu_iovec_reset(&hd_qiov); qemu_iovec_add(&hd_qiov, cluster_data, 512 * cur_nr_sectors); } BLKDBG_EVENT(bs->file, BLKDBG_READ_AIO); qemu_co_mutex_unlock(&s->lock); ret = bdrv_co_readv(bs->file, (cluster_offset >> 9) + index_in_cluster, cur_nr_sectors, &hd_qiov); qemu_co_mutex_lock(&s->lock); if (ret < 0) {
goto fail; } if (s->crypt_method) {
qcow2_encrypt_sectors(s, sector_num, cluster_data, cluster_data, cur_nr_sectors, 0, &s->aes_decrypt_key); qemu_iovec_from_buf(qiov, bytes_done, cluster_data, 512 * cur_nr_sectors); } break; default: g_assert_not_reached(); ret = -EIO; goto fail; } remaining_sectors -= cur_nr_sectors; sector_num += cur_nr_sectors; bytes_done += cur_nr_sectors * 512; } ret = 0; fail: qemu_co_mutex_unlock(&s->lock); qemu_iovec_destroy(&hd_qiov); qemu_vfree(cluster_data); return ret; } ",qemu,0
"static int usb_parse(const char *cmdline)

{

    int r;

    r = usb_device_add(cmdline);

    if (r < 0) {

        fprintf(stderr, ""qemu: could not add USB device '%s'\n"", cmdline);

    }

    return r;

}
","static int usb_parse(const char *cmdline) {
int r; r = usb_device_add(cmdline); if (r < 0) {
fprintf(stderr, ""qemu: could not add USB device '%s'\n"", cmdline); } return r; } ",qemu,0
"static int qcow2_open(BlockDriverState *bs, QDict *options, int flags,

                      Error **errp)

{

    BDRVQcowState *s = bs->opaque;

    int len, i, ret = 0;

    QCowHeader header;

    QemuOpts *opts;

    Error *local_err = NULL;

    uint64_t ext_end;

    uint64_t l1_vm_state_index;

    const char *opt_overlap_check;

    int overlap_check_template = 0;



    ret = bdrv_pread(bs->file, 0, &header, sizeof(header));

    if (ret < 0) {

        error_setg_errno(errp, -ret, ""Could not read qcow2 header"");

        goto fail;

    }

    be32_to_cpus(&header.magic);

    be32_to_cpus(&header.version);

    be64_to_cpus(&header.backing_file_offset);

    be32_to_cpus(&header.backing_file_size);

    be64_to_cpus(&header.size);

    be32_to_cpus(&header.cluster_bits);

    be32_to_cpus(&header.crypt_method);

    be64_to_cpus(&header.l1_table_offset);

    be32_to_cpus(&header.l1_size);

    be64_to_cpus(&header.refcount_table_offset);

    be32_to_cpus(&header.refcount_table_clusters);

    be64_to_cpus(&header.snapshots_offset);

    be32_to_cpus(&header.nb_snapshots);



    if (header.magic != QCOW_MAGIC) {

        error_setg(errp, ""Image is not in qcow2 format"");

        ret = -EINVAL;

        goto fail;

    }

    if (header.version < 2 || header.version > 3) {

        report_unsupported(bs, errp, ""QCOW version %d"", header.version);

        ret = -ENOTSUP;

        goto fail;

    }



    s->qcow_version = header.version;



    /* Initialise version 3 header fields */

    if (header.version == 2) {

        header.incompatible_features    = 0;

        header.compatible_features      = 0;

        header.autoclear_features       = 0;

        header.refcount_order           = 4;

        header.header_length            = 72;

    } else {

        be64_to_cpus(&header.incompatible_features);

        be64_to_cpus(&header.compatible_features);

        be64_to_cpus(&header.autoclear_features);

        be32_to_cpus(&header.refcount_order);

        be32_to_cpus(&header.header_length);

    }



    if (header.header_length > sizeof(header)) {

        s->unknown_header_fields_size = header.header_length - sizeof(header);

        s->unknown_header_fields = g_malloc(s->unknown_header_fields_size);

        ret = bdrv_pread(bs->file, sizeof(header), s->unknown_header_fields,

                         s->unknown_header_fields_size);

        if (ret < 0) {

            error_setg_errno(errp, -ret, ""Could not read unknown qcow2 header ""

                             ""fields"");

            goto fail;

        }

    }



    if (header.backing_file_offset) {

        ext_end = header.backing_file_offset;

    } else {

        ext_end = 1 << header.cluster_bits;

    }



    /* Handle feature bits */

    s->incompatible_features    = header.incompatible_features;

    s->compatible_features      = header.compatible_features;

    s->autoclear_features       = header.autoclear_features;



    if (s->incompatible_features & ~QCOW2_INCOMPAT_MASK) {

        void *feature_table = NULL;

        qcow2_read_extensions(bs, header.header_length, ext_end,

                              &feature_table, NULL);

        report_unsupported_feature(bs, errp, feature_table,

                                   s->incompatible_features &

                                   ~QCOW2_INCOMPAT_MASK);

        ret = -ENOTSUP;

        g_free(feature_table);

        goto fail;

    }



    if (s->incompatible_features & QCOW2_INCOMPAT_CORRUPT) {

        /* Corrupt images may not be written to unless they are being repaired

         */

        if ((flags & BDRV_O_RDWR) && !(flags & BDRV_O_CHECK)) {

            error_setg(errp, ""qcow2: Image is corrupt; cannot be opened ""

                       ""read/write"");

            ret = -EACCES;

            goto fail;

        }

    }



    /* Check support for various header values */

    if (header.refcount_order != 4) {

        report_unsupported(bs, errp, ""%d bit reference counts"",

                           1 << header.refcount_order);

        ret = -ENOTSUP;

        goto fail;

    }

    s->refcount_order = header.refcount_order;



    if (header.cluster_bits < MIN_CLUSTER_BITS ||

        header.cluster_bits > MAX_CLUSTER_BITS) {

        error_setg(errp, ""Unsupported cluster size: 2^%i"", header.cluster_bits);

        ret = -EINVAL;

        goto fail;

    }

    if (header.crypt_method > QCOW_CRYPT_AES) {

        error_setg(errp, ""Unsupported encryption method: %i"",

                   header.crypt_method);

        ret = -EINVAL;

        goto fail;

    }

    s->crypt_method_header = header.crypt_method;

    if (s->crypt_method_header) {

        bs->encrypted = 1;

    }

    s->cluster_bits = header.cluster_bits;

    s->cluster_size = 1 << s->cluster_bits;

    s->cluster_sectors = 1 << (s->cluster_bits - 9);

    s->l2_bits = s->cluster_bits - 3; /* L2 is always one cluster */

    s->l2_size = 1 << s->l2_bits;

    bs->total_sectors = header.size / 512;

    s->csize_shift = (62 - (s->cluster_bits - 8));

    s->csize_mask = (1 << (s->cluster_bits - 8)) - 1;

    s->cluster_offset_mask = (1LL << s->csize_shift) - 1;

    s->refcount_table_offset = header.refcount_table_offset;

    s->refcount_table_size =

        header.refcount_table_clusters << (s->cluster_bits - 3);



    s->snapshots_offset = header.snapshots_offset;

    s->nb_snapshots = header.nb_snapshots;



    /* read the level 1 table */

    s->l1_size = header.l1_size;



    l1_vm_state_index = size_to_l1(s, header.size);

    if (l1_vm_state_index > INT_MAX) {

        error_setg(errp, ""Image is too big"");

        ret = -EFBIG;

        goto fail;

    }

    s->l1_vm_state_index = l1_vm_state_index;



    /* the L1 table must contain at least enough entries to put

       header.size bytes */

    if (s->l1_size < s->l1_vm_state_index) {

        error_setg(errp, ""L1 table is too small"");

        ret = -EINVAL;

        goto fail;

    }

    s->l1_table_offset = header.l1_table_offset;

    if (s->l1_size > 0) {

        s->l1_table = g_malloc0(

            align_offset(s->l1_size * sizeof(uint64_t), 512));

        ret = bdrv_pread(bs->file, s->l1_table_offset, s->l1_table,

                         s->l1_size * sizeof(uint64_t));

        if (ret < 0) {

            error_setg_errno(errp, -ret, ""Could not read L1 table"");

            goto fail;

        }

        for(i = 0;i < s->l1_size; i++) {

            be64_to_cpus(&s->l1_table[i]);

        }

    }



    /* alloc L2 table/refcount block cache */

    s->l2_table_cache = qcow2_cache_create(bs, L2_CACHE_SIZE);

    s->refcount_block_cache = qcow2_cache_create(bs, REFCOUNT_CACHE_SIZE);



    s->cluster_cache = g_malloc(s->cluster_size);

    /* one more sector for decompressed data alignment */

    s->cluster_data = qemu_blockalign(bs, QCOW_MAX_CRYPT_CLUSTERS * s->cluster_size

                                  + 512);

    s->cluster_cache_offset = -1;

    s->flags = flags;



    ret = qcow2_refcount_init(bs);

    if (ret != 0) {

        error_setg_errno(errp, -ret, ""Could not initialize refcount handling"");

        goto fail;

    }



    QLIST_INIT(&s->cluster_allocs);

    QTAILQ_INIT(&s->discards);



    /* read qcow2 extensions */

    if (qcow2_read_extensions(bs, header.header_length, ext_end, NULL,

        &local_err)) {

        error_propagate(errp, local_err);

        ret = -EINVAL;

        goto fail;

    }



    /* read the backing file name */

    if (header.backing_file_offset != 0) {

        len = header.backing_file_size;

        if (len > 1023) {

            len = 1023;

        }

        ret = bdrv_pread(bs->file, header.backing_file_offset,

                         bs->backing_file, len);

        if (ret < 0) {

            error_setg_errno(errp, -ret, ""Could not read backing file name"");

            goto fail;

        }

        bs->backing_file[len] = '\0';

    }



    ret = qcow2_read_snapshots(bs);

    if (ret < 0) {

        error_setg_errno(errp, -ret, ""Could not read snapshots"");

        goto fail;

    }



    /* Clear unknown autoclear feature bits */

    if (!bs->read_only && !(flags & BDRV_O_INCOMING) && s->autoclear_features) {

        s->autoclear_features = 0;

        ret = qcow2_update_header(bs);

        if (ret < 0) {

            error_setg_errno(errp, -ret, ""Could not update qcow2 header"");

            goto fail;

        }

    }



    /* Initialise locks */

    qemu_co_mutex_init(&s->lock);



    /* Repair image if dirty */

    if (!(flags & (BDRV_O_CHECK | BDRV_O_INCOMING)) && !bs->read_only &&

        (s->incompatible_features & QCOW2_INCOMPAT_DIRTY)) {

        BdrvCheckResult result = {0};



        ret = qcow2_check(bs, &result, BDRV_FIX_ERRORS);

        if (ret < 0) {

            error_setg_errno(errp, -ret, ""Could not repair dirty image"");

            goto fail;

        }

    }



    /* Enable lazy_refcounts according to image and command line options */

    opts = qemu_opts_create(&qcow2_runtime_opts, NULL, 0, &error_abort);

    qemu_opts_absorb_qdict(opts, options, &local_err);

    if (local_err) {

        error_propagate(errp, local_err);

        ret = -EINVAL;

        goto fail;

    }



    s->use_lazy_refcounts = qemu_opt_get_bool(opts, QCOW2_OPT_LAZY_REFCOUNTS,

        (s->compatible_features & QCOW2_COMPAT_LAZY_REFCOUNTS));



    s->discard_passthrough[QCOW2_DISCARD_NEVER] = false;

    s->discard_passthrough[QCOW2_DISCARD_ALWAYS] = true;

    s->discard_passthrough[QCOW2_DISCARD_REQUEST] =

        qemu_opt_get_bool(opts, QCOW2_OPT_DISCARD_REQUEST,

                          flags & BDRV_O_UNMAP);

    s->discard_passthrough[QCOW2_DISCARD_SNAPSHOT] =

        qemu_opt_get_bool(opts, QCOW2_OPT_DISCARD_SNAPSHOT, true);

    s->discard_passthrough[QCOW2_DISCARD_OTHER] =

        qemu_opt_get_bool(opts, QCOW2_OPT_DISCARD_OTHER, false);



    opt_overlap_check = qemu_opt_get(opts, ""overlap-check"") ?: ""cached"";

    if (!strcmp(opt_overlap_check, ""none"")) {

        overlap_check_template = 0;

    } else if (!strcmp(opt_overlap_check, ""constant"")) {

        overlap_check_template = QCOW2_OL_CONSTANT;

    } else if (!strcmp(opt_overlap_check, ""cached"")) {

        overlap_check_template = QCOW2_OL_CACHED;

    } else if (!strcmp(opt_overlap_check, ""all"")) {

        overlap_check_template = QCOW2_OL_ALL;

    } else {

        error_setg(errp, ""Unsupported value '%s' for qcow2 option ""

                   ""'overlap-check'. Allowed are either of the following: ""

                   ""none, constant, cached, all"", opt_overlap_check);

        qemu_opts_del(opts);

        ret = -EINVAL;

        goto fail;

    }



    s->overlap_check = 0;

    for (i = 0; i < QCOW2_OL_MAX_BITNR; i++) {

        /* overlap-check defines a template bitmask, but every flag may be

         * overwritten through the associated boolean option */

        s->overlap_check |=

            qemu_opt_get_bool(opts, overlap_bool_option_names[i],

                              overlap_check_template & (1 << i)) << i;

    }



    qemu_opts_del(opts);



    if (s->use_lazy_refcounts && s->qcow_version < 3) {

        error_setg(errp, ""Lazy refcounts require a qcow2 image with at least ""

                   ""qemu 1.1 compatibility level"");

        ret = -EINVAL;

        goto fail;

    }



#ifdef DEBUG_ALLOC

    {

        BdrvCheckResult result = {0};

        qcow2_check_refcounts(bs, &result, 0);

    }

#endif

    return ret;



 fail:

    g_free(s->unknown_header_fields);

    cleanup_unknown_header_ext(bs);

    qcow2_free_snapshots(bs);

    qcow2_refcount_close(bs);

    g_free(s->l1_table);

    /* else pre-write overlap checks in cache_destroy may crash */

    s->l1_table = NULL;

    if (s->l2_table_cache) {

        qcow2_cache_destroy(bs, s->l2_table_cache);

    }

    if (s->refcount_block_cache) {

        qcow2_cache_destroy(bs, s->refcount_block_cache);

    }

    g_free(s->cluster_cache);

    qemu_vfree(s->cluster_data);

    return ret;

}
","static int qcow2_open(BlockDriverState *bs, QDict *options, int flags, Error **errp) {
BDRVQcowState *s = bs->opaque; int len, i, ret = 0; QCowHeader header; QemuOpts *opts; Error *local_err = NULL; uint64_t ext_end; uint64_t l1_vm_state_index; const char *opt_overlap_check; int overlap_check_template = 0; ret = bdrv_pread(bs->file, 0, &header, sizeof(header)); if (ret < 0) {
error_setg_errno(errp, -ret, ""Could not read qcow2 header""); goto fail; } be32_to_cpus(&header.magic); be32_to_cpus(&header.version); be64_to_cpus(&header.backing_file_offset); be32_to_cpus(&header.backing_file_size); be64_to_cpus(&header.size); be32_to_cpus(&header.cluster_bits); be32_to_cpus(&header.crypt_method); be64_to_cpus(&header.l1_table_offset); be32_to_cpus(&header.l1_size); be64_to_cpus(&header.refcount_table_offset); be32_to_cpus(&header.refcount_table_clusters); be64_to_cpus(&header.snapshots_offset); be32_to_cpus(&header.nb_snapshots); if (header.magic != QCOW_MAGIC) {
error_setg(errp, ""Image is not in qcow2 format""); ret = -EINVAL; goto fail; } if (header.version < 2 || header.version > 3) {
report_unsupported(bs, errp, ""QCOW version %d"", header.version); ret = -ENOTSUP; goto fail; } s->qcow_version = header.version; /* Initialise version 3 header fields */ if (header.version == 2) {
header.incompatible_features = 0; header.compatible_features = 0; header.autoclear_features = 0; header.refcount_order = 4; header.header_length = 72; } else {
be64_to_cpus(&header.incompatible_features); be64_to_cpus(&header.compatible_features); be64_to_cpus(&header.autoclear_features); be32_to_cpus(&header.refcount_order); be32_to_cpus(&header.header_length); } if (header.header_length > sizeof(header)) {
s->unknown_header_fields_size = header.header_length - sizeof(header); s->unknown_header_fields = g_malloc(s->unknown_header_fields_size); ret = bdrv_pread(bs->file, sizeof(header), s->unknown_header_fields, s->unknown_header_fields_size); if (ret < 0) {
error_setg_errno(errp, -ret, ""Could not read unknown qcow2 header "" ""fields""); goto fail; } } if (header.backing_file_offset) {
ext_end = header.backing_file_offset; } else {
ext_end = 1 << header.cluster_bits; } /* Handle feature bits */ s->incompatible_features = header.incompatible_features; s->compatible_features = header.compatible_features; s->autoclear_features = header.autoclear_features; if (s->incompatible_features & ~QCOW2_INCOMPAT_MASK) {
void *feature_table = NULL; qcow2_read_extensions(bs, header.header_length, ext_end, &feature_table, NULL); report_unsupported_feature(bs, errp, feature_table, s->incompatible_features & ~QCOW2_INCOMPAT_MASK); ret = -ENOTSUP; g_free(feature_table); goto fail; } if (s->incompatible_features & QCOW2_INCOMPAT_CORRUPT) {
/* Corrupt images may not be written to unless they are being repaired */ if ((flags & BDRV_O_RDWR) && !(flags & BDRV_O_CHECK)) {
error_setg(errp, ""qcow2: Image is corrupt; cannot be opened "" ""read/write""); ret = -EACCES; goto fail; } } /* Check support for various header values */ if (header.refcount_order != 4) {
report_unsupported(bs, errp, ""%d bit reference counts"", 1 << header.refcount_order); ret = -ENOTSUP; goto fail; } s->refcount_order = header.refcount_order; if (header.cluster_bits < MIN_CLUSTER_BITS || header.cluster_bits > MAX_CLUSTER_BITS) {
error_setg(errp, ""Unsupported cluster size: 2^%i"", header.cluster_bits); ret = -EINVAL; goto fail; } if (header.crypt_method > QCOW_CRYPT_AES) {
error_setg(errp, ""Unsupported encryption method: %i"", header.crypt_method); ret = -EINVAL; goto fail; } s->crypt_method_header = header.crypt_method; if (s->crypt_method_header) {
bs->encrypted = 1; } s->cluster_bits = header.cluster_bits; s->cluster_size = 1 << s->cluster_bits; s->cluster_sectors = 1 << (s->cluster_bits - 9); s->l2_bits = s->cluster_bits - 3; /* L2 is always one cluster */ s->l2_size = 1 << s->l2_bits; bs->total_sectors = header.size / 512; s->csize_shift = (62 - (s->cluster_bits - 8)); s->csize_mask = (1 << (s->cluster_bits - 8)) - 1; s->cluster_offset_mask = (1LL << s->csize_shift) - 1; s->refcount_table_offset = header.refcount_table_offset; s->refcount_table_size = header.refcount_table_clusters << (s->cluster_bits - 3); s->snapshots_offset = header.snapshots_offset; s->nb_snapshots = header.nb_snapshots; /* read the level 1 table */ s->l1_size = header.l1_size; l1_vm_state_index = size_to_l1(s, header.size); if (l1_vm_state_index > INT_MAX) {
error_setg(errp, ""Image is too big""); ret = -EFBIG; goto fail; } s->l1_vm_state_index = l1_vm_state_index; /* the L1 table must contain at least enough entries to put header.size bytes */ if (s->l1_size < s->l1_vm_state_index) {
error_setg(errp, ""L1 table is too small""); ret = -EINVAL; goto fail; } s->l1_table_offset = header.l1_table_offset; if (s->l1_size > 0) {
s->l1_table = g_malloc0( align_offset(s->l1_size * sizeof(uint64_t), 512)); ret = bdrv_pread(bs->file, s->l1_table_offset, s->l1_table, s->l1_size * sizeof(uint64_t)); if (ret < 0) {
error_setg_errno(errp, -ret, ""Could not read L1 table""); goto fail; } for(i = 0;i < s->l1_size; i++) {
be64_to_cpus(&s->l1_table[i]); } } /* alloc L2 table/refcount block cache */ s->l2_table_cache = qcow2_cache_create(bs, L2_CACHE_SIZE); s->refcount_block_cache = qcow2_cache_create(bs, REFCOUNT_CACHE_SIZE); s->cluster_cache = g_malloc(s->cluster_size); /* one more sector for decompressed data alignment */ s->cluster_data = qemu_blockalign(bs, QCOW_MAX_CRYPT_CLUSTERS * s->cluster_size + 512); s->cluster_cache_offset = -1; s->flags = flags; ret = qcow2_refcount_init(bs); if (ret != 0) {
error_setg_errno(errp, -ret, ""Could not initialize refcount handling""); goto fail; } QLIST_INIT(&s->cluster_allocs); QTAILQ_INIT(&s->discards); /* read qcow2 extensions */ if (qcow2_read_extensions(bs, header.header_length, ext_end, NULL, &local_err)) {
error_propagate(errp, local_err); ret = -EINVAL; goto fail; } /* read the backing file name */ if (header.backing_file_offset != 0) {
len = header.backing_file_size; if (len > 1023) {
len = 1023; } ret = bdrv_pread(bs->file, header.backing_file_offset, bs->backing_file, len); if (ret < 0) {
error_setg_errno(errp, -ret, ""Could not read backing file name""); goto fail; } bs->backing_file[len] = '\0'; } ret = qcow2_read_snapshots(bs); if (ret < 0) {
error_setg_errno(errp, -ret, ""Could not read snapshots""); goto fail; } /* Clear unknown autoclear feature bits */ if (!bs->read_only && !(flags & BDRV_O_INCOMING) && s->autoclear_features) {
s->autoclear_features = 0; ret = qcow2_update_header(bs); if (ret < 0) {
error_setg_errno(errp, -ret, ""Could not update qcow2 header""); goto fail; } } /* Initialise locks */ qemu_co_mutex_init(&s->lock); /* Repair image if dirty */ if (!(flags & (BDRV_O_CHECK | BDRV_O_INCOMING)) && !bs->read_only && (s->incompatible_features & QCOW2_INCOMPAT_DIRTY)) {
BdrvCheckResult result = {
0}; ret = qcow2_check(bs, &result, BDRV_FIX_ERRORS); if (ret < 0) {
error_setg_errno(errp, -ret, ""Could not repair dirty image""); goto fail; } } /* Enable lazy_refcounts according to image and command line options */ opts = qemu_opts_create(&qcow2_runtime_opts, NULL, 0, &error_abort); qemu_opts_absorb_qdict(opts, options, &local_err); if (local_err) {
error_propagate(errp, local_err); ret = -EINVAL; goto fail; } s->use_lazy_refcounts = qemu_opt_get_bool(opts, QCOW2_OPT_LAZY_REFCOUNTS, (s->compatible_features & QCOW2_COMPAT_LAZY_REFCOUNTS)); s->discard_passthrough[QCOW2_DISCARD_NEVER] = false; s->discard_passthrough[QCOW2_DISCARD_ALWAYS] = true; s->discard_passthrough[QCOW2_DISCARD_REQUEST] = qemu_opt_get_bool(opts, QCOW2_OPT_DISCARD_REQUEST, flags & BDRV_O_UNMAP); s->discard_passthrough[QCOW2_DISCARD_SNAPSHOT] = qemu_opt_get_bool(opts, QCOW2_OPT_DISCARD_SNAPSHOT, true); s->discard_passthrough[QCOW2_DISCARD_OTHER] = qemu_opt_get_bool(opts, QCOW2_OPT_DISCARD_OTHER, false); opt_overlap_check = qemu_opt_get(opts, ""overlap-check"") ?: ""cached""; if (!strcmp(opt_overlap_check, ""none"")) {
overlap_check_template = 0; } else if (!strcmp(opt_overlap_check, ""constant"")) {
overlap_check_template = QCOW2_OL_CONSTANT; } else if (!strcmp(opt_overlap_check, ""cached"")) {
overlap_check_template = QCOW2_OL_CACHED; } else if (!strcmp(opt_overlap_check, ""all"")) {
overlap_check_template = QCOW2_OL_ALL; } else {
error_setg(errp, ""Unsupported value '%s' for qcow2 option "" ""'overlap-check'. Allowed are either of the following: "" ""none, constant, cached, all"", opt_overlap_check); qemu_opts_del(opts); ret = -EINVAL; goto fail; } s->overlap_check = 0; for (i = 0; i < QCOW2_OL_MAX_BITNR; i++) {
/* overlap-check defines a template bitmask, but every flag may be * overwritten through the associated boolean option */ s->overlap_check |= qemu_opt_get_bool(opts, overlap_bool_option_names[i], overlap_check_template & (1 << i)) << i; } qemu_opts_del(opts); if (s->use_lazy_refcounts && s->qcow_version < 3) {
error_setg(errp, ""Lazy refcounts require a qcow2 image with at least "" ""qemu 1.1 compatibility level""); ret = -EINVAL; goto fail; } #ifdef DEBUG_ALLOC {
BdrvCheckResult result = {
0}; qcow2_check_refcounts(bs, &result, 0); } #endif return ret; fail: g_free(s->unknown_header_fields); cleanup_unknown_header_ext(bs); qcow2_free_snapshots(bs); qcow2_refcount_close(bs); g_free(s->l1_table); /* else pre-write overlap checks in cache_destroy may crash */ s->l1_table = NULL; if (s->l2_table_cache) {
qcow2_cache_destroy(bs, s->l2_table_cache); } if (s->refcount_block_cache) {
qcow2_cache_destroy(bs, s->refcount_block_cache); } g_free(s->cluster_cache); qemu_vfree(s->cluster_data); return ret; } ",qemu,1
"static int omap_validate_tipb_mpui_addr(struct omap_mpu_state_s *s,

                target_phys_addr_t addr)

{

    return range_covers_byte(0xe1010000, 0xe1020004 - 0xe1010000, addr);

}
","static int omap_validate_tipb_mpui_addr(struct omap_mpu_state_s *s, target_phys_addr_t addr) {
return range_covers_byte(0xe1010000, 0xe1020004 - 0xe1010000, addr); } ",qemu,0
"static void gen_spr_power5p_lpar(CPUPPCState *env)

{

#if !defined(CONFIG_USER_ONLY)

    /* Logical partitionning */

    spr_register_kvm(env, SPR_LPCR, ""LPCR"",


                     &spr_read_generic, &spr_write_lpcr,

                     KVM_REG_PPC_LPCR, LPCR_LPES0 | LPCR_LPES1);





#endif

}","static void gen_spr_power5p_lpar(CPUPPCState *env) {
#if !defined(CONFIG_USER_ONLY) /* Logical partitionning */ spr_register_kvm(env, SPR_LPCR, ""LPCR"", &spr_read_generic, &spr_write_lpcr, KVM_REG_PPC_LPCR, LPCR_LPES0 | LPCR_LPES1); #endif }",qemu,1
"static int bmdma_prepare_buf(IDEDMA *dma, int is_write)

{

    BMDMAState *bm = DO_UPCAST(BMDMAState, dma, dma);

    IDEState *s = bmdma_active_if(bm);

    struct {

        uint32_t addr;

        uint32_t size;

    } prd;

    int l, len;



    qemu_sglist_init(&s->sg, s->nsector / (BMDMA_PAGE_SIZE / 512) + 1);

    s->io_buffer_size = 0;

    for(;;) {

        if (bm->cur_prd_len == 0) {

            /* end of table (with a fail safe of one page) */

            if (bm->cur_prd_last ||

                (bm->cur_addr - bm->addr) >= BMDMA_PAGE_SIZE)

                return s->io_buffer_size != 0;

            cpu_physical_memory_read(bm->cur_addr, (uint8_t *)&prd, 8);

            bm->cur_addr += 8;

            prd.addr = le32_to_cpu(prd.addr);

            prd.size = le32_to_cpu(prd.size);

            len = prd.size & 0xfffe;

            if (len == 0)

                len = 0x10000;

            bm->cur_prd_len = len;

            bm->cur_prd_addr = prd.addr;

            bm->cur_prd_last = (prd.size & 0x80000000);

        }

        l = bm->cur_prd_len;

        if (l > 0) {

            qemu_sglist_add(&s->sg, bm->cur_prd_addr, l);

            bm->cur_prd_addr += l;

            bm->cur_prd_len -= l;

            s->io_buffer_size += l;

        }

    }

    return 1;

}
","static int bmdma_prepare_buf(IDEDMA *dma, int is_write) {
BMDMAState *bm = DO_UPCAST(BMDMAState, dma, dma); IDEState *s = bmdma_active_if(bm); struct {
uint32_t addr; uint32_t size; } prd; int l, len; qemu_sglist_init(&s->sg, s->nsector / (BMDMA_PAGE_SIZE / 512) + 1); s->io_buffer_size = 0; for(;;) {
if (bm->cur_prd_len == 0) {
/* end of table (with a fail safe of one page) */ if (bm->cur_prd_last || (bm->cur_addr - bm->addr) >= BMDMA_PAGE_SIZE) return s->io_buffer_size != 0; cpu_physical_memory_read(bm->cur_addr, (uint8_t *)&prd, 8); bm->cur_addr += 8; prd.addr = le32_to_cpu(prd.addr); prd.size = le32_to_cpu(prd.size); len = prd.size & 0xfffe; if (len == 0) len = 0x10000; bm->cur_prd_len = len; bm->cur_prd_addr = prd.addr; bm->cur_prd_last = (prd.size & 0x80000000); } l = bm->cur_prd_len; if (l > 0) {
qemu_sglist_add(&s->sg, bm->cur_prd_addr, l); bm->cur_prd_addr += l; bm->cur_prd_len -= l; s->io_buffer_size += l; } } return 1; } ",qemu,0
"print_ipc_cmd(int cmd)

{

#define output_cmd(val) \

if( cmd == val ) { \

    gemu_log(#val); \

    return; \

}



    cmd &= 0xff;



    /* General IPC commands */

    output_cmd( IPC_RMID );

    output_cmd( IPC_SET );

    output_cmd( IPC_STAT );

    output_cmd( IPC_INFO );

    /* msgctl() commands */

    #ifdef __USER_MISC

    output_cmd( MSG_STAT );

    output_cmd( MSG_INFO );

    #endif

    /* shmctl() commands */

    output_cmd( SHM_LOCK );

    output_cmd( SHM_UNLOCK );

    output_cmd( SHM_STAT );

    output_cmd( SHM_INFO );

    /* semctl() commands */

    output_cmd( GETPID );

    output_cmd( GETVAL );

    output_cmd( GETALL );

    output_cmd( GETNCNT );

    output_cmd( GETZCNT );

    output_cmd( SETVAL );

    output_cmd( SETALL );

    output_cmd( SEM_STAT );

    output_cmd( SEM_INFO );

    output_cmd( IPC_RMID );

    output_cmd( IPC_RMID );

    output_cmd( IPC_RMID );

    output_cmd( IPC_RMID );

    output_cmd( IPC_RMID );

    output_cmd( IPC_RMID );

    output_cmd( IPC_RMID );

    output_cmd( IPC_RMID );

    output_cmd( IPC_RMID );



    /* Some value we don't recognize */

    gemu_log(""%d"",cmd);

}
","print_ipc_cmd(int cmd) {
#define output_cmd(val) \ if( cmd == val ) {
\ gemu_log(#val); \ return; \ } cmd &= 0xff; /* General IPC commands */ output_cmd( IPC_RMID ); output_cmd( IPC_SET ); output_cmd( IPC_STAT ); output_cmd( IPC_INFO ); /* msgctl() commands */ #ifdef __USER_MISC output_cmd( MSG_STAT ); output_cmd( MSG_INFO ); #endif /* shmctl() commands */ output_cmd( SHM_LOCK ); output_cmd( SHM_UNLOCK ); output_cmd( SHM_STAT ); output_cmd( SHM_INFO ); /* semctl() commands */ output_cmd( GETPID ); output_cmd( GETVAL ); output_cmd( GETALL ); output_cmd( GETNCNT ); output_cmd( GETZCNT ); output_cmd( SETVAL ); output_cmd( SETALL ); output_cmd( SEM_STAT ); output_cmd( SEM_INFO ); output_cmd( IPC_RMID ); output_cmd( IPC_RMID ); output_cmd( IPC_RMID ); output_cmd( IPC_RMID ); output_cmd( IPC_RMID ); output_cmd( IPC_RMID ); output_cmd( IPC_RMID ); output_cmd( IPC_RMID ); output_cmd( IPC_RMID ); /* Some value we don't recognize */ gemu_log(""%d"",cmd); } ",qemu,1
"static int elf_core_dump(int signr, const CPUArchState *env)

{

    const CPUState *cpu = ENV_GET_CPU((CPUArchState *)env);

    const TaskState *ts = (const TaskState *)cpu->opaque;

    struct vm_area_struct *vma = NULL;

    char corefile[PATH_MAX];

    struct elf_note_info info;

    struct elfhdr elf;

    struct elf_phdr phdr;

    struct rlimit dumpsize;

    struct mm_struct *mm = NULL;

    off_t offset = 0, data_offset = 0;

    int segs = 0;

    int fd = -1;



    init_note_info(&info);



    errno = 0;

    getrlimit(RLIMIT_CORE, &dumpsize);

    if (dumpsize.rlim_cur == 0)

        return 0;



    if (core_dump_filename(ts, corefile, sizeof (corefile)) < 0)

        return (-errno);



    if ((fd = open(corefile, O_WRONLY | O_CREAT,

                   S_IRUSR|S_IWUSR|S_IRGRP|S_IROTH)) < 0)

        return (-errno);



    /*

     * Walk through target process memory mappings and

     * set up structure containing this information.  After

     * this point vma_xxx functions can be used.

     */

    if ((mm = vma_init()) == NULL)

        goto out;



    walk_memory_regions(mm, vma_walker);

    segs = vma_get_mapping_count(mm);



    /*

     * Construct valid coredump ELF header.  We also

     * add one more segment for notes.

     */

    fill_elf_header(&elf, segs + 1, ELF_MACHINE, 0);

    if (dump_write(fd, &elf, sizeof (elf)) != 0)

        goto out;



    /* fill in the in-memory version of notes */

    if (fill_note_info(&info, signr, env) < 0)

        goto out;



    offset += sizeof (elf);                             /* elf header */

    offset += (segs + 1) * sizeof (struct elf_phdr);    /* program headers */



    /* write out notes program header */

    fill_elf_note_phdr(&phdr, info.notes_size, offset);



    offset += info.notes_size;

    if (dump_write(fd, &phdr, sizeof (phdr)) != 0)

        goto out;



    /*

     * ELF specification wants data to start at page boundary so

     * we align it here.

     */

    data_offset = offset = roundup(offset, ELF_EXEC_PAGESIZE);



    /*

     * Write program headers for memory regions mapped in

     * the target process.

     */

    for (vma = vma_first(mm); vma != NULL; vma = vma_next(vma)) {

        (void) memset(&phdr, 0, sizeof (phdr));



        phdr.p_type = PT_LOAD;

        phdr.p_offset = offset;

        phdr.p_vaddr = vma->vma_start;

        phdr.p_paddr = 0;

        phdr.p_filesz = vma_dump_size(vma);

        offset += phdr.p_filesz;

        phdr.p_memsz = vma->vma_end - vma->vma_start;

        phdr.p_flags = vma->vma_flags & PROT_READ ? PF_R : 0;

        if (vma->vma_flags & PROT_WRITE)

            phdr.p_flags |= PF_W;

        if (vma->vma_flags & PROT_EXEC)

            phdr.p_flags |= PF_X;

        phdr.p_align = ELF_EXEC_PAGESIZE;



        bswap_phdr(&phdr, 1);

        dump_write(fd, &phdr, sizeof (phdr));

    }



    /*

     * Next we write notes just after program headers.  No

     * alignment needed here.

     */

    if (write_note_info(&info, fd) < 0)

        goto out;



    /* align data to page boundary */

    if (lseek(fd, data_offset, SEEK_SET) != data_offset)

        goto out;



    /*

     * Finally we can dump process memory into corefile as well.

     */

    for (vma = vma_first(mm); vma != NULL; vma = vma_next(vma)) {

        abi_ulong addr;

        abi_ulong end;



        end = vma->vma_start + vma_dump_size(vma);



        for (addr = vma->vma_start; addr < end;

             addr += TARGET_PAGE_SIZE) {

            char page[TARGET_PAGE_SIZE];

            int error;



            /*

             *  Read in page from target process memory and

             *  write it to coredump file.

             */

            error = copy_from_user(page, addr, sizeof (page));

            if (error != 0) {

                (void) fprintf(stderr, ""unable to dump "" TARGET_ABI_FMT_lx ""\n"",

                               addr);

                errno = -error;

                goto out;

            }

            if (dump_write(fd, page, TARGET_PAGE_SIZE) < 0)

                goto out;

        }

    }



 out:

    free_note_info(&info);

    if (mm != NULL)

        vma_delete(mm);

    (void) close(fd);



    if (errno != 0)

        return (-errno);

    return (0);

}
","static int elf_core_dump(int signr, const CPUArchState *env) {
const CPUState *cpu = ENV_GET_CPU((CPUArchState *)env); const TaskState *ts = (const TaskState *)cpu->opaque; struct vm_area_struct *vma = NULL; char corefile[PATH_MAX]; struct elf_note_info info; struct elfhdr elf; struct elf_phdr phdr; struct rlimit dumpsize; struct mm_struct *mm = NULL; off_t offset = 0, data_offset = 0; int segs = 0; int fd = -1; init_note_info(&info); errno = 0; getrlimit(RLIMIT_CORE, &dumpsize); if (dumpsize.rlim_cur == 0) return 0; if (core_dump_filename(ts, corefile, sizeof (corefile)) < 0) return (-errno); if ((fd = open(corefile, O_WRONLY | O_CREAT, S_IRUSR|S_IWUSR|S_IRGRP|S_IROTH)) < 0) return (-errno); /* * Walk through target process memory mappings and * set up structure containing this information. After * this point vma_xxx functions can be used. */ if ((mm = vma_init()) == NULL) goto out; walk_memory_regions(mm, vma_walker); segs = vma_get_mapping_count(mm); /* * Construct valid coredump ELF header. We also * add one more segment for notes. */ fill_elf_header(&elf, segs + 1, ELF_MACHINE, 0); if (dump_write(fd, &elf, sizeof (elf)) != 0) goto out; /* fill in the in-memory version of notes */ if (fill_note_info(&info, signr, env) < 0) goto out; offset += sizeof (elf); /* elf header */ offset += (segs + 1) * sizeof (struct elf_phdr); /* program headers */ /* write out notes program header */ fill_elf_note_phdr(&phdr, info.notes_size, offset); offset += info.notes_size; if (dump_write(fd, &phdr, sizeof (phdr)) != 0) goto out; /* * ELF specification wants data to start at page boundary so * we align it here. */ data_offset = offset = roundup(offset, ELF_EXEC_PAGESIZE); /* * Write program headers for memory regions mapped in * the target process. */ for (vma = vma_first(mm); vma != NULL; vma = vma_next(vma)) {
(void) memset(&phdr, 0, sizeof (phdr)); phdr.p_type = PT_LOAD; phdr.p_offset = offset; phdr.p_vaddr = vma->vma_start; phdr.p_paddr = 0; phdr.p_filesz = vma_dump_size(vma); offset += phdr.p_filesz; phdr.p_memsz = vma->vma_end - vma->vma_start; phdr.p_flags = vma->vma_flags & PROT_READ ? PF_R : 0; if (vma->vma_flags & PROT_WRITE) phdr.p_flags |= PF_W; if (vma->vma_flags & PROT_EXEC) phdr.p_flags |= PF_X; phdr.p_align = ELF_EXEC_PAGESIZE; bswap_phdr(&phdr, 1); dump_write(fd, &phdr, sizeof (phdr)); } /* * Next we write notes just after program headers. No * alignment needed here. */ if (write_note_info(&info, fd) < 0) goto out; /* align data to page boundary */ if (lseek(fd, data_offset, SEEK_SET) != data_offset) goto out; /* * Finally we can dump process memory into corefile as well. */ for (vma = vma_first(mm); vma != NULL; vma = vma_next(vma)) {
abi_ulong addr; abi_ulong end; end = vma->vma_start + vma_dump_size(vma); for (addr = vma->vma_start; addr < end; addr += TARGET_PAGE_SIZE) {
char page[TARGET_PAGE_SIZE]; int error; /* * Read in page from target process memory and * write it to coredump file. */ error = copy_from_user(page, addr, sizeof (page)); if (error != 0) {
(void) fprintf(stderr, ""unable to dump "" TARGET_ABI_FMT_lx ""\n"", addr); errno = -error; goto out; } if (dump_write(fd, page, TARGET_PAGE_SIZE) < 0) goto out; } } out: free_note_info(&info); if (mm != NULL) vma_delete(mm); (void) close(fd); if (errno != 0) return (-errno); return (0); } ",qemu,1
"bool write_cpustate_to_list(ARMCPU *cpu)

{

    /* Write the coprocessor state from cpu->env to the (index,value) list. */

    int i;

    bool ok = true;



    for (i = 0; i < cpu->cpreg_array_len; i++) {

        uint32_t regidx = kvm_to_cpreg_id(cpu->cpreg_indexes[i]);

        const ARMCPRegInfo *ri;



        ri = get_arm_cp_reginfo(cpu->cp_regs, regidx);

        if (!ri) {

            ok = false;

            continue;

        }

        if (ri->type & ARM_CP_NO_MIGRATE) {

            continue;

        }

        cpu->cpreg_values[i] = read_raw_cp_reg(&cpu->env, ri);

    }

    return ok;

}
","bool write_cpustate_to_list(ARMCPU *cpu) {
/* Write the coprocessor state from cpu->env to the (index,value) list. */ int i; bool ok = true; for (i = 0; i < cpu->cpreg_array_len; i++) {
uint32_t regidx = kvm_to_cpreg_id(cpu->cpreg_indexes[i]); const ARMCPRegInfo *ri; ri = get_arm_cp_reginfo(cpu->cp_regs, regidx); if (!ri) {
ok = false; continue; } if (ri->type & ARM_CP_NO_MIGRATE) {
continue; } cpu->cpreg_values[i] = read_raw_cp_reg(&cpu->env, ri); } return ok; } ",qemu,0
"static void spapr_msi_setmsg(PCIDevice *pdev, hwaddr addr, bool msix,

                             unsigned first_irq, unsigned req_num)

{

    unsigned i;

    MSIMessage msg = { .address = addr, .data = first_irq };



    if (!msix) {

        msi_set_message(pdev, msg);

        trace_spapr_pci_msi_setup(pdev->name, 0, msg.address);

        return;

    }



    for (i = 0; i < req_num; ++i, ++msg.data) {

        msix_set_message(pdev, i, msg);

        trace_spapr_pci_msi_setup(pdev->name, i, msg.address);

    }

}
","static void spapr_msi_setmsg(PCIDevice *pdev, hwaddr addr, bool msix, unsigned first_irq, unsigned req_num) {
unsigned i; MSIMessage msg = {
.address = addr, .data = first_irq }; if (!msix) {
msi_set_message(pdev, msg); trace_spapr_pci_msi_setup(pdev->name, 0, msg.address); return; } for (i = 0; i < req_num; ++i, ++msg.data) {
msix_set_message(pdev, i, msg); trace_spapr_pci_msi_setup(pdev->name, i, msg.address); } } ",qemu,1
"static void gen_ld (CPUMIPSState *env, DisasContext *ctx, uint32_t opc,

                    int rt, int base, int16_t offset)

{

    const char *opn = ""ld"";

    TCGv t0, t1;



    if (rt == 0 && env->insn_flags & (INSN_LOONGSON2E | INSN_LOONGSON2F)) {

        /* Loongson CPU uses a load to zero register for prefetch.

           We emulate it as a NOP. On other CPU we must perform the

           actual memory access. */

        MIPS_DEBUG(""NOP"");

        return;

    }



    t0 = tcg_temp_new();

    gen_base_offset_addr(ctx, t0, base, offset);



    switch (opc) {

#if defined(TARGET_MIPS64)

    case OPC_LWU:

        tcg_gen_qemu_ld32u(t0, t0, ctx->mem_idx);

        gen_store_gpr(t0, rt);

        opn = ""lwu"";

        break;

    case OPC_LD:

        tcg_gen_qemu_ld64(t0, t0, ctx->mem_idx);

        gen_store_gpr(t0, rt);

        opn = ""ld"";

        break;

    case OPC_LLD:

        save_cpu_state(ctx, 1);

        op_ld_lld(t0, t0, ctx);

        gen_store_gpr(t0, rt);

        opn = ""lld"";

        break;

    case OPC_LDL:

        save_cpu_state(ctx, 1);

        t1 = tcg_temp_new();

        gen_load_gpr(t1, rt);

        gen_helper_1e2i(ldl, t1, t1, t0, ctx->mem_idx);

        gen_store_gpr(t1, rt);

        tcg_temp_free(t1);

        opn = ""ldl"";

        break;

    case OPC_LDR:

        save_cpu_state(ctx, 1);

        t1 = tcg_temp_new();

        gen_load_gpr(t1, rt);

        gen_helper_1e2i(ldr, t1, t1, t0, ctx->mem_idx);

        gen_store_gpr(t1, rt);

        tcg_temp_free(t1);

        opn = ""ldr"";

        break;

    case OPC_LDPC:

        t1 = tcg_const_tl(pc_relative_pc(ctx));

        gen_op_addr_add(ctx, t0, t0, t1);

        tcg_temp_free(t1);

        tcg_gen_qemu_ld64(t0, t0, ctx->mem_idx);

        gen_store_gpr(t0, rt);

        opn = ""ldpc"";

        break;

#endif

    case OPC_LWPC:

        t1 = tcg_const_tl(pc_relative_pc(ctx));

        gen_op_addr_add(ctx, t0, t0, t1);

        tcg_temp_free(t1);

        tcg_gen_qemu_ld32s(t0, t0, ctx->mem_idx);

        gen_store_gpr(t0, rt);

        opn = ""lwpc"";

        break;

    case OPC_LW:

        tcg_gen_qemu_ld32s(t0, t0, ctx->mem_idx);

        gen_store_gpr(t0, rt);

        opn = ""lw"";

        break;

    case OPC_LH:

        tcg_gen_qemu_ld16s(t0, t0, ctx->mem_idx);

        gen_store_gpr(t0, rt);

        opn = ""lh"";

        break;

    case OPC_LHU:

        tcg_gen_qemu_ld16u(t0, t0, ctx->mem_idx);

        gen_store_gpr(t0, rt);

        opn = ""lhu"";

        break;

    case OPC_LB:

        tcg_gen_qemu_ld8s(t0, t0, ctx->mem_idx);

        gen_store_gpr(t0, rt);

        opn = ""lb"";

        break;

    case OPC_LBU:

        tcg_gen_qemu_ld8u(t0, t0, ctx->mem_idx);

        gen_store_gpr(t0, rt);

        opn = ""lbu"";

        break;

    case OPC_LWL:

        save_cpu_state(ctx, 1);

        t1 = tcg_temp_new();

        gen_load_gpr(t1, rt);

        gen_helper_1e2i(lwl, t1, t1, t0, ctx->mem_idx);

        gen_store_gpr(t1, rt);

        tcg_temp_free(t1);

        opn = ""lwl"";

        break;

    case OPC_LWR:

        save_cpu_state(ctx, 1);

        t1 = tcg_temp_new();

        gen_load_gpr(t1, rt);

        gen_helper_1e2i(lwr, t1, t1, t0, ctx->mem_idx);

        gen_store_gpr(t1, rt);

        tcg_temp_free(t1);

        opn = ""lwr"";

        break;

    case OPC_LL:

        save_cpu_state(ctx, 1);

        op_ld_ll(t0, t0, ctx);

        gen_store_gpr(t0, rt);

        opn = ""ll"";

        break;

    }

    (void)opn; /* avoid a compiler warning */

    MIPS_DEBUG(""%s %s, %d(%s)"", opn, regnames[rt], offset, regnames[base]);

    tcg_temp_free(t0);

}
","static void gen_ld (CPUMIPSState *env, DisasContext *ctx, uint32_t opc, int rt, int base, int16_t offset) {
const char *opn = ""ld""; TCGv t0, t1; if (rt == 0 && env->insn_flags & (INSN_LOONGSON2E | INSN_LOONGSON2F)) {
/* Loongson CPU uses a load to zero register for prefetch. We emulate it as a NOP. On other CPU we must perform the actual memory access. */ MIPS_DEBUG(""NOP""); return; } t0 = tcg_temp_new(); gen_base_offset_addr(ctx, t0, base, offset); switch (opc) {
#if defined(TARGET_MIPS64) case OPC_LWU: tcg_gen_qemu_ld32u(t0, t0, ctx->mem_idx); gen_store_gpr(t0, rt); opn = ""lwu""; break; case OPC_LD: tcg_gen_qemu_ld64(t0, t0, ctx->mem_idx); gen_store_gpr(t0, rt); opn = ""ld""; break; case OPC_LLD: save_cpu_state(ctx, 1); op_ld_lld(t0, t0, ctx); gen_store_gpr(t0, rt); opn = ""lld""; break; case OPC_LDL: save_cpu_state(ctx, 1); t1 = tcg_temp_new(); gen_load_gpr(t1, rt); gen_helper_1e2i(ldl, t1, t1, t0, ctx->mem_idx); gen_store_gpr(t1, rt); tcg_temp_free(t1); opn = ""ldl""; break; case OPC_LDR: save_cpu_state(ctx, 1); t1 = tcg_temp_new(); gen_load_gpr(t1, rt); gen_helper_1e2i(ldr, t1, t1, t0, ctx->mem_idx); gen_store_gpr(t1, rt); tcg_temp_free(t1); opn = ""ldr""; break; case OPC_LDPC: t1 = tcg_const_tl(pc_relative_pc(ctx)); gen_op_addr_add(ctx, t0, t0, t1); tcg_temp_free(t1); tcg_gen_qemu_ld64(t0, t0, ctx->mem_idx); gen_store_gpr(t0, rt); opn = ""ldpc""; break; #endif case OPC_LWPC: t1 = tcg_const_tl(pc_relative_pc(ctx)); gen_op_addr_add(ctx, t0, t0, t1); tcg_temp_free(t1); tcg_gen_qemu_ld32s(t0, t0, ctx->mem_idx); gen_store_gpr(t0, rt); opn = ""lwpc""; break; case OPC_LW: tcg_gen_qemu_ld32s(t0, t0, ctx->mem_idx); gen_store_gpr(t0, rt); opn = ""lw""; break; case OPC_LH: tcg_gen_qemu_ld16s(t0, t0, ctx->mem_idx); gen_store_gpr(t0, rt); opn = ""lh""; break; case OPC_LHU: tcg_gen_qemu_ld16u(t0, t0, ctx->mem_idx); gen_store_gpr(t0, rt); opn = ""lhu""; break; case OPC_LB: tcg_gen_qemu_ld8s(t0, t0, ctx->mem_idx); gen_store_gpr(t0, rt); opn = ""lb""; break; case OPC_LBU: tcg_gen_qemu_ld8u(t0, t0, ctx->mem_idx); gen_store_gpr(t0, rt); opn = ""lbu""; break; case OPC_LWL: save_cpu_state(ctx, 1); t1 = tcg_temp_new(); gen_load_gpr(t1, rt); gen_helper_1e2i(lwl, t1, t1, t0, ctx->mem_idx); gen_store_gpr(t1, rt); tcg_temp_free(t1); opn = ""lwl""; break; case OPC_LWR: save_cpu_state(ctx, 1); t1 = tcg_temp_new(); gen_load_gpr(t1, rt); gen_helper_1e2i(lwr, t1, t1, t0, ctx->mem_idx); gen_store_gpr(t1, rt); tcg_temp_free(t1); opn = ""lwr""; break; case OPC_LL: save_cpu_state(ctx, 1); op_ld_ll(t0, t0, ctx); gen_store_gpr(t0, rt); opn = ""ll""; break; } (void)opn; /* avoid a compiler warning */ MIPS_DEBUG(""%s %s, %d(%s)"", opn, regnames[rt], offset, regnames[base]); tcg_temp_free(t0); } ",qemu,1
"static void tcg_out_st (TCGContext *s, TCGType type, int arg, int arg1,

                        tcg_target_long arg2)

{

    if (type == TCG_TYPE_I32)

        tcg_out_ldst (s, arg, arg1, arg2, STW, STWX);

    else

        tcg_out_ldst (s, arg, arg1, arg2, STD, STDX);

}
","static void tcg_out_st (TCGContext *s, TCGType type, int arg, int arg1, tcg_target_long arg2) {
if (type == TCG_TYPE_I32) tcg_out_ldst (s, arg, arg1, arg2, STW, STWX); else tcg_out_ldst (s, arg, arg1, arg2, STD, STDX); } ",qemu,1
"static inline TCGv gen_ld32(TCGv addr, int index)

{

    TCGv tmp = new_tmp();

    tcg_gen_qemu_ld32u(tmp, addr, index);

    return tmp;

}
","static inline TCGv gen_ld32(TCGv addr, int index) {
TCGv tmp = new_tmp(); tcg_gen_qemu_ld32u(tmp, addr, index); return tmp; } ",qemu,1
"m48t59_t *m48t59_init (qemu_irq IRQ, target_phys_addr_t mem_base,

                       uint32_t io_base, uint16_t size,

                       int type)

{

    DeviceState *dev;

    SysBusDevice *s;

    M48t59SysBusState *d;



    dev = qdev_create(NULL, ""m48t59"");

    qdev_prop_set_uint32(dev, ""type"", type);

    qdev_prop_set_uint32(dev, ""size"", size);

    qdev_prop_set_uint32(dev, ""io_base"", io_base);

    qdev_init(dev);

    s = sysbus_from_qdev(dev);

    sysbus_connect_irq(s, 0, IRQ);

    if (io_base != 0) {

        register_ioport_read(io_base, 0x04, 1, NVRAM_readb, s);

        register_ioport_write(io_base, 0x04, 1, NVRAM_writeb, s);

    }

    if (mem_base != 0) {

        sysbus_mmio_map(s, 0, mem_base);

    }



    d = FROM_SYSBUS(M48t59SysBusState, s);



    return &d->state;

}
","m48t59_t *m48t59_init (qemu_irq IRQ, target_phys_addr_t mem_base, uint32_t io_base, uint16_t size, int type) {
DeviceState *dev; SysBusDevice *s; M48t59SysBusState *d; dev = qdev_create(NULL, ""m48t59""); qdev_prop_set_uint32(dev, ""type"", type); qdev_prop_set_uint32(dev, ""size"", size); qdev_prop_set_uint32(dev, ""io_base"", io_base); qdev_init(dev); s = sysbus_from_qdev(dev); sysbus_connect_irq(s, 0, IRQ); if (io_base != 0) {
register_ioport_read(io_base, 0x04, 1, NVRAM_readb, s); register_ioport_write(io_base, 0x04, 1, NVRAM_writeb, s); } if (mem_base != 0) {
sysbus_mmio_map(s, 0, mem_base); } d = FROM_SYSBUS(M48t59SysBusState, s); return &d->state; } ",qemu,1
"static void rtas_query_cpu_stopped_state(sPAPREnvironment *spapr,

                                         uint32_t token, uint32_t nargs,

                                         target_ulong args,

                                         uint32_t nret, target_ulong rets)

{

    target_ulong id;

    CPUState *cpu;



    if (nargs != 1 || nret != 2) {

        rtas_st(rets, 0, -3);

        return;

    }



    id = rtas_ld(args, 0);

    cpu = qemu_get_cpu(id);

    if (cpu != NULL) {

        if (cpu->halted) {

            rtas_st(rets, 1, 0);

        } else {

            rtas_st(rets, 1, 2);

        }



        rtas_st(rets, 0, 0);

        return;

    }



    /* Didn't find a matching cpu */

    rtas_st(rets, 0, -3);

}
","static void rtas_query_cpu_stopped_state(sPAPREnvironment *spapr, uint32_t token, uint32_t nargs, target_ulong args, uint32_t nret, target_ulong rets) {
target_ulong id; CPUState *cpu; if (nargs != 1 || nret != 2) {
rtas_st(rets, 0, -3); return; } id = rtas_ld(args, 0); cpu = qemu_get_cpu(id); if (cpu != NULL) {
if (cpu->halted) {
rtas_st(rets, 1, 0); } else {
rtas_st(rets, 1, 2); } rtas_st(rets, 0, 0); return; } /* Didn't find a matching cpu */ rtas_st(rets, 0, -3); } ",qemu,0
"static void json_message_process_token(JSONLexer *lexer, QString *token, JSONTokenType type, int x, int y)

{

    JSONMessageParser *parser = container_of(lexer, JSONMessageParser, lexer);

    QDict *dict;



    if (type == JSON_OPERATOR) {

        switch (qstring_get_str(token)[0]) {

        case '{':

            parser->brace_count++;

            break;

        case '}':

            parser->brace_count--;

            break;

        case '[':

            parser->bracket_count++;

            break;

        case ']':

            parser->bracket_count--;

            break;

        default:

            break;

        }

    }



    dict = qdict_new();

    qdict_put(dict, ""type"", qint_from_int(type));

    QINCREF(token);

    qdict_put(dict, ""token"", token);

    qdict_put(dict, ""x"", qint_from_int(x));

    qdict_put(dict, ""y"", qint_from_int(y));



    parser->token_size += token->length;



    qlist_append(parser->tokens, dict);



    if (type == JSON_ERROR) {

        goto out_emit_bad;

    } else if (parser->brace_count < 0 ||

        parser->bracket_count < 0 ||

        (parser->brace_count == 0 &&

         parser->bracket_count == 0)) {

        goto out_emit;

    } else if (parser->token_size > MAX_TOKEN_SIZE ||

               parser->bracket_count > MAX_NESTING ||

               parser->brace_count > MAX_NESTING) {

        /* Security consideration, we limit total memory allocated per object

         * and the maximum recursion depth that a message can force.

         */

        goto out_emit;

    }



    return;



out_emit_bad:

    /* clear out token list and tell the parser to emit and error

     * indication by passing it a NULL list

     */

    QDECREF(parser->tokens);

    parser->tokens = NULL;

out_emit:

    /* send current list of tokens to parser and reset tokenizer */

    parser->brace_count = 0;

    parser->bracket_count = 0;

    parser->emit(parser, parser->tokens);

    if (parser->tokens) {

        QDECREF(parser->tokens);

    }

    parser->tokens = qlist_new();

    parser->token_size = 0;

}
","static void json_message_process_token(JSONLexer *lexer, QString *token, JSONTokenType type, int x, int y) {
JSONMessageParser *parser = container_of(lexer, JSONMessageParser, lexer); QDict *dict; if (type == JSON_OPERATOR) {
switch (qstring_get_str(token)[0]) {
case ' {
': parser->brace_count++; break; case '}': parser->brace_count--; break; case '[': parser->bracket_count++; break; case ']': parser->bracket_count--; break; default: break; } } dict = qdict_new(); qdict_put(dict, ""type"", qint_from_int(type)); QINCREF(token); qdict_put(dict, ""token"", token); qdict_put(dict, ""x"", qint_from_int(x)); qdict_put(dict, ""y"", qint_from_int(y)); parser->token_size += token->length; qlist_append(parser->tokens, dict); if (type == JSON_ERROR) {
goto out_emit_bad; } else if (parser->brace_count < 0 || parser->bracket_count < 0 || (parser->brace_count == 0 && parser->bracket_count == 0)) {
goto out_emit; } else if (parser->token_size > MAX_TOKEN_SIZE || parser->bracket_count > MAX_NESTING || parser->brace_count > MAX_NESTING) {
/* Security consideration, we limit total memory allocated per object * and the maximum recursion depth that a message can force. */ goto out_emit; } return; out_emit_bad: /* clear out token list and tell the parser to emit and error * indication by passing it a NULL list */ QDECREF(parser->tokens); parser->tokens = NULL; out_emit: /* send current list of tokens to parser and reset tokenizer */ parser->brace_count = 0; parser->bracket_count = 0; parser->emit(parser, parser->tokens); if (parser->tokens) {
QDECREF(parser->tokens); } parser->tokens = qlist_new(); parser->token_size = 0; } ",qemu,1
"void gen_intermediate_code(CPUARMState *env, TranslationBlock *tb)

{

    ARMCPU *cpu = arm_env_get_cpu(env);

    CPUState *cs = CPU(cpu);

    DisasContext dc1, *dc = &dc1;

    target_ulong pc_start;

    target_ulong next_page_start;

    int num_insns;

    int max_insns;



    /* generate intermediate code */



    /* The A64 decoder has its own top level loop, because it doesn't need

     * the A32/T32 complexity to do with conditional execution/IT blocks/etc.

     */

    if (ARM_TBFLAG_AARCH64_STATE(tb->flags)) {

        gen_intermediate_code_a64(cpu, tb);

        return;

    }



    pc_start = tb->pc;



    dc->tb = tb;



    dc->is_jmp = DISAS_NEXT;

    dc->pc = pc_start;

    dc->singlestep_enabled = cs->singlestep_enabled;

    dc->condjmp = 0;



    dc->aarch64 = 0;

    /* If we are coming from secure EL0 in a system with a 32-bit EL3, then

     * there is no secure EL1, so we route exceptions to EL3.

     */

    dc->secure_routed_to_el3 = arm_feature(env, ARM_FEATURE_EL3) &&

                               !arm_el_is_aa64(env, 3);

    dc->thumb = ARM_TBFLAG_THUMB(tb->flags);

    dc->bswap_code = ARM_TBFLAG_BSWAP_CODE(tb->flags);

    dc->condexec_mask = (ARM_TBFLAG_CONDEXEC(tb->flags) & 0xf) << 1;

    dc->condexec_cond = ARM_TBFLAG_CONDEXEC(tb->flags) >> 4;

    dc->mmu_idx = ARM_TBFLAG_MMUIDX(tb->flags);

    dc->current_el = arm_mmu_idx_to_el(dc->mmu_idx);

#if !defined(CONFIG_USER_ONLY)

    dc->user = (dc->current_el == 0);

#endif

    dc->ns = ARM_TBFLAG_NS(tb->flags);

    dc->fp_excp_el = ARM_TBFLAG_FPEXC_EL(tb->flags);

    dc->vfp_enabled = ARM_TBFLAG_VFPEN(tb->flags);

    dc->vec_len = ARM_TBFLAG_VECLEN(tb->flags);

    dc->vec_stride = ARM_TBFLAG_VECSTRIDE(tb->flags);

    dc->c15_cpar = ARM_TBFLAG_XSCALE_CPAR(tb->flags);

    dc->cp_regs = cpu->cp_regs;

    dc->features = env->features;



    /* Single step state. The code-generation logic here is:

     *  SS_ACTIVE == 0:

     *   generate code with no special handling for single-stepping (except

     *   that anything that can make us go to SS_ACTIVE == 1 must end the TB;

     *   this happens anyway because those changes are all system register or

     *   PSTATE writes).

     *  SS_ACTIVE == 1, PSTATE.SS == 1: (active-not-pending)

     *   emit code for one insn

     *   emit code to clear PSTATE.SS

     *   emit code to generate software step exception for completed step

     *   end TB (as usual for having generated an exception)

     *  SS_ACTIVE == 1, PSTATE.SS == 0: (active-pending)

     *   emit code to generate a software step exception

     *   end the TB

     */

    dc->ss_active = ARM_TBFLAG_SS_ACTIVE(tb->flags);

    dc->pstate_ss = ARM_TBFLAG_PSTATE_SS(tb->flags);

    dc->is_ldex = false;

    dc->ss_same_el = false; /* Can't be true since EL_d must be AArch64 */



    cpu_F0s = tcg_temp_new_i32();

    cpu_F1s = tcg_temp_new_i32();

    cpu_F0d = tcg_temp_new_i64();

    cpu_F1d = tcg_temp_new_i64();

    cpu_V0 = cpu_F0d;

    cpu_V1 = cpu_F1d;

    /* FIXME: cpu_M0 can probably be the same as cpu_V0.  */

    cpu_M0 = tcg_temp_new_i64();

    next_page_start = (pc_start & TARGET_PAGE_MASK) + TARGET_PAGE_SIZE;

    num_insns = 0;

    max_insns = tb->cflags & CF_COUNT_MASK;

    if (max_insns == 0) {

        max_insns = CF_COUNT_MASK;

    }

    if (max_insns > TCG_MAX_INSNS) {

        max_insns = TCG_MAX_INSNS;

    }



    gen_tb_start(tb);



    tcg_clear_temp_count();



    /* A note on handling of the condexec (IT) bits:

     *

     * We want to avoid the overhead of having to write the updated condexec

     * bits back to the CPUARMState for every instruction in an IT block. So:

     * (1) if the condexec bits are not already zero then we write

     * zero back into the CPUARMState now. This avoids complications trying

     * to do it at the end of the block. (For example if we don't do this

     * it's hard to identify whether we can safely skip writing condexec

     * at the end of the TB, which we definitely want to do for the case

     * where a TB doesn't do anything with the IT state at all.)

     * (2) if we are going to leave the TB then we call gen_set_condexec()

     * which will write the correct value into CPUARMState if zero is wrong.

     * This is done both for leaving the TB at the end, and for leaving

     * it because of an exception we know will happen, which is done in

     * gen_exception_insn(). The latter is necessary because we need to

     * leave the TB with the PC/IT state just prior to execution of the

     * instruction which caused the exception.

     * (3) if we leave the TB unexpectedly (eg a data abort on a load)

     * then the CPUARMState will be wrong and we need to reset it.

     * This is handled in the same way as restoration of the

     * PC in these situations; we save the value of the condexec bits

     * for each PC via tcg_gen_insn_start(), and restore_state_to_opc()

     * then uses this to restore them after an exception.

     *

     * Note that there are no instructions which can read the condexec

     * bits, and none which can write non-static values to them, so

     * we don't need to care about whether CPUARMState is correct in the

     * middle of a TB.

     */



    /* Reset the conditional execution bits immediately. This avoids

       complications trying to do it at the end of the block.  */

    if (dc->condexec_mask || dc->condexec_cond)

      {

        TCGv_i32 tmp = tcg_temp_new_i32();

        tcg_gen_movi_i32(tmp, 0);

        store_cpu_field(tmp, condexec_bits);

      }

    do {

        tcg_gen_insn_start(dc->pc,

                           (dc->condexec_cond << 4) | (dc->condexec_mask >> 1));

        num_insns++;



#ifdef CONFIG_USER_ONLY

        /* Intercept jump to the magic kernel page.  */

        if (dc->pc >= 0xffff0000) {

            /* We always get here via a jump, so know we are not in a

               conditional execution block.  */

            gen_exception_internal(EXCP_KERNEL_TRAP);

            dc->is_jmp = DISAS_UPDATE;

            break;

        }

#else

        if (dc->pc >= 0xfffffff0 && arm_dc_feature(dc, ARM_FEATURE_M)) {

            /* We always get here via a jump, so know we are not in a

               conditional execution block.  */

            gen_exception_internal(EXCP_EXCEPTION_EXIT);

            dc->is_jmp = DISAS_UPDATE;

            break;

        }

#endif



        if (unlikely(!QTAILQ_EMPTY(&cs->breakpoints))) {

            CPUBreakpoint *bp;

            QTAILQ_FOREACH(bp, &cs->breakpoints, entry) {

                if (bp->pc == dc->pc) {

                    gen_exception_internal_insn(dc, 0, EXCP_DEBUG);

                    /* Advance PC so that clearing the breakpoint will

                       invalidate this TB.  */

                    dc->pc += 2;

                    goto done_generating;

                }

            }

        }



        if (num_insns == max_insns && (tb->cflags & CF_LAST_IO)) {

            gen_io_start();

        }



        if (dc->ss_active && !dc->pstate_ss) {

            /* Singlestep state is Active-pending.

             * If we're in this state at the start of a TB then either

             *  a) we just took an exception to an EL which is being debugged

             *     and this is the first insn in the exception handler

             *  b) debug exceptions were masked and we just unmasked them

             *     without changing EL (eg by clearing PSTATE.D)

             * In either case we're going to take a swstep exception in the

             * ""did not step an insn"" case, and so the syndrome ISV and EX

             * bits should be zero.

             */

            assert(num_insns == 1);

            gen_exception(EXCP_UDEF, syn_swstep(dc->ss_same_el, 0, 0),

                          default_exception_el(dc));

            goto done_generating;

        }



        if (dc->thumb) {

            disas_thumb_insn(env, dc);

            if (dc->condexec_mask) {

                dc->condexec_cond = (dc->condexec_cond & 0xe)

                                   | ((dc->condexec_mask >> 4) & 1);

                dc->condexec_mask = (dc->condexec_mask << 1) & 0x1f;

                if (dc->condexec_mask == 0) {

                    dc->condexec_cond = 0;

                }

            }

        } else {

            unsigned int insn = arm_ldl_code(env, dc->pc, dc->bswap_code);

            dc->pc += 4;

            disas_arm_insn(dc, insn);

        }



        if (dc->condjmp && !dc->is_jmp) {

            gen_set_label(dc->condlabel);

            dc->condjmp = 0;

        }



        if (tcg_check_temp_count()) {

            fprintf(stderr, ""TCG temporary leak before ""TARGET_FMT_lx""\n"",

                    dc->pc);

        }



        /* Translation stops when a conditional branch is encountered.

         * Otherwise the subsequent code could get translated several times.

         * Also stop translation when a page boundary is reached.  This

         * ensures prefetch aborts occur at the right place.  */

    } while (!dc->is_jmp && !tcg_op_buf_full() &&

             !cs->singlestep_enabled &&

             !singlestep &&

             !dc->ss_active &&

             dc->pc < next_page_start &&

             num_insns < max_insns);



    if (tb->cflags & CF_LAST_IO) {

        if (dc->condjmp) {

            /* FIXME:  This can theoretically happen with self-modifying

               code.  */

            cpu_abort(cs, ""IO on conditional branch instruction"");

        }

        gen_io_end();

    }



    /* At this stage dc->condjmp will only be set when the skipped

       instruction was a conditional branch or trap, and the PC has

       already been written.  */

    if (unlikely(cs->singlestep_enabled || dc->ss_active)) {

        /* Make sure the pc is updated, and raise a debug exception.  */

        if (dc->condjmp) {

            gen_set_condexec(dc);

            if (dc->is_jmp == DISAS_SWI) {

                gen_ss_advance(dc);

                gen_exception(EXCP_SWI, syn_aa32_svc(dc->svc_imm, dc->thumb),

                              default_exception_el(dc));

            } else if (dc->is_jmp == DISAS_HVC) {

                gen_ss_advance(dc);

                gen_exception(EXCP_HVC, syn_aa32_hvc(dc->svc_imm), 2);

            } else if (dc->is_jmp == DISAS_SMC) {

                gen_ss_advance(dc);

                gen_exception(EXCP_SMC, syn_aa32_smc(), 3);

            } else if (dc->ss_active) {

                gen_step_complete_exception(dc);

            } else {

                gen_exception_internal(EXCP_DEBUG);

            }

            gen_set_label(dc->condlabel);

        }

        if (dc->condjmp || !dc->is_jmp) {

            gen_set_pc_im(dc, dc->pc);

            dc->condjmp = 0;

        }

        gen_set_condexec(dc);

        if (dc->is_jmp == DISAS_SWI && !dc->condjmp) {

            gen_ss_advance(dc);

            gen_exception(EXCP_SWI, syn_aa32_svc(dc->svc_imm, dc->thumb),

                          default_exception_el(dc));

        } else if (dc->is_jmp == DISAS_HVC && !dc->condjmp) {

            gen_ss_advance(dc);

            gen_exception(EXCP_HVC, syn_aa32_hvc(dc->svc_imm), 2);

        } else if (dc->is_jmp == DISAS_SMC && !dc->condjmp) {

            gen_ss_advance(dc);

            gen_exception(EXCP_SMC, syn_aa32_smc(), 3);

        } else if (dc->ss_active) {

            gen_step_complete_exception(dc);

        } else {

            /* FIXME: Single stepping a WFI insn will not halt

               the CPU.  */

            gen_exception_internal(EXCP_DEBUG);

        }

    } else {

        /* While branches must always occur at the end of an IT block,

           there are a few other things that can cause us to terminate

           the TB in the middle of an IT block:

            - Exception generating instructions (bkpt, swi, undefined).

            - Page boundaries.

            - Hardware watchpoints.

           Hardware breakpoints have already been handled and skip this code.

         */

        gen_set_condexec(dc);

        switch(dc->is_jmp) {

        case DISAS_NEXT:

            gen_goto_tb(dc, 1, dc->pc);

            break;

        default:

        case DISAS_JUMP:

        case DISAS_UPDATE:

            /* indicate that the hash table must be used to find the next TB */

            tcg_gen_exit_tb(0);

            break;

        case DISAS_TB_JUMP:

            /* nothing more to generate */

            break;

        case DISAS_WFI:

            gen_helper_wfi(cpu_env);

            /* The helper doesn't necessarily throw an exception, but we

             * must go back to the main loop to check for interrupts anyway.

             */

            tcg_gen_exit_tb(0);

            break;

        case DISAS_WFE:

            gen_helper_wfe(cpu_env);

            break;

        case DISAS_YIELD:

            gen_helper_yield(cpu_env);

            break;

        case DISAS_SWI:

            gen_exception(EXCP_SWI, syn_aa32_svc(dc->svc_imm, dc->thumb),

                          default_exception_el(dc));

            break;

        case DISAS_HVC:

            gen_exception(EXCP_HVC, syn_aa32_hvc(dc->svc_imm), 2);

            break;

        case DISAS_SMC:

            gen_exception(EXCP_SMC, syn_aa32_smc(), 3);

            break;

        }

        if (dc->condjmp) {

            gen_set_label(dc->condlabel);

            gen_set_condexec(dc);

            gen_goto_tb(dc, 1, dc->pc);

            dc->condjmp = 0;

        }

    }



done_generating:

    gen_tb_end(tb, num_insns);



#ifdef DEBUG_DISAS

    if (qemu_loglevel_mask(CPU_LOG_TB_IN_ASM)) {

        qemu_log(""----------------\n"");

        qemu_log(""IN: %s\n"", lookup_symbol(pc_start));

        log_target_disas(cs, pc_start, dc->pc - pc_start,

                         dc->thumb | (dc->bswap_code << 1));

        qemu_log(""\n"");

    }

#endif

    tb->size = dc->pc - pc_start;

    tb->icount = num_insns;

}
","void gen_intermediate_code(CPUARMState *env, TranslationBlock *tb) {
ARMCPU *cpu = arm_env_get_cpu(env); CPUState *cs = CPU(cpu); DisasContext dc1, *dc = &dc1; target_ulong pc_start; target_ulong next_page_start; int num_insns; int max_insns; /* generate intermediate code */ /* The A64 decoder has its own top level loop, because it doesn't need * the A32/T32 complexity to do with conditional execution/IT blocks/etc. */ if (ARM_TBFLAG_AARCH64_STATE(tb->flags)) {
gen_intermediate_code_a64(cpu, tb); return; } pc_start = tb->pc; dc->tb = tb; dc->is_jmp = DISAS_NEXT; dc->pc = pc_start; dc->singlestep_enabled = cs->singlestep_enabled; dc->condjmp = 0; dc->aarch64 = 0; /* If we are coming from secure EL0 in a system with a 32-bit EL3, then * there is no secure EL1, so we route exceptions to EL3. */ dc->secure_routed_to_el3 = arm_feature(env, ARM_FEATURE_EL3) && !arm_el_is_aa64(env, 3); dc->thumb = ARM_TBFLAG_THUMB(tb->flags); dc->bswap_code = ARM_TBFLAG_BSWAP_CODE(tb->flags); dc->condexec_mask = (ARM_TBFLAG_CONDEXEC(tb->flags) & 0xf) << 1; dc->condexec_cond = ARM_TBFLAG_CONDEXEC(tb->flags) >> 4; dc->mmu_idx = ARM_TBFLAG_MMUIDX(tb->flags); dc->current_el = arm_mmu_idx_to_el(dc->mmu_idx); #if !defined(CONFIG_USER_ONLY) dc->user = (dc->current_el == 0); #endif dc->ns = ARM_TBFLAG_NS(tb->flags); dc->fp_excp_el = ARM_TBFLAG_FPEXC_EL(tb->flags); dc->vfp_enabled = ARM_TBFLAG_VFPEN(tb->flags); dc->vec_len = ARM_TBFLAG_VECLEN(tb->flags); dc->vec_stride = ARM_TBFLAG_VECSTRIDE(tb->flags); dc->c15_cpar = ARM_TBFLAG_XSCALE_CPAR(tb->flags); dc->cp_regs = cpu->cp_regs; dc->features = env->features; /* Single step state. The code-generation logic here is: * SS_ACTIVE == 0: * generate code with no special handling for single-stepping (except * that anything that can make us go to SS_ACTIVE == 1 must end the TB; * this happens anyway because those changes are all system register or * PSTATE writes). * SS_ACTIVE == 1, PSTATE.SS == 1: (active-not-pending) * emit code for one insn * emit code to clear PSTATE.SS * emit code to generate software step exception for completed step * end TB (as usual for having generated an exception) * SS_ACTIVE == 1, PSTATE.SS == 0: (active-pending) * emit code to generate a software step exception * end the TB */ dc->ss_active = ARM_TBFLAG_SS_ACTIVE(tb->flags); dc->pstate_ss = ARM_TBFLAG_PSTATE_SS(tb->flags); dc->is_ldex = false; dc->ss_same_el = false; /* Can't be true since EL_d must be AArch64 */ cpu_F0s = tcg_temp_new_i32(); cpu_F1s = tcg_temp_new_i32(); cpu_F0d = tcg_temp_new_i64(); cpu_F1d = tcg_temp_new_i64(); cpu_V0 = cpu_F0d; cpu_V1 = cpu_F1d; /* FIXME: cpu_M0 can probably be the same as cpu_V0. */ cpu_M0 = tcg_temp_new_i64(); next_page_start = (pc_start & TARGET_PAGE_MASK) + TARGET_PAGE_SIZE; num_insns = 0; max_insns = tb->cflags & CF_COUNT_MASK; if (max_insns == 0) {
max_insns = CF_COUNT_MASK; } if (max_insns > TCG_MAX_INSNS) {
max_insns = TCG_MAX_INSNS; } gen_tb_start(tb); tcg_clear_temp_count(); /* A note on handling of the condexec (IT) bits: * * We want to avoid the overhead of having to write the updated condexec * bits back to the CPUARMState for every instruction in an IT block. So: * (1) if the condexec bits are not already zero then we write * zero back into the CPUARMState now. This avoids complications trying * to do it at the end of the block. (For example if we don't do this * it's hard to identify whether we can safely skip writing condexec * at the end of the TB, which we definitely want to do for the case * where a TB doesn't do anything with the IT state at all.) * (2) if we are going to leave the TB then we call gen_set_condexec() * which will write the correct value into CPUARMState if zero is wrong. * This is done both for leaving the TB at the end, and for leaving * it because of an exception we know will happen, which is done in * gen_exception_insn(). The latter is necessary because we need to * leave the TB with the PC/IT state just prior to execution of the * instruction which caused the exception. * (3) if we leave the TB unexpectedly (eg a data abort on a load) * then the CPUARMState will be wrong and we need to reset it. * This is handled in the same way as restoration of the * PC in these situations; we save the value of the condexec bits * for each PC via tcg_gen_insn_start(), and restore_state_to_opc() * then uses this to restore them after an exception. * * Note that there are no instructions which can read the condexec * bits, and none which can write non-static values to them, so * we don't need to care about whether CPUARMState is correct in the * middle of a TB. */ /* Reset the conditional execution bits immediately. This avoids complications trying to do it at the end of the block. */ if (dc->condexec_mask || dc->condexec_cond) {
TCGv_i32 tmp = tcg_temp_new_i32(); tcg_gen_movi_i32(tmp, 0); store_cpu_field(tmp, condexec_bits); } do {
tcg_gen_insn_start(dc->pc, (dc->condexec_cond << 4) | (dc->condexec_mask >> 1)); num_insns++; #ifdef CONFIG_USER_ONLY /* Intercept jump to the magic kernel page. */ if (dc->pc >= 0xffff0000) {
/* We always get here via a jump, so know we are not in a conditional execution block. */ gen_exception_internal(EXCP_KERNEL_TRAP); dc->is_jmp = DISAS_UPDATE; break; } #else if (dc->pc >= 0xfffffff0 && arm_dc_feature(dc, ARM_FEATURE_M)) {
/* We always get here via a jump, so know we are not in a conditional execution block. */ gen_exception_internal(EXCP_EXCEPTION_EXIT); dc->is_jmp = DISAS_UPDATE; break; } #endif if (unlikely(!QTAILQ_EMPTY(&cs->breakpoints))) {
CPUBreakpoint *bp; QTAILQ_FOREACH(bp, &cs->breakpoints, entry) {
if (bp->pc == dc->pc) {
gen_exception_internal_insn(dc, 0, EXCP_DEBUG); /* Advance PC so that clearing the breakpoint will invalidate this TB. */ dc->pc += 2; goto done_generating; } } } if (num_insns == max_insns && (tb->cflags & CF_LAST_IO)) {
gen_io_start(); } if (dc->ss_active && !dc->pstate_ss) {
/* Singlestep state is Active-pending. * If we're in this state at the start of a TB then either * a) we just took an exception to an EL which is being debugged * and this is the first insn in the exception handler * b) debug exceptions were masked and we just unmasked them * without changing EL (eg by clearing PSTATE.D) * In either case we're going to take a swstep exception in the * ""did not step an insn"" case, and so the syndrome ISV and EX * bits should be zero. */ assert(num_insns == 1); gen_exception(EXCP_UDEF, syn_swstep(dc->ss_same_el, 0, 0), default_exception_el(dc)); goto done_generating; } if (dc->thumb) {
disas_thumb_insn(env, dc); if (dc->condexec_mask) {
dc->condexec_cond = (dc->condexec_cond & 0xe) | ((dc->condexec_mask >> 4) & 1); dc->condexec_mask = (dc->condexec_mask << 1) & 0x1f; if (dc->condexec_mask == 0) {
dc->condexec_cond = 0; } } } else {
unsigned int insn = arm_ldl_code(env, dc->pc, dc->bswap_code); dc->pc += 4; disas_arm_insn(dc, insn); } if (dc->condjmp && !dc->is_jmp) {
gen_set_label(dc->condlabel); dc->condjmp = 0; } if (tcg_check_temp_count()) {
fprintf(stderr, ""TCG temporary leak before ""TARGET_FMT_lx""\n"", dc->pc); } /* Translation stops when a conditional branch is encountered. * Otherwise the subsequent code could get translated several times. * Also stop translation when a page boundary is reached. This * ensures prefetch aborts occur at the right place. */ } while (!dc->is_jmp && !tcg_op_buf_full() && !cs->singlestep_enabled && !singlestep && !dc->ss_active && dc->pc < next_page_start && num_insns < max_insns); if (tb->cflags & CF_LAST_IO) {
if (dc->condjmp) {
/* FIXME: This can theoretically happen with self-modifying code. */ cpu_abort(cs, ""IO on conditional branch instruction""); } gen_io_end(); } /* At this stage dc->condjmp will only be set when the skipped instruction was a conditional branch or trap, and the PC has already been written. */ if (unlikely(cs->singlestep_enabled || dc->ss_active)) {
/* Make sure the pc is updated, and raise a debug exception. */ if (dc->condjmp) {
gen_set_condexec(dc); if (dc->is_jmp == DISAS_SWI) {
gen_ss_advance(dc); gen_exception(EXCP_SWI, syn_aa32_svc(dc->svc_imm, dc->thumb), default_exception_el(dc)); } else if (dc->is_jmp == DISAS_HVC) {
gen_ss_advance(dc); gen_exception(EXCP_HVC, syn_aa32_hvc(dc->svc_imm), 2); } else if (dc->is_jmp == DISAS_SMC) {
gen_ss_advance(dc); gen_exception(EXCP_SMC, syn_aa32_smc(), 3); } else if (dc->ss_active) {
gen_step_complete_exception(dc); } else {
gen_exception_internal(EXCP_DEBUG); } gen_set_label(dc->condlabel); } if (dc->condjmp || !dc->is_jmp) {
gen_set_pc_im(dc, dc->pc); dc->condjmp = 0; } gen_set_condexec(dc); if (dc->is_jmp == DISAS_SWI && !dc->condjmp) {
gen_ss_advance(dc); gen_exception(EXCP_SWI, syn_aa32_svc(dc->svc_imm, dc->thumb), default_exception_el(dc)); } else if (dc->is_jmp == DISAS_HVC && !dc->condjmp) {
gen_ss_advance(dc); gen_exception(EXCP_HVC, syn_aa32_hvc(dc->svc_imm), 2); } else if (dc->is_jmp == DISAS_SMC && !dc->condjmp) {
gen_ss_advance(dc); gen_exception(EXCP_SMC, syn_aa32_smc(), 3); } else if (dc->ss_active) {
gen_step_complete_exception(dc); } else {
/* FIXME: Single stepping a WFI insn will not halt the CPU. */ gen_exception_internal(EXCP_DEBUG); } } else {
/* While branches must always occur at the end of an IT block, there are a few other things that can cause us to terminate the TB in the middle of an IT block: - Exception generating instructions (bkpt, swi, undefined). - Page boundaries. - Hardware watchpoints. Hardware breakpoints have already been handled and skip this code. */ gen_set_condexec(dc); switch(dc->is_jmp) {
case DISAS_NEXT: gen_goto_tb(dc, 1, dc->pc); break; default: case DISAS_JUMP: case DISAS_UPDATE: /* indicate that the hash table must be used to find the next TB */ tcg_gen_exit_tb(0); break; case DISAS_TB_JUMP: /* nothing more to generate */ break; case DISAS_WFI: gen_helper_wfi(cpu_env); /* The helper doesn't necessarily throw an exception, but we * must go back to the main loop to check for interrupts anyway. */ tcg_gen_exit_tb(0); break; case DISAS_WFE: gen_helper_wfe(cpu_env); break; case DISAS_YIELD: gen_helper_yield(cpu_env); break; case DISAS_SWI: gen_exception(EXCP_SWI, syn_aa32_svc(dc->svc_imm, dc->thumb), default_exception_el(dc)); break; case DISAS_HVC: gen_exception(EXCP_HVC, syn_aa32_hvc(dc->svc_imm), 2); break; case DISAS_SMC: gen_exception(EXCP_SMC, syn_aa32_smc(), 3); break; } if (dc->condjmp) {
gen_set_label(dc->condlabel); gen_set_condexec(dc); gen_goto_tb(dc, 1, dc->pc); dc->condjmp = 0; } } done_generating: gen_tb_end(tb, num_insns); #ifdef DEBUG_DISAS if (qemu_loglevel_mask(CPU_LOG_TB_IN_ASM)) {
qemu_log(""----------------\n""); qemu_log(""IN: %s\n"", lookup_symbol(pc_start)); log_target_disas(cs, pc_start, dc->pc - pc_start, dc->thumb | (dc->bswap_code << 1)); qemu_log(""\n""); } #endif tb->size = dc->pc - pc_start; tb->icount = num_insns; } ",qemu,0
"static int scsi_disk_initfn(SCSIDevice *dev)
{
    SCSIDiskState *s = DO_UPCAST(SCSIDiskState, qdev, dev);
    int is_cd;
    DriveInfo *dinfo;
    if (!s->qdev.conf.bs) {
        error_report(""scsi-disk: drive property not set"");
    s->bs = s->qdev.conf.bs;
    is_cd = bdrv_get_type_hint(s->bs) == BDRV_TYPE_CDROM;
    if (bdrv_get_on_error(s->bs, 1) != BLOCK_ERR_REPORT) {
        error_report(""Device doesn't support drive option rerror"");
    if (!s->serial) {
        /* try to fall back to value set with legacy -drive serial=... */
        dinfo = drive_get_by_blockdev(s->bs);
        s->serial = qemu_strdup(*dinfo->serial ? dinfo->serial : ""0"");
    if (!s->version) {
        s->version = qemu_strdup(QEMU_VERSION);
    if (bdrv_is_sg(s->bs)) {
        error_report(""scsi-disk: unwanted /dev/sg*"");
    if (is_cd) {
        s->qdev.blocksize = 2048;
    } else {
        s->qdev.blocksize = s->qdev.conf.logical_block_size;
    s->cluster_size = s->qdev.blocksize / 512;
    s->qdev.type = TYPE_DISK;
    qemu_add_vm_change_state_handler(scsi_dma_restart_cb, s);
    bdrv_set_removable(s->bs, is_cd);
    return 0;","static int scsi_disk_initfn(SCSIDevice *dev) {
SCSIDiskState *s = DO_UPCAST(SCSIDiskState, qdev, dev); int is_cd; DriveInfo *dinfo; if (!s->qdev.conf.bs) {
error_report(""scsi-disk: drive property not set""); s->bs = s->qdev.conf.bs; is_cd = bdrv_get_type_hint(s->bs) == BDRV_TYPE_CDROM; if (bdrv_get_on_error(s->bs, 1) != BLOCK_ERR_REPORT) {
error_report(""Device doesn't support drive option rerror""); if (!s->serial) {
/* try to fall back to value set with legacy -drive serial=... */ dinfo = drive_get_by_blockdev(s->bs); s->serial = qemu_strdup(*dinfo->serial ? dinfo->serial : ""0""); if (!s->version) {
s->version = qemu_strdup(QEMU_VERSION); if (bdrv_is_sg(s->bs)) {
error_report(""scsi-disk: unwanted /dev/sg*""); if (is_cd) {
s->qdev.blocksize = 2048; } else {
s->qdev.blocksize = s->qdev.conf.logical_block_size; s->cluster_size = s->qdev.blocksize / 512; s->qdev.type = TYPE_DISK; qemu_add_vm_change_state_handler(scsi_dma_restart_cb, s); bdrv_set_removable(s->bs, is_cd); return 0;",qemu,1
"PPC_OP(setcrfbit)

{

    T1 = (T1 & PARAM(1)) | (T0 << PARAM(2)); 

    RETURN();

}
","PPC_OP(setcrfbit) {
T1 = (T1 & PARAM(1)) | (T0 << PARAM(2)); RETURN(); } ",qemu,1
"void virtio_queue_set_notification(VirtQueue *vq, int enable)

{

    vq->notification = enable;

    if (vq->vdev->guest_features & (1 << VIRTIO_RING_F_EVENT_IDX)) {

        vring_avail_event(vq, vring_avail_idx(vq));

    } else if (enable) {

        vring_used_flags_unset_bit(vq, VRING_USED_F_NO_NOTIFY);

    } else {

        vring_used_flags_set_bit(vq, VRING_USED_F_NO_NOTIFY);





","void virtio_queue_set_notification(VirtQueue *vq, int enable) {
vq->notification = enable; if (vq->vdev->guest_features & (1 << VIRTIO_RING_F_EVENT_IDX)) {
vring_avail_event(vq, vring_avail_idx(vq)); } else if (enable) {
vring_used_flags_unset_bit(vq, VRING_USED_F_NO_NOTIFY); } else {
vring_used_flags_set_bit(vq, VRING_USED_F_NO_NOTIFY); ",qemu,1
"static int qemu_rdma_registration_stop(QEMUFile *f, void *opaque,

                                       uint64_t flags)

{

    Error *local_err = NULL, **errp = &local_err;

    QEMUFileRDMA *rfile = opaque;

    RDMAContext *rdma = rfile->rdma;

    RDMAControlHeader head = { .len = 0, .repeat = 1 };

    int ret = 0;



    CHECK_ERROR_STATE();



    qemu_fflush(f);

    ret = qemu_rdma_drain_cq(f, rdma);



    if (ret < 0) {

        goto err;

    }



    if (flags == RAM_CONTROL_SETUP) {

        RDMAControlHeader resp = {.type = RDMA_CONTROL_RAM_BLOCKS_RESULT };

        RDMALocalBlocks *local = &rdma->local_ram_blocks;

        int reg_result_idx, i, j, nb_remote_blocks;



        head.type = RDMA_CONTROL_RAM_BLOCKS_REQUEST;

        DPRINTF(""Sending registration setup for ram blocks...\n"");



        /*

         * Make sure that we parallelize the pinning on both sides.

         * For very large guests, doing this serially takes a really

         * long time, so we have to 'interleave' the pinning locally

         * with the control messages by performing the pinning on this

         * side before we receive the control response from the other

         * side that the pinning has completed.

         */

        ret = qemu_rdma_exchange_send(rdma, &head, NULL, &resp,

                    &reg_result_idx, rdma->pin_all ?

                    qemu_rdma_reg_whole_ram_blocks : NULL);

        if (ret < 0) {

            ERROR(errp, ""receiving remote info!"");

            return ret;

        }



        nb_remote_blocks = resp.len / sizeof(RDMARemoteBlock);



        /*

         * The protocol uses two different sets of rkeys (mutually exclusive):

         * 1. One key to represent the virtual address of the entire ram block.

         *    (dynamic chunk registration disabled - pin everything with one rkey.)

         * 2. One to represent individual chunks within a ram block.

         *    (dynamic chunk registration enabled - pin individual chunks.)

         *

         * Once the capability is successfully negotiated, the destination transmits

         * the keys to use (or sends them later) including the virtual addresses

         * and then propagates the remote ram block descriptions to his local copy.

         */



        if (local->nb_blocks != nb_remote_blocks) {

            ERROR(errp, ""ram blocks mismatch #1! ""

                        ""Your QEMU command line parameters are probably ""

                        ""not identical on both the source and destination."");

            return -EINVAL;

        }



        qemu_rdma_move_header(rdma, reg_result_idx, &resp);

        memcpy(rdma->block,

            rdma->wr_data[reg_result_idx].control_curr, resp.len);

        for (i = 0; i < nb_remote_blocks; i++) {

            network_to_remote_block(&rdma->block[i]);



            /* search local ram blocks */

            for (j = 0; j < local->nb_blocks; j++) {

                if (rdma->block[i].offset != local->block[j].offset) {

                    continue;

                }



                if (rdma->block[i].length != local->block[j].length) {

                    ERROR(errp, ""ram blocks mismatch #2! ""

                        ""Your QEMU command line parameters are probably ""

                        ""not identical on both the source and destination."");

                    return -EINVAL;

                }

                local->block[j].remote_host_addr =

                        rdma->block[i].remote_host_addr;

                local->block[j].remote_rkey = rdma->block[i].remote_rkey;

                break;

            }



            if (j >= local->nb_blocks) {

                ERROR(errp, ""ram blocks mismatch #3! ""

                        ""Your QEMU command line parameters are probably ""

                        ""not identical on both the source and destination."");

                return -EINVAL;

            }

        }

    }



    DDDPRINTF(""Sending registration finish %"" PRIu64 ""...\n"", flags);



    head.type = RDMA_CONTROL_REGISTER_FINISHED;

    ret = qemu_rdma_exchange_send(rdma, &head, NULL, NULL, NULL, NULL);



    if (ret < 0) {

        goto err;

    }



    return 0;

err:

    rdma->error_state = ret;

    return ret;

}
","static int qemu_rdma_registration_stop(QEMUFile *f, void *opaque, uint64_t flags) {
Error *local_err = NULL, **errp = &local_err; QEMUFileRDMA *rfile = opaque; RDMAContext *rdma = rfile->rdma; RDMAControlHeader head = {
.len = 0, .repeat = 1 }; int ret = 0; CHECK_ERROR_STATE(); qemu_fflush(f); ret = qemu_rdma_drain_cq(f, rdma); if (ret < 0) {
goto err; } if (flags == RAM_CONTROL_SETUP) {
RDMAControlHeader resp = {
.type = RDMA_CONTROL_RAM_BLOCKS_RESULT }; RDMALocalBlocks *local = &rdma->local_ram_blocks; int reg_result_idx, i, j, nb_remote_blocks; head.type = RDMA_CONTROL_RAM_BLOCKS_REQUEST; DPRINTF(""Sending registration setup for ram blocks...\n""); /* * Make sure that we parallelize the pinning on both sides. * For very large guests, doing this serially takes a really * long time, so we have to 'interleave' the pinning locally * with the control messages by performing the pinning on this * side before we receive the control response from the other * side that the pinning has completed. */ ret = qemu_rdma_exchange_send(rdma, &head, NULL, &resp, &reg_result_idx, rdma->pin_all ? qemu_rdma_reg_whole_ram_blocks : NULL); if (ret < 0) {
ERROR(errp, ""receiving remote info!""); return ret; } nb_remote_blocks = resp.len / sizeof(RDMARemoteBlock); /* * The protocol uses two different sets of rkeys (mutually exclusive): * 1. One key to represent the virtual address of the entire ram block. * (dynamic chunk registration disabled - pin everything with one rkey.) * 2. One to represent individual chunks within a ram block. * (dynamic chunk registration enabled - pin individual chunks.) * * Once the capability is successfully negotiated, the destination transmits * the keys to use (or sends them later) including the virtual addresses * and then propagates the remote ram block descriptions to his local copy. */ if (local->nb_blocks != nb_remote_blocks) {
ERROR(errp, ""ram blocks mismatch #1! "" ""Your QEMU command line parameters are probably "" ""not identical on both the source and destination.""); return -EINVAL; } qemu_rdma_move_header(rdma, reg_result_idx, &resp); memcpy(rdma->block, rdma->wr_data[reg_result_idx].control_curr, resp.len); for (i = 0; i < nb_remote_blocks; i++) {
network_to_remote_block(&rdma->block[i]); /* search local ram blocks */ for (j = 0; j < local->nb_blocks; j++) {
if (rdma->block[i].offset != local->block[j].offset) {
continue; } if (rdma->block[i].length != local->block[j].length) {
ERROR(errp, ""ram blocks mismatch #2! "" ""Your QEMU command line parameters are probably "" ""not identical on both the source and destination.""); return -EINVAL; } local->block[j].remote_host_addr = rdma->block[i].remote_host_addr; local->block[j].remote_rkey = rdma->block[i].remote_rkey; break; } if (j >= local->nb_blocks) {
ERROR(errp, ""ram blocks mismatch #3! "" ""Your QEMU command line parameters are probably "" ""not identical on both the source and destination.""); return -EINVAL; } } } DDDPRINTF(""Sending registration finish %"" PRIu64 ""...\n"", flags); head.type = RDMA_CONTROL_REGISTER_FINISHED; ret = qemu_rdma_exchange_send(rdma, &head, NULL, NULL, NULL, NULL); if (ret < 0) {
goto err; } return 0; err: rdma->error_state = ret; return ret; } ",qemu,1
"static void vhost_log_put(struct vhost_dev *dev, bool sync)

{

    struct vhost_log *log = dev->log;



    if (!log) {

        return;

    }

    dev->log = NULL;

    dev->log_size = 0;



    --log->refcnt;

    if (log->refcnt == 0) {

        /* Sync only the range covered by the old log */

        if (dev->log_size && sync) {

            vhost_log_sync_range(dev, 0, dev->log_size * VHOST_LOG_CHUNK - 1);

        }



        if (vhost_log == log) {

            g_free(log->log);

            vhost_log = NULL;

        } else if (vhost_log_shm == log) {

            qemu_memfd_free(log->log, log->size * sizeof(*(log->log)),

                            log->fd);

            vhost_log_shm = NULL;

        }



        g_free(log);

    }

}
","static void vhost_log_put(struct vhost_dev *dev, bool sync) {
struct vhost_log *log = dev->log; if (!log) {
return; } dev->log = NULL; dev->log_size = 0; --log->refcnt; if (log->refcnt == 0) {
/* Sync only the range covered by the old log */ if (dev->log_size && sync) {
vhost_log_sync_range(dev, 0, dev->log_size * VHOST_LOG_CHUNK - 1); } if (vhost_log == log) {
g_free(log->log); vhost_log = NULL; } else if (vhost_log_shm == log) {
qemu_memfd_free(log->log, log->size * sizeof(*(log->log)), log->fd); vhost_log_shm = NULL; } g_free(log); } } ",qemu,1
"uint8_t qpci_io_readb(QPCIDevice *dev, void *data)

{

    uintptr_t addr = (uintptr_t)data;



    if (addr < QPCI_PIO_LIMIT) {

        return dev->bus->pio_readb(dev->bus, addr);

    } else {

        uint8_t val;

        dev->bus->memread(dev->bus, addr, &val, sizeof(val));

        return val;

    }

}
","uint8_t qpci_io_readb(QPCIDevice *dev, void *data) {
uintptr_t addr = (uintptr_t)data; if (addr < QPCI_PIO_LIMIT) {
return dev->bus->pio_readb(dev->bus, addr); } else {
uint8_t val; dev->bus->memread(dev->bus, addr, &val, sizeof(val)); return val; } } ",qemu,1
"iscsi_aio_readv(BlockDriverState *bs, int64_t sector_num,

                QEMUIOVector *qiov, int nb_sectors,

                BlockDriverCompletionFunc *cb,

                void *opaque)

{

    IscsiLun *iscsilun = bs->opaque;

    struct iscsi_context *iscsi = iscsilun->iscsi;

    IscsiAIOCB *acb;

    size_t qemu_read_size;

    int i;

    uint64_t lba;

    uint32_t num_sectors;



    qemu_read_size = BDRV_SECTOR_SIZE * (size_t)nb_sectors;



    acb = qemu_aio_get(&iscsi_aiocb_info, bs, cb, opaque);

    trace_iscsi_aio_readv(iscsi, sector_num, nb_sectors, opaque, acb);



    acb->iscsilun = iscsilun;

    acb->qiov     = qiov;



    acb->canceled    = 0;

    acb->bh          = NULL;

    acb->status      = -EINPROGRESS;

    acb->read_size   = qemu_read_size;

    acb->buf         = NULL;



    /* If LUN blocksize is bigger than BDRV_BLOCK_SIZE a read from QEMU

     * may be misaligned to the LUN, so we may need to read some extra

     * data.

     */

    acb->read_offset = 0;

    if (iscsilun->block_size > BDRV_SECTOR_SIZE) {

        uint64_t bdrv_offset = BDRV_SECTOR_SIZE * sector_num;



        acb->read_offset  = bdrv_offset % iscsilun->block_size;

    }



    num_sectors  = (qemu_read_size + iscsilun->block_size

                    + acb->read_offset - 1)

                    / iscsilun->block_size;



    acb->task = malloc(sizeof(struct scsi_task));

    if (acb->task == NULL) {

        error_report(""iSCSI: Failed to allocate task for scsi READ16 ""

                     ""command. %s"", iscsi_get_error(iscsi));

        qemu_aio_release(acb);

        return NULL;

    }

    memset(acb->task, 0, sizeof(struct scsi_task));



    acb->task->xfer_dir = SCSI_XFER_READ;

    lba = sector_qemu2lun(sector_num, iscsilun);

    acb->task->expxferlen = qemu_read_size;



    switch (iscsilun->type) {

    case TYPE_DISK:

        acb->task->cdb_size = 16;

        acb->task->cdb[0]  = 0x88;

        *(uint32_t *)&acb->task->cdb[2]  = htonl(lba >> 32);

        *(uint32_t *)&acb->task->cdb[6]  = htonl(lba & 0xffffffff);

        *(uint32_t *)&acb->task->cdb[10] = htonl(num_sectors);

        break;

    default:

        acb->task->cdb_size = 10;

        acb->task->cdb[0]  = 0x28;

        *(uint32_t *)&acb->task->cdb[2] = htonl(lba);

        *(uint16_t *)&acb->task->cdb[7] = htons(num_sectors);

        break;

    }

    

    if (iscsi_scsi_command_async(iscsi, iscsilun->lun, acb->task,

                                 iscsi_aio_read16_cb,

                                 NULL,

                                 acb) != 0) {

        scsi_free_scsi_task(acb->task);

        qemu_aio_release(acb);

        return NULL;

    }



    for (i = 0; i < acb->qiov->niov; i++) {

        scsi_task_add_data_in_buffer(acb->task,

                acb->qiov->iov[i].iov_len,

                acb->qiov->iov[i].iov_base);

    }



    iscsi_set_events(iscsilun);



    return &acb->common;

}
","iscsi_aio_readv(BlockDriverState *bs, int64_t sector_num, QEMUIOVector *qiov, int nb_sectors, BlockDriverCompletionFunc *cb, void *opaque) {
IscsiLun *iscsilun = bs->opaque; struct iscsi_context *iscsi = iscsilun->iscsi; IscsiAIOCB *acb; size_t qemu_read_size; int i; uint64_t lba; uint32_t num_sectors; qemu_read_size = BDRV_SECTOR_SIZE * (size_t)nb_sectors; acb = qemu_aio_get(&iscsi_aiocb_info, bs, cb, opaque); trace_iscsi_aio_readv(iscsi, sector_num, nb_sectors, opaque, acb); acb->iscsilun = iscsilun; acb->qiov = qiov; acb->canceled = 0; acb->bh = NULL; acb->status = -EINPROGRESS; acb->read_size = qemu_read_size; acb->buf = NULL; /* If LUN blocksize is bigger than BDRV_BLOCK_SIZE a read from QEMU * may be misaligned to the LUN, so we may need to read some extra * data. */ acb->read_offset = 0; if (iscsilun->block_size > BDRV_SECTOR_SIZE) {
uint64_t bdrv_offset = BDRV_SECTOR_SIZE * sector_num; acb->read_offset = bdrv_offset % iscsilun->block_size; } num_sectors = (qemu_read_size + iscsilun->block_size + acb->read_offset - 1) / iscsilun->block_size; acb->task = malloc(sizeof(struct scsi_task)); if (acb->task == NULL) {
error_report(""iSCSI: Failed to allocate task for scsi READ16 "" ""command. %s"", iscsi_get_error(iscsi)); qemu_aio_release(acb); return NULL; } memset(acb->task, 0, sizeof(struct scsi_task)); acb->task->xfer_dir = SCSI_XFER_READ; lba = sector_qemu2lun(sector_num, iscsilun); acb->task->expxferlen = qemu_read_size; switch (iscsilun->type) {
case TYPE_DISK: acb->task->cdb_size = 16; acb->task->cdb[0] = 0x88; *(uint32_t *)&acb->task->cdb[2] = htonl(lba >> 32); *(uint32_t *)&acb->task->cdb[6] = htonl(lba & 0xffffffff); *(uint32_t *)&acb->task->cdb[10] = htonl(num_sectors); break; default: acb->task->cdb_size = 10; acb->task->cdb[0] = 0x28; *(uint32_t *)&acb->task->cdb[2] = htonl(lba); *(uint16_t *)&acb->task->cdb[7] = htons(num_sectors); break; } if (iscsi_scsi_command_async(iscsi, iscsilun->lun, acb->task, iscsi_aio_read16_cb, NULL, acb) != 0) {
scsi_free_scsi_task(acb->task); qemu_aio_release(acb); return NULL; } for (i = 0; i < acb->qiov->niov; i++) {
scsi_task_add_data_in_buffer(acb->task, acb->qiov->iov[i].iov_len, acb->qiov->iov[i].iov_base); } iscsi_set_events(iscsilun); return &acb->common; } ",qemu,1
"static uint8_t send_read_command(void)

{

    uint8_t drive = 0;

    uint8_t head = 0;

    uint8_t cyl = 0;

    uint8_t sect_addr = 1;

    uint8_t sect_size = 2;

    uint8_t eot = 1;

    uint8_t gap = 0x1b;

    uint8_t gpl = 0xff;



    uint8_t msr = 0;

    uint8_t st0;



    uint8_t ret = 0;



    floppy_send(CMD_READ);

    floppy_send(head << 2 | drive);

    g_assert(!get_irq(FLOPPY_IRQ));

    floppy_send(cyl);

    floppy_send(head);

    floppy_send(sect_addr);

    floppy_send(sect_size);

    floppy_send(eot);

    floppy_send(gap);

    floppy_send(gpl);



    uint8_t i = 0;

    uint8_t n = 2;

    for (; i < n; i++) {

        msr = inb(FLOPPY_BASE + reg_msr);

        if (msr == 0xd0) {

            break;

        }

        sleep(1);

    }



    if (i >= n) {

        return 1;

    }



    st0 = floppy_recv();

    if (st0 != 0x40) {

        ret = 1;

    }



    floppy_recv();

    floppy_recv();

    floppy_recv();

    floppy_recv();

    floppy_recv();

    floppy_recv();



    return ret;

}
","static uint8_t send_read_command(void) {
uint8_t drive = 0; uint8_t head = 0; uint8_t cyl = 0; uint8_t sect_addr = 1; uint8_t sect_size = 2; uint8_t eot = 1; uint8_t gap = 0x1b; uint8_t gpl = 0xff; uint8_t msr = 0; uint8_t st0; uint8_t ret = 0; floppy_send(CMD_READ); floppy_send(head << 2 | drive); g_assert(!get_irq(FLOPPY_IRQ)); floppy_send(cyl); floppy_send(head); floppy_send(sect_addr); floppy_send(sect_size); floppy_send(eot); floppy_send(gap); floppy_send(gpl); uint8_t i = 0; uint8_t n = 2; for (; i < n; i++) {
msr = inb(FLOPPY_BASE + reg_msr); if (msr == 0xd0) {
break; } sleep(1); } if (i >= n) {
return 1; } st0 = floppy_recv(); if (st0 != 0x40) {
ret = 1; } floppy_recv(); floppy_recv(); floppy_recv(); floppy_recv(); floppy_recv(); floppy_recv(); return ret; } ",qemu,1
"int ppcmas_tlb_check(CPUState *env, ppcmas_tlb_t *tlb,

                     target_phys_addr_t *raddrp,

                     target_ulong address, uint32_t pid)

{

    target_ulong mask;

    uint32_t tlb_pid;



    /* Check valid flag */

    if (!(tlb->mas1 & MAS1_VALID)) {

        return -1;

    }



    mask = ~(booke206_tlb_to_page_size(env, tlb) - 1);

    LOG_SWTLB(""%s: TLB ADDR=0x"" TARGET_FMT_lx "" PID=0x%x MAS1=0x%x MAS2=0x%""

              PRIx64 "" mask=0x"" TARGET_FMT_lx "" MAS7_3=0x%"" PRIx64 "" MAS8=%x\n"",

              __func__, address, pid, tlb->mas1, tlb->mas2, mask, tlb->mas7_3,

              tlb->mas8);



    /* Check PID */

    tlb_pid = (tlb->mas1 & MAS1_TID_MASK) >> MAS1_TID_SHIFT;

    if (tlb_pid != 0 && tlb_pid != pid) {

        return -1;

    }



    /* Check effective address */

    if ((address & mask) != (tlb->mas2 & MAS2_EPN_MASK)) {

        return -1;

    }

    *raddrp = (tlb->mas7_3 & mask) | (address & ~mask);



    return 0;

}
","int ppcmas_tlb_check(CPUState *env, ppcmas_tlb_t *tlb, target_phys_addr_t *raddrp, target_ulong address, uint32_t pid) {
target_ulong mask; uint32_t tlb_pid; /* Check valid flag */ if (!(tlb->mas1 & MAS1_VALID)) {
return -1; } mask = ~(booke206_tlb_to_page_size(env, tlb) - 1); LOG_SWTLB(""%s: TLB ADDR=0x"" TARGET_FMT_lx "" PID=0x%x MAS1=0x%x MAS2=0x%"" PRIx64 "" mask=0x"" TARGET_FMT_lx "" MAS7_3=0x%"" PRIx64 "" MAS8=%x\n"", __func__, address, pid, tlb->mas1, tlb->mas2, mask, tlb->mas7_3, tlb->mas8); /* Check PID */ tlb_pid = (tlb->mas1 & MAS1_TID_MASK) >> MAS1_TID_SHIFT; if (tlb_pid != 0 && tlb_pid != pid) {
return -1; } /* Check effective address */ if ((address & mask) != (tlb->mas2 & MAS2_EPN_MASK)) {
return -1; } *raddrp = (tlb->mas7_3 & mask) | (address & ~mask); return 0; } ",qemu,0
"static void scsi_disk_emulate_mode_select(SCSIDiskReq *r, uint8_t *inbuf)

{

    SCSIDiskState *s = DO_UPCAST(SCSIDiskState, qdev, r->req.dev);

    uint8_t *p = inbuf;

    int cmd = r->req.cmd.buf[0];

    int len = r->req.cmd.xfer;

    int hdr_len = (cmd == MODE_SELECT ? 4 : 8);

    int bd_len;

    int pass;



    /* We only support PF=1, SP=0.  */

    if ((r->req.cmd.buf[1] & 0x11) != 0x10) {

        goto invalid_field;

    }



    if (len < hdr_len) {

        goto invalid_param_len;

    }



    bd_len = (cmd == MODE_SELECT ? p[3] : lduw_be_p(&p[6]));

    len -= hdr_len;

    p += hdr_len;

    if (len < bd_len) {

        goto invalid_param_len;

    }

    if (bd_len != 0 && bd_len != 8) {

        goto invalid_param;

    }



    len -= bd_len;

    p += bd_len;



    /* Ensure no change is made if there is an error!  */

    for (pass = 0; pass < 2; pass++) {

        if (mode_select_pages(r, p, len, pass == 1) < 0) {

            assert(pass == 0);

            return;

        }

    }

    if (!bdrv_enable_write_cache(s->qdev.conf.bs)) {

        /* The request is used as the AIO opaque value, so add a ref.  */

        scsi_req_ref(&r->req);

        block_acct_start(bdrv_get_stats(s->qdev.conf.bs), &r->acct, 0,

                         BLOCK_ACCT_FLUSH);

        r->req.aiocb = bdrv_aio_flush(s->qdev.conf.bs, scsi_aio_complete, r);

        return;

    }



    scsi_req_complete(&r->req, GOOD);

    return;



invalid_param:

    scsi_check_condition(r, SENSE_CODE(INVALID_PARAM));

    return;



invalid_param_len:

    scsi_check_condition(r, SENSE_CODE(INVALID_PARAM_LEN));

    return;



invalid_field:

    scsi_check_condition(r, SENSE_CODE(INVALID_FIELD));

}
","static void scsi_disk_emulate_mode_select(SCSIDiskReq *r, uint8_t *inbuf) {
SCSIDiskState *s = DO_UPCAST(SCSIDiskState, qdev, r->req.dev); uint8_t *p = inbuf; int cmd = r->req.cmd.buf[0]; int len = r->req.cmd.xfer; int hdr_len = (cmd == MODE_SELECT ? 4 : 8); int bd_len; int pass; /* We only support PF=1, SP=0. */ if ((r->req.cmd.buf[1] & 0x11) != 0x10) {
goto invalid_field; } if (len < hdr_len) {
goto invalid_param_len; } bd_len = (cmd == MODE_SELECT ? p[3] : lduw_be_p(&p[6])); len -= hdr_len; p += hdr_len; if (len < bd_len) {
goto invalid_param_len; } if (bd_len != 0 && bd_len != 8) {
goto invalid_param; } len -= bd_len; p += bd_len; /* Ensure no change is made if there is an error! */ for (pass = 0; pass < 2; pass++) {
if (mode_select_pages(r, p, len, pass == 1) < 0) {
assert(pass == 0); return; } } if (!bdrv_enable_write_cache(s->qdev.conf.bs)) {
/* The request is used as the AIO opaque value, so add a ref. */ scsi_req_ref(&r->req); block_acct_start(bdrv_get_stats(s->qdev.conf.bs), &r->acct, 0, BLOCK_ACCT_FLUSH); r->req.aiocb = bdrv_aio_flush(s->qdev.conf.bs, scsi_aio_complete, r); return; } scsi_req_complete(&r->req, GOOD); return; invalid_param: scsi_check_condition(r, SENSE_CODE(INVALID_PARAM)); return; invalid_param_len: scsi_check_condition(r, SENSE_CODE(INVALID_PARAM_LEN)); return; invalid_field: scsi_check_condition(r, SENSE_CODE(INVALID_FIELD)); } ",qemu,0
"int cpu_ppc_handle_mmu_fault (CPUState *env, target_ulong address, int rw,

                              int mmu_idx, int is_softmmu)

{

    mmu_ctx_t ctx;

    int access_type;

    int ret = 0;



    if (rw == 2) {

        /* code access */

        rw = 0;

        access_type = ACCESS_CODE;

    } else {

        /* data access */

        access_type = env->access_type;

    }

    ret = get_physical_address(env, &ctx, address, rw, access_type);

    if (ret == 0) {

        ret = tlb_set_page_exec(env, address & TARGET_PAGE_MASK,

                                ctx.raddr & TARGET_PAGE_MASK, ctx.prot,

                                mmu_idx, is_softmmu);

    } else if (ret < 0) {

        LOG_MMU_STATE(env);

        if (access_type == ACCESS_CODE) {

            switch (ret) {

            case -1:

                /* No matches in page tables or TLB */

                switch (env->mmu_model) {

                case POWERPC_MMU_SOFT_6xx:

                    env->exception_index = POWERPC_EXCP_IFTLB;

                    env->error_code = 1 << 18;

                    env->spr[SPR_IMISS] = address;

                    env->spr[SPR_ICMP] = 0x80000000 | ctx.ptem;

                    goto tlb_miss;

                case POWERPC_MMU_SOFT_74xx:

                    env->exception_index = POWERPC_EXCP_IFTLB;

                    goto tlb_miss_74xx;

                case POWERPC_MMU_SOFT_4xx:

                case POWERPC_MMU_SOFT_4xx_Z:

                    env->exception_index = POWERPC_EXCP_ITLB;

                    env->error_code = 0;

                    env->spr[SPR_40x_DEAR] = address;

                    env->spr[SPR_40x_ESR] = 0x00000000;

                    break;

                case POWERPC_MMU_32B:

                case POWERPC_MMU_601:

#if defined(TARGET_PPC64)

                case POWERPC_MMU_620:

                case POWERPC_MMU_64B:

#endif

                    env->exception_index = POWERPC_EXCP_ISI;

                    env->error_code = 0x40000000;

                    break;

                case POWERPC_MMU_BOOKE:

                    /* XXX: TODO */

                    cpu_abort(env, ""BookE MMU model is not implemented\n"");

                    return -1;

                case POWERPC_MMU_BOOKE_FSL:

                    /* XXX: TODO */

                    cpu_abort(env, ""BookE FSL MMU model is not implemented\n"");

                    return -1;

                case POWERPC_MMU_MPC8xx:

                    /* XXX: TODO */

                    cpu_abort(env, ""MPC8xx MMU model is not implemented\n"");

                    break;

                case POWERPC_MMU_REAL:

                    cpu_abort(env, ""PowerPC in real mode should never raise ""

                              ""any MMU exceptions\n"");

                    return -1;

                default:

                    cpu_abort(env, ""Unknown or invalid MMU model\n"");

                    return -1;

                }

                break;

            case -2:

                /* Access rights violation */

                env->exception_index = POWERPC_EXCP_ISI;

                env->error_code = 0x08000000;

                break;

            case -3:

                /* No execute protection violation */

                env->exception_index = POWERPC_EXCP_ISI;

                env->error_code = 0x10000000;

                break;

            case -4:

                /* Direct store exception */

                /* No code fetch is allowed in direct-store areas */

                env->exception_index = POWERPC_EXCP_ISI;

                env->error_code = 0x10000000;

                break;

#if defined(TARGET_PPC64)

            case -5:

                /* No match in segment table */

                if (env->mmu_model == POWERPC_MMU_620) {

                    env->exception_index = POWERPC_EXCP_ISI;

                    /* XXX: this might be incorrect */

                    env->error_code = 0x40000000;

                } else {

                    env->exception_index = POWERPC_EXCP_ISEG;

                    env->error_code = 0;

                }

                break;

#endif

            }

        } else {

            switch (ret) {

            case -1:

                /* No matches in page tables or TLB */

                switch (env->mmu_model) {

                case POWERPC_MMU_SOFT_6xx:

                    if (rw == 1) {

                        env->exception_index = POWERPC_EXCP_DSTLB;

                        env->error_code = 1 << 16;

                    } else {

                        env->exception_index = POWERPC_EXCP_DLTLB;

                        env->error_code = 0;

                    }

                    env->spr[SPR_DMISS] = address;

                    env->spr[SPR_DCMP] = 0x80000000 | ctx.ptem;

                tlb_miss:

                    env->error_code |= ctx.key << 19;

                    env->spr[SPR_HASH1] = ctx.pg_addr[0];

                    env->spr[SPR_HASH2] = ctx.pg_addr[1];

                    break;

                case POWERPC_MMU_SOFT_74xx:

                    if (rw == 1) {

                        env->exception_index = POWERPC_EXCP_DSTLB;

                    } else {

                        env->exception_index = POWERPC_EXCP_DLTLB;

                    }

                tlb_miss_74xx:

                    /* Implement LRU algorithm */

                    env->error_code = ctx.key << 19;

                    env->spr[SPR_TLBMISS] = (address & ~((target_ulong)0x3)) |

                        ((env->last_way + 1) & (env->nb_ways - 1));

                    env->spr[SPR_PTEHI] = 0x80000000 | ctx.ptem;

                    break;

                case POWERPC_MMU_SOFT_4xx:

                case POWERPC_MMU_SOFT_4xx_Z:

                    env->exception_index = POWERPC_EXCP_DTLB;

                    env->error_code = 0;

                    env->spr[SPR_40x_DEAR] = address;

                    if (rw)

                        env->spr[SPR_40x_ESR] = 0x00800000;

                    else

                        env->spr[SPR_40x_ESR] = 0x00000000;

                    break;

                case POWERPC_MMU_32B:

                case POWERPC_MMU_601:

#if defined(TARGET_PPC64)

                case POWERPC_MMU_620:

                case POWERPC_MMU_64B:

#endif

                    env->exception_index = POWERPC_EXCP_DSI;

                    env->error_code = 0;

                    env->spr[SPR_DAR] = address;

                    if (rw == 1)

                        env->spr[SPR_DSISR] = 0x42000000;

                    else

                        env->spr[SPR_DSISR] = 0x40000000;

                    break;

                case POWERPC_MMU_MPC8xx:

                    /* XXX: TODO */

                    cpu_abort(env, ""MPC8xx MMU model is not implemented\n"");

                    break;

                case POWERPC_MMU_BOOKE:

                    /* XXX: TODO */

                    cpu_abort(env, ""BookE MMU model is not implemented\n"");

                    return -1;

                case POWERPC_MMU_BOOKE_FSL:

                    /* XXX: TODO */

                    cpu_abort(env, ""BookE FSL MMU model is not implemented\n"");

                    return -1;

                case POWERPC_MMU_REAL:

                    cpu_abort(env, ""PowerPC in real mode should never raise ""

                              ""any MMU exceptions\n"");

                    return -1;

                default:

                    cpu_abort(env, ""Unknown or invalid MMU model\n"");

                    return -1;

                }

                break;

            case -2:

                /* Access rights violation */

                env->exception_index = POWERPC_EXCP_DSI;

                env->error_code = 0;

                env->spr[SPR_DAR] = address;

                if (rw == 1)

                    env->spr[SPR_DSISR] = 0x0A000000;

                else

                    env->spr[SPR_DSISR] = 0x08000000;

                break;

            case -4:

                /* Direct store exception */

                switch (access_type) {

                case ACCESS_FLOAT:

                    /* Floating point load/store */

                    env->exception_index = POWERPC_EXCP_ALIGN;

                    env->error_code = POWERPC_EXCP_ALIGN_FP;

                    env->spr[SPR_DAR] = address;

                    break;

                case ACCESS_RES:

                    /* lwarx, ldarx or stwcx. */

                    env->exception_index = POWERPC_EXCP_DSI;

                    env->error_code = 0;

                    env->spr[SPR_DAR] = address;

                    if (rw == 1)

                        env->spr[SPR_DSISR] = 0x06000000;

                    else

                        env->spr[SPR_DSISR] = 0x04000000;

                    break;

                case ACCESS_EXT:

                    /* eciwx or ecowx */

                    env->exception_index = POWERPC_EXCP_DSI;

                    env->error_code = 0;

                    env->spr[SPR_DAR] = address;

                    if (rw == 1)

                        env->spr[SPR_DSISR] = 0x06100000;

                    else

                        env->spr[SPR_DSISR] = 0x04100000;

                    break;

                default:

                    printf(""DSI: invalid exception (%d)\n"", ret);

                    env->exception_index = POWERPC_EXCP_PROGRAM;

                    env->error_code =

                        POWERPC_EXCP_INVAL | POWERPC_EXCP_INVAL_INVAL;

                    env->spr[SPR_DAR] = address;

                    break;

                }

                break;

#if defined(TARGET_PPC64)

            case -5:

                /* No match in segment table */

                if (env->mmu_model == POWERPC_MMU_620) {

                    env->exception_index = POWERPC_EXCP_DSI;

                    env->error_code = 0;

                    env->spr[SPR_DAR] = address;

                    /* XXX: this might be incorrect */

                    if (rw == 1)

                        env->spr[SPR_DSISR] = 0x42000000;

                    else

                        env->spr[SPR_DSISR] = 0x40000000;

                } else {

                    env->exception_index = POWERPC_EXCP_DSEG;

                    env->error_code = 0;

                    env->spr[SPR_DAR] = address;

                }

                break;

#endif

            }

        }

#if 0

        printf(""%s: set exception to %d %02x\n"", __func__,

               env->exception, env->error_code);

#endif

        ret = 1;

    }



    return ret;

}
","int cpu_ppc_handle_mmu_fault (CPUState *env, target_ulong address, int rw, int mmu_idx, int is_softmmu) {
mmu_ctx_t ctx; int access_type; int ret = 0; if (rw == 2) {
/* code access */ rw = 0; access_type = ACCESS_CODE; } else {
/* data access */ access_type = env->access_type; } ret = get_physical_address(env, &ctx, address, rw, access_type); if (ret == 0) {
ret = tlb_set_page_exec(env, address & TARGET_PAGE_MASK, ctx.raddr & TARGET_PAGE_MASK, ctx.prot, mmu_idx, is_softmmu); } else if (ret < 0) {
LOG_MMU_STATE(env); if (access_type == ACCESS_CODE) {
switch (ret) {
case -1: /* No matches in page tables or TLB */ switch (env->mmu_model) {
case POWERPC_MMU_SOFT_6xx: env->exception_index = POWERPC_EXCP_IFTLB; env->error_code = 1 << 18; env->spr[SPR_IMISS] = address; env->spr[SPR_ICMP] = 0x80000000 | ctx.ptem; goto tlb_miss; case POWERPC_MMU_SOFT_74xx: env->exception_index = POWERPC_EXCP_IFTLB; goto tlb_miss_74xx; case POWERPC_MMU_SOFT_4xx: case POWERPC_MMU_SOFT_4xx_Z: env->exception_index = POWERPC_EXCP_ITLB; env->error_code = 0; env->spr[SPR_40x_DEAR] = address; env->spr[SPR_40x_ESR] = 0x00000000; break; case POWERPC_MMU_32B: case POWERPC_MMU_601: #if defined(TARGET_PPC64) case POWERPC_MMU_620: case POWERPC_MMU_64B: #endif env->exception_index = POWERPC_EXCP_ISI; env->error_code = 0x40000000; break; case POWERPC_MMU_BOOKE: /* XXX: TODO */ cpu_abort(env, ""BookE MMU model is not implemented\n""); return -1; case POWERPC_MMU_BOOKE_FSL: /* XXX: TODO */ cpu_abort(env, ""BookE FSL MMU model is not implemented\n""); return -1; case POWERPC_MMU_MPC8xx: /* XXX: TODO */ cpu_abort(env, ""MPC8xx MMU model is not implemented\n""); break; case POWERPC_MMU_REAL: cpu_abort(env, ""PowerPC in real mode should never raise "" ""any MMU exceptions\n""); return -1; default: cpu_abort(env, ""Unknown or invalid MMU model\n""); return -1; } break; case -2: /* Access rights violation */ env->exception_index = POWERPC_EXCP_ISI; env->error_code = 0x08000000; break; case -3: /* No execute protection violation */ env->exception_index = POWERPC_EXCP_ISI; env->error_code = 0x10000000; break; case -4: /* Direct store exception */ /* No code fetch is allowed in direct-store areas */ env->exception_index = POWERPC_EXCP_ISI; env->error_code = 0x10000000; break; #if defined(TARGET_PPC64) case -5: /* No match in segment table */ if (env->mmu_model == POWERPC_MMU_620) {
env->exception_index = POWERPC_EXCP_ISI; /* XXX: this might be incorrect */ env->error_code = 0x40000000; } else {
env->exception_index = POWERPC_EXCP_ISEG; env->error_code = 0; } break; #endif } } else {
switch (ret) {
case -1: /* No matches in page tables or TLB */ switch (env->mmu_model) {
case POWERPC_MMU_SOFT_6xx: if (rw == 1) {
env->exception_index = POWERPC_EXCP_DSTLB; env->error_code = 1 << 16; } else {
env->exception_index = POWERPC_EXCP_DLTLB; env->error_code = 0; } env->spr[SPR_DMISS] = address; env->spr[SPR_DCMP] = 0x80000000 | ctx.ptem; tlb_miss: env->error_code |= ctx.key << 19; env->spr[SPR_HASH1] = ctx.pg_addr[0]; env->spr[SPR_HASH2] = ctx.pg_addr[1]; break; case POWERPC_MMU_SOFT_74xx: if (rw == 1) {
env->exception_index = POWERPC_EXCP_DSTLB; } else {
env->exception_index = POWERPC_EXCP_DLTLB; } tlb_miss_74xx: /* Implement LRU algorithm */ env->error_code = ctx.key << 19; env->spr[SPR_TLBMISS] = (address & ~((target_ulong)0x3)) | ((env->last_way + 1) & (env->nb_ways - 1)); env->spr[SPR_PTEHI] = 0x80000000 | ctx.ptem; break; case POWERPC_MMU_SOFT_4xx: case POWERPC_MMU_SOFT_4xx_Z: env->exception_index = POWERPC_EXCP_DTLB; env->error_code = 0; env->spr[SPR_40x_DEAR] = address; if (rw) env->spr[SPR_40x_ESR] = 0x00800000; else env->spr[SPR_40x_ESR] = 0x00000000; break; case POWERPC_MMU_32B: case POWERPC_MMU_601: #if defined(TARGET_PPC64) case POWERPC_MMU_620: case POWERPC_MMU_64B: #endif env->exception_index = POWERPC_EXCP_DSI; env->error_code = 0; env->spr[SPR_DAR] = address; if (rw == 1) env->spr[SPR_DSISR] = 0x42000000; else env->spr[SPR_DSISR] = 0x40000000; break; case POWERPC_MMU_MPC8xx: /* XXX: TODO */ cpu_abort(env, ""MPC8xx MMU model is not implemented\n""); break; case POWERPC_MMU_BOOKE: /* XXX: TODO */ cpu_abort(env, ""BookE MMU model is not implemented\n""); return -1; case POWERPC_MMU_BOOKE_FSL: /* XXX: TODO */ cpu_abort(env, ""BookE FSL MMU model is not implemented\n""); return -1; case POWERPC_MMU_REAL: cpu_abort(env, ""PowerPC in real mode should never raise "" ""any MMU exceptions\n""); return -1; default: cpu_abort(env, ""Unknown or invalid MMU model\n""); return -1; } break; case -2: /* Access rights violation */ env->exception_index = POWERPC_EXCP_DSI; env->error_code = 0; env->spr[SPR_DAR] = address; if (rw == 1) env->spr[SPR_DSISR] = 0x0A000000; else env->spr[SPR_DSISR] = 0x08000000; break; case -4: /* Direct store exception */ switch (access_type) {
case ACCESS_FLOAT: /* Floating point load/store */ env->exception_index = POWERPC_EXCP_ALIGN; env->error_code = POWERPC_EXCP_ALIGN_FP; env->spr[SPR_DAR] = address; break; case ACCESS_RES: /* lwarx, ldarx or stwcx. */ env->exception_index = POWERPC_EXCP_DSI; env->error_code = 0; env->spr[SPR_DAR] = address; if (rw == 1) env->spr[SPR_DSISR] = 0x06000000; else env->spr[SPR_DSISR] = 0x04000000; break; case ACCESS_EXT: /* eciwx or ecowx */ env->exception_index = POWERPC_EXCP_DSI; env->error_code = 0; env->spr[SPR_DAR] = address; if (rw == 1) env->spr[SPR_DSISR] = 0x06100000; else env->spr[SPR_DSISR] = 0x04100000; break; default: printf(""DSI: invalid exception (%d)\n"", ret); env->exception_index = POWERPC_EXCP_PROGRAM; env->error_code = POWERPC_EXCP_INVAL | POWERPC_EXCP_INVAL_INVAL; env->spr[SPR_DAR] = address; break; } break; #if defined(TARGET_PPC64) case -5: /* No match in segment table */ if (env->mmu_model == POWERPC_MMU_620) {
env->exception_index = POWERPC_EXCP_DSI; env->error_code = 0; env->spr[SPR_DAR] = address; /* XXX: this might be incorrect */ if (rw == 1) env->spr[SPR_DSISR] = 0x42000000; else env->spr[SPR_DSISR] = 0x40000000; } else {
env->exception_index = POWERPC_EXCP_DSEG; env->error_code = 0; env->spr[SPR_DAR] = address; } break; #endif } } #if 0 printf(""%s: set exception to %d %02x\n"", __func__, env->exception, env->error_code); #endif ret = 1; } return ret; } ",qemu,0
"static void IRQ_local_pipe(OpenPICState *opp, int n_CPU, int n_IRQ)

{

    IRQ_dst_t *dst;

    IRQ_src_t *src;

    int priority;



    dst = &opp->dst[n_CPU];

    src = &opp->src[n_IRQ];

    priority = IPVP_PRIORITY(src->ipvp);

    if (priority <= dst->pctp) {

        /* Too low priority */

        DPRINTF(""%s: IRQ %d has too low priority on CPU %d\n"",

                __func__, n_IRQ, n_CPU);

        return;

    }

    if (IRQ_testbit(&dst->raised, n_IRQ)) {

        /* Interrupt miss */

        DPRINTF(""%s: IRQ %d was missed on CPU %d\n"",

                __func__, n_IRQ, n_CPU);

        return;

    }

    src->ipvp |= IPVP_ACTIVITY_MASK;

    IRQ_setbit(&dst->raised, n_IRQ);

    if (priority < dst->raised.priority) {

        /* An higher priority IRQ is already raised */

        DPRINTF(""%s: IRQ %d is hidden by raised IRQ %d on CPU %d\n"",

                __func__, n_IRQ, dst->raised.next, n_CPU);

        return;

    }

    IRQ_get_next(opp, &dst->raised);

    if (IRQ_get_next(opp, &dst->servicing) != -1 &&

        priority <= dst->servicing.priority) {

        DPRINTF(""%s: IRQ %d is hidden by servicing IRQ %d on CPU %d\n"",

                __func__, n_IRQ, dst->servicing.next, n_CPU);

        /* Already servicing a higher priority IRQ */

        return;

    }

    DPRINTF(""Raise OpenPIC INT output cpu %d irq %d\n"", n_CPU, n_IRQ);

    openpic_irq_raise(opp, n_CPU, src);

}
","static void IRQ_local_pipe(OpenPICState *opp, int n_CPU, int n_IRQ) {
IRQ_dst_t *dst; IRQ_src_t *src; int priority; dst = &opp->dst[n_CPU]; src = &opp->src[n_IRQ]; priority = IPVP_PRIORITY(src->ipvp); if (priority <= dst->pctp) {
/* Too low priority */ DPRINTF(""%s: IRQ %d has too low priority on CPU %d\n"", __func__, n_IRQ, n_CPU); return; } if (IRQ_testbit(&dst->raised, n_IRQ)) {
/* Interrupt miss */ DPRINTF(""%s: IRQ %d was missed on CPU %d\n"", __func__, n_IRQ, n_CPU); return; } src->ipvp |= IPVP_ACTIVITY_MASK; IRQ_setbit(&dst->raised, n_IRQ); if (priority < dst->raised.priority) {
/* An higher priority IRQ is already raised */ DPRINTF(""%s: IRQ %d is hidden by raised IRQ %d on CPU %d\n"", __func__, n_IRQ, dst->raised.next, n_CPU); return; } IRQ_get_next(opp, &dst->raised); if (IRQ_get_next(opp, &dst->servicing) != -1 && priority <= dst->servicing.priority) {
DPRINTF(""%s: IRQ %d is hidden by servicing IRQ %d on CPU %d\n"", __func__, n_IRQ, dst->servicing.next, n_CPU); /* Already servicing a higher priority IRQ */ return; } DPRINTF(""Raise OpenPIC INT output cpu %d irq %d\n"", n_CPU, n_IRQ); openpic_irq_raise(opp, n_CPU, src); } ",qemu,1
"static uint64_t virtio_pci_common_read(void *opaque, hwaddr addr,

                                       unsigned size)

{

    VirtIOPCIProxy *proxy = opaque;

    VirtIODevice *vdev = virtio_bus_get_device(&proxy->bus);

    uint32_t val = 0;

    int i;



    switch (addr) {

    case VIRTIO_PCI_COMMON_DFSELECT:

        val = proxy->dfselect;

        break;

    case VIRTIO_PCI_COMMON_DF:

        if (proxy->dfselect <= 1) {

            val = vdev->host_features >> (32 * proxy->dfselect);

        }

        break;

    case VIRTIO_PCI_COMMON_GFSELECT:

        val = proxy->gfselect;

        break;

    case VIRTIO_PCI_COMMON_GF:

        if (proxy->gfselect <= ARRAY_SIZE(proxy->guest_features)) {

            val = proxy->guest_features[proxy->gfselect];

        }

        break;

    case VIRTIO_PCI_COMMON_MSIX:

        val = vdev->config_vector;

        break;

    case VIRTIO_PCI_COMMON_NUMQ:

        for (i = 0; i < VIRTIO_QUEUE_MAX; ++i) {

            if (virtio_queue_get_num(vdev, i)) {

                val = i + 1;

            }

        }

        break;

    case VIRTIO_PCI_COMMON_STATUS:

        val = vdev->status;

        break;

    case VIRTIO_PCI_COMMON_CFGGENERATION:

        val = vdev->generation;

        break;

    case VIRTIO_PCI_COMMON_Q_SELECT:

        val = vdev->queue_sel;

        break;

    case VIRTIO_PCI_COMMON_Q_SIZE:

        val = virtio_queue_get_num(vdev, vdev->queue_sel);

        break;

    case VIRTIO_PCI_COMMON_Q_MSIX:

        val = virtio_queue_vector(vdev, vdev->queue_sel);

        break;

    case VIRTIO_PCI_COMMON_Q_ENABLE:

        val = proxy->vqs[vdev->queue_sel].enabled;

        break;

    case VIRTIO_PCI_COMMON_Q_NOFF:

        /* Simply map queues in order */

        val = vdev->queue_sel;

        break;

    case VIRTIO_PCI_COMMON_Q_DESCLO:

        val = proxy->vqs[vdev->queue_sel].desc[0];

        break;

    case VIRTIO_PCI_COMMON_Q_DESCHI:

        val = proxy->vqs[vdev->queue_sel].desc[1];

        break;

    case VIRTIO_PCI_COMMON_Q_AVAILLO:

        val = proxy->vqs[vdev->queue_sel].avail[0];

        break;

    case VIRTIO_PCI_COMMON_Q_AVAILHI:

        val = proxy->vqs[vdev->queue_sel].avail[1];

        break;

    case VIRTIO_PCI_COMMON_Q_USEDLO:

        val = proxy->vqs[vdev->queue_sel].used[0];

        break;

    case VIRTIO_PCI_COMMON_Q_USEDHI:

        val = proxy->vqs[vdev->queue_sel].used[1];

        break;

    default:

        val = 0;

    }



    return val;

}
","static uint64_t virtio_pci_common_read(void *opaque, hwaddr addr, unsigned size) {
VirtIOPCIProxy *proxy = opaque; VirtIODevice *vdev = virtio_bus_get_device(&proxy->bus); uint32_t val = 0; int i; switch (addr) {
case VIRTIO_PCI_COMMON_DFSELECT: val = proxy->dfselect; break; case VIRTIO_PCI_COMMON_DF: if (proxy->dfselect <= 1) {
val = vdev->host_features >> (32 * proxy->dfselect); } break; case VIRTIO_PCI_COMMON_GFSELECT: val = proxy->gfselect; break; case VIRTIO_PCI_COMMON_GF: if (proxy->gfselect <= ARRAY_SIZE(proxy->guest_features)) {
val = proxy->guest_features[proxy->gfselect]; } break; case VIRTIO_PCI_COMMON_MSIX: val = vdev->config_vector; break; case VIRTIO_PCI_COMMON_NUMQ: for (i = 0; i < VIRTIO_QUEUE_MAX; ++i) {
if (virtio_queue_get_num(vdev, i)) {
val = i + 1; } } break; case VIRTIO_PCI_COMMON_STATUS: val = vdev->status; break; case VIRTIO_PCI_COMMON_CFGGENERATION: val = vdev->generation; break; case VIRTIO_PCI_COMMON_Q_SELECT: val = vdev->queue_sel; break; case VIRTIO_PCI_COMMON_Q_SIZE: val = virtio_queue_get_num(vdev, vdev->queue_sel); break; case VIRTIO_PCI_COMMON_Q_MSIX: val = virtio_queue_vector(vdev, vdev->queue_sel); break; case VIRTIO_PCI_COMMON_Q_ENABLE: val = proxy->vqs[vdev->queue_sel].enabled; break; case VIRTIO_PCI_COMMON_Q_NOFF: /* Simply map queues in order */ val = vdev->queue_sel; break; case VIRTIO_PCI_COMMON_Q_DESCLO: val = proxy->vqs[vdev->queue_sel].desc[0]; break; case VIRTIO_PCI_COMMON_Q_DESCHI: val = proxy->vqs[vdev->queue_sel].desc[1]; break; case VIRTIO_PCI_COMMON_Q_AVAILLO: val = proxy->vqs[vdev->queue_sel].avail[0]; break; case VIRTIO_PCI_COMMON_Q_AVAILHI: val = proxy->vqs[vdev->queue_sel].avail[1]; break; case VIRTIO_PCI_COMMON_Q_USEDLO: val = proxy->vqs[vdev->queue_sel].used[0]; break; case VIRTIO_PCI_COMMON_Q_USEDHI: val = proxy->vqs[vdev->queue_sel].used[1]; break; default: val = 0; } return val; } ",qemu,1
"static int qcow_write(BlockDriverState *bs, int64_t sector_num,

                     const uint8_t *buf, int nb_sectors)

{

    BDRVQcowState *s = bs->opaque;

    int ret, index_in_cluster, n;

    uint64_t cluster_offset;

    int n_end;



    while (nb_sectors > 0) {

        index_in_cluster = sector_num & (s->cluster_sectors - 1);

        n_end = index_in_cluster + nb_sectors;

        if (s->crypt_method &&

            n_end > QCOW_MAX_CRYPT_CLUSTERS * s->cluster_sectors)

            n_end = QCOW_MAX_CRYPT_CLUSTERS * s->cluster_sectors;

        cluster_offset = alloc_cluster_offset(bs, sector_num << 9,

                                              index_in_cluster,

                                              n_end, &n);

        if (!cluster_offset)

            return -1;

        if (s->crypt_method) {

            encrypt_sectors(s, sector_num, s->cluster_data, buf, n, 1,

                            &s->aes_encrypt_key);

            ret = bdrv_pwrite(s->hd, cluster_offset + index_in_cluster * 512,

                              s->cluster_data, n * 512);

        } else {

            ret = bdrv_pwrite(s->hd, cluster_offset + index_in_cluster * 512, buf, n * 512);

        }

        if (ret != n * 512)

            return -1;

        nb_sectors -= n;

        sector_num += n;

        buf += n * 512;

    }

    s->cluster_cache_offset = -1; /* disable compressed cache */

    return 0;

}
","static int qcow_write(BlockDriverState *bs, int64_t sector_num, const uint8_t *buf, int nb_sectors) {
BDRVQcowState *s = bs->opaque; int ret, index_in_cluster, n; uint64_t cluster_offset; int n_end; while (nb_sectors > 0) {
index_in_cluster = sector_num & (s->cluster_sectors - 1); n_end = index_in_cluster + nb_sectors; if (s->crypt_method && n_end > QCOW_MAX_CRYPT_CLUSTERS * s->cluster_sectors) n_end = QCOW_MAX_CRYPT_CLUSTERS * s->cluster_sectors; cluster_offset = alloc_cluster_offset(bs, sector_num << 9, index_in_cluster, n_end, &n); if (!cluster_offset) return -1; if (s->crypt_method) {
encrypt_sectors(s, sector_num, s->cluster_data, buf, n, 1, &s->aes_encrypt_key); ret = bdrv_pwrite(s->hd, cluster_offset + index_in_cluster * 512, s->cluster_data, n * 512); } else {
ret = bdrv_pwrite(s->hd, cluster_offset + index_in_cluster * 512, buf, n * 512); } if (ret != n * 512) return -1; nb_sectors -= n; sector_num += n; buf += n * 512; } s->cluster_cache_offset = -1; /* disable compressed cache */ return 0; } ",qemu,1
"static int cpu_load_old(QEMUFile *f, void *opaque, int version_id)

{

    PowerPCCPU *cpu = opaque;

    CPUPPCState *env = &cpu->env;

    unsigned int i, j;

    target_ulong sdr1;

    uint32_t fpscr;

    target_ulong xer;



    for (i = 0; i < 32; i++)

        qemu_get_betls(f, &env->gpr[i]);

#if !defined(TARGET_PPC64)

    for (i = 0; i < 32; i++)

        qemu_get_betls(f, &env->gprh[i]);

#endif

    qemu_get_betls(f, &env->lr);

    qemu_get_betls(f, &env->ctr);

    for (i = 0; i < 8; i++)

        qemu_get_be32s(f, &env->crf[i]);

    qemu_get_betls(f, &xer);

    cpu_write_xer(env, xer);

    qemu_get_betls(f, &env->reserve_addr);

    qemu_get_betls(f, &env->msr);

    for (i = 0; i < 4; i++)

        qemu_get_betls(f, &env->tgpr[i]);

    for (i = 0; i < 32; i++) {

        union {

            float64 d;

            uint64_t l;

        } u;

        u.l = qemu_get_be64(f);

        env->fpr[i] = u.d;

    }

    qemu_get_be32s(f, &fpscr);

    env->fpscr = fpscr;

    qemu_get_sbe32s(f, &env->access_type);

#if defined(TARGET_PPC64)

    qemu_get_betls(f, &env->spr[SPR_ASR]);

    qemu_get_sbe32s(f, &env->slb_nr);

#endif

    qemu_get_betls(f, &sdr1);

    for (i = 0; i < 32; i++)

        qemu_get_betls(f, &env->sr[i]);

    for (i = 0; i < 2; i++)

        for (j = 0; j < 8; j++)

            qemu_get_betls(f, &env->DBAT[i][j]);

    for (i = 0; i < 2; i++)

        for (j = 0; j < 8; j++)

            qemu_get_betls(f, &env->IBAT[i][j]);

    qemu_get_sbe32s(f, &env->nb_tlb);

    qemu_get_sbe32s(f, &env->tlb_per_way);

    qemu_get_sbe32s(f, &env->nb_ways);

    qemu_get_sbe32s(f, &env->last_way);

    qemu_get_sbe32s(f, &env->id_tlbs);

    qemu_get_sbe32s(f, &env->nb_pids);

    if (env->tlb.tlb6) {

        // XXX assumes 6xx

        for (i = 0; i < env->nb_tlb; i++) {

            qemu_get_betls(f, &env->tlb.tlb6[i].pte0);

            qemu_get_betls(f, &env->tlb.tlb6[i].pte1);

            qemu_get_betls(f, &env->tlb.tlb6[i].EPN);

        }

    }

    for (i = 0; i < 4; i++)

        qemu_get_betls(f, &env->pb[i]);

    for (i = 0; i < 1024; i++)

        qemu_get_betls(f, &env->spr[i]);

    if (!env->external_htab) {

        ppc_store_sdr1(env, sdr1);

    }

    qemu_get_be32s(f, &env->vscr);

    qemu_get_be64s(f, &env->spe_acc);

    qemu_get_be32s(f, &env->spe_fscr);

    qemu_get_betls(f, &env->msr_mask);

    qemu_get_be32s(f, &env->flags);

    qemu_get_sbe32s(f, &env->error_code);

    qemu_get_be32s(f, &env->pending_interrupts);

    qemu_get_be32s(f, &env->irq_input_state);

    for (i = 0; i < POWERPC_EXCP_NB; i++)

        qemu_get_betls(f, &env->excp_vectors[i]);

    qemu_get_betls(f, &env->excp_prefix);

    qemu_get_betls(f, &env->ivor_mask);

    qemu_get_betls(f, &env->ivpr_mask);

    qemu_get_betls(f, &env->hreset_vector);

    qemu_get_betls(f, &env->nip);

    qemu_get_betls(f, &env->hflags);

    qemu_get_betls(f, &env->hflags_nmsr);

    qemu_get_sbe32(f); /* Discard unused mmu_idx */

    qemu_get_sbe32(f); /* Discard unused power_mode */



    /* Recompute mmu indices */

    hreg_compute_mem_idx(env);



    return 0;

}
","static int cpu_load_old(QEMUFile *f, void *opaque, int version_id) {
PowerPCCPU *cpu = opaque; CPUPPCState *env = &cpu->env; unsigned int i, j; target_ulong sdr1; uint32_t fpscr; target_ulong xer; for (i = 0; i < 32; i++) qemu_get_betls(f, &env->gpr[i]); #if !defined(TARGET_PPC64) for (i = 0; i < 32; i++) qemu_get_betls(f, &env->gprh[i]); #endif qemu_get_betls(f, &env->lr); qemu_get_betls(f, &env->ctr); for (i = 0; i < 8; i++) qemu_get_be32s(f, &env->crf[i]); qemu_get_betls(f, &xer); cpu_write_xer(env, xer); qemu_get_betls(f, &env->reserve_addr); qemu_get_betls(f, &env->msr); for (i = 0; i < 4; i++) qemu_get_betls(f, &env->tgpr[i]); for (i = 0; i < 32; i++) {
union {
float64 d; uint64_t l; } u; u.l = qemu_get_be64(f); env->fpr[i] = u.d; } qemu_get_be32s(f, &fpscr); env->fpscr = fpscr; qemu_get_sbe32s(f, &env->access_type); #if defined(TARGET_PPC64) qemu_get_betls(f, &env->spr[SPR_ASR]); qemu_get_sbe32s(f, &env->slb_nr); #endif qemu_get_betls(f, &sdr1); for (i = 0; i < 32; i++) qemu_get_betls(f, &env->sr[i]); for (i = 0; i < 2; i++) for (j = 0; j < 8; j++) qemu_get_betls(f, &env->DBAT[i][j]); for (i = 0; i < 2; i++) for (j = 0; j < 8; j++) qemu_get_betls(f, &env->IBAT[i][j]); qemu_get_sbe32s(f, &env->nb_tlb); qemu_get_sbe32s(f, &env->tlb_per_way); qemu_get_sbe32s(f, &env->nb_ways); qemu_get_sbe32s(f, &env->last_way); qemu_get_sbe32s(f, &env->id_tlbs); qemu_get_sbe32s(f, &env->nb_pids); if (env->tlb.tlb6) {
// XXX assumes 6xx for (i = 0; i < env->nb_tlb; i++) {
qemu_get_betls(f, &env->tlb.tlb6[i].pte0); qemu_get_betls(f, &env->tlb.tlb6[i].pte1); qemu_get_betls(f, &env->tlb.tlb6[i].EPN); } } for (i = 0; i < 4; i++) qemu_get_betls(f, &env->pb[i]); for (i = 0; i < 1024; i++) qemu_get_betls(f, &env->spr[i]); if (!env->external_htab) {
ppc_store_sdr1(env, sdr1); } qemu_get_be32s(f, &env->vscr); qemu_get_be64s(f, &env->spe_acc); qemu_get_be32s(f, &env->spe_fscr); qemu_get_betls(f, &env->msr_mask); qemu_get_be32s(f, &env->flags); qemu_get_sbe32s(f, &env->error_code); qemu_get_be32s(f, &env->pending_interrupts); qemu_get_be32s(f, &env->irq_input_state); for (i = 0; i < POWERPC_EXCP_NB; i++) qemu_get_betls(f, &env->excp_vectors[i]); qemu_get_betls(f, &env->excp_prefix); qemu_get_betls(f, &env->ivor_mask); qemu_get_betls(f, &env->ivpr_mask); qemu_get_betls(f, &env->hreset_vector); qemu_get_betls(f, &env->nip); qemu_get_betls(f, &env->hflags); qemu_get_betls(f, &env->hflags_nmsr); qemu_get_sbe32(f); /* Discard unused mmu_idx */ qemu_get_sbe32(f); /* Discard unused power_mode */ /* Recompute mmu indices */ hreg_compute_mem_idx(env); return 0; } ",qemu,0
"void qpci_io_writeb(QPCIDevice *dev, void *data, uint8_t value)

{

    uintptr_t addr = (uintptr_t)data;



    if (addr < QPCI_PIO_LIMIT) {

        dev->bus->pio_writeb(dev->bus, addr, value);

    } else {

        dev->bus->memwrite(dev->bus, addr, &value, sizeof(value));

    }

}
","void qpci_io_writeb(QPCIDevice *dev, void *data, uint8_t value) {
uintptr_t addr = (uintptr_t)data; if (addr < QPCI_PIO_LIMIT) {
dev->bus->pio_writeb(dev->bus, addr, value); } else {
dev->bus->memwrite(dev->bus, addr, &value, sizeof(value)); } } ",qemu,1
"static inline void gen_op_fcmpes(int fccno, TCGv_i32 r_rs1, TCGv_i32 r_rs2)

{

    switch (fccno) {

    case 0:

        gen_helper_fcmpes(cpu_env, r_rs1, r_rs2);

        break;

    case 1:

        gen_helper_fcmpes_fcc1(cpu_env, r_rs1, r_rs2);

        break;

    case 2:

        gen_helper_fcmpes_fcc2(cpu_env, r_rs1, r_rs2);

        break;

    case 3:

        gen_helper_fcmpes_fcc3(cpu_env, r_rs1, r_rs2);

        break;

    }

}
","static inline void gen_op_fcmpes(int fccno, TCGv_i32 r_rs1, TCGv_i32 r_rs2) {
switch (fccno) {
case 0: gen_helper_fcmpes(cpu_env, r_rs1, r_rs2); break; case 1: gen_helper_fcmpes_fcc1(cpu_env, r_rs1, r_rs2); break; case 2: gen_helper_fcmpes_fcc2(cpu_env, r_rs1, r_rs2); break; case 3: gen_helper_fcmpes_fcc3(cpu_env, r_rs1, r_rs2); break; } } ",qemu,0
"static void pc_compat_1_7(MachineState *machine)

{

    pc_compat_2_0(machine);

    smbios_defaults = false;

    gigabyte_align = false;

    option_rom_has_mr = true;

    x86_cpu_change_kvm_default(""x2apic"", NULL);

}
","static void pc_compat_1_7(MachineState *machine) {
pc_compat_2_0(machine); smbios_defaults = false; gigabyte_align = false; option_rom_has_mr = true; x86_cpu_change_kvm_default(""x2apic"", NULL); } ",qemu,0
"static void fpu_dump_state(CPUState *env, FILE *f,

                           int (*fpu_fprintf)(FILE *f, const char *fmt, ...),

                           int flags)

{

    int i;

    int is_fpu64 = !!(env->hflags & MIPS_HFLAG_F64);



#define printfpr(fp)                                                    \

    do {                                                                \

        if (is_fpu64)                                                   \

            fpu_fprintf(f, ""w:%08x d:%016"" PRIx64                       \

                        "" fd:%13g fs:%13g psu: %13g\n"",                 \

                        (fp)->w[FP_ENDIAN_IDX], (fp)->d,                \

                        (double)(fp)->fd,                               \

                        (double)(fp)->fs[FP_ENDIAN_IDX],                \

                        (double)(fp)->fs[!FP_ENDIAN_IDX]);              \

        else {                                                          \

            fpr_t tmp;                                                  \

            tmp.w[FP_ENDIAN_IDX] = (fp)->w[FP_ENDIAN_IDX];              \

            tmp.w[!FP_ENDIAN_IDX] = ((fp) + 1)->w[FP_ENDIAN_IDX];       \

            fpu_fprintf(f, ""w:%08x d:%016"" PRIx64                       \

                        "" fd:%13g fs:%13g psu:%13g\n"",                  \

                        tmp.w[FP_ENDIAN_IDX], tmp.d,                    \

                        (double)tmp.fd,                                 \

                        (double)tmp.fs[FP_ENDIAN_IDX],                  \

                        (double)tmp.fs[!FP_ENDIAN_IDX]);                \

        }                                                               \

    } while(0)





    fpu_fprintf(f, ""CP1 FCR0 0x%08x  FCR31 0x%08x  SR.FR %d  fp_status 0x%08x(0x%02x)\n"",

                env->active_fpu.fcr0, env->active_fpu.fcr31, is_fpu64, env->active_fpu.fp_status,

                get_float_exception_flags(&env->active_fpu.fp_status));

    for (i = 0; i < 32; (is_fpu64) ? i++ : (i += 2)) {

        fpu_fprintf(f, ""%3s: "", fregnames[i]);

        printfpr(&env->active_fpu.fpr[i]);

    }



#undef printfpr

}
","static void fpu_dump_state(CPUState *env, FILE *f, int (*fpu_fprintf)(FILE *f, const char *fmt, ...), int flags) {
int i; int is_fpu64 = !!(env->hflags & MIPS_HFLAG_F64); #define printfpr(fp) \ do {
\ if (is_fpu64) \ fpu_fprintf(f, ""w:%08x d:%016"" PRIx64 \ "" fd:%13g fs:%13g psu: %13g\n"", \ (fp)->w[FP_ENDIAN_IDX], (fp)->d, \ (double)(fp)->fd, \ (double)(fp)->fs[FP_ENDIAN_IDX], \ (double)(fp)->fs[!FP_ENDIAN_IDX]); \ else {
\ fpr_t tmp; \ tmp.w[FP_ENDIAN_IDX] = (fp)->w[FP_ENDIAN_IDX]; \ tmp.w[!FP_ENDIAN_IDX] = ((fp) + 1)->w[FP_ENDIAN_IDX]; \ fpu_fprintf(f, ""w:%08x d:%016"" PRIx64 \ "" fd:%13g fs:%13g psu:%13g\n"", \ tmp.w[FP_ENDIAN_IDX], tmp.d, \ (double)tmp.fd, \ (double)tmp.fs[FP_ENDIAN_IDX], \ (double)tmp.fs[!FP_ENDIAN_IDX]); \ } \ } while(0) fpu_fprintf(f, ""CP1 FCR0 0x%08x FCR31 0x%08x SR.FR %d fp_status 0x%08x(0x%02x)\n"", env->active_fpu.fcr0, env->active_fpu.fcr31, is_fpu64, env->active_fpu.fp_status, get_float_exception_flags(&env->active_fpu.fp_status)); for (i = 0; i < 32; (is_fpu64) ? i++ : (i += 2)) {
fpu_fprintf(f, ""%3s: "", fregnames[i]); printfpr(&env->active_fpu.fpr[i]); } #undef printfpr } ",qemu,0
"static void watch_mem_write(void *opaque, target_phys_addr_t addr,

                            uint64_t val, unsigned size)

{

    check_watchpoint(addr & ~TARGET_PAGE_MASK, ~(size - 1), BP_MEM_WRITE);

    switch (size) {

    case 1:

        stb_phys(addr, val);

        break;

    case 2:

        stw_phys(addr, val);

        break;

    case 4:

        stl_phys(addr, val);

        break;

    default: abort();

    }

}
","static void watch_mem_write(void *opaque, target_phys_addr_t addr, uint64_t val, unsigned size) {
check_watchpoint(addr & ~TARGET_PAGE_MASK, ~(size - 1), BP_MEM_WRITE); switch (size) {
case 1: stb_phys(addr, val); break; case 2: stw_phys(addr, val); break; case 4: stl_phys(addr, val); break; default: abort(); } } ",qemu,0
"static int coroutine_fn blkreplay_co_preadv(BlockDriverState *bs,

    uint64_t offset, uint64_t bytes, QEMUIOVector *qiov, int flags)

{

    uint64_t reqid = request_id++;

    int ret = bdrv_co_preadv(bs->file->bs, offset, bytes, qiov, flags);

    block_request_create(reqid, bs, qemu_coroutine_self());

    qemu_coroutine_yield();



    return ret;

}
","static int coroutine_fn blkreplay_co_preadv(BlockDriverState *bs, uint64_t offset, uint64_t bytes, QEMUIOVector *qiov, int flags) {
uint64_t reqid = request_id++; int ret = bdrv_co_preadv(bs->file->bs, offset, bytes, qiov, flags); block_request_create(reqid, bs, qemu_coroutine_self()); qemu_coroutine_yield(); return ret; } ",qemu,0
"static int protocol_client_auth(VncState *vs, uint8_t *data, size_t len)

{

    /* We only advertise 1 auth scheme at a time, so client

     * must pick the one we sent. Verify this */

    if (data[0] != vs->vd->auth) { /* Reject auth */

       VNC_DEBUG(""Reject auth %d\n"", (int)data[0]);

       vnc_write_u32(vs, 1);

       if (vs->minor >= 8) {

           static const char err[] = ""Authentication failed"";

           vnc_write_u32(vs, sizeof(err));

           vnc_write(vs, err, sizeof(err));

       }

       vnc_client_error(vs);

    } else { /* Accept requested auth */

       VNC_DEBUG(""Client requested auth %d\n"", (int)data[0]);

       switch (vs->vd->auth) {

       case VNC_AUTH_NONE:

           VNC_DEBUG(""Accept auth none\n"");

           if (vs->minor >= 8) {

               vnc_write_u32(vs, 0); /* Accept auth completion */

               vnc_flush(vs);

           }

           start_client_init(vs);

           break;



       case VNC_AUTH_VNC:

           VNC_DEBUG(""Start VNC auth\n"");

           start_auth_vnc(vs);

           break;



#ifdef CONFIG_VNC_TLS

       case VNC_AUTH_VENCRYPT:

           VNC_DEBUG(""Accept VeNCrypt auth\n"");;

           start_auth_vencrypt(vs);

           break;

#endif /* CONFIG_VNC_TLS */



#ifdef CONFIG_VNC_SASL

       case VNC_AUTH_SASL:

           VNC_DEBUG(""Accept SASL auth\n"");

           start_auth_sasl(vs);

           break;

#endif /* CONFIG_VNC_SASL */



       default: /* Should not be possible, but just in case */

           VNC_DEBUG(""Reject auth %d\n"", vs->vd->auth);

           vnc_write_u8(vs, 1);

           if (vs->minor >= 8) {

               static const char err[] = ""Authentication failed"";

               vnc_write_u32(vs, sizeof(err));

               vnc_write(vs, err, sizeof(err));

           }

           vnc_client_error(vs);

       }

    }

    return 0;

}
","static int protocol_client_auth(VncState *vs, uint8_t *data, size_t len) {
/* We only advertise 1 auth scheme at a time, so client * must pick the one we sent. Verify this */ if (data[0] != vs->vd->auth) {
/* Reject auth */ VNC_DEBUG(""Reject auth %d\n"", (int)data[0]); vnc_write_u32(vs, 1); if (vs->minor >= 8) {
static const char err[] = ""Authentication failed""; vnc_write_u32(vs, sizeof(err)); vnc_write(vs, err, sizeof(err)); } vnc_client_error(vs); } else {
/* Accept requested auth */ VNC_DEBUG(""Client requested auth %d\n"", (int)data[0]); switch (vs->vd->auth) {
case VNC_AUTH_NONE: VNC_DEBUG(""Accept auth none\n""); if (vs->minor >= 8) {
vnc_write_u32(vs, 0); /* Accept auth completion */ vnc_flush(vs); } start_client_init(vs); break; case VNC_AUTH_VNC: VNC_DEBUG(""Start VNC auth\n""); start_auth_vnc(vs); break; #ifdef CONFIG_VNC_TLS case VNC_AUTH_VENCRYPT: VNC_DEBUG(""Accept VeNCrypt auth\n"");; start_auth_vencrypt(vs); break; #endif /* CONFIG_VNC_TLS */ #ifdef CONFIG_VNC_SASL case VNC_AUTH_SASL: VNC_DEBUG(""Accept SASL auth\n""); start_auth_sasl(vs); break; #endif /* CONFIG_VNC_SASL */ default: /* Should not be possible, but just in case */ VNC_DEBUG(""Reject auth %d\n"", vs->vd->auth); vnc_write_u8(vs, 1); if (vs->minor >= 8) {
static const char err[] = ""Authentication failed""; vnc_write_u32(vs, sizeof(err)); vnc_write(vs, err, sizeof(err)); } vnc_client_error(vs); } } return 0; } ",qemu,0
"static void qcow_close(BlockDriverState *bs)

{

    BDRVQcowState *s = bs->opaque;



    g_free(s->l1_table);

    g_free(s->l2_cache);

    g_free(s->cluster_cache);

    g_free(s->cluster_data);



    migrate_del_blocker(s->migration_blocker);

    error_free(s->migration_blocker);

}
","static void qcow_close(BlockDriverState *bs) {
BDRVQcowState *s = bs->opaque; g_free(s->l1_table); g_free(s->l2_cache); g_free(s->cluster_cache); g_free(s->cluster_data); migrate_del_blocker(s->migration_blocker); error_free(s->migration_blocker); } ",qemu,1
"static int pit_initfn(ISADevice *dev)

{

    PITState *pit = DO_UPCAST(PITState, dev, dev);

    PITChannelState *s;



    s = &pit->channels[0];

    /* the timer 0 is connected to an IRQ */

    s->irq_timer = qemu_new_timer(vm_clock, pit_irq_timer, s);

    s->irq = isa_reserve_irq(pit->irq);



    register_ioport_write(pit->iobase, 4, 1, pit_ioport_write, pit);

    register_ioport_read(pit->iobase, 3, 1, pit_ioport_read, pit);

    isa_init_ioport(dev, pit->iobase);



    return 0;

}
","static int pit_initfn(ISADevice *dev) {
PITState *pit = DO_UPCAST(PITState, dev, dev); PITChannelState *s; s = &pit->channels[0]; /* the timer 0 is connected to an IRQ */ s->irq_timer = qemu_new_timer(vm_clock, pit_irq_timer, s); s->irq = isa_reserve_irq(pit->irq); register_ioport_write(pit->iobase, 4, 1, pit_ioport_write, pit); register_ioport_read(pit->iobase, 3, 1, pit_ioport_read, pit); isa_init_ioport(dev, pit->iobase); return 0; } ",qemu,0
"static int ssd0303_init(I2CSlave *i2c)

{

    ssd0303_state *s = FROM_I2C_SLAVE(ssd0303_state, i2c);



    s->con = graphic_console_init(ssd0303_update_display,

                                  ssd0303_invalidate_display,

                                  NULL, NULL, s);

    qemu_console_resize(s->con, 96 * MAGNIFY, 16 * MAGNIFY);

    return 0;

}
","static int ssd0303_init(I2CSlave *i2c) {
ssd0303_state *s = FROM_I2C_SLAVE(ssd0303_state, i2c); s->con = graphic_console_init(ssd0303_update_display, ssd0303_invalidate_display, NULL, NULL, s); qemu_console_resize(s->con, 96 * MAGNIFY, 16 * MAGNIFY); return 0; } ",qemu,0
"static void e1000_pre_save(void *opaque)

{

    E1000State *s = opaque;

    NetClientState *nc = qemu_get_queue(s->nic);



    /* If the mitigation timer is active, emulate a timeout now. */

    if (s->mit_timer_on) {

        e1000_mit_timer(s);

    }



    /*

     * If link is down and auto-negotiation is supported and ongoing,

     * complete auto-negotiation immediately. This allows us to look

     * at MII_SR_AUTONEG_COMPLETE to infer link status on load.

     */

    if (nc->link_down &&

        s->compat_flags & E1000_FLAG_AUTONEG &&

        s->phy_reg[PHY_CTRL] & MII_CR_AUTO_NEG_EN &&

        s->phy_reg[PHY_CTRL] & MII_CR_RESTART_AUTO_NEG) {

         s->phy_reg[PHY_STATUS] |= MII_SR_AUTONEG_COMPLETE;

    }

}
","static void e1000_pre_save(void *opaque) {
E1000State *s = opaque; NetClientState *nc = qemu_get_queue(s->nic); /* If the mitigation timer is active, emulate a timeout now. */ if (s->mit_timer_on) {
e1000_mit_timer(s); } /* * If link is down and auto-negotiation is supported and ongoing, * complete auto-negotiation immediately. This allows us to look * at MII_SR_AUTONEG_COMPLETE to infer link status on load. */ if (nc->link_down && s->compat_flags & E1000_FLAG_AUTONEG && s->phy_reg[PHY_CTRL] & MII_CR_AUTO_NEG_EN && s->phy_reg[PHY_CTRL] & MII_CR_RESTART_AUTO_NEG) {
s->phy_reg[PHY_STATUS] |= MII_SR_AUTONEG_COMPLETE; } } ",qemu,0
"int scsi_req_parse(SCSIRequest *req, uint8_t *buf)

{

    int rc;



    if (req->dev->type == TYPE_TAPE) {

        rc = scsi_req_stream_length(&req->cmd, req->dev, buf);

    } else {

        rc = scsi_req_length(&req->cmd, req->dev, buf);

    }

    if (rc != 0)

        return rc;



    assert(buf == req->cmd.buf);

    scsi_cmd_xfer_mode(&req->cmd);

    req->cmd.lba = scsi_cmd_lba(&req->cmd);

    trace_scsi_req_parsed(req->dev->id, req->lun, req->tag, buf[0],

                          req->cmd.mode, req->cmd.xfer);

    if (req->cmd.lba != -1) {

        trace_scsi_req_parsed_lba(req->dev->id, req->lun, req->tag, buf[0],

                              req->cmd.lba);

    }

    return 0;

}
","int scsi_req_parse(SCSIRequest *req, uint8_t *buf) {
int rc; if (req->dev->type == TYPE_TAPE) {
rc = scsi_req_stream_length(&req->cmd, req->dev, buf); } else {
rc = scsi_req_length(&req->cmd, req->dev, buf); } if (rc != 0) return rc; assert(buf == req->cmd.buf); scsi_cmd_xfer_mode(&req->cmd); req->cmd.lba = scsi_cmd_lba(&req->cmd); trace_scsi_req_parsed(req->dev->id, req->lun, req->tag, buf[0], req->cmd.mode, req->cmd.xfer); if (req->cmd.lba != -1) {
trace_scsi_req_parsed_lba(req->dev->id, req->lun, req->tag, buf[0], req->cmd.lba); } return 0; } ",qemu,0
"static void dec_mul(DisasContext *dc)

{

    if (dc->format == OP_FMT_RI) {

        LOG_DIS(""muli r%d, r%d, %d\n"", dc->r0, dc->r1,

                sign_extend(dc->imm16, 16));

    } else {

        LOG_DIS(""mul r%d, r%d, r%d\n"", dc->r2, dc->r0, dc->r1);

    }



    if (!(dc->env->features & LM32_FEATURE_MULTIPLY)) {

        cpu_abort(dc->env, ""hardware multiplier is not available\n"");

    }



    if (dc->format == OP_FMT_RI) {

        tcg_gen_muli_tl(cpu_R[dc->r1], cpu_R[dc->r0],

                sign_extend(dc->imm16, 16));

    } else {

        tcg_gen_mul_tl(cpu_R[dc->r2], cpu_R[dc->r0], cpu_R[dc->r1]);

    }

}
","static void dec_mul(DisasContext *dc) {
if (dc->format == OP_FMT_RI) {
LOG_DIS(""muli r%d, r%d, %d\n"", dc->r0, dc->r1, sign_extend(dc->imm16, 16)); } else {
LOG_DIS(""mul r%d, r%d, r%d\n"", dc->r2, dc->r0, dc->r1); } if (!(dc->env->features & LM32_FEATURE_MULTIPLY)) {
cpu_abort(dc->env, ""hardware multiplier is not available\n""); } if (dc->format == OP_FMT_RI) {
tcg_gen_muli_tl(cpu_R[dc->r1], cpu_R[dc->r0], sign_extend(dc->imm16, 16)); } else {
tcg_gen_mul_tl(cpu_R[dc->r2], cpu_R[dc->r0], cpu_R[dc->r1]); } } ",qemu,1
"static hwaddr ppc_hash64_pteg_search(PowerPCCPU *cpu, hwaddr hash,

                                     ppc_slb_t *slb, bool secondary,

                                     target_ulong ptem, ppc_hash_pte64_t *pte)

{

    CPUPPCState *env = &cpu->env;

    int i;

    uint64_t token;

    target_ulong pte0, pte1;

    target_ulong pte_index;



    pte_index = (hash & env->htab_mask) * HPTES_PER_GROUP;

    token = ppc_hash64_start_access(cpu, pte_index);

    if (!token) {

        return -1;

    }

    for (i = 0; i < HPTES_PER_GROUP; i++) {

        pte0 = ppc_hash64_load_hpte0(cpu, token, i);

        pte1 = ppc_hash64_load_hpte1(cpu, token, i);



        if ((pte0 & HPTE64_V_VALID)

            && (secondary == !!(pte0 & HPTE64_V_SECONDARY))

            && HPTE64_V_COMPARE(pte0, ptem)) {

            unsigned pshift = hpte_page_shift(slb->sps, pte0, pte1);

            /*

             * If there is no match, ignore the PTE, it could simply

             * be for a different segment size encoding and the

             * architecture specifies we should not match. Linux will

             * potentially leave behind PTEs for the wrong base page

             * size when demoting segments.

             */

            if (pshift == 0) {

                continue;

            }

            /* We don't do anything with pshift yet as qemu TLB only deals

             * with 4K pages anyway

             */

            pte->pte0 = pte0;

            pte->pte1 = pte1;

            ppc_hash64_stop_access(cpu, token);

            return (pte_index + i) * HASH_PTE_SIZE_64;

        }

    }

    ppc_hash64_stop_access(cpu, token);

    /*

     * We didn't find a valid entry.

     */

    return -1;

}
","static hwaddr ppc_hash64_pteg_search(PowerPCCPU *cpu, hwaddr hash, ppc_slb_t *slb, bool secondary, target_ulong ptem, ppc_hash_pte64_t *pte) {
CPUPPCState *env = &cpu->env; int i; uint64_t token; target_ulong pte0, pte1; target_ulong pte_index; pte_index = (hash & env->htab_mask) * HPTES_PER_GROUP; token = ppc_hash64_start_access(cpu, pte_index); if (!token) {
return -1; } for (i = 0; i < HPTES_PER_GROUP; i++) {
pte0 = ppc_hash64_load_hpte0(cpu, token, i); pte1 = ppc_hash64_load_hpte1(cpu, token, i); if ((pte0 & HPTE64_V_VALID) && (secondary == !!(pte0 & HPTE64_V_SECONDARY)) && HPTE64_V_COMPARE(pte0, ptem)) {
unsigned pshift = hpte_page_shift(slb->sps, pte0, pte1); /* * If there is no match, ignore the PTE, it could simply * be for a different segment size encoding and the * architecture specifies we should not match. Linux will * potentially leave behind PTEs for the wrong base page * size when demoting segments. */ if (pshift == 0) {
continue; } /* We don't do anything with pshift yet as qemu TLB only deals * with 4K pages anyway */ pte->pte0 = pte0; pte->pte1 = pte1; ppc_hash64_stop_access(cpu, token); return (pte_index + i) * HASH_PTE_SIZE_64; } } ppc_hash64_stop_access(cpu, token); /* * We didn't find a valid entry. */ return -1; } ",qemu,1
"static int virtio_net_device_exit(DeviceState *qdev)

{

    VirtIONet *n = VIRTIO_NET(qdev);

    VirtIODevice *vdev = VIRTIO_DEVICE(qdev);

    int i;



    /* This will stop vhost backend if appropriate. */

    virtio_net_set_status(vdev, 0);



    unregister_savevm(qdev, ""virtio-net"", n);



    if (n->netclient_name) {

        g_free(n->netclient_name);

        n->netclient_name = NULL;

    }

    if (n->netclient_type) {

        g_free(n->netclient_type);

        n->netclient_type = NULL;

    }



    g_free(n->mac_table.macs);

    g_free(n->vlans);



    for (i = 0; i < n->max_queues; i++) {

        VirtIONetQueue *q = &n->vqs[i];

        NetClientState *nc = qemu_get_subqueue(n->nic, i);



        qemu_purge_queued_packets(nc);



        if (q->tx_timer) {

            timer_del(q->tx_timer);

            timer_free(q->tx_timer);

        } else if (q->tx_bh) {

            qemu_bh_delete(q->tx_bh);

        }

    }



    g_free(n->vqs);

    qemu_del_nic(n->nic);

    virtio_cleanup(vdev);



    return 0;

}
","static int virtio_net_device_exit(DeviceState *qdev) {
VirtIONet *n = VIRTIO_NET(qdev); VirtIODevice *vdev = VIRTIO_DEVICE(qdev); int i; /* This will stop vhost backend if appropriate. */ virtio_net_set_status(vdev, 0); unregister_savevm(qdev, ""virtio-net"", n); if (n->netclient_name) {
g_free(n->netclient_name); n->netclient_name = NULL; } if (n->netclient_type) {
g_free(n->netclient_type); n->netclient_type = NULL; } g_free(n->mac_table.macs); g_free(n->vlans); for (i = 0; i < n->max_queues; i++) {
VirtIONetQueue *q = &n->vqs[i]; NetClientState *nc = qemu_get_subqueue(n->nic, i); qemu_purge_queued_packets(nc); if (q->tx_timer) {
timer_del(q->tx_timer); timer_free(q->tx_timer); } else if (q->tx_bh) {
qemu_bh_delete(q->tx_bh); } } g_free(n->vqs); qemu_del_nic(n->nic); virtio_cleanup(vdev); return 0; } ",qemu,1
"static void spr_write_dbatl (void *opaque, int sprn)

{

    DisasContext *ctx = opaque;



    gen_op_store_dbatl((sprn - SPR_DBAT0L) / 2);

    RET_STOP(ctx);

}
","static void spr_write_dbatl (void *opaque, int sprn) {
DisasContext *ctx = opaque; gen_op_store_dbatl((sprn - SPR_DBAT0L) / 2); RET_STOP(ctx); } ",qemu,0
"static int do_cont(Monitor *mon, const QDict *qdict, QObject **ret_data)

{

    struct bdrv_iterate_context context = { mon, 0 };







    bdrv_iterate(encrypted_bdrv_it, &context);

    /* only resume the vm if all keys are set and valid */

    if (!context.err) {

        vm_start();

        return 0;

    } else {


","static int do_cont(Monitor *mon, const QDict *qdict, QObject **ret_data) {
struct bdrv_iterate_context context = {
mon, 0 }; bdrv_iterate(encrypted_bdrv_it, &context); /* only resume the vm if all keys are set and valid */ if (!context.err) {
vm_start(); return 0; } else {
",qemu,1
"void timerlist_notify(QEMUTimerList *timer_list)

{

    if (timer_list->notify_cb) {

        timer_list->notify_cb(timer_list->notify_opaque);

    } else {

        qemu_notify_event();

    }

}
","void timerlist_notify(QEMUTimerList *timer_list) {
if (timer_list->notify_cb) {
timer_list->notify_cb(timer_list->notify_opaque); } else {
qemu_notify_event(); } } ",qemu,0
"int pci_add_capability(PCIDevice *pdev, uint8_t cap_id,

                       uint8_t offset, uint8_t size)

{

    int ret;

    Error *local_err = NULL;



    ret = pci_add_capability2(pdev, cap_id, offset, size, &local_err);

    if (local_err) {

        assert(ret < 0);

        error_report_err(local_err);

    } else {

        /* success implies a positive offset in config space */

        assert(ret > 0);

    }

    return ret;

}
","int pci_add_capability(PCIDevice *pdev, uint8_t cap_id, uint8_t offset, uint8_t size) {
int ret; Error *local_err = NULL; ret = pci_add_capability2(pdev, cap_id, offset, size, &local_err); if (local_err) {
assert(ret < 0); error_report_err(local_err); } else {
/* success implies a positive offset in config space */ assert(ret > 0); } return ret; } ",qemu,1
"bool throttle_is_valid(ThrottleConfig *cfg, Error **errp)
{
    int i;
    bool bps_flag, ops_flag;
    bool bps_max_flag, ops_max_flag;
    bps_flag = cfg->buckets[THROTTLE_BPS_TOTAL].avg &&
               (cfg->buckets[THROTTLE_BPS_READ].avg ||
                cfg->buckets[THROTTLE_BPS_WRITE].avg);
    ops_flag = cfg->buckets[THROTTLE_OPS_TOTAL].avg &&
               (cfg->buckets[THROTTLE_OPS_READ].avg ||
                cfg->buckets[THROTTLE_OPS_WRITE].avg);
    bps_max_flag = cfg->buckets[THROTTLE_BPS_TOTAL].max &&
                  (cfg->buckets[THROTTLE_BPS_READ].max  ||
                   cfg->buckets[THROTTLE_BPS_WRITE].max);
    ops_max_flag = cfg->buckets[THROTTLE_OPS_TOTAL].max &&
                   (cfg->buckets[THROTTLE_OPS_READ].max ||
                   cfg->buckets[THROTTLE_OPS_WRITE].max);
    if (bps_flag || ops_flag || bps_max_flag || ops_max_flag) {
        error_setg(errp, ""bps/iops/max total values and read/write values""
                   "" cannot be used at the same time"");
    if (cfg->op_size &&
        !cfg->buckets[THROTTLE_OPS_TOTAL].avg &&
        !cfg->buckets[THROTTLE_OPS_READ].avg &&
        !cfg->buckets[THROTTLE_OPS_WRITE].avg) {
        error_setg(errp, ""iops size requires an iops value to be set"");
    for (i = 0; i < BUCKETS_COUNT; i++) {
        LeakyBucket *bkt = &cfg->buckets[i];
        if (bkt->avg > THROTTLE_VALUE_MAX || bkt->max > THROTTLE_VALUE_MAX) {
            error_setg(errp, ""bps/iops/max values must be within [0, %lld]"",
                       THROTTLE_VALUE_MAX);
        if (!bkt->burst_length) {
            error_setg(errp, ""the burst length cannot be 0"");
        if (bkt->burst_length > 1 && !bkt->max) {
            error_setg(errp, ""burst length set without burst rate"");
        if (bkt->max && !bkt->avg) {
            error_setg(errp, ""bps_max/iops_max require corresponding""
                       "" bps/iops values"");
        if (bkt->max && bkt->max < bkt->avg) {
            error_setg(errp, ""bps_max/iops_max cannot be lower than bps/iops"");
    return true;","bool throttle_is_valid(ThrottleConfig *cfg, Error **errp) {
int i; bool bps_flag, ops_flag; bool bps_max_flag, ops_max_flag; bps_flag = cfg->buckets[THROTTLE_BPS_TOTAL].avg && (cfg->buckets[THROTTLE_BPS_READ].avg || cfg->buckets[THROTTLE_BPS_WRITE].avg); ops_flag = cfg->buckets[THROTTLE_OPS_TOTAL].avg && (cfg->buckets[THROTTLE_OPS_READ].avg || cfg->buckets[THROTTLE_OPS_WRITE].avg); bps_max_flag = cfg->buckets[THROTTLE_BPS_TOTAL].max && (cfg->buckets[THROTTLE_BPS_READ].max || cfg->buckets[THROTTLE_BPS_WRITE].max); ops_max_flag = cfg->buckets[THROTTLE_OPS_TOTAL].max && (cfg->buckets[THROTTLE_OPS_READ].max || cfg->buckets[THROTTLE_OPS_WRITE].max); if (bps_flag || ops_flag || bps_max_flag || ops_max_flag) {
error_setg(errp, ""bps/iops/max total values and read/write values"" "" cannot be used at the same time""); if (cfg->op_size && !cfg->buckets[THROTTLE_OPS_TOTAL].avg && !cfg->buckets[THROTTLE_OPS_READ].avg && !cfg->buckets[THROTTLE_OPS_WRITE].avg) {
error_setg(errp, ""iops size requires an iops value to be set""); for (i = 0; i < BUCKETS_COUNT; i++) {
LeakyBucket *bkt = &cfg->buckets[i]; if (bkt->avg > THROTTLE_VALUE_MAX || bkt->max > THROTTLE_VALUE_MAX) {
error_setg(errp, ""bps/iops/max values must be within [0, %lld]"", THROTTLE_VALUE_MAX); if (!bkt->burst_length) {
error_setg(errp, ""the burst length cannot be 0""); if (bkt->burst_length > 1 && !bkt->max) {
error_setg(errp, ""burst length set without burst rate""); if (bkt->max && !bkt->avg) {
error_setg(errp, ""bps_max/iops_max require corresponding"" "" bps/iops values""); if (bkt->max && bkt->max < bkt->avg) {
error_setg(errp, ""bps_max/iops_max cannot be lower than bps/iops""); return true;",qemu,1
"static void exynos4210_pwm_write(void *opaque, target_phys_addr_t offset,

        uint64_t value, unsigned size)

{

    Exynos4210PWMState *s = (Exynos4210PWMState *)opaque;

    int index;

    uint32_t new_val;

    int i;



    switch (offset) {

    case TCFG0: case TCFG1:

        index = (offset - TCFG0) >> 2;

        s->reg_tcfg[index] = value;



        /* update timers frequencies */

        for (i = 0; i < EXYNOS4210_PWM_TIMERS_NUM; i++) {

            exynos4210_pwm_update_freq(s, s->timer[i].id);

        }

        break;



    case TCON:

        for (i = 0; i < EXYNOS4210_PWM_TIMERS_NUM; i++) {

            if ((value & TCON_TIMER_MANUAL_UPD(i)) >

            (s->reg_tcon & TCON_TIMER_MANUAL_UPD(i))) {

                /*

                 * TCNTB and TCMPB are loaded into TCNT and TCMP.

                 * Update timers.

                 */



                /* this will start timer to run, this ok, because

                 * during processing start bit timer will be stopped

                 * if needed */

                ptimer_set_count(s->timer[i].ptimer, s->timer[i].reg_tcntb);

                DPRINTF(""set timer %d count to %x\n"", i,

                        s->timer[i].reg_tcntb);

            }



            if ((value & TCON_TIMER_START(i)) >

            (s->reg_tcon & TCON_TIMER_START(i))) {

                /* changed to start */

                ptimer_run(s->timer[i].ptimer, 1);

                DPRINTF(""run timer %d\n"", i);

            }



            if ((value & TCON_TIMER_START(i)) <

                    (s->reg_tcon & TCON_TIMER_START(i))) {

                /* changed to stop */

                ptimer_stop(s->timer[i].ptimer);

                DPRINTF(""stop timer %d\n"", i);

            }

        }

        s->reg_tcon = value;

        break;



    case TCNTB0: case TCNTB1:

    case TCNTB2: case TCNTB3: case TCNTB4:

        index = (offset - TCNTB0) / 0xC;

        s->timer[index].reg_tcntb = value;

        break;



    case TCMPB0: case TCMPB1:

    case TCMPB2: case TCMPB3:

        index = (offset - TCMPB0) / 0xC;

        s->timer[index].reg_tcmpb = value;

        break;



    case TINT_CSTAT:

        new_val = (s->reg_tint_cstat & 0x3E0) + (0x1F & value);

        new_val &= ~(0x3E0 & value);



        for (i = 0; i < EXYNOS4210_PWM_TIMERS_NUM; i++) {

            if ((new_val & TINT_CSTAT_STATUS(i)) <

                    (s->reg_tint_cstat & TINT_CSTAT_STATUS(i))) {

                qemu_irq_lower(s->timer[i].irq);

            }

        }



        s->reg_tint_cstat = new_val;

        break;



    default:

        fprintf(stderr,

                ""[exynos4210.pwm: bad write offset "" TARGET_FMT_plx ""]\n"",

                offset);

        break;



    }

}
","static void exynos4210_pwm_write(void *opaque, target_phys_addr_t offset, uint64_t value, unsigned size) {
Exynos4210PWMState *s = (Exynos4210PWMState *)opaque; int index; uint32_t new_val; int i; switch (offset) {
case TCFG0: case TCFG1: index = (offset - TCFG0) >> 2; s->reg_tcfg[index] = value; /* update timers frequencies */ for (i = 0; i < EXYNOS4210_PWM_TIMERS_NUM; i++) {
exynos4210_pwm_update_freq(s, s->timer[i].id); } break; case TCON: for (i = 0; i < EXYNOS4210_PWM_TIMERS_NUM; i++) {
if ((value & TCON_TIMER_MANUAL_UPD(i)) > (s->reg_tcon & TCON_TIMER_MANUAL_UPD(i))) {
/* * TCNTB and TCMPB are loaded into TCNT and TCMP. * Update timers. */ /* this will start timer to run, this ok, because * during processing start bit timer will be stopped * if needed */ ptimer_set_count(s->timer[i].ptimer, s->timer[i].reg_tcntb); DPRINTF(""set timer %d count to %x\n"", i, s->timer[i].reg_tcntb); } if ((value & TCON_TIMER_START(i)) > (s->reg_tcon & TCON_TIMER_START(i))) {
/* changed to start */ ptimer_run(s->timer[i].ptimer, 1); DPRINTF(""run timer %d\n"", i); } if ((value & TCON_TIMER_START(i)) < (s->reg_tcon & TCON_TIMER_START(i))) {
/* changed to stop */ ptimer_stop(s->timer[i].ptimer); DPRINTF(""stop timer %d\n"", i); } } s->reg_tcon = value; break; case TCNTB0: case TCNTB1: case TCNTB2: case TCNTB3: case TCNTB4: index = (offset - TCNTB0) / 0xC; s->timer[index].reg_tcntb = value; break; case TCMPB0: case TCMPB1: case TCMPB2: case TCMPB3: index = (offset - TCMPB0) / 0xC; s->timer[index].reg_tcmpb = value; break; case TINT_CSTAT: new_val = (s->reg_tint_cstat & 0x3E0) + (0x1F & value); new_val &= ~(0x3E0 & value); for (i = 0; i < EXYNOS4210_PWM_TIMERS_NUM; i++) {
if ((new_val & TINT_CSTAT_STATUS(i)) < (s->reg_tint_cstat & TINT_CSTAT_STATUS(i))) {
qemu_irq_lower(s->timer[i].irq); } } s->reg_tint_cstat = new_val; break; default: fprintf(stderr, ""[exynos4210.pwm: bad write offset "" TARGET_FMT_plx ""]\n"", offset); break; } } ",qemu,0
"static void free_note_info(struct elf_note_info *info)

{

    struct elf_thread_status *ets;



    while (!TAILQ_EMPTY(&info->thread_list)) {

        ets = TAILQ_FIRST(&info->thread_list);

        TAILQ_REMOVE(&info->thread_list, ets, ets_link);

        qemu_free(ets);

    }



    qemu_free(info->prstatus);

    qemu_free(info->psinfo);

    qemu_free(info->notes);

}
","static void free_note_info(struct elf_note_info *info) {
struct elf_thread_status *ets; while (!TAILQ_EMPTY(&info->thread_list)) {
ets = TAILQ_FIRST(&info->thread_list); TAILQ_REMOVE(&info->thread_list, ets, ets_link); qemu_free(ets); } qemu_free(info->prstatus); qemu_free(info->psinfo); qemu_free(info->notes); } ",qemu,0
"static void blk_mig_cleanup(void)

{

    BlkMigDevState *bmds;

    BlkMigBlock *blk;



    bdrv_drain_all();



    unset_dirty_tracking();



    blk_mig_lock();

    while ((bmds = QSIMPLEQ_FIRST(&block_mig_state.bmds_list)) != NULL) {

        QSIMPLEQ_REMOVE_HEAD(&block_mig_state.bmds_list, entry);

        bdrv_op_unblock_all(bmds->bs, bmds->blocker);

        error_free(bmds->blocker);

        bdrv_unref(bmds->bs);

        g_free(bmds->aio_bitmap);

        g_free(bmds);

    }



    while ((blk = QSIMPLEQ_FIRST(&block_mig_state.blk_list)) != NULL) {

        QSIMPLEQ_REMOVE_HEAD(&block_mig_state.blk_list, entry);

        g_free(blk->buf);

        g_free(blk);

    }

    blk_mig_unlock();

}
","static void blk_mig_cleanup(void) {
BlkMigDevState *bmds; BlkMigBlock *blk; bdrv_drain_all(); unset_dirty_tracking(); blk_mig_lock(); while ((bmds = QSIMPLEQ_FIRST(&block_mig_state.bmds_list)) != NULL) {
QSIMPLEQ_REMOVE_HEAD(&block_mig_state.bmds_list, entry); bdrv_op_unblock_all(bmds->bs, bmds->blocker); error_free(bmds->blocker); bdrv_unref(bmds->bs); g_free(bmds->aio_bitmap); g_free(bmds); } while ((blk = QSIMPLEQ_FIRST(&block_mig_state.blk_list)) != NULL) {
QSIMPLEQ_REMOVE_HEAD(&block_mig_state.blk_list, entry); g_free(blk->buf); g_free(blk); } blk_mig_unlock(); } ",qemu,1
"static void sys_write(void *opaque, hwaddr addr,

                      uint64_t value, unsigned size)

{

    LM32SysState *s = opaque;

    char *testname;



    trace_lm32_sys_memory_write(addr, value);



    addr >>= 2;

    switch (addr) {

    case R_CTRL:

        qemu_system_shutdown_request();

        break;

    case R_PASSFAIL:

        s->regs[addr] = value;

        testname = (char *)s->testname;

        qemu_log(""TC  %-16s %s\n"", testname, (value) ? ""FAILED"" : ""OK"");

        break;

    case R_TESTNAME:

        s->regs[addr] = value;

        copy_testname(s);

        break;



    default:

        error_report(""lm32_sys: write access to unknown register 0x""

                TARGET_FMT_plx, addr << 2);

        break;

    }

}
","static void sys_write(void *opaque, hwaddr addr, uint64_t value, unsigned size) {
LM32SysState *s = opaque; char *testname; trace_lm32_sys_memory_write(addr, value); addr >>= 2; switch (addr) {
case R_CTRL: qemu_system_shutdown_request(); break; case R_PASSFAIL: s->regs[addr] = value; testname = (char *)s->testname; qemu_log(""TC %-16s %s\n"", testname, (value) ? ""FAILED"" : ""OK""); break; case R_TESTNAME: s->regs[addr] = value; copy_testname(s); break; default: error_report(""lm32_sys: write access to unknown register 0x"" TARGET_FMT_plx, addr << 2); break; } } ",qemu,0
"uint64_t helper_fmul(CPUPPCState *env, uint64_t arg1, uint64_t arg2)

{

    CPU_DoubleU farg1, farg2;



    farg1.ll = arg1;

    farg2.ll = arg2;



    if (unlikely((float64_is_infinity(farg1.d) && float64_is_zero(farg2.d)) ||

                 (float64_is_zero(farg1.d) && float64_is_infinity(farg2.d)))) {

        /* Multiplication of zero by infinity */

        farg1.ll = fload_invalid_op_excp(env, POWERPC_EXCP_FP_VXIMZ);

    } else {

        if (unlikely(float64_is_signaling_nan(farg1.d) ||

                     float64_is_signaling_nan(farg2.d))) {

            /* sNaN multiplication */

            fload_invalid_op_excp(env, POWERPC_EXCP_FP_VXSNAN);

        }

        farg1.d = float64_mul(farg1.d, farg2.d, &env->fp_status);

    }



    return farg1.ll;

}
","uint64_t helper_fmul(CPUPPCState *env, uint64_t arg1, uint64_t arg2) {
CPU_DoubleU farg1, farg2; farg1.ll = arg1; farg2.ll = arg2; if (unlikely((float64_is_infinity(farg1.d) && float64_is_zero(farg2.d)) || (float64_is_zero(farg1.d) && float64_is_infinity(farg2.d)))) {
/* Multiplication of zero by infinity */ farg1.ll = fload_invalid_op_excp(env, POWERPC_EXCP_FP_VXIMZ); } else {
if (unlikely(float64_is_signaling_nan(farg1.d) || float64_is_signaling_nan(farg2.d))) {
/* sNaN multiplication */ fload_invalid_op_excp(env, POWERPC_EXCP_FP_VXSNAN); } farg1.d = float64_mul(farg1.d, farg2.d, &env->fp_status); } return farg1.ll; } ",qemu,0
"static void slavio_timer_get_out(SLAVIO_TIMERState *s)

{

    uint64_t count;



    count = s->limit - PERIODS_TO_LIMIT(ptimer_get_count(s->timer));

    DPRINTF(""get_out: limit %"" PRIx64 "" count %x%08x\n"", s->limit,

            s->counthigh, s->count);

    s->count = count & TIMER_COUNT_MASK32;

    s->counthigh = count >> 32;

}
","static void slavio_timer_get_out(SLAVIO_TIMERState *s) {
uint64_t count; count = s->limit - PERIODS_TO_LIMIT(ptimer_get_count(s->timer)); DPRINTF(""get_out: limit %"" PRIx64 "" count %x%08x\n"", s->limit, s->counthigh, s->count); s->count = count & TIMER_COUNT_MASK32; s->counthigh = count >> 32; } ",qemu,0
"void replay_shutdown_request(void)

{

    if (replay_mode == REPLAY_MODE_RECORD) {

        replay_mutex_lock();

        replay_put_event(EVENT_SHUTDOWN);

        replay_mutex_unlock();

    }

}
","void replay_shutdown_request(void) {
if (replay_mode == REPLAY_MODE_RECORD) {
replay_mutex_lock(); replay_put_event(EVENT_SHUTDOWN); replay_mutex_unlock(); } } ",qemu,0
"static int qcow2_set_key(BlockDriverState *bs, const char *key)

{

    BDRVQcow2State *s = bs->opaque;

    uint8_t keybuf[16];

    int len, i;

    Error *err = NULL;



    memset(keybuf, 0, 16);

    len = strlen(key);

    if (len > 16)

        len = 16;

    /* XXX: we could compress the chars to 7 bits to increase

       entropy */

    for(i = 0;i < len;i++) {

        keybuf[i] = key[i];

    }

    assert(bs->encrypted);



    qcrypto_cipher_free(s->cipher);

    s->cipher = qcrypto_cipher_new(

        QCRYPTO_CIPHER_ALG_AES_128,

        QCRYPTO_CIPHER_MODE_CBC,

        keybuf, G_N_ELEMENTS(keybuf),

        &err);



    if (!s->cipher) {

        /* XXX would be nice if errors in this method could

         * be properly propagate to the caller. Would need

         * the bdrv_set_key() API signature to be fixed. */

        error_free(err);

        return -1;

    }

    return 0;

}
","static int qcow2_set_key(BlockDriverState *bs, const char *key) {
BDRVQcow2State *s = bs->opaque; uint8_t keybuf[16]; int len, i; Error *err = NULL; memset(keybuf, 0, 16); len = strlen(key); if (len > 16) len = 16; /* XXX: we could compress the chars to 7 bits to increase entropy */ for(i = 0;i < len;i++) {
keybuf[i] = key[i]; } assert(bs->encrypted); qcrypto_cipher_free(s->cipher); s->cipher = qcrypto_cipher_new( QCRYPTO_CIPHER_ALG_AES_128, QCRYPTO_CIPHER_MODE_CBC, keybuf, G_N_ELEMENTS(keybuf), &err); if (!s->cipher) {
/* XXX would be nice if errors in this method could * be properly propagate to the caller. Would need * the bdrv_set_key() API signature to be fixed. */ error_free(err); return -1; } return 0; } ",qemu,0
"static int nbd_handle_reply_err(uint32_t opt, uint32_t type, Error **errp)

{

    if (!(type & (1 << 31))) {

        return 0;

    }



    switch (type) {

    case NBD_REP_ERR_UNSUP:

        error_setg(errp, ""Unsupported option type %x"", opt);

        break;



    case NBD_REP_ERR_POLICY:

        error_setg(errp, ""Denied by server for option %x"", opt);

        break;



    case NBD_REP_ERR_INVALID:

        error_setg(errp, ""Invalid data length for option %x"", opt);

        break;



    case NBD_REP_ERR_TLS_REQD:

        error_setg(errp, ""TLS negotiation required before option %x"", opt);

        break;



    default:

        error_setg(errp, ""Unknown error code when asking for option %x"", opt);

        break;

    }



    return -1;

}
","static int nbd_handle_reply_err(uint32_t opt, uint32_t type, Error **errp) {
if (!(type & (1 << 31))) {
return 0; } switch (type) {
case NBD_REP_ERR_UNSUP: error_setg(errp, ""Unsupported option type %x"", opt); break; case NBD_REP_ERR_POLICY: error_setg(errp, ""Denied by server for option %x"", opt); break; case NBD_REP_ERR_INVALID: error_setg(errp, ""Invalid data length for option %x"", opt); break; case NBD_REP_ERR_TLS_REQD: error_setg(errp, ""TLS negotiation required before option %x"", opt); break; default: error_setg(errp, ""Unknown error code when asking for option %x"", opt); break; } return -1; } ",qemu,1
"static void vapic_write(void *opaque, target_phys_addr_t addr, uint64_t data,

                        unsigned int size)

{

    CPUX86State *env = cpu_single_env;

    target_phys_addr_t rom_paddr;

    VAPICROMState *s = opaque;



    cpu_synchronize_state(env);



    /*

     * The VAPIC supports two PIO-based hypercalls, both via port 0x7E.

     *  o 16-bit write access:

     *    Reports the option ROM initialization to the hypervisor. Written

     *    value is the offset of the state structure in the ROM.

     *  o 8-bit write access:

     *    Reactivates the VAPIC after a guest hibernation, i.e. after the

     *    option ROM content has been re-initialized by a guest power cycle.

     *  o 32-bit write access:

     *    Poll for pending IRQs, considering the current VAPIC state.

     */

    switch (size) {

    case 2:

        if (s->state == VAPIC_INACTIVE) {

            rom_paddr = (env->segs[R_CS].base + env->eip) & ROM_BLOCK_MASK;

            s->rom_state_paddr = rom_paddr + data;



            s->state = VAPIC_STANDBY;

        }

        if (vapic_prepare(s) < 0) {

            s->state = VAPIC_INACTIVE;

            break;

        }

        break;

    case 1:

        if (kvm_enabled()) {

            /*

             * Disable triggering instruction in ROM by writing a NOP.

             *

             * We cannot do this in TCG mode as the reported IP is not

             * accurate.

             */

            pause_all_vcpus();

            patch_byte(env, env->eip - 2, 0x66);

            patch_byte(env, env->eip - 1, 0x90);

            resume_all_vcpus();

        }



        if (s->state == VAPIC_ACTIVE) {

            break;

        }

        if (update_rom_mapping(s, env, env->eip) < 0) {

            break;

        }

        if (find_real_tpr_addr(s, env) < 0) {

            break;

        }

        vapic_enable(s, env);

        break;

    default:

    case 4:

        if (!kvm_irqchip_in_kernel()) {

            apic_poll_irq(env->apic_state);

        }

        break;

    }

}
","static void vapic_write(void *opaque, target_phys_addr_t addr, uint64_t data, unsigned int size) {
CPUX86State *env = cpu_single_env; target_phys_addr_t rom_paddr; VAPICROMState *s = opaque; cpu_synchronize_state(env); /* * The VAPIC supports two PIO-based hypercalls, both via port 0x7E. * o 16-bit write access: * Reports the option ROM initialization to the hypervisor. Written * value is the offset of the state structure in the ROM. * o 8-bit write access: * Reactivates the VAPIC after a guest hibernation, i.e. after the * option ROM content has been re-initialized by a guest power cycle. * o 32-bit write access: * Poll for pending IRQs, considering the current VAPIC state. */ switch (size) {
case 2: if (s->state == VAPIC_INACTIVE) {
rom_paddr = (env->segs[R_CS].base + env->eip) & ROM_BLOCK_MASK; s->rom_state_paddr = rom_paddr + data; s->state = VAPIC_STANDBY; } if (vapic_prepare(s) < 0) {
s->state = VAPIC_INACTIVE; break; } break; case 1: if (kvm_enabled()) {
/* * Disable triggering instruction in ROM by writing a NOP. * * We cannot do this in TCG mode as the reported IP is not * accurate. */ pause_all_vcpus(); patch_byte(env, env->eip - 2, 0x66); patch_byte(env, env->eip - 1, 0x90); resume_all_vcpus(); } if (s->state == VAPIC_ACTIVE) {
break; } if (update_rom_mapping(s, env, env->eip) < 0) {
break; } if (find_real_tpr_addr(s, env) < 0) {
break; } vapic_enable(s, env); break; default: case 4: if (!kvm_irqchip_in_kernel()) {
apic_poll_irq(env->apic_state); } break; } } ",qemu,0
"static int qcow_create2(const char *filename, int64_t total_size,

                        const char *backing_file, const char *backing_format,

                        int flags, size_t cluster_size, int prealloc)

{



    int fd, header_size, backing_filename_len, l1_size, i, shift, l2_bits;

    int ref_clusters, backing_format_len = 0;

    int rounded_ext_bf_len = 0;

    QCowHeader header;

    uint64_t tmp, offset;

    QCowCreateState s1, *s = &s1;

    QCowExtension ext_bf = {0, 0};

    int ret;



    memset(s, 0, sizeof(*s));



    fd = open(filename, O_WRONLY | O_CREAT | O_TRUNC | O_BINARY, 0644);

    if (fd < 0)

        return -1;

    memset(&header, 0, sizeof(header));

    header.magic = cpu_to_be32(QCOW_MAGIC);

    header.version = cpu_to_be32(QCOW_VERSION);

    header.size = cpu_to_be64(total_size * 512);

    header_size = sizeof(header);

    backing_filename_len = 0;

    if (backing_file) {

        if (backing_format) {

            ext_bf.magic = QCOW_EXT_MAGIC_BACKING_FORMAT;

            backing_format_len = strlen(backing_format);

            ext_bf.len = backing_format_len;

            rounded_ext_bf_len = (sizeof(ext_bf) + ext_bf.len + 7) & ~7;

            header_size += rounded_ext_bf_len;

        }

        header.backing_file_offset = cpu_to_be64(header_size);

        backing_filename_len = strlen(backing_file);

        header.backing_file_size = cpu_to_be32(backing_filename_len);

        header_size += backing_filename_len;

    }



    /* Cluster size */

    s->cluster_bits = get_bits_from_size(cluster_size);

    if (s->cluster_bits < MIN_CLUSTER_BITS ||

        s->cluster_bits > MAX_CLUSTER_BITS)

    {

        fprintf(stderr, ""Cluster size must be a power of two between ""

            ""%d and %dk\n"",

            1 << MIN_CLUSTER_BITS,

            1 << (MAX_CLUSTER_BITS - 10));

        return -EINVAL;

    }

    s->cluster_size = 1 << s->cluster_bits;



    header.cluster_bits = cpu_to_be32(s->cluster_bits);

    header_size = (header_size + 7) & ~7;

    if (flags & BLOCK_FLAG_ENCRYPT) {

        header.crypt_method = cpu_to_be32(QCOW_CRYPT_AES);

    } else {

        header.crypt_method = cpu_to_be32(QCOW_CRYPT_NONE);

    }

    l2_bits = s->cluster_bits - 3;

    shift = s->cluster_bits + l2_bits;

    l1_size = (((total_size * 512) + (1LL << shift) - 1) >> shift);

    offset = align_offset(header_size, s->cluster_size);

    s->l1_table_offset = offset;

    header.l1_table_offset = cpu_to_be64(s->l1_table_offset);

    header.l1_size = cpu_to_be32(l1_size);

    offset += align_offset(l1_size * sizeof(uint64_t), s->cluster_size);



    s->refcount_table = qemu_mallocz(s->cluster_size);



    s->refcount_table_offset = offset;

    header.refcount_table_offset = cpu_to_be64(offset);

    header.refcount_table_clusters = cpu_to_be32(1);

    offset += s->cluster_size;

    s->refcount_block_offset = offset;



    /* count how many refcount blocks needed */

    tmp = offset >> s->cluster_bits;

    ref_clusters = (tmp >> (s->cluster_bits - REFCOUNT_SHIFT)) + 1;

    for (i=0; i < ref_clusters; i++) {

        s->refcount_table[i] = cpu_to_be64(offset);

        offset += s->cluster_size;

    }



    s->refcount_block = qemu_mallocz(ref_clusters * s->cluster_size);



    /* update refcounts */

    qcow2_create_refcount_update(s, 0, header_size);

    qcow2_create_refcount_update(s, s->l1_table_offset,

        l1_size * sizeof(uint64_t));

    qcow2_create_refcount_update(s, s->refcount_table_offset, s->cluster_size);

    qcow2_create_refcount_update(s, s->refcount_block_offset,

        ref_clusters * s->cluster_size);



    /* write all the data */

    ret = qemu_write_full(fd, &header, sizeof(header));

    if (ret != sizeof(header)) {

        ret = -1;

        goto exit;

    }

    if (backing_file) {

        if (backing_format_len) {

            char zero[16];

            int padding = rounded_ext_bf_len - (ext_bf.len + sizeof(ext_bf));



            memset(zero, 0, sizeof(zero));

            cpu_to_be32s(&ext_bf.magic);

            cpu_to_be32s(&ext_bf.len);

            ret = qemu_write_full(fd, &ext_bf, sizeof(ext_bf));

            if (ret != sizeof(ext_bf)) {

                ret = -1;

                goto exit;

            }

            ret = qemu_write_full(fd, backing_format, backing_format_len);

            if (ret != backing_format_len) {

                ret = -1;

                goto exit;

            }

            if (padding > 0) {

                ret = qemu_write_full(fd, zero, padding);

                if (ret != padding) {

                    ret = -1;

                    goto exit;

                }

            }

        }

        ret = qemu_write_full(fd, backing_file, backing_filename_len);

        if (ret != backing_filename_len) {

            ret = -1;

            goto exit;

        }

    }

    lseek(fd, s->l1_table_offset, SEEK_SET);

    tmp = 0;

    for(i = 0;i < l1_size; i++) {

        ret = qemu_write_full(fd, &tmp, sizeof(tmp));

        if (ret != sizeof(tmp)) {

            ret = -1;

            goto exit;

        }

    }

    lseek(fd, s->refcount_table_offset, SEEK_SET);

    ret = qemu_write_full(fd, s->refcount_table, s->cluster_size);

    if (ret != s->cluster_size) {

        ret = -1;

        goto exit;

    }



    lseek(fd, s->refcount_block_offset, SEEK_SET);

    ret = qemu_write_full(fd, s->refcount_block,

		    ref_clusters * s->cluster_size);

    if (ret != s->cluster_size) {

        ret = -1;

        goto exit;

    }



    ret = 0;

exit:

    qemu_free(s->refcount_table);

    qemu_free(s->refcount_block);

    close(fd);



    /* Preallocate metadata */

    if (prealloc) {

        BlockDriverState *bs;

        bs = bdrv_new("""");

        bdrv_open(bs, filename, BDRV_O_CACHE_WB | BDRV_O_RDWR);

        preallocate(bs);

        bdrv_close(bs);

    }



    return ret;

}
","static int qcow_create2(const char *filename, int64_t total_size, const char *backing_file, const char *backing_format, int flags, size_t cluster_size, int prealloc) {
int fd, header_size, backing_filename_len, l1_size, i, shift, l2_bits; int ref_clusters, backing_format_len = 0; int rounded_ext_bf_len = 0; QCowHeader header; uint64_t tmp, offset; QCowCreateState s1, *s = &s1; QCowExtension ext_bf = {
0, 0}; int ret; memset(s, 0, sizeof(*s)); fd = open(filename, O_WRONLY | O_CREAT | O_TRUNC | O_BINARY, 0644); if (fd < 0) return -1; memset(&header, 0, sizeof(header)); header.magic = cpu_to_be32(QCOW_MAGIC); header.version = cpu_to_be32(QCOW_VERSION); header.size = cpu_to_be64(total_size * 512); header_size = sizeof(header); backing_filename_len = 0; if (backing_file) {
if (backing_format) {
ext_bf.magic = QCOW_EXT_MAGIC_BACKING_FORMAT; backing_format_len = strlen(backing_format); ext_bf.len = backing_format_len; rounded_ext_bf_len = (sizeof(ext_bf) + ext_bf.len + 7) & ~7; header_size += rounded_ext_bf_len; } header.backing_file_offset = cpu_to_be64(header_size); backing_filename_len = strlen(backing_file); header.backing_file_size = cpu_to_be32(backing_filename_len); header_size += backing_filename_len; } /* Cluster size */ s->cluster_bits = get_bits_from_size(cluster_size); if (s->cluster_bits < MIN_CLUSTER_BITS || s->cluster_bits > MAX_CLUSTER_BITS) {
fprintf(stderr, ""Cluster size must be a power of two between "" ""%d and %dk\n"", 1 << MIN_CLUSTER_BITS, 1 << (MAX_CLUSTER_BITS - 10)); return -EINVAL; } s->cluster_size = 1 << s->cluster_bits; header.cluster_bits = cpu_to_be32(s->cluster_bits); header_size = (header_size + 7) & ~7; if (flags & BLOCK_FLAG_ENCRYPT) {
header.crypt_method = cpu_to_be32(QCOW_CRYPT_AES); } else {
header.crypt_method = cpu_to_be32(QCOW_CRYPT_NONE); } l2_bits = s->cluster_bits - 3; shift = s->cluster_bits + l2_bits; l1_size = (((total_size * 512) + (1LL << shift) - 1) >> shift); offset = align_offset(header_size, s->cluster_size); s->l1_table_offset = offset; header.l1_table_offset = cpu_to_be64(s->l1_table_offset); header.l1_size = cpu_to_be32(l1_size); offset += align_offset(l1_size * sizeof(uint64_t), s->cluster_size); s->refcount_table = qemu_mallocz(s->cluster_size); s->refcount_table_offset = offset; header.refcount_table_offset = cpu_to_be64(offset); header.refcount_table_clusters = cpu_to_be32(1); offset += s->cluster_size; s->refcount_block_offset = offset; /* count how many refcount blocks needed */ tmp = offset >> s->cluster_bits; ref_clusters = (tmp >> (s->cluster_bits - REFCOUNT_SHIFT)) + 1; for (i=0; i < ref_clusters; i++) {
s->refcount_table[i] = cpu_to_be64(offset); offset += s->cluster_size; } s->refcount_block = qemu_mallocz(ref_clusters * s->cluster_size); /* update refcounts */ qcow2_create_refcount_update(s, 0, header_size); qcow2_create_refcount_update(s, s->l1_table_offset, l1_size * sizeof(uint64_t)); qcow2_create_refcount_update(s, s->refcount_table_offset, s->cluster_size); qcow2_create_refcount_update(s, s->refcount_block_offset, ref_clusters * s->cluster_size); /* write all the data */ ret = qemu_write_full(fd, &header, sizeof(header)); if (ret != sizeof(header)) {
ret = -1; goto exit; } if (backing_file) {
if (backing_format_len) {
char zero[16]; int padding = rounded_ext_bf_len - (ext_bf.len + sizeof(ext_bf)); memset(zero, 0, sizeof(zero)); cpu_to_be32s(&ext_bf.magic); cpu_to_be32s(&ext_bf.len); ret = qemu_write_full(fd, &ext_bf, sizeof(ext_bf)); if (ret != sizeof(ext_bf)) {
ret = -1; goto exit; } ret = qemu_write_full(fd, backing_format, backing_format_len); if (ret != backing_format_len) {
ret = -1; goto exit; } if (padding > 0) {
ret = qemu_write_full(fd, zero, padding); if (ret != padding) {
ret = -1; goto exit; } } } ret = qemu_write_full(fd, backing_file, backing_filename_len); if (ret != backing_filename_len) {
ret = -1; goto exit; } } lseek(fd, s->l1_table_offset, SEEK_SET); tmp = 0; for(i = 0;i < l1_size; i++) {
ret = qemu_write_full(fd, &tmp, sizeof(tmp)); if (ret != sizeof(tmp)) {
ret = -1; goto exit; } } lseek(fd, s->refcount_table_offset, SEEK_SET); ret = qemu_write_full(fd, s->refcount_table, s->cluster_size); if (ret != s->cluster_size) {
ret = -1; goto exit; } lseek(fd, s->refcount_block_offset, SEEK_SET); ret = qemu_write_full(fd, s->refcount_block, ref_clusters * s->cluster_size); if (ret != s->cluster_size) {
ret = -1; goto exit; } ret = 0; exit: qemu_free(s->refcount_table); qemu_free(s->refcount_block); close(fd); /* Preallocate metadata */ if (prealloc) {
BlockDriverState *bs; bs = bdrv_new(""""); bdrv_open(bs, filename, BDRV_O_CACHE_WB | BDRV_O_RDWR); preallocate(bs); bdrv_close(bs); } return ret; } ",qemu,1
"static void mirror_write_complete(void *opaque, int ret)

{

    MirrorOp *op = opaque;

    MirrorBlockJob *s = op->s;

    if (ret < 0) {

        BlockDriverState *source = s->common.bs;

        BlockErrorAction action;



        bdrv_set_dirty(source, op->sector_num, op->nb_sectors);

        action = mirror_error_action(s, false, -ret);

        if (action == BLOCK_ERROR_ACTION_REPORT && s->ret >= 0) {

            s->ret = ret;

        }

    }

    mirror_iteration_done(op, ret);

}
","static void mirror_write_complete(void *opaque, int ret) {
MirrorOp *op = opaque; MirrorBlockJob *s = op->s; if (ret < 0) {
BlockDriverState *source = s->common.bs; BlockErrorAction action; bdrv_set_dirty(source, op->sector_num, op->nb_sectors); action = mirror_error_action(s, false, -ret); if (action == BLOCK_ERROR_ACTION_REPORT && s->ret >= 0) {
s->ret = ret; } } mirror_iteration_done(op, ret); } ",qemu,1
"int qemu_show_nic_models(const char *arg, const char *const *models)

{

    int i;



    if (!arg || strcmp(arg, ""?""))

        return 0;



    fprintf(stderr, ""qemu: Supported NIC models: "");

    for (i = 0 ; models[i]; i++)

        fprintf(stderr, ""%s%c"", models[i], models[i+1] ? ',' : '\n');

    return 1;

}
","int qemu_show_nic_models(const char *arg, const char *const *models) {
int i; if (!arg || strcmp(arg, ""?"")) return 0; fprintf(stderr, ""qemu: Supported NIC models: ""); for (i = 0 ; models[i]; i++) fprintf(stderr, ""%s%c"", models[i], models[i+1] ? ',' : '\n'); return 1; } ",qemu,1
"static int32_t scsi_disk_emulate_command(SCSIRequest *req, uint8_t *buf)

{

    SCSIDiskReq *r = DO_UPCAST(SCSIDiskReq, req, req);

    SCSIDiskState *s = DO_UPCAST(SCSIDiskState, qdev, req->dev);

    uint64_t nb_sectors;

    uint8_t *outbuf;

    int buflen;



    switch (req->cmd.buf[0]) {

    case INQUIRY:

    case MODE_SENSE:

    case MODE_SENSE_10:

    case RESERVE:

    case RESERVE_10:

    case RELEASE:

    case RELEASE_10:

    case START_STOP:

    case ALLOW_MEDIUM_REMOVAL:

    case GET_CONFIGURATION:

    case GET_EVENT_STATUS_NOTIFICATION:

    case MECHANISM_STATUS:

    case REQUEST_SENSE:

        break;



    default:

        if (s->tray_open || !bdrv_is_inserted(s->qdev.conf.bs)) {

            scsi_check_condition(r, SENSE_CODE(NO_MEDIUM));

            return 0;

        }

        break;

    }



    /*

     * FIXME: we shouldn't return anything bigger than 4k, but the code

     * requires the buffer to be as big as req->cmd.xfer in several

     * places.  So, do not allow CDBs with a very large ALLOCATION

     * LENGTH.  The real fix would be to modify scsi_read_data and

     * dma_buf_read, so that they return data beyond the buflen

     * as all zeros.

     */

    if (req->cmd.xfer > 65536) {

        goto illegal_request;

    }

    r->buflen = MAX(4096, req->cmd.xfer);



    if (!r->iov.iov_base) {

        r->iov.iov_base = qemu_blockalign(s->qdev.conf.bs, r->buflen);

    }



    buflen = req->cmd.xfer;

    outbuf = r->iov.iov_base;

    memset(outbuf, 0, r->buflen);

    switch (req->cmd.buf[0]) {

    case TEST_UNIT_READY:

        assert(!s->tray_open && bdrv_is_inserted(s->qdev.conf.bs));

        break;

    case INQUIRY:

        buflen = scsi_disk_emulate_inquiry(req, outbuf);

        if (buflen < 0) {

            goto illegal_request;

        }

        break;

    case MODE_SENSE:

    case MODE_SENSE_10:

        buflen = scsi_disk_emulate_mode_sense(r, outbuf);

        if (buflen < 0) {

            goto illegal_request;

        }

        break;

    case READ_TOC:

        buflen = scsi_disk_emulate_read_toc(req, outbuf);

        if (buflen < 0) {

            goto illegal_request;

        }

        break;

    case RESERVE:

        if (req->cmd.buf[1] & 1) {

            goto illegal_request;

        }

        break;

    case RESERVE_10:

        if (req->cmd.buf[1] & 3) {

            goto illegal_request;

        }

        break;

    case RELEASE:

        if (req->cmd.buf[1] & 1) {

            goto illegal_request;

        }

        break;

    case RELEASE_10:

        if (req->cmd.buf[1] & 3) {

            goto illegal_request;

        }

        break;

    case START_STOP:

        if (scsi_disk_emulate_start_stop(r) < 0) {

            return 0;

        }

        break;

    case ALLOW_MEDIUM_REMOVAL:

        s->tray_locked = req->cmd.buf[4] & 1;

        bdrv_lock_medium(s->qdev.conf.bs, req->cmd.buf[4] & 1);

        break;

    case READ_CAPACITY_10:

        /* The normal LEN field for this command is zero.  */

        memset(outbuf, 0, 8);

        bdrv_get_geometry(s->qdev.conf.bs, &nb_sectors);

        if (!nb_sectors) {

            scsi_check_condition(r, SENSE_CODE(LUN_NOT_READY));

            return 0;

        }

        if ((req->cmd.buf[8] & 1) == 0 && req->cmd.lba) {

            goto illegal_request;

        }

        nb_sectors /= s->qdev.blocksize / 512;

        /* Returned value is the address of the last sector.  */

        nb_sectors--;

        /* Remember the new size for read/write sanity checking. */

        s->qdev.max_lba = nb_sectors;

        /* Clip to 2TB, instead of returning capacity modulo 2TB. */

        if (nb_sectors > UINT32_MAX) {

            nb_sectors = UINT32_MAX;

        }

        outbuf[0] = (nb_sectors >> 24) & 0xff;

        outbuf[1] = (nb_sectors >> 16) & 0xff;

        outbuf[2] = (nb_sectors >> 8) & 0xff;

        outbuf[3] = nb_sectors & 0xff;

        outbuf[4] = 0;

        outbuf[5] = 0;

        outbuf[6] = s->qdev.blocksize >> 8;

        outbuf[7] = 0;

        break;

    case REQUEST_SENSE:

        /* Just return ""NO SENSE"".  */

        buflen = scsi_build_sense(NULL, 0, outbuf, r->buflen,

                                  (req->cmd.buf[1] & 1) == 0);

        if (buflen < 0) {

            goto illegal_request;

        }

        break;

    case MECHANISM_STATUS:

        buflen = scsi_emulate_mechanism_status(s, outbuf);

        if (buflen < 0) {

            goto illegal_request;

        }

        break;

    case GET_CONFIGURATION:

        buflen = scsi_get_configuration(s, outbuf);

        if (buflen < 0) {

            goto illegal_request;

        }

        break;

    case GET_EVENT_STATUS_NOTIFICATION:

        buflen = scsi_get_event_status_notification(s, r, outbuf);

        if (buflen < 0) {

            goto illegal_request;

        }

        break;

    case READ_DISC_INFORMATION:

        buflen = scsi_read_disc_information(s, r, outbuf);

        if (buflen < 0) {

            goto illegal_request;

        }

        break;

    case READ_DVD_STRUCTURE:

        buflen = scsi_read_dvd_structure(s, r, outbuf);

        if (buflen < 0) {

            goto illegal_request;

        }

        break;

    case SERVICE_ACTION_IN_16:

        /* Service Action In subcommands. */

        if ((req->cmd.buf[1] & 31) == SAI_READ_CAPACITY_16) {

            DPRINTF(""SAI READ CAPACITY(16)\n"");

            memset(outbuf, 0, req->cmd.xfer);

            bdrv_get_geometry(s->qdev.conf.bs, &nb_sectors);

            if (!nb_sectors) {

                scsi_check_condition(r, SENSE_CODE(LUN_NOT_READY));

                return 0;

            }

            if ((req->cmd.buf[14] & 1) == 0 && req->cmd.lba) {

                goto illegal_request;

            }

            nb_sectors /= s->qdev.blocksize / 512;

            /* Returned value is the address of the last sector.  */

            nb_sectors--;

            /* Remember the new size for read/write sanity checking. */

            s->qdev.max_lba = nb_sectors;

            outbuf[0] = (nb_sectors >> 56) & 0xff;

            outbuf[1] = (nb_sectors >> 48) & 0xff;

            outbuf[2] = (nb_sectors >> 40) & 0xff;

            outbuf[3] = (nb_sectors >> 32) & 0xff;

            outbuf[4] = (nb_sectors >> 24) & 0xff;

            outbuf[5] = (nb_sectors >> 16) & 0xff;

            outbuf[6] = (nb_sectors >> 8) & 0xff;

            outbuf[7] = nb_sectors & 0xff;

            outbuf[8] = 0;

            outbuf[9] = 0;

            outbuf[10] = s->qdev.blocksize >> 8;

            outbuf[11] = 0;

            outbuf[12] = 0;

            outbuf[13] = get_physical_block_exp(&s->qdev.conf);



            /* set TPE bit if the format supports discard */

            if (s->qdev.conf.discard_granularity) {

                outbuf[14] = 0x80;

            }



            /* Protection, exponent and lowest lba field left blank. */

            break;

        }

        DPRINTF(""Unsupported Service Action In\n"");

        goto illegal_request;

    case SYNCHRONIZE_CACHE:

        /* The request is used as the AIO opaque value, so add a ref.  */

        scsi_req_ref(&r->req);

        bdrv_acct_start(s->qdev.conf.bs, &r->acct, 0, BDRV_ACCT_FLUSH);

        r->req.aiocb = bdrv_aio_flush(s->qdev.conf.bs, scsi_aio_complete, r);

        return 0;

    case SEEK_10:

        DPRINTF(""Seek(10) (sector %"" PRId64 "")\n"", r->req.cmd.lba);

        if (r->req.cmd.lba > s->qdev.max_lba) {

            goto illegal_lba;

        }

        break;

    case MODE_SELECT:

        DPRINTF(""Mode Select(6) (len %lu)\n"", (long)r->req.cmd.xfer);

        break;

    case MODE_SELECT_10:

        DPRINTF(""Mode Select(10) (len %lu)\n"", (long)r->req.cmd.xfer);

        break;

    case UNMAP:

        DPRINTF(""Unmap (len %lu)\n"", (long)r->req.cmd.xfer);

        break;

    case WRITE_SAME_10:

    case WRITE_SAME_16:

        nb_sectors = scsi_data_cdb_length(r->req.cmd.buf);

        if (bdrv_is_read_only(s->qdev.conf.bs)) {

            scsi_check_condition(r, SENSE_CODE(WRITE_PROTECTED));

            return 0;

        }

        if (!check_lba_range(s, r->req.cmd.lba, nb_sectors)) {

            goto illegal_lba;

        }



        /*

         * We only support WRITE SAME with the unmap bit set for now.

         */

        if (!(req->cmd.buf[1] & 0x8)) {

            goto illegal_request;

        }



        /* The request is used as the AIO opaque value, so add a ref.  */

        scsi_req_ref(&r->req);

        r->req.aiocb = bdrv_aio_discard(s->qdev.conf.bs,

                                        r->req.cmd.lba * (s->qdev.blocksize / 512),

                                        nb_sectors * (s->qdev.blocksize / 512),

                                        scsi_aio_complete, r);

        return 0;

    default:

        DPRINTF(""Unknown SCSI command (%2.2x)\n"", buf[0]);

        scsi_check_condition(r, SENSE_CODE(INVALID_OPCODE));

        return 0;

    }

    assert(!r->req.aiocb);

    r->iov.iov_len = MIN(r->buflen, req->cmd.xfer);

    if (r->iov.iov_len == 0) {

        scsi_req_complete(&r->req, GOOD);

    }

    if (r->req.cmd.mode == SCSI_XFER_TO_DEV) {

        assert(r->iov.iov_len == req->cmd.xfer);

        return -r->iov.iov_len;

    } else {

        return r->iov.iov_len;

    }



illegal_request:

    if (r->req.status == -1) {

        scsi_check_condition(r, SENSE_CODE(INVALID_FIELD));

    }

    return 0;



illegal_lba:

    scsi_check_condition(r, SENSE_CODE(LBA_OUT_OF_RANGE));

    return 0;

}
","static int32_t scsi_disk_emulate_command(SCSIRequest *req, uint8_t *buf) {
SCSIDiskReq *r = DO_UPCAST(SCSIDiskReq, req, req); SCSIDiskState *s = DO_UPCAST(SCSIDiskState, qdev, req->dev); uint64_t nb_sectors; uint8_t *outbuf; int buflen; switch (req->cmd.buf[0]) {
case INQUIRY: case MODE_SENSE: case MODE_SENSE_10: case RESERVE: case RESERVE_10: case RELEASE: case RELEASE_10: case START_STOP: case ALLOW_MEDIUM_REMOVAL: case GET_CONFIGURATION: case GET_EVENT_STATUS_NOTIFICATION: case MECHANISM_STATUS: case REQUEST_SENSE: break; default: if (s->tray_open || !bdrv_is_inserted(s->qdev.conf.bs)) {
scsi_check_condition(r, SENSE_CODE(NO_MEDIUM)); return 0; } break; } /* * FIXME: we shouldn't return anything bigger than 4k, but the code * requires the buffer to be as big as req->cmd.xfer in several * places. So, do not allow CDBs with a very large ALLOCATION * LENGTH. The real fix would be to modify scsi_read_data and * dma_buf_read, so that they return data beyond the buflen * as all zeros. */ if (req->cmd.xfer > 65536) {
goto illegal_request; } r->buflen = MAX(4096, req->cmd.xfer); if (!r->iov.iov_base) {
r->iov.iov_base = qemu_blockalign(s->qdev.conf.bs, r->buflen); } buflen = req->cmd.xfer; outbuf = r->iov.iov_base; memset(outbuf, 0, r->buflen); switch (req->cmd.buf[0]) {
case TEST_UNIT_READY: assert(!s->tray_open && bdrv_is_inserted(s->qdev.conf.bs)); break; case INQUIRY: buflen = scsi_disk_emulate_inquiry(req, outbuf); if (buflen < 0) {
goto illegal_request; } break; case MODE_SENSE: case MODE_SENSE_10: buflen = scsi_disk_emulate_mode_sense(r, outbuf); if (buflen < 0) {
goto illegal_request; } break; case READ_TOC: buflen = scsi_disk_emulate_read_toc(req, outbuf); if (buflen < 0) {
goto illegal_request; } break; case RESERVE: if (req->cmd.buf[1] & 1) {
goto illegal_request; } break; case RESERVE_10: if (req->cmd.buf[1] & 3) {
goto illegal_request; } break; case RELEASE: if (req->cmd.buf[1] & 1) {
goto illegal_request; } break; case RELEASE_10: if (req->cmd.buf[1] & 3) {
goto illegal_request; } break; case START_STOP: if (scsi_disk_emulate_start_stop(r) < 0) {
return 0; } break; case ALLOW_MEDIUM_REMOVAL: s->tray_locked = req->cmd.buf[4] & 1; bdrv_lock_medium(s->qdev.conf.bs, req->cmd.buf[4] & 1); break; case READ_CAPACITY_10: /* The normal LEN field for this command is zero. */ memset(outbuf, 0, 8); bdrv_get_geometry(s->qdev.conf.bs, &nb_sectors); if (!nb_sectors) {
scsi_check_condition(r, SENSE_CODE(LUN_NOT_READY)); return 0; } if ((req->cmd.buf[8] & 1) == 0 && req->cmd.lba) {
goto illegal_request; } nb_sectors /= s->qdev.blocksize / 512; /* Returned value is the address of the last sector. */ nb_sectors--; /* Remember the new size for read/write sanity checking. */ s->qdev.max_lba = nb_sectors; /* Clip to 2TB, instead of returning capacity modulo 2TB. */ if (nb_sectors > UINT32_MAX) {
nb_sectors = UINT32_MAX; } outbuf[0] = (nb_sectors >> 24) & 0xff; outbuf[1] = (nb_sectors >> 16) & 0xff; outbuf[2] = (nb_sectors >> 8) & 0xff; outbuf[3] = nb_sectors & 0xff; outbuf[4] = 0; outbuf[5] = 0; outbuf[6] = s->qdev.blocksize >> 8; outbuf[7] = 0; break; case REQUEST_SENSE: /* Just return ""NO SENSE"". */ buflen = scsi_build_sense(NULL, 0, outbuf, r->buflen, (req->cmd.buf[1] & 1) == 0); if (buflen < 0) {
goto illegal_request; } break; case MECHANISM_STATUS: buflen = scsi_emulate_mechanism_status(s, outbuf); if (buflen < 0) {
goto illegal_request; } break; case GET_CONFIGURATION: buflen = scsi_get_configuration(s, outbuf); if (buflen < 0) {
goto illegal_request; } break; case GET_EVENT_STATUS_NOTIFICATION: buflen = scsi_get_event_status_notification(s, r, outbuf); if (buflen < 0) {
goto illegal_request; } break; case READ_DISC_INFORMATION: buflen = scsi_read_disc_information(s, r, outbuf); if (buflen < 0) {
goto illegal_request; } break; case READ_DVD_STRUCTURE: buflen = scsi_read_dvd_structure(s, r, outbuf); if (buflen < 0) {
goto illegal_request; } break; case SERVICE_ACTION_IN_16: /* Service Action In subcommands. */ if ((req->cmd.buf[1] & 31) == SAI_READ_CAPACITY_16) {
DPRINTF(""SAI READ CAPACITY(16)\n""); memset(outbuf, 0, req->cmd.xfer); bdrv_get_geometry(s->qdev.conf.bs, &nb_sectors); if (!nb_sectors) {
scsi_check_condition(r, SENSE_CODE(LUN_NOT_READY)); return 0; } if ((req->cmd.buf[14] & 1) == 0 && req->cmd.lba) {
goto illegal_request; } nb_sectors /= s->qdev.blocksize / 512; /* Returned value is the address of the last sector. */ nb_sectors--; /* Remember the new size for read/write sanity checking. */ s->qdev.max_lba = nb_sectors; outbuf[0] = (nb_sectors >> 56) & 0xff; outbuf[1] = (nb_sectors >> 48) & 0xff; outbuf[2] = (nb_sectors >> 40) & 0xff; outbuf[3] = (nb_sectors >> 32) & 0xff; outbuf[4] = (nb_sectors >> 24) & 0xff; outbuf[5] = (nb_sectors >> 16) & 0xff; outbuf[6] = (nb_sectors >> 8) & 0xff; outbuf[7] = nb_sectors & 0xff; outbuf[8] = 0; outbuf[9] = 0; outbuf[10] = s->qdev.blocksize >> 8; outbuf[11] = 0; outbuf[12] = 0; outbuf[13] = get_physical_block_exp(&s->qdev.conf); /* set TPE bit if the format supports discard */ if (s->qdev.conf.discard_granularity) {
outbuf[14] = 0x80; } /* Protection, exponent and lowest lba field left blank. */ break; } DPRINTF(""Unsupported Service Action In\n""); goto illegal_request; case SYNCHRONIZE_CACHE: /* The request is used as the AIO opaque value, so add a ref. */ scsi_req_ref(&r->req); bdrv_acct_start(s->qdev.conf.bs, &r->acct, 0, BDRV_ACCT_FLUSH); r->req.aiocb = bdrv_aio_flush(s->qdev.conf.bs, scsi_aio_complete, r); return 0; case SEEK_10: DPRINTF(""Seek(10) (sector %"" PRId64 "")\n"", r->req.cmd.lba); if (r->req.cmd.lba > s->qdev.max_lba) {
goto illegal_lba; } break; case MODE_SELECT: DPRINTF(""Mode Select(6) (len %lu)\n"", (long)r->req.cmd.xfer); break; case MODE_SELECT_10: DPRINTF(""Mode Select(10) (len %lu)\n"", (long)r->req.cmd.xfer); break; case UNMAP: DPRINTF(""Unmap (len %lu)\n"", (long)r->req.cmd.xfer); break; case WRITE_SAME_10: case WRITE_SAME_16: nb_sectors = scsi_data_cdb_length(r->req.cmd.buf); if (bdrv_is_read_only(s->qdev.conf.bs)) {
scsi_check_condition(r, SENSE_CODE(WRITE_PROTECTED)); return 0; } if (!check_lba_range(s, r->req.cmd.lba, nb_sectors)) {
goto illegal_lba; } /* * We only support WRITE SAME with the unmap bit set for now. */ if (!(req->cmd.buf[1] & 0x8)) {
goto illegal_request; } /* The request is used as the AIO opaque value, so add a ref. */ scsi_req_ref(&r->req); r->req.aiocb = bdrv_aio_discard(s->qdev.conf.bs, r->req.cmd.lba * (s->qdev.blocksize / 512), nb_sectors * (s->qdev.blocksize / 512), scsi_aio_complete, r); return 0; default: DPRINTF(""Unknown SCSI command (%2.2x)\n"", buf[0]); scsi_check_condition(r, SENSE_CODE(INVALID_OPCODE)); return 0; } assert(!r->req.aiocb); r->iov.iov_len = MIN(r->buflen, req->cmd.xfer); if (r->iov.iov_len == 0) {
scsi_req_complete(&r->req, GOOD); } if (r->req.cmd.mode == SCSI_XFER_TO_DEV) {
assert(r->iov.iov_len == req->cmd.xfer); return -r->iov.iov_len; } else {
return r->iov.iov_len; } illegal_request: if (r->req.status == -1) {
scsi_check_condition(r, SENSE_CODE(INVALID_FIELD)); } return 0; illegal_lba: scsi_check_condition(r, SENSE_CODE(LBA_OUT_OF_RANGE)); return 0; } ",qemu,1
"static void bdrv_co_maybe_schedule_bh(BlockAIOCBCoroutine *acb)

{

    acb->need_bh = false;

    if (acb->req.error != -EINPROGRESS) {

        BlockDriverState *bs = acb->common.bs;



        acb->bh = aio_bh_new(bdrv_get_aio_context(bs), bdrv_co_em_bh, acb);

        qemu_bh_schedule(acb->bh);

    }

}
","static void bdrv_co_maybe_schedule_bh(BlockAIOCBCoroutine *acb) {
acb->need_bh = false; if (acb->req.error != -EINPROGRESS) {
BlockDriverState *bs = acb->common.bs; acb->bh = aio_bh_new(bdrv_get_aio_context(bs), bdrv_co_em_bh, acb); qemu_bh_schedule(acb->bh); } } ",qemu,0
"xilinx_pcie_init(MemoryRegion *sys_mem, uint32_t bus_nr,

                 hwaddr cfg_base, uint64_t cfg_size,

                 hwaddr mmio_base, uint64_t mmio_size,

                 qemu_irq irq, bool link_up)

{

    DeviceState *dev;

    MemoryRegion *cfg, *mmio;



    dev = qdev_create(NULL, TYPE_XILINX_PCIE_HOST);



    qdev_prop_set_uint32(dev, ""bus_nr"", bus_nr);

    qdev_prop_set_uint64(dev, ""cfg_base"", cfg_base);

    qdev_prop_set_uint64(dev, ""cfg_size"", cfg_size);

    qdev_prop_set_uint64(dev, ""mmio_base"", mmio_base);

    qdev_prop_set_uint64(dev, ""mmio_size"", mmio_size);

    qdev_prop_set_bit(dev, ""link_up"", link_up);



    qdev_init_nofail(dev);



    cfg = sysbus_mmio_get_region(SYS_BUS_DEVICE(dev), 0);

    memory_region_add_subregion_overlap(sys_mem, cfg_base, cfg, 0);



    mmio = sysbus_mmio_get_region(SYS_BUS_DEVICE(dev), 1);

    memory_region_add_subregion_overlap(sys_mem, 0, mmio, 0);



    qdev_connect_gpio_out_named(dev, ""interrupt_out"", 0, irq);



    return XILINX_PCIE_HOST(dev);

}
","xilinx_pcie_init(MemoryRegion *sys_mem, uint32_t bus_nr, hwaddr cfg_base, uint64_t cfg_size, hwaddr mmio_base, uint64_t mmio_size, qemu_irq irq, bool link_up) {
DeviceState *dev; MemoryRegion *cfg, *mmio; dev = qdev_create(NULL, TYPE_XILINX_PCIE_HOST); qdev_prop_set_uint32(dev, ""bus_nr"", bus_nr); qdev_prop_set_uint64(dev, ""cfg_base"", cfg_base); qdev_prop_set_uint64(dev, ""cfg_size"", cfg_size); qdev_prop_set_uint64(dev, ""mmio_base"", mmio_base); qdev_prop_set_uint64(dev, ""mmio_size"", mmio_size); qdev_prop_set_bit(dev, ""link_up"", link_up); qdev_init_nofail(dev); cfg = sysbus_mmio_get_region(SYS_BUS_DEVICE(dev), 0); memory_region_add_subregion_overlap(sys_mem, cfg_base, cfg, 0); mmio = sysbus_mmio_get_region(SYS_BUS_DEVICE(dev), 1); memory_region_add_subregion_overlap(sys_mem, 0, mmio, 0); qdev_connect_gpio_out_named(dev, ""interrupt_out"", 0, irq); return XILINX_PCIE_HOST(dev); } ",qemu,1
"static void slirp_state_save(QEMUFile *f, void *opaque)

{

    Slirp *slirp = opaque;

    struct ex_list *ex_ptr;



    for (ex_ptr = slirp->exec_list; ex_ptr; ex_ptr = ex_ptr->ex_next)

        if (ex_ptr->ex_pty == 3) {

            struct socket *so;

            so = slirp_find_ctl_socket(slirp, ex_ptr->ex_addr,

                                       ntohs(ex_ptr->ex_fport));

            if (!so)

                continue;



            qemu_put_byte(f, 42);

            vmstate_save_state(f, &vmstate_slirp_socket, so, NULL);

        }

    qemu_put_byte(f, 0);



    qemu_put_be16(f, slirp->ip_id);



    slirp_bootp_save(f, slirp);

}
","static void slirp_state_save(QEMUFile *f, void *opaque) {
Slirp *slirp = opaque; struct ex_list *ex_ptr; for (ex_ptr = slirp->exec_list; ex_ptr; ex_ptr = ex_ptr->ex_next) if (ex_ptr->ex_pty == 3) {
struct socket *so; so = slirp_find_ctl_socket(slirp, ex_ptr->ex_addr, ntohs(ex_ptr->ex_fport)); if (!so) continue; qemu_put_byte(f, 42); vmstate_save_state(f, &vmstate_slirp_socket, so, NULL); } qemu_put_byte(f, 0); qemu_put_be16(f, slirp->ip_id); slirp_bootp_save(f, slirp); } ",qemu,0
"int bdrv_all_find_snapshot(const char *name, BlockDriverState **first_bad_bs)

{

    QEMUSnapshotInfo sn;

    int err = 0;

    BlockDriverState *bs;

    BdrvNextIterator *it = NULL;



    while (err == 0 && (it = bdrv_next(it, &bs))) {

        AioContext *ctx = bdrv_get_aio_context(bs);



        aio_context_acquire(ctx);

        if (bdrv_can_snapshot(bs)) {

            err = bdrv_snapshot_find(bs, &sn, name);

        }

        aio_context_release(ctx);

    }



    *first_bad_bs = bs;

    return err;

}
","int bdrv_all_find_snapshot(const char *name, BlockDriverState **first_bad_bs) {
QEMUSnapshotInfo sn; int err = 0; BlockDriverState *bs; BdrvNextIterator *it = NULL; while (err == 0 && (it = bdrv_next(it, &bs))) {
AioContext *ctx = bdrv_get_aio_context(bs); aio_context_acquire(ctx); if (bdrv_can_snapshot(bs)) {
err = bdrv_snapshot_find(bs, &sn, name); } aio_context_release(ctx); } *first_bad_bs = bs; return err; } ",qemu,1
"static int vnc_display_get_address(const char *addrstr,

                                   bool websocket,

                                   bool reverse,

                                   int displaynum,

                                   int to,

                                   bool has_ipv4,

                                   bool has_ipv6,

                                   bool ipv4,

                                   bool ipv6,

                                   SocketAddressLegacy **retaddr,

                                   Error **errp)

{

    int ret = -1;

    SocketAddressLegacy *addr = NULL;



    addr = g_new0(SocketAddressLegacy, 1);



    if (strncmp(addrstr, ""unix:"", 5) == 0) {

        addr->type = SOCKET_ADDRESS_LEGACY_KIND_UNIX;

        addr->u.q_unix.data = g_new0(UnixSocketAddress, 1);

        addr->u.q_unix.data->path = g_strdup(addrstr + 5);



        if (websocket) {

            error_setg(errp, ""UNIX sockets not supported with websock"");

            goto cleanup;

        }



        if (to) {

            error_setg(errp, ""Port range not support with UNIX socket"");

            goto cleanup;

        }

        ret = 0;

    } else {

        const char *port;

        size_t hostlen;

        unsigned long long baseport = 0;

        InetSocketAddress *inet;



        port = strrchr(addrstr, ':');

        if (!port) {

            if (websocket) {

                hostlen = 0;

                port = addrstr;

            } else {

                error_setg(errp, ""no vnc port specified"");

                goto cleanup;

            }

        } else {

            hostlen = port - addrstr;

            port++;

            if (*port == '\0') {

                error_setg(errp, ""vnc port cannot be empty"");

                goto cleanup;

            }

        }



        addr->type = SOCKET_ADDRESS_LEGACY_KIND_INET;

        inet = addr->u.inet.data = g_new0(InetSocketAddress, 1);

        if (addrstr[0] == '[' && addrstr[hostlen - 1] == ']') {

            inet->host = g_strndup(addrstr + 1, hostlen - 2);

        } else {

            inet->host = g_strndup(addrstr, hostlen);

        }

        /* plain VNC port is just an offset, for websocket

         * port is absolute */

        if (websocket) {

            if (g_str_equal(addrstr, """") ||

                g_str_equal(addrstr, ""on"")) {

                if (displaynum == -1) {

                    error_setg(errp, ""explicit websocket port is required"");

                    goto cleanup;

                }

                inet->port = g_strdup_printf(

                    ""%d"", displaynum + 5700);

                if (to) {

                    inet->has_to = true;

                    inet->to = to + 5700;

                }

            } else {

                inet->port = g_strdup(port);

            }

        } else {

            int offset = reverse ? 0 : 5900;

            if (parse_uint_full(port, &baseport, 10) < 0) {

                error_setg(errp, ""can't convert to a number: %s"", port);

                goto cleanup;

            }

            if (baseport > 65535 ||

                baseport + offset > 65535) {

                error_setg(errp, ""port %s out of range"", port);

                goto cleanup;

            }

            inet->port = g_strdup_printf(

                ""%d"", (int)baseport + offset);



            if (to) {

                inet->has_to = true;

                inet->to = to + offset;

            }

        }



        inet->ipv4 = ipv4;

        inet->has_ipv4 = has_ipv4;

        inet->ipv6 = ipv6;

        inet->has_ipv6 = has_ipv6;



        ret = baseport;

    }



    *retaddr = addr;



 cleanup:

    if (ret < 0) {

        qapi_free_SocketAddressLegacy(addr);

    }

    return ret;

}
","static int vnc_display_get_address(const char *addrstr, bool websocket, bool reverse, int displaynum, int to, bool has_ipv4, bool has_ipv6, bool ipv4, bool ipv6, SocketAddressLegacy **retaddr, Error **errp) {
int ret = -1; SocketAddressLegacy *addr = NULL; addr = g_new0(SocketAddressLegacy, 1); if (strncmp(addrstr, ""unix:"", 5) == 0) {
addr->type = SOCKET_ADDRESS_LEGACY_KIND_UNIX; addr->u.q_unix.data = g_new0(UnixSocketAddress, 1); addr->u.q_unix.data->path = g_strdup(addrstr + 5); if (websocket) {
error_setg(errp, ""UNIX sockets not supported with websock""); goto cleanup; } if (to) {
error_setg(errp, ""Port range not support with UNIX socket""); goto cleanup; } ret = 0; } else {
const char *port; size_t hostlen; unsigned long long baseport = 0; InetSocketAddress *inet; port = strrchr(addrstr, ':'); if (!port) {
if (websocket) {
hostlen = 0; port = addrstr; } else {
error_setg(errp, ""no vnc port specified""); goto cleanup; } } else {
hostlen = port - addrstr; port++; if (*port == '\0') {
error_setg(errp, ""vnc port cannot be empty""); goto cleanup; } } addr->type = SOCKET_ADDRESS_LEGACY_KIND_INET; inet = addr->u.inet.data = g_new0(InetSocketAddress, 1); if (addrstr[0] == '[' && addrstr[hostlen - 1] == ']') {
inet->host = g_strndup(addrstr + 1, hostlen - 2); } else {
inet->host = g_strndup(addrstr, hostlen); } /* plain VNC port is just an offset, for websocket * port is absolute */ if (websocket) {
if (g_str_equal(addrstr, """") || g_str_equal(addrstr, ""on"")) {
if (displaynum == -1) {
error_setg(errp, ""explicit websocket port is required""); goto cleanup; } inet->port = g_strdup_printf( ""%d"", displaynum + 5700); if (to) {
inet->has_to = true; inet->to = to + 5700; } } else {
inet->port = g_strdup(port); } } else {
int offset = reverse ? 0 : 5900; if (parse_uint_full(port, &baseport, 10) < 0) {
error_setg(errp, ""can't convert to a number: %s"", port); goto cleanup; } if (baseport > 65535 || baseport + offset > 65535) {
error_setg(errp, ""port %s out of range"", port); goto cleanup; } inet->port = g_strdup_printf( ""%d"", (int)baseport + offset); if (to) {
inet->has_to = true; inet->to = to + offset; } } inet->ipv4 = ipv4; inet->has_ipv4 = has_ipv4; inet->ipv6 = ipv6; inet->has_ipv6 = has_ipv6; ret = baseport; } *retaddr = addr; cleanup: if (ret < 0) {
qapi_free_SocketAddressLegacy(addr); } return ret; } ",qemu,0
"static void rtas_set_tce_bypass(PowerPCCPU *cpu, sPAPREnvironment *spapr,

                                uint32_t token,

                                uint32_t nargs, target_ulong args,

                                uint32_t nret, target_ulong rets)

{

    VIOsPAPRBus *bus = spapr->vio_bus;

    VIOsPAPRDevice *dev;

    uint32_t unit, enable;



    if (nargs != 2) {

        rtas_st(rets, 0, RTAS_OUT_PARAM_ERROR);

        return;

    }

    unit = rtas_ld(args, 0);

    enable = rtas_ld(args, 1);

    dev = spapr_vio_find_by_reg(bus, unit);

    if (!dev) {

        rtas_st(rets, 0, RTAS_OUT_PARAM_ERROR);

        return;

    }



    if (!dev->tcet) {

        rtas_st(rets, 0, RTAS_OUT_PARAM_ERROR);

        return;

    }



    spapr_tce_set_bypass(dev->tcet, !!enable);



    rtas_st(rets, 0, RTAS_OUT_SUCCESS);

}
","static void rtas_set_tce_bypass(PowerPCCPU *cpu, sPAPREnvironment *spapr, uint32_t token, uint32_t nargs, target_ulong args, uint32_t nret, target_ulong rets) {
VIOsPAPRBus *bus = spapr->vio_bus; VIOsPAPRDevice *dev; uint32_t unit, enable; if (nargs != 2) {
rtas_st(rets, 0, RTAS_OUT_PARAM_ERROR); return; } unit = rtas_ld(args, 0); enable = rtas_ld(args, 1); dev = spapr_vio_find_by_reg(bus, unit); if (!dev) {
rtas_st(rets, 0, RTAS_OUT_PARAM_ERROR); return; } if (!dev->tcet) {
rtas_st(rets, 0, RTAS_OUT_PARAM_ERROR); return; } spapr_tce_set_bypass(dev->tcet, !!enable); rtas_st(rets, 0, RTAS_OUT_SUCCESS); } ",qemu,0
"static void qemu_clock_init(QEMUClockType type)
{
    QEMUClock *clock = qemu_clock_ptr(type);
    clock->type = type;
    clock->enabled = true;
    clock->last = INT64_MIN;
    QLIST_INIT(&clock->timerlists);
    notifier_list_init(&clock->reset_notifiers);
    main_loop_tlg.tl[type] = timerlist_new(type, NULL, NULL);
}","static void qemu_clock_init(QEMUClockType type) {
QEMUClock *clock = qemu_clock_ptr(type); clock->type = type; clock->enabled = true; clock->last = INT64_MIN; QLIST_INIT(&clock->timerlists); notifier_list_init(&clock->reset_notifiers); main_loop_tlg.tl[type] = timerlist_new(type, NULL, NULL); }",qemu,1
"static void vfio_exitfn(PCIDevice *pdev)

{

    VFIOPCIDevice *vdev = DO_UPCAST(VFIOPCIDevice, pdev, pdev);

    VFIOGroup *group = vdev->vbasedev.group;



    vfio_unregister_err_notifier(vdev);

    pci_device_set_intx_routing_notifier(&vdev->pdev, NULL);

    vfio_disable_interrupts(vdev);

    if (vdev->intx.mmap_timer) {

        timer_free(vdev->intx.mmap_timer);

    }

    vfio_teardown_msi(vdev);

    vfio_unmap_bars(vdev);

    g_free(vdev->emulated_config_bits);

    g_free(vdev->rom);

    vfio_put_device(vdev);

    vfio_put_group(group);

}
","static void vfio_exitfn(PCIDevice *pdev) {
VFIOPCIDevice *vdev = DO_UPCAST(VFIOPCIDevice, pdev, pdev); VFIOGroup *group = vdev->vbasedev.group; vfio_unregister_err_notifier(vdev); pci_device_set_intx_routing_notifier(&vdev->pdev, NULL); vfio_disable_interrupts(vdev); if (vdev->intx.mmap_timer) {
timer_free(vdev->intx.mmap_timer); } vfio_teardown_msi(vdev); vfio_unmap_bars(vdev); g_free(vdev->emulated_config_bits); g_free(vdev->rom); vfio_put_device(vdev); vfio_put_group(group); } ",qemu,1
"static void vtd_do_iommu_translate(VTDAddressSpace *vtd_as, PCIBus *bus,

                                   uint8_t devfn, hwaddr addr, bool is_write,

                                   IOMMUTLBEntry *entry)

{

    IntelIOMMUState *s = vtd_as->iommu_state;

    VTDContextEntry ce;

    uint8_t bus_num = pci_bus_num(bus);

    VTDContextCacheEntry *cc_entry = &vtd_as->context_cache_entry;

    uint64_t slpte;

    uint32_t level;

    uint16_t source_id = vtd_make_source_id(bus_num, devfn);

    int ret_fr;

    bool is_fpd_set = false;

    bool reads = true;

    bool writes = true;

    VTDIOTLBEntry *iotlb_entry;



    /* Check if the request is in interrupt address range */

    if (vtd_is_interrupt_addr(addr)) {

        if (is_write) {

            /* FIXME: since we don't know the length of the access here, we

             * treat Non-DWORD length write requests without PASID as

             * interrupt requests, too. Withoud interrupt remapping support,

             * we just use 1:1 mapping.

             */

            VTD_DPRINTF(MMU, ""write request to interrupt address ""

                        ""gpa 0x%""PRIx64, addr);

            entry->iova = addr & VTD_PAGE_MASK_4K;

            entry->translated_addr = addr & VTD_PAGE_MASK_4K;

            entry->addr_mask = ~VTD_PAGE_MASK_4K;

            entry->perm = IOMMU_WO;

            return;

        } else {

            VTD_DPRINTF(GENERAL, ""error: read request from interrupt address ""

                        ""gpa 0x%""PRIx64, addr);

            vtd_report_dmar_fault(s, source_id, addr, VTD_FR_READ, is_write);

            return;

        }

    }

    /* Try to fetch slpte form IOTLB */

    iotlb_entry = vtd_lookup_iotlb(s, source_id, addr);

    if (iotlb_entry) {

        VTD_DPRINTF(CACHE, ""hit iotlb sid 0x%""PRIx16 "" gpa 0x%""PRIx64

                    "" slpte 0x%""PRIx64 "" did 0x%""PRIx16, source_id, addr,

                    iotlb_entry->slpte, iotlb_entry->domain_id);

        slpte = iotlb_entry->slpte;

        reads = iotlb_entry->read_flags;

        writes = iotlb_entry->write_flags;

        goto out;

    }

    /* Try to fetch context-entry from cache first */

    if (cc_entry->context_cache_gen == s->context_cache_gen) {

        VTD_DPRINTF(CACHE, ""hit context-cache bus %d devfn %d ""

                    ""(hi %""PRIx64 "" lo %""PRIx64 "" gen %""PRIu32 "")"",

                    bus_num, devfn, cc_entry->context_entry.hi,

                    cc_entry->context_entry.lo, cc_entry->context_cache_gen);

        ce = cc_entry->context_entry;

        is_fpd_set = ce.lo & VTD_CONTEXT_ENTRY_FPD;

    } else {

        ret_fr = vtd_dev_to_context_entry(s, bus_num, devfn, &ce);

        is_fpd_set = ce.lo & VTD_CONTEXT_ENTRY_FPD;

        if (ret_fr) {

            ret_fr = -ret_fr;

            if (is_fpd_set && vtd_is_qualified_fault(ret_fr)) {

                VTD_DPRINTF(FLOG, ""fault processing is disabled for DMA ""

                            ""requests through this context-entry ""

                            ""(with FPD Set)"");

            } else {

                vtd_report_dmar_fault(s, source_id, addr, ret_fr, is_write);

            }

            return;

        }

        /* Update context-cache */

        VTD_DPRINTF(CACHE, ""update context-cache bus %d devfn %d ""

                    ""(hi %""PRIx64 "" lo %""PRIx64 "" gen %""PRIu32 ""->%""PRIu32 "")"",

                    bus_num, devfn, ce.hi, ce.lo,

                    cc_entry->context_cache_gen, s->context_cache_gen);

        cc_entry->context_entry = ce;

        cc_entry->context_cache_gen = s->context_cache_gen;

    }



    ret_fr = vtd_gpa_to_slpte(&ce, addr, is_write, &slpte, &level,

                              &reads, &writes);

    if (ret_fr) {

        ret_fr = -ret_fr;

        if (is_fpd_set && vtd_is_qualified_fault(ret_fr)) {

            VTD_DPRINTF(FLOG, ""fault processing is disabled for DMA requests ""

                        ""through this context-entry (with FPD Set)"");

        } else {

            vtd_report_dmar_fault(s, source_id, addr, ret_fr, is_write);

        }

        return;

    }



    vtd_update_iotlb(s, source_id, VTD_CONTEXT_ENTRY_DID(ce.hi), addr, slpte,

                     reads, writes);

out:

    entry->iova = addr & VTD_PAGE_MASK_4K;

    entry->translated_addr = vtd_get_slpte_addr(slpte) & VTD_PAGE_MASK_4K;

    entry->addr_mask = ~VTD_PAGE_MASK_4K;

    entry->perm = (writes ? 2 : 0) + (reads ? 1 : 0);

}
","static void vtd_do_iommu_translate(VTDAddressSpace *vtd_as, PCIBus *bus, uint8_t devfn, hwaddr addr, bool is_write, IOMMUTLBEntry *entry) {
IntelIOMMUState *s = vtd_as->iommu_state; VTDContextEntry ce; uint8_t bus_num = pci_bus_num(bus); VTDContextCacheEntry *cc_entry = &vtd_as->context_cache_entry; uint64_t slpte; uint32_t level; uint16_t source_id = vtd_make_source_id(bus_num, devfn); int ret_fr; bool is_fpd_set = false; bool reads = true; bool writes = true; VTDIOTLBEntry *iotlb_entry; /* Check if the request is in interrupt address range */ if (vtd_is_interrupt_addr(addr)) {
if (is_write) {
/* FIXME: since we don't know the length of the access here, we * treat Non-DWORD length write requests without PASID as * interrupt requests, too. Withoud interrupt remapping support, * we just use 1:1 mapping. */ VTD_DPRINTF(MMU, ""write request to interrupt address "" ""gpa 0x%""PRIx64, addr); entry->iova = addr & VTD_PAGE_MASK_4K; entry->translated_addr = addr & VTD_PAGE_MASK_4K; entry->addr_mask = ~VTD_PAGE_MASK_4K; entry->perm = IOMMU_WO; return; } else {
VTD_DPRINTF(GENERAL, ""error: read request from interrupt address "" ""gpa 0x%""PRIx64, addr); vtd_report_dmar_fault(s, source_id, addr, VTD_FR_READ, is_write); return; } } /* Try to fetch slpte form IOTLB */ iotlb_entry = vtd_lookup_iotlb(s, source_id, addr); if (iotlb_entry) {
VTD_DPRINTF(CACHE, ""hit iotlb sid 0x%""PRIx16 "" gpa 0x%""PRIx64 "" slpte 0x%""PRIx64 "" did 0x%""PRIx16, source_id, addr, iotlb_entry->slpte, iotlb_entry->domain_id); slpte = iotlb_entry->slpte; reads = iotlb_entry->read_flags; writes = iotlb_entry->write_flags; goto out; } /* Try to fetch context-entry from cache first */ if (cc_entry->context_cache_gen == s->context_cache_gen) {
VTD_DPRINTF(CACHE, ""hit context-cache bus %d devfn %d "" ""(hi %""PRIx64 "" lo %""PRIx64 "" gen %""PRIu32 "")"", bus_num, devfn, cc_entry->context_entry.hi, cc_entry->context_entry.lo, cc_entry->context_cache_gen); ce = cc_entry->context_entry; is_fpd_set = ce.lo & VTD_CONTEXT_ENTRY_FPD; } else {
ret_fr = vtd_dev_to_context_entry(s, bus_num, devfn, &ce); is_fpd_set = ce.lo & VTD_CONTEXT_ENTRY_FPD; if (ret_fr) {
ret_fr = -ret_fr; if (is_fpd_set && vtd_is_qualified_fault(ret_fr)) {
VTD_DPRINTF(FLOG, ""fault processing is disabled for DMA "" ""requests through this context-entry "" ""(with FPD Set)""); } else {
vtd_report_dmar_fault(s, source_id, addr, ret_fr, is_write); } return; } /* Update context-cache */ VTD_DPRINTF(CACHE, ""update context-cache bus %d devfn %d "" ""(hi %""PRIx64 "" lo %""PRIx64 "" gen %""PRIu32 ""->%""PRIu32 "")"", bus_num, devfn, ce.hi, ce.lo, cc_entry->context_cache_gen, s->context_cache_gen); cc_entry->context_entry = ce; cc_entry->context_cache_gen = s->context_cache_gen; } ret_fr = vtd_gpa_to_slpte(&ce, addr, is_write, &slpte, &level, &reads, &writes); if (ret_fr) {
ret_fr = -ret_fr; if (is_fpd_set && vtd_is_qualified_fault(ret_fr)) {
VTD_DPRINTF(FLOG, ""fault processing is disabled for DMA requests "" ""through this context-entry (with FPD Set)""); } else {
vtd_report_dmar_fault(s, source_id, addr, ret_fr, is_write); } return; } vtd_update_iotlb(s, source_id, VTD_CONTEXT_ENTRY_DID(ce.hi), addr, slpte, reads, writes); out: entry->iova = addr & VTD_PAGE_MASK_4K; entry->translated_addr = vtd_get_slpte_addr(slpte) & VTD_PAGE_MASK_4K; entry->addr_mask = ~VTD_PAGE_MASK_4K; entry->perm = (writes ? 2 : 0) + (reads ? 1 : 0); } ",qemu,0
"static uint64_t jazz_led_read(void *opaque, target_phys_addr_t addr,

                              unsigned int size)

{

    LedState *s = opaque;

    uint8_t val;



    val = s->segments;

    trace_jazz_led_read(addr, val);



    return val;

}
","static uint64_t jazz_led_read(void *opaque, target_phys_addr_t addr, unsigned int size) {
LedState *s = opaque; uint8_t val; val = s->segments; trace_jazz_led_read(addr, val); return val; } ",qemu,0
"static int mp_dacl_setxattr(FsContext *ctx, const char *path, const char *name,

                            void *value, size_t size, int flags)

{

    char buffer[PATH_MAX];

    return lsetxattr(rpath(ctx, path, buffer), MAP_ACL_DEFAULT, value,

            size, flags);

}
","static int mp_dacl_setxattr(FsContext *ctx, const char *path, const char *name, void *value, size_t size, int flags) {
char buffer[PATH_MAX]; return lsetxattr(rpath(ctx, path, buffer), MAP_ACL_DEFAULT, value, size, flags); } ",qemu,0
"static int raw_eject(BlockDriverState *bs, int eject_flag)

{

    BDRVRawState *s = bs->opaque;



    switch(s->type) {

    case FTYPE_CD:

        if (eject_flag) {

            if (ioctl (s->fd, CDROMEJECT, NULL) < 0)

                perror(""CDROMEJECT"");

        } else {

            if (ioctl (s->fd, CDROMCLOSETRAY, NULL) < 0)

                perror(""CDROMEJECT"");

        }

        break;

    case FTYPE_FD:

        {

            int fd;

            if (s->fd >= 0) {

                close(s->fd);

                s->fd = -1;

                raw_close_fd_pool(s);

            }

            fd = open(bs->filename, s->fd_open_flags | O_NONBLOCK);

            if (fd >= 0) {

                if (ioctl(fd, FDEJECT, 0) < 0)

                    perror(""FDEJECT"");

                close(fd);

            }

        }

        break;

    default:

        return -ENOTSUP;

    }

    return 0;

}
","static int raw_eject(BlockDriverState *bs, int eject_flag) {
BDRVRawState *s = bs->opaque; switch(s->type) {
case FTYPE_CD: if (eject_flag) {
if (ioctl (s->fd, CDROMEJECT, NULL) < 0) perror(""CDROMEJECT""); } else {
if (ioctl (s->fd, CDROMCLOSETRAY, NULL) < 0) perror(""CDROMEJECT""); } break; case FTYPE_FD: {
int fd; if (s->fd >= 0) {
close(s->fd); s->fd = -1; raw_close_fd_pool(s); } fd = open(bs->filename, s->fd_open_flags | O_NONBLOCK); if (fd >= 0) {
if (ioctl(fd, FDEJECT, 0) < 0) perror(""FDEJECT""); close(fd); } } break; default: return -ENOTSUP; } return 0; } ",qemu,0
"QmpOutputVisitor *qmp_output_visitor_new(void)

{

    QmpOutputVisitor *v;



    v = g_malloc0(sizeof(*v));



    v->visitor.type = VISITOR_OUTPUT;

    v->visitor.start_struct = qmp_output_start_struct;

    v->visitor.end_struct = qmp_output_end_struct;

    v->visitor.start_list = qmp_output_start_list;

    v->visitor.next_list = qmp_output_next_list;

    v->visitor.end_list = qmp_output_end_list;

    v->visitor.type_int64 = qmp_output_type_int64;

    v->visitor.type_uint64 = qmp_output_type_uint64;

    v->visitor.type_bool = qmp_output_type_bool;

    v->visitor.type_str = qmp_output_type_str;

    v->visitor.type_number = qmp_output_type_number;

    v->visitor.type_any = qmp_output_type_any;

    v->visitor.type_null = qmp_output_type_null;

    v->visitor.free = qmp_output_free;



    QTAILQ_INIT(&v->stack);



    return v;

}
","QmpOutputVisitor *qmp_output_visitor_new(void) {
QmpOutputVisitor *v; v = g_malloc0(sizeof(*v)); v->visitor.type = VISITOR_OUTPUT; v->visitor.start_struct = qmp_output_start_struct; v->visitor.end_struct = qmp_output_end_struct; v->visitor.start_list = qmp_output_start_list; v->visitor.next_list = qmp_output_next_list; v->visitor.end_list = qmp_output_end_list; v->visitor.type_int64 = qmp_output_type_int64; v->visitor.type_uint64 = qmp_output_type_uint64; v->visitor.type_bool = qmp_output_type_bool; v->visitor.type_str = qmp_output_type_str; v->visitor.type_number = qmp_output_type_number; v->visitor.type_any = qmp_output_type_any; v->visitor.type_null = qmp_output_type_null; v->visitor.free = qmp_output_free; QTAILQ_INIT(&v->stack); return v; } ",qemu,0
"static void qemu_sgl_init_external(VirtIOSCSIReq *req, struct iovec *sg,

                                   hwaddr *addr, int num)

{

    QEMUSGList *qsgl = &req->qsgl;



    qemu_sglist_init(qsgl, DEVICE(req->dev), num, &address_space_memory);

    while (num--) {

        qemu_sglist_add(qsgl, *(addr++), (sg++)->iov_len);

    }

}
","static void qemu_sgl_init_external(VirtIOSCSIReq *req, struct iovec *sg, hwaddr *addr, int num) {
QEMUSGList *qsgl = &req->qsgl; qemu_sglist_init(qsgl, DEVICE(req->dev), num, &address_space_memory); while (num--) {
qemu_sglist_add(qsgl, *(addr++), (sg++)->iov_len); } } ",qemu,0
"void unix_start_incoming_migration(const char *path, Error **errp)

{

    int s;



    s = unix_listen(path, NULL, 0, errp);

    if (s < 0) {

        return;

    }



    qemu_set_fd_handler2(s, NULL, unix_accept_incoming_migration, NULL,

                         (void *)(intptr_t)s);

}
","void unix_start_incoming_migration(const char *path, Error **errp) {
int s; s = unix_listen(path, NULL, 0, errp); if (s < 0) {
return; } qemu_set_fd_handler2(s, NULL, unix_accept_incoming_migration, NULL, (void *)(intptr_t)s); } ",qemu,1
"int qemu_file_rate_limit(QEMUFile *f)

{

    if (qemu_file_get_error(f)) {

        return 1;

    }

    if (f->xfer_limit > 0 && f->bytes_xfer > f->xfer_limit) {

        return 1;

    }

    return 0;

}
","int qemu_file_rate_limit(QEMUFile *f) {
if (qemu_file_get_error(f)) {
return 1; } if (f->xfer_limit > 0 && f->bytes_xfer > f->xfer_limit) {
return 1; } return 0; } ",qemu,1
"static void vfio_platform_realize(DeviceState *dev, Error **errp)

{

    VFIOPlatformDevice *vdev = VFIO_PLATFORM_DEVICE(dev);

    SysBusDevice *sbdev = SYS_BUS_DEVICE(dev);

    VFIODevice *vbasedev = &vdev->vbasedev;

    int i, ret;



    vbasedev->type = VFIO_DEVICE_TYPE_PLATFORM;

    vbasedev->ops = &vfio_platform_ops;



    trace_vfio_platform_realize(vbasedev->name, vdev->compat);



    ret = vfio_base_device_init(vbasedev);

    if (ret) {

        error_setg(errp, ""vfio: vfio_base_device_init failed for %s"",

                   vbasedev->name);

        return;

    }



    for (i = 0; i < vbasedev->num_regions; i++) {

        vfio_map_region(vdev, i);

        sysbus_init_mmio(sbdev, &vdev->regions[i]->mem);

    }

}
","static void vfio_platform_realize(DeviceState *dev, Error **errp) {
VFIOPlatformDevice *vdev = VFIO_PLATFORM_DEVICE(dev); SysBusDevice *sbdev = SYS_BUS_DEVICE(dev); VFIODevice *vbasedev = &vdev->vbasedev; int i, ret; vbasedev->type = VFIO_DEVICE_TYPE_PLATFORM; vbasedev->ops = &vfio_platform_ops; trace_vfio_platform_realize(vbasedev->name, vdev->compat); ret = vfio_base_device_init(vbasedev); if (ret) {
error_setg(errp, ""vfio: vfio_base_device_init failed for %s"", vbasedev->name); return; } for (i = 0; i < vbasedev->num_regions; i++) {
vfio_map_region(vdev, i); sysbus_init_mmio(sbdev, &vdev->regions[i]->mem); } } ",qemu,0
"void tcp_start_outgoing_migration(MigrationState *s, const char *host_port, Error **errp)

{

    inet_nonblocking_connect(host_port, tcp_wait_for_connect, s, errp);

}
","void tcp_start_outgoing_migration(MigrationState *s, const char *host_port, Error **errp) {
inet_nonblocking_connect(host_port, tcp_wait_for_connect, s, errp); } ",qemu,1
"CharDriverState *chr_testdev_init(void)

{

    TestdevCharState *testdev;

    CharDriverState *chr;



    testdev = g_malloc0(sizeof(TestdevCharState));

    testdev->chr = chr = g_malloc0(sizeof(CharDriverState));



    chr->opaque = testdev;

    chr->chr_write = testdev_write;

    chr->chr_close = testdev_close;



    return chr;

}
","CharDriverState *chr_testdev_init(void) {
TestdevCharState *testdev; CharDriverState *chr; testdev = g_malloc0(sizeof(TestdevCharState)); testdev->chr = chr = g_malloc0(sizeof(CharDriverState)); chr->opaque = testdev; chr->chr_write = testdev_write; chr->chr_close = testdev_close; return chr; } ",qemu,1
"static gint ppc_cpu_compare_class_pvr_mask(gconstpointer a, gconstpointer b)

{

    ObjectClass *oc = (ObjectClass *)a;

    uint32_t pvr = *(uint32_t *)b;

    PowerPCCPUClass *pcc = (PowerPCCPUClass *)a;

    gint ret;



    /* -cpu host does a PVR lookup during construction */

    if (unlikely(strcmp(object_class_get_name(oc),

                        TYPE_HOST_POWERPC_CPU) == 0)) {

        return -1;

    }



    if (!ppc_cpu_is_valid(pcc)) {

        return -1;

    }



    ret = (((pcc->pvr & pcc->pvr_mask) == (pvr & pcc->pvr_mask)) ? 0 : -1);



    return ret;

}
","static gint ppc_cpu_compare_class_pvr_mask(gconstpointer a, gconstpointer b) {
ObjectClass *oc = (ObjectClass *)a; uint32_t pvr = *(uint32_t *)b; PowerPCCPUClass *pcc = (PowerPCCPUClass *)a; gint ret; /* -cpu host does a PVR lookup during construction */ if (unlikely(strcmp(object_class_get_name(oc), TYPE_HOST_POWERPC_CPU) == 0)) {
return -1; } if (!ppc_cpu_is_valid(pcc)) {
return -1; } ret = (((pcc->pvr & pcc->pvr_mask) == (pvr & pcc->pvr_mask)) ? 0 : -1); return ret; } ",qemu,0
"int bdrv_open(BlockDriverState *bs, const char *filename, int flags,

              BlockDriver *drv)

{

    int ret;

    char tmp_filename[PATH_MAX];



    if (flags & BDRV_O_SNAPSHOT) {

        BlockDriverState *bs1;

        int64_t total_size;

        int is_protocol = 0;

        BlockDriver *bdrv_qcow2;

        QEMUOptionParameter *options;

        char backing_filename[PATH_MAX];



        /* if snapshot, we create a temporary backing file and open it

           instead of opening 'filename' directly */



        /* if there is a backing file, use it */

        bs1 = bdrv_new("""");

        ret = bdrv_open(bs1, filename, 0, drv);

        if (ret < 0) {

            bdrv_delete(bs1);

            return ret;

        }

        total_size = bdrv_getlength(bs1) & BDRV_SECTOR_MASK;



        if (bs1->drv && bs1->drv->protocol_name)

            is_protocol = 1;



        bdrv_delete(bs1);



        get_tmp_filename(tmp_filename, sizeof(tmp_filename));



        /* Real path is meaningless for protocols */

        if (is_protocol)

            snprintf(backing_filename, sizeof(backing_filename),

                     ""%s"", filename);

        else if (!realpath(filename, backing_filename))

            return -errno;



        bdrv_qcow2 = bdrv_find_format(""qcow2"");

        options = parse_option_parameters("""", bdrv_qcow2->create_options, NULL);



        set_option_parameter_int(options, BLOCK_OPT_SIZE, total_size);

        set_option_parameter(options, BLOCK_OPT_BACKING_FILE, backing_filename);

        if (drv) {

            set_option_parameter(options, BLOCK_OPT_BACKING_FMT,

                drv->format_name);

        }



        ret = bdrv_create(bdrv_qcow2, tmp_filename, options);

        free_option_parameters(options);

        if (ret < 0) {

            return ret;

        }



        filename = tmp_filename;

        drv = bdrv_qcow2;

        bs->is_temporary = 1;

    }



    /* Find the right image format driver */

    if (!drv) {

        ret = find_image_format(filename, &drv);

    }



    if (!drv) {

        goto unlink_and_fail;

    }



    /* Open the image */

    ret = bdrv_open_common(bs, filename, flags, drv);

    if (ret < 0) {

        goto unlink_and_fail;

    }



    /* If there is a backing file, use it */

    if ((flags & BDRV_O_NO_BACKING) == 0 && bs->backing_file[0] != '\0') {

        char backing_filename[PATH_MAX];

        int back_flags;

        BlockDriver *back_drv = NULL;



        bs->backing_hd = bdrv_new("""");

        bdrv_get_full_backing_filename(bs, backing_filename,

                                       sizeof(backing_filename));



        if (bs->backing_format[0] != '\0') {

            back_drv = bdrv_find_format(bs->backing_format);

        }



        /* backing files always opened read-only */

        back_flags =

            flags & ~(BDRV_O_RDWR | BDRV_O_SNAPSHOT | BDRV_O_NO_BACKING);



        ret = bdrv_open(bs->backing_hd, backing_filename, back_flags, back_drv);

        if (ret < 0) {

            bdrv_close(bs);

            return ret;

        }

        if (bs->is_temporary) {

            bs->backing_hd->keep_read_only = !(flags & BDRV_O_RDWR);

        } else {

            /* base image inherits from ""parent"" */

            bs->backing_hd->keep_read_only = bs->keep_read_only;

        }

    }



    if (!bdrv_key_required(bs)) {

        bdrv_dev_change_media_cb(bs, true);

    }



    /* throttling disk I/O limits */

    if (bs->io_limits_enabled) {

        bdrv_io_limits_enable(bs);

    }



    return 0;



unlink_and_fail:

    if (bs->is_temporary) {

        unlink(filename);

    }

    return ret;

}
","int bdrv_open(BlockDriverState *bs, const char *filename, int flags, BlockDriver *drv) {
int ret; char tmp_filename[PATH_MAX]; if (flags & BDRV_O_SNAPSHOT) {
BlockDriverState *bs1; int64_t total_size; int is_protocol = 0; BlockDriver *bdrv_qcow2; QEMUOptionParameter *options; char backing_filename[PATH_MAX]; /* if snapshot, we create a temporary backing file and open it instead of opening 'filename' directly */ /* if there is a backing file, use it */ bs1 = bdrv_new(""""); ret = bdrv_open(bs1, filename, 0, drv); if (ret < 0) {
bdrv_delete(bs1); return ret; } total_size = bdrv_getlength(bs1) & BDRV_SECTOR_MASK; if (bs1->drv && bs1->drv->protocol_name) is_protocol = 1; bdrv_delete(bs1); get_tmp_filename(tmp_filename, sizeof(tmp_filename)); /* Real path is meaningless for protocols */ if (is_protocol) snprintf(backing_filename, sizeof(backing_filename), ""%s"", filename); else if (!realpath(filename, backing_filename)) return -errno; bdrv_qcow2 = bdrv_find_format(""qcow2""); options = parse_option_parameters("""", bdrv_qcow2->create_options, NULL); set_option_parameter_int(options, BLOCK_OPT_SIZE, total_size); set_option_parameter(options, BLOCK_OPT_BACKING_FILE, backing_filename); if (drv) {
set_option_parameter(options, BLOCK_OPT_BACKING_FMT, drv->format_name); } ret = bdrv_create(bdrv_qcow2, tmp_filename, options); free_option_parameters(options); if (ret < 0) {
return ret; } filename = tmp_filename; drv = bdrv_qcow2; bs->is_temporary = 1; } /* Find the right image format driver */ if (!drv) {
ret = find_image_format(filename, &drv); } if (!drv) {
goto unlink_and_fail; } /* Open the image */ ret = bdrv_open_common(bs, filename, flags, drv); if (ret < 0) {
goto unlink_and_fail; } /* If there is a backing file, use it */ if ((flags & BDRV_O_NO_BACKING) == 0 && bs->backing_file[0] != '\0') {
char backing_filename[PATH_MAX]; int back_flags; BlockDriver *back_drv = NULL; bs->backing_hd = bdrv_new(""""); bdrv_get_full_backing_filename(bs, backing_filename, sizeof(backing_filename)); if (bs->backing_format[0] != '\0') {
back_drv = bdrv_find_format(bs->backing_format); } /* backing files always opened read-only */ back_flags = flags & ~(BDRV_O_RDWR | BDRV_O_SNAPSHOT | BDRV_O_NO_BACKING); ret = bdrv_open(bs->backing_hd, backing_filename, back_flags, back_drv); if (ret < 0) {
bdrv_close(bs); return ret; } if (bs->is_temporary) {
bs->backing_hd->keep_read_only = !(flags & BDRV_O_RDWR); } else {
/* base image inherits from ""parent"" */ bs->backing_hd->keep_read_only = bs->keep_read_only; } } if (!bdrv_key_required(bs)) {
bdrv_dev_change_media_cb(bs, true); } /* throttling disk I/O limits */ if (bs->io_limits_enabled) {
bdrv_io_limits_enable(bs); } return 0; unlink_and_fail: if (bs->is_temporary) {
unlink(filename); } return ret; } ",qemu,1
"static void qxl_track_command(PCIQXLDevice *qxl, struct QXLCommandExt *ext)

{

    switch (le32_to_cpu(ext->cmd.type)) {

    case QXL_CMD_SURFACE:

    {

        QXLSurfaceCmd *cmd = qxl_phys2virt(qxl, ext->cmd.data, ext->group_id);

        uint32_t id = le32_to_cpu(cmd->surface_id);

        PANIC_ON(id >= NUM_SURFACES);

        qemu_mutex_lock(&qxl->track_lock);

        if (cmd->type == QXL_SURFACE_CMD_CREATE) {

            qxl->guest_surfaces.cmds[id] = ext->cmd.data;

            qxl->guest_surfaces.count++;

            if (qxl->guest_surfaces.max < qxl->guest_surfaces.count)

                qxl->guest_surfaces.max = qxl->guest_surfaces.count;

        }

        if (cmd->type == QXL_SURFACE_CMD_DESTROY) {

            qxl->guest_surfaces.cmds[id] = 0;

            qxl->guest_surfaces.count--;

        }

        qemu_mutex_unlock(&qxl->track_lock);

        break;

    }

    case QXL_CMD_CURSOR:

    {

        QXLCursorCmd *cmd = qxl_phys2virt(qxl, ext->cmd.data, ext->group_id);

        if (cmd->type == QXL_CURSOR_SET) {

            qemu_mutex_lock(&qxl->track_lock);

            qxl->guest_cursor = ext->cmd.data;

            qemu_mutex_unlock(&qxl->track_lock);

        }

        break;

    }

    }

}
","static void qxl_track_command(PCIQXLDevice *qxl, struct QXLCommandExt *ext) {
switch (le32_to_cpu(ext->cmd.type)) {
case QXL_CMD_SURFACE: {
QXLSurfaceCmd *cmd = qxl_phys2virt(qxl, ext->cmd.data, ext->group_id); uint32_t id = le32_to_cpu(cmd->surface_id); PANIC_ON(id >= NUM_SURFACES); qemu_mutex_lock(&qxl->track_lock); if (cmd->type == QXL_SURFACE_CMD_CREATE) {
qxl->guest_surfaces.cmds[id] = ext->cmd.data; qxl->guest_surfaces.count++; if (qxl->guest_surfaces.max < qxl->guest_surfaces.count) qxl->guest_surfaces.max = qxl->guest_surfaces.count; } if (cmd->type == QXL_SURFACE_CMD_DESTROY) {
qxl->guest_surfaces.cmds[id] = 0; qxl->guest_surfaces.count--; } qemu_mutex_unlock(&qxl->track_lock); break; } case QXL_CMD_CURSOR: {
QXLCursorCmd *cmd = qxl_phys2virt(qxl, ext->cmd.data, ext->group_id); if (cmd->type == QXL_CURSOR_SET) {
qemu_mutex_lock(&qxl->track_lock); qxl->guest_cursor = ext->cmd.data; qemu_mutex_unlock(&qxl->track_lock); } break; } } } ",qemu,0
"static void s390_cpu_plug(HotplugHandler *hotplug_dev,

                        DeviceState *dev, Error **errp)

{

    gchar *name;

    S390CPU *cpu = S390_CPU(dev);

    CPUState *cs = CPU(dev);



    name = g_strdup_printf(""cpu[%i]"", cpu->env.cpu_num);

    object_property_set_link(OBJECT(hotplug_dev), OBJECT(cs), name,

                             errp);

    g_free(name);

}
","static void s390_cpu_plug(HotplugHandler *hotplug_dev, DeviceState *dev, Error **errp) {
gchar *name; S390CPU *cpu = S390_CPU(dev); CPUState *cs = CPU(dev); name = g_strdup_printf(""cpu[%i]"", cpu->env.cpu_num); object_property_set_link(OBJECT(hotplug_dev), OBJECT(cs), name, errp); g_free(name); } ",qemu,0
"static inline void *host_from_stream_offset(QEMUFile *f,

                                            ram_addr_t offset,

                                            int flags)

{

    static RAMBlock *block = NULL;

    char id[256];

    uint8_t len;



    if (flags & RAM_SAVE_FLAG_CONTINUE) {

        if (!block) {

            error_report(""Ack, bad migration stream!"");

            return NULL;

        }



        return memory_region_get_ram_ptr(block->mr) + offset;

    }



    len = qemu_get_byte(f);

    qemu_get_buffer(f, (uint8_t *)id, len);

    id[len] = 0;



    QTAILQ_FOREACH(block, &ram_list.blocks, next) {

        if (!strncmp(id, block->idstr, sizeof(id)))

            return memory_region_get_ram_ptr(block->mr) + offset;

    }



    error_report(""Can't find block %s!"", id);

    return NULL;

}
","static inline void *host_from_stream_offset(QEMUFile *f, ram_addr_t offset, int flags) {
static RAMBlock *block = NULL; char id[256]; uint8_t len; if (flags & RAM_SAVE_FLAG_CONTINUE) {
if (!block) {
error_report(""Ack, bad migration stream!""); return NULL; } return memory_region_get_ram_ptr(block->mr) + offset; } len = qemu_get_byte(f); qemu_get_buffer(f, (uint8_t *)id, len); id[len] = 0; QTAILQ_FOREACH(block, &ram_list.blocks, next) {
if (!strncmp(id, block->idstr, sizeof(id))) return memory_region_get_ram_ptr(block->mr) + offset; } error_report(""Can't find block %s!"", id); return NULL; } ",qemu,1
"static int bdrv_open_common(BlockDriverState *bs, BlockDriverState *file,

    QDict *options, int flags, BlockDriver *drv, Error **errp)

{

    int ret, open_flags;

    const char *filename;

    const char *node_name = NULL;

    Error *local_err = NULL;



    assert(drv != NULL);

    assert(bs->file == NULL);

    assert(options != NULL && bs->options != options);



    if (file != NULL) {

        filename = file->filename;

    } else {

        filename = qdict_get_try_str(options, ""filename"");

    }



    if (drv->bdrv_needs_filename && !filename) {

        error_setg(errp, ""The '%s' block driver requires a file name"",

                   drv->format_name);

        return -EINVAL;

    }



    trace_bdrv_open_common(bs, filename ?: """", flags, drv->format_name);



    node_name = qdict_get_try_str(options, ""node-name"");

    bdrv_assign_node_name(bs, node_name, &local_err);

    if (local_err) {

        error_propagate(errp, local_err);

        return -EINVAL;

    }

    qdict_del(options, ""node-name"");



    /* bdrv_open() with directly using a protocol as drv. This layer is already

     * opened, so assign it to bs (while file becomes a closed BlockDriverState)

     * and return immediately. */

    if (file != NULL && drv->bdrv_file_open) {

        bdrv_swap(file, bs);

        return 0;

    }



    bs->open_flags = flags;

    bs->guest_block_size = 512;

    bs->request_alignment = 512;

    bs->zero_beyond_eof = true;

    open_flags = bdrv_open_flags(bs, flags);

    bs->read_only = !(open_flags & BDRV_O_RDWR);



    if (use_bdrv_whitelist && !bdrv_is_whitelisted(drv, bs->read_only)) {

        error_setg(errp,

                   !bs->read_only && bdrv_is_whitelisted(drv, true)

                        ? ""Driver '%s' can only be used for read-only devices""

                        : ""Driver '%s' is not whitelisted"",

                   drv->format_name);

        return -ENOTSUP;

    }



    assert(bs->copy_on_read == 0); /* bdrv_new() and bdrv_close() make it so */

    if (flags & BDRV_O_COPY_ON_READ) {

        if (!bs->read_only) {

            bdrv_enable_copy_on_read(bs);

        } else {

            error_setg(errp, ""Can't use copy-on-read on read-only device"");

            return -EINVAL;

        }

    }



    if (filename != NULL) {

        pstrcpy(bs->filename, sizeof(bs->filename), filename);

    } else {

        bs->filename[0] = '\0';

    }

    pstrcpy(bs->exact_filename, sizeof(bs->exact_filename), bs->filename);



    bs->drv = drv;

    bs->opaque = g_malloc0(drv->instance_size);



    bs->enable_write_cache = !!(flags & BDRV_O_CACHE_WB);



    /* Open the image, either directly or using a protocol */

    if (drv->bdrv_file_open) {

        assert(file == NULL);

        assert(!drv->bdrv_needs_filename || filename != NULL);

        ret = drv->bdrv_file_open(bs, options, open_flags, &local_err);

    } else {

        if (file == NULL) {

            error_setg(errp, ""Can't use '%s' as a block driver for the ""

                       ""protocol level"", drv->format_name);

            ret = -EINVAL;

            goto free_and_fail;

        }

        bs->file = file;

        ret = drv->bdrv_open(bs, options, open_flags, &local_err);

    }



    if (ret < 0) {

        if (local_err) {

            error_propagate(errp, local_err);

        } else if (bs->filename[0]) {

            error_setg_errno(errp, -ret, ""Could not open '%s'"", bs->filename);

        } else {

            error_setg_errno(errp, -ret, ""Could not open image"");

        }

        goto free_and_fail;

    }



    if (bs->encrypted) {

        error_report(""Encrypted images are deprecated"");

        error_printf(""Support for them will be removed in a future release.\n""

                     ""You can use 'qemu-img convert' to convert your image""

                     "" to an unencrypted one.\n"");

    }



    ret = refresh_total_sectors(bs, bs->total_sectors);

    if (ret < 0) {

        error_setg_errno(errp, -ret, ""Could not refresh total sector count"");

        goto free_and_fail;

    }



    bdrv_refresh_limits(bs, &local_err);

    if (local_err) {

        error_propagate(errp, local_err);

        ret = -EINVAL;

        goto free_and_fail;

    }



    assert(bdrv_opt_mem_align(bs) != 0);

    assert(bdrv_min_mem_align(bs) != 0);

    assert((bs->request_alignment != 0) || bs->sg);

    return 0;



free_and_fail:

    bs->file = NULL;

    g_free(bs->opaque);

    bs->opaque = NULL;

    bs->drv = NULL;

    return ret;

}
","static int bdrv_open_common(BlockDriverState *bs, BlockDriverState *file, QDict *options, int flags, BlockDriver *drv, Error **errp) {
int ret, open_flags; const char *filename; const char *node_name = NULL; Error *local_err = NULL; assert(drv != NULL); assert(bs->file == NULL); assert(options != NULL && bs->options != options); if (file != NULL) {
filename = file->filename; } else {
filename = qdict_get_try_str(options, ""filename""); } if (drv->bdrv_needs_filename && !filename) {
error_setg(errp, ""The '%s' block driver requires a file name"", drv->format_name); return -EINVAL; } trace_bdrv_open_common(bs, filename ?: """", flags, drv->format_name); node_name = qdict_get_try_str(options, ""node-name""); bdrv_assign_node_name(bs, node_name, &local_err); if (local_err) {
error_propagate(errp, local_err); return -EINVAL; } qdict_del(options, ""node-name""); /* bdrv_open() with directly using a protocol as drv. This layer is already * opened, so assign it to bs (while file becomes a closed BlockDriverState) * and return immediately. */ if (file != NULL && drv->bdrv_file_open) {
bdrv_swap(file, bs); return 0; } bs->open_flags = flags; bs->guest_block_size = 512; bs->request_alignment = 512; bs->zero_beyond_eof = true; open_flags = bdrv_open_flags(bs, flags); bs->read_only = !(open_flags & BDRV_O_RDWR); if (use_bdrv_whitelist && !bdrv_is_whitelisted(drv, bs->read_only)) {
error_setg(errp, !bs->read_only && bdrv_is_whitelisted(drv, true) ? ""Driver '%s' can only be used for read-only devices"" : ""Driver '%s' is not whitelisted"", drv->format_name); return -ENOTSUP; } assert(bs->copy_on_read == 0); /* bdrv_new() and bdrv_close() make it so */ if (flags & BDRV_O_COPY_ON_READ) {
if (!bs->read_only) {
bdrv_enable_copy_on_read(bs); } else {
error_setg(errp, ""Can't use copy-on-read on read-only device""); return -EINVAL; } } if (filename != NULL) {
pstrcpy(bs->filename, sizeof(bs->filename), filename); } else {
bs->filename[0] = '\0'; } pstrcpy(bs->exact_filename, sizeof(bs->exact_filename), bs->filename); bs->drv = drv; bs->opaque = g_malloc0(drv->instance_size); bs->enable_write_cache = !!(flags & BDRV_O_CACHE_WB); /* Open the image, either directly or using a protocol */ if (drv->bdrv_file_open) {
assert(file == NULL); assert(!drv->bdrv_needs_filename || filename != NULL); ret = drv->bdrv_file_open(bs, options, open_flags, &local_err); } else {
if (file == NULL) {
error_setg(errp, ""Can't use '%s' as a block driver for the "" ""protocol level"", drv->format_name); ret = -EINVAL; goto free_and_fail; } bs->file = file; ret = drv->bdrv_open(bs, options, open_flags, &local_err); } if (ret < 0) {
if (local_err) {
error_propagate(errp, local_err); } else if (bs->filename[0]) {
error_setg_errno(errp, -ret, ""Could not open '%s'"", bs->filename); } else {
error_setg_errno(errp, -ret, ""Could not open image""); } goto free_and_fail; } if (bs->encrypted) {
error_report(""Encrypted images are deprecated""); error_printf(""Support for them will be removed in a future release.\n"" ""You can use 'qemu-img convert' to convert your image"" "" to an unencrypted one.\n""); } ret = refresh_total_sectors(bs, bs->total_sectors); if (ret < 0) {
error_setg_errno(errp, -ret, ""Could not refresh total sector count""); goto free_and_fail; } bdrv_refresh_limits(bs, &local_err); if (local_err) {
error_propagate(errp, local_err); ret = -EINVAL; goto free_and_fail; } assert(bdrv_opt_mem_align(bs) != 0); assert(bdrv_min_mem_align(bs) != 0); assert((bs->request_alignment != 0) || bs->sg); return 0; free_and_fail: bs->file = NULL; g_free(bs->opaque); bs->opaque = NULL; bs->drv = NULL; return ret; } ",qemu,0
"TPMVersion tpm_tis_get_tpm_version(Object *obj)
{
    TPMState *s = TPM(obj);
    return tpm_backend_get_tpm_version(s->be_driver);","TPMVersion tpm_tis_get_tpm_version(Object *obj) {
TPMState *s = TPM(obj); return tpm_backend_get_tpm_version(s->be_driver);",qemu,1
"void pc_guest_info_init(PCMachineState *pcms)

{

    int i, j;



    pcms->apic_xrupt_override = kvm_allows_irq0_override();

    pcms->numa_nodes = nb_numa_nodes;

    pcms->node_mem = g_malloc0(pcms->numa_nodes *

                                    sizeof *pcms->node_mem);

    for (i = 0; i < nb_numa_nodes; i++) {

        pcms->node_mem[i] = numa_info[i].node_mem;

    }



    pcms->node_cpu = g_malloc0(pcms->apic_id_limit *

                                     sizeof *pcms->node_cpu);



    for (i = 0; i < max_cpus; i++) {

        unsigned int apic_id = x86_cpu_apic_id_from_index(i);

        assert(apic_id < pcms->apic_id_limit);

        for (j = 0; j < nb_numa_nodes; j++) {

            if (test_bit(i, numa_info[j].node_cpu)) {

                pcms->node_cpu[apic_id] = j;

                break;

            }

        }

    }



    pcms->machine_done.notify = pc_machine_done;

    qemu_add_machine_init_done_notifier(&pcms->machine_done);

}
","void pc_guest_info_init(PCMachineState *pcms) {
int i, j; pcms->apic_xrupt_override = kvm_allows_irq0_override(); pcms->numa_nodes = nb_numa_nodes; pcms->node_mem = g_malloc0(pcms->numa_nodes * sizeof *pcms->node_mem); for (i = 0; i < nb_numa_nodes; i++) {
pcms->node_mem[i] = numa_info[i].node_mem; } pcms->node_cpu = g_malloc0(pcms->apic_id_limit * sizeof *pcms->node_cpu); for (i = 0; i < max_cpus; i++) {
unsigned int apic_id = x86_cpu_apic_id_from_index(i); assert(apic_id < pcms->apic_id_limit); for (j = 0; j < nb_numa_nodes; j++) {
if (test_bit(i, numa_info[j].node_cpu)) {
pcms->node_cpu[apic_id] = j; break; } } } pcms->machine_done.notify = pc_machine_done; qemu_add_machine_init_done_notifier(&pcms->machine_done); } ",qemu,0
"static void test_hmac_speed(const void *opaque)

{

    size_t chunk_size = (size_t)opaque;

    QCryptoHmac *hmac = NULL;

    uint8_t *in = NULL, *out = NULL;

    size_t out_len = 0;

    double total = 0.0;

    struct iovec iov;

    Error *err = NULL;

    int ret;



    if (!qcrypto_hmac_supports(QCRYPTO_HASH_ALG_SHA256)) {

        return;

    }



    in = g_new0(uint8_t, chunk_size);

    memset(in, g_test_rand_int(), chunk_size);



    iov.iov_base = (char *)in;

    iov.iov_len = chunk_size;



    g_test_timer_start();

    do {

        hmac = qcrypto_hmac_new(QCRYPTO_HASH_ALG_SHA256,

                                (const uint8_t *)KEY, strlen(KEY), &err);

        g_assert(err == NULL);

        g_assert(hmac != NULL);



        ret = qcrypto_hmac_bytesv(hmac, &iov, 1, &out, &out_len, &err);

        g_assert(ret == 0);

        g_assert(err == NULL);



        qcrypto_hmac_free(hmac);



        total += chunk_size;

    } while (g_test_timer_elapsed() < 5.0);



    total /= 1024 * 1024; /* to MB */



    g_print(""hmac(sha256): "");

    g_print(""Testing chunk_size %ld bytes "", chunk_size);

    g_print(""done: %.2f MB in %.2f secs: "", total, g_test_timer_last());

    g_print(""%.2f MB/sec\n"", total / g_test_timer_last());



    g_free(out);

    g_free(in);

}
","static void test_hmac_speed(const void *opaque) {
size_t chunk_size = (size_t)opaque; QCryptoHmac *hmac = NULL; uint8_t *in = NULL, *out = NULL; size_t out_len = 0; double total = 0.0; struct iovec iov; Error *err = NULL; int ret; if (!qcrypto_hmac_supports(QCRYPTO_HASH_ALG_SHA256)) {
return; } in = g_new0(uint8_t, chunk_size); memset(in, g_test_rand_int(), chunk_size); iov.iov_base = (char *)in; iov.iov_len = chunk_size; g_test_timer_start(); do {
hmac = qcrypto_hmac_new(QCRYPTO_HASH_ALG_SHA256, (const uint8_t *)KEY, strlen(KEY), &err); g_assert(err == NULL); g_assert(hmac != NULL); ret = qcrypto_hmac_bytesv(hmac, &iov, 1, &out, &out_len, &err); g_assert(ret == 0); g_assert(err == NULL); qcrypto_hmac_free(hmac); total += chunk_size; } while (g_test_timer_elapsed() < 5.0); total /= 1024 * 1024; /* to MB */ g_print(""hmac(sha256): ""); g_print(""Testing chunk_size %ld bytes "", chunk_size); g_print(""done: %.2f MB in %.2f secs: "", total, g_test_timer_last()); g_print(""%.2f MB/sec\n"", total / g_test_timer_last()); g_free(out); g_free(in); } ",qemu,1
"static ssize_t nbd_receive_request(int csock, struct nbd_request *request)

{

    uint8_t buf[4 + 4 + 8 + 8 + 4];

    uint32_t magic;



    if (read_sync(csock, buf, sizeof(buf)) != sizeof(buf)) {

        LOG(""read failed"");

        errno = EINVAL;

        return -1;

    }



    /* Request

       [ 0 ..  3]   magic   (NBD_REQUEST_MAGIC)

       [ 4 ..  7]   type    (0 == READ, 1 == WRITE)

       [ 8 .. 15]   handle

       [16 .. 23]   from

       [24 .. 27]   len

     */



    magic = be32_to_cpup((uint32_t*)buf);

    request->type  = be32_to_cpup((uint32_t*)(buf + 4));

    request->handle = be64_to_cpup((uint64_t*)(buf + 8));

    request->from  = be64_to_cpup((uint64_t*)(buf + 16));

    request->len   = be32_to_cpup((uint32_t*)(buf + 24));



    TRACE(""Got request: ""

          ""{ magic = 0x%x, .type = %d, from = %"" PRIu64"" , len = %u }"",

          magic, request->type, request->from, request->len);



    if (magic != NBD_REQUEST_MAGIC) {

        LOG(""invalid magic (got 0x%x)"", magic);

        errno = EINVAL;

        return -1;

    }

    return 0;

}
","static ssize_t nbd_receive_request(int csock, struct nbd_request *request) {
uint8_t buf[4 + 4 + 8 + 8 + 4]; uint32_t magic; if (read_sync(csock, buf, sizeof(buf)) != sizeof(buf)) {
LOG(""read failed""); errno = EINVAL; return -1; } /* Request [ 0 .. 3] magic (NBD_REQUEST_MAGIC) [ 4 .. 7] type (0 == READ, 1 == WRITE) [ 8 .. 15] handle [16 .. 23] from [24 .. 27] len */ magic = be32_to_cpup((uint32_t*)buf); request->type = be32_to_cpup((uint32_t*)(buf + 4)); request->handle = be64_to_cpup((uint64_t*)(buf + 8)); request->from = be64_to_cpup((uint64_t*)(buf + 16)); request->len = be32_to_cpup((uint32_t*)(buf + 24)); TRACE(""Got request: "" "" {
magic = 0x%x, .type = %d, from = %"" PRIu64"" , len = %u }"", magic, request->type, request->from, request->len); if (magic != NBD_REQUEST_MAGIC) {
LOG(""invalid magic (got 0x%x)"", magic); errno = EINVAL; return -1; } return 0; } ",qemu,0
"static int guess_disk_lchs(IDEState *s,

                           int *pcylinders, int *pheads, int *psectors)

{

    uint8_t *buf;

    int ret, i, heads, sectors, cylinders;

    struct partition *p;

    uint32_t nr_sects;



    buf = qemu_memalign(512, 512);

    if (buf == NULL)

        return -1;

    ret = bdrv_read(s->bs, 0, buf, 1);

    if (ret < 0) {

        qemu_free(buf);

        return -1;

    }

    /* test msdos magic */

    if (buf[510] != 0x55 || buf[511] != 0xaa) {

        qemu_free(buf);

        return -1;

    }

    for(i = 0; i < 4; i++) {

        p = ((struct partition *)(buf + 0x1be)) + i;

        nr_sects = le32_to_cpu(p->nr_sects);

        if (nr_sects && p->end_head) {

            /* We make the assumption that the partition terminates on

               a cylinder boundary */

            heads = p->end_head + 1;

            sectors = p->end_sector & 63;

            if (sectors == 0)

                continue;

            cylinders = s->nb_sectors / (heads * sectors);

            if (cylinders < 1 || cylinders > 16383)

                continue;

            *pheads = heads;

            *psectors = sectors;

            *pcylinders = cylinders;

#if 0

            printf(""guessed geometry: LCHS=%d %d %d\n"",

                   cylinders, heads, sectors);

#endif

            qemu_free(buf);

            return 0;

        }

    }

    qemu_free(buf);

    return -1;

}
","static int guess_disk_lchs(IDEState *s, int *pcylinders, int *pheads, int *psectors) {
uint8_t *buf; int ret, i, heads, sectors, cylinders; struct partition *p; uint32_t nr_sects; buf = qemu_memalign(512, 512); if (buf == NULL) return -1; ret = bdrv_read(s->bs, 0, buf, 1); if (ret < 0) {
qemu_free(buf); return -1; } /* test msdos magic */ if (buf[510] != 0x55 || buf[511] != 0xaa) {
qemu_free(buf); return -1; } for(i = 0; i < 4; i++) {
p = ((struct partition *)(buf + 0x1be)) + i; nr_sects = le32_to_cpu(p->nr_sects); if (nr_sects && p->end_head) {
/* We make the assumption that the partition terminates on a cylinder boundary */ heads = p->end_head + 1; sectors = p->end_sector & 63; if (sectors == 0) continue; cylinders = s->nb_sectors / (heads * sectors); if (cylinders < 1 || cylinders > 16383) continue; *pheads = heads; *psectors = sectors; *pcylinders = cylinders; #if 0 printf(""guessed geometry: LCHS=%d %d %d\n"", cylinders, heads, sectors); #endif qemu_free(buf); return 0; } } qemu_free(buf); return -1; } ",qemu,1
"static void control_out(VirtIODevice *vdev, VirtQueue *vq)

{

    VirtQueueElement elem;

    VirtIOSerial *vser;



    vser = DO_UPCAST(VirtIOSerial, vdev, vdev);



    while (virtqueue_pop(vq, &elem)) {

        handle_control_message(vser, elem.out_sg[0].iov_base);

        virtqueue_push(vq, &elem, elem.out_sg[0].iov_len);

    }

    virtio_notify(vdev, vq);

}
","static void control_out(VirtIODevice *vdev, VirtQueue *vq) {
VirtQueueElement elem; VirtIOSerial *vser; vser = DO_UPCAST(VirtIOSerial, vdev, vdev); while (virtqueue_pop(vq, &elem)) {
handle_control_message(vser, elem.out_sg[0].iov_base); virtqueue_push(vq, &elem, elem.out_sg[0].iov_len); } virtio_notify(vdev, vq); } ",qemu,1
"static void icount_dummy_timer(void *opaque)

{

    (void)opaque;

}
","static void icount_dummy_timer(void *opaque) {
(void)opaque; } ",qemu,0
"m_get(Slirp *slirp)

{

	register struct mbuf *m;

	int flags = 0;



	DEBUG_CALL(""m_get"");



	if (slirp->m_freelist.m_next == &slirp->m_freelist) {

		m = (struct mbuf *)malloc(SLIRP_MSIZE);

		if (m == NULL) goto end_error;

		slirp->mbuf_alloced++;

		if (slirp->mbuf_alloced > MBUF_THRESH)

			flags = M_DOFREE;

		m->slirp = slirp;

	} else {

		m = slirp->m_freelist.m_next;

		remque(m);

	}



	/* Insert it in the used list */

	insque(m,&slirp->m_usedlist);

	m->m_flags = (flags | M_USEDLIST);



	/* Initialise it */

	m->m_size = SLIRP_MSIZE - sizeof(struct m_hdr);

	m->m_data = m->m_dat;

	m->m_len = 0;

        m->m_nextpkt = NULL;

        m->m_prevpkt = NULL;

end_error:

	DEBUG_ARG(""m = %lx"", (long )m);

	return m;

}
","m_get(Slirp *slirp) {
register struct mbuf *m; int flags = 0; DEBUG_CALL(""m_get""); if (slirp->m_freelist.m_next == &slirp->m_freelist) {
m = (struct mbuf *)malloc(SLIRP_MSIZE); if (m == NULL) goto end_error; slirp->mbuf_alloced++; if (slirp->mbuf_alloced > MBUF_THRESH) flags = M_DOFREE; m->slirp = slirp; } else {
m = slirp->m_freelist.m_next; remque(m); } /* Insert it in the used list */ insque(m,&slirp->m_usedlist); m->m_flags = (flags | M_USEDLIST); /* Initialise it */ m->m_size = SLIRP_MSIZE - sizeof(struct m_hdr); m->m_data = m->m_dat; m->m_len = 0; m->m_nextpkt = NULL; m->m_prevpkt = NULL; end_error: DEBUG_ARG(""m = %lx"", (long )m); return m; } ",qemu,1
"static void pc_init_pci_1_2(QEMUMachineInitArgs *args)

{

    disable_kvm_pv_eoi();

    enable_compat_apic_id_mode();

    pc_sysfw_flash_vs_rom_bug_compatible = true;

    has_pvpanic = false;

    pc_init_pci(args);

}
","static void pc_init_pci_1_2(QEMUMachineInitArgs *args) {
disable_kvm_pv_eoi(); enable_compat_apic_id_mode(); pc_sysfw_flash_vs_rom_bug_compatible = true; has_pvpanic = false; pc_init_pci(args); } ",qemu,1
"static QEMUFile *open_test_file(bool write)

{

    int fd = dup(temp_fd);

    QIOChannel *ioc;

    lseek(fd, 0, SEEK_SET);

    if (write) {

        g_assert_cmpint(ftruncate(fd, 0), ==, 0);

    }

    ioc = QIO_CHANNEL(qio_channel_file_new_fd(fd));

    if (write) {

        return qemu_fopen_channel_output(ioc);

    } else {

        return qemu_fopen_channel_input(ioc);

    }

}
","static QEMUFile *open_test_file(bool write) {
int fd = dup(temp_fd); QIOChannel *ioc; lseek(fd, 0, SEEK_SET); if (write) {
g_assert_cmpint(ftruncate(fd, 0), ==, 0); } ioc = QIO_CHANNEL(qio_channel_file_new_fd(fd)); if (write) {
return qemu_fopen_channel_output(ioc); } else {
return qemu_fopen_channel_input(ioc); } } ",qemu,1
"static void v9fs_read(void *opaque)

{

    int32_t fid;

    int64_t off;

    ssize_t err = 0;

    int32_t count = 0;

    size_t offset = 7;

    int32_t max_count;

    V9fsFidState *fidp;

    V9fsPDU *pdu = opaque;

    V9fsState *s = pdu->s;



    pdu_unmarshal(pdu, offset, ""dqd"", &fid, &off, &max_count);




    fidp = get_fid(pdu, fid);

    if (fidp == NULL) {

        err = -EINVAL;

        goto out_nofid;

    }

    if (fidp->fid_type == P9_FID_DIR) {



        if (off == 0) {

            v9fs_co_rewinddir(pdu, fidp);

        }

        count = v9fs_do_readdir_with_stat(pdu, fidp, max_count);

        if (count < 0) {

            err = count;

            goto out;

        }

        err = offset;

        err += pdu_marshal(pdu, offset, ""d"", count);

        err += count;

    } else if (fidp->fid_type == P9_FID_FILE) {

        int32_t cnt;

        int32_t len;

        struct iovec *sg;

        struct iovec iov[128]; /* FIXME: bad, bad, bad */



        sg = iov;

        pdu_marshal(pdu, offset + 4, ""v"", sg, &cnt);

        sg = cap_sg(sg, max_count, &cnt);

        do {

            if (0) {

                print_sg(sg, cnt);

            }

            /* Loop in case of EINTR */

            do {

                len = v9fs_co_preadv(pdu, fidp, sg, cnt, off);

                if (len >= 0) {

                    off   += len;

                    count += len;

                }

            } while (len == -EINTR && !pdu->cancelled);

            if (len < 0) {

                /* IO error return the error */

                err = len;

                goto out;

            }

            sg = adjust_sg(sg, len, &cnt);

        } while (count < max_count && len > 0);

        err = offset;

        err += pdu_marshal(pdu, offset, ""d"", count);

        err += count;

    } else if (fidp->fid_type == P9_FID_XATTR) {

        err = v9fs_xattr_read(s, pdu, fidp, off, max_count);

    } else {

        err = -EINVAL;

    }

out:

    put_fid(pdu, fidp);

out_nofid:

    trace_v9fs_read_return(pdu->tag, pdu->id, count, err);

    complete_pdu(s, pdu, err);

}","static void v9fs_read(void *opaque) {
int32_t fid; int64_t off; ssize_t err = 0; int32_t count = 0; size_t offset = 7; int32_t max_count; V9fsFidState *fidp; V9fsPDU *pdu = opaque; V9fsState *s = pdu->s; pdu_unmarshal(pdu, offset, ""dqd"", &fid, &off, &max_count); fidp = get_fid(pdu, fid); if (fidp == NULL) {
err = -EINVAL; goto out_nofid; } if (fidp->fid_type == P9_FID_DIR) {
if (off == 0) {
v9fs_co_rewinddir(pdu, fidp); } count = v9fs_do_readdir_with_stat(pdu, fidp, max_count); if (count < 0) {
err = count; goto out; } err = offset; err += pdu_marshal(pdu, offset, ""d"", count); err += count; } else if (fidp->fid_type == P9_FID_FILE) {
int32_t cnt; int32_t len; struct iovec *sg; struct iovec iov[128]; /* FIXME: bad, bad, bad */ sg = iov; pdu_marshal(pdu, offset + 4, ""v"", sg, &cnt); sg = cap_sg(sg, max_count, &cnt); do {
if (0) {
print_sg(sg, cnt); } /* Loop in case of EINTR */ do {
len = v9fs_co_preadv(pdu, fidp, sg, cnt, off); if (len >= 0) {
off += len; count += len; } } while (len == -EINTR && !pdu->cancelled); if (len < 0) {
/* IO error return the error */ err = len; goto out; } sg = adjust_sg(sg, len, &cnt); } while (count < max_count && len > 0); err = offset; err += pdu_marshal(pdu, offset, ""d"", count); err += count; } else if (fidp->fid_type == P9_FID_XATTR) {
err = v9fs_xattr_read(s, pdu, fidp, off, max_count); } else {
err = -EINVAL; } out: put_fid(pdu, fidp); out_nofid: trace_v9fs_read_return(pdu->tag, pdu->id, count, err); complete_pdu(s, pdu, err); }",qemu,1
"bool bdrv_all_can_snapshot(BlockDriverState **first_bad_bs)

{

    bool ok = true;

    BlockDriverState *bs;

    BdrvNextIterator it;



    for (bs = bdrv_first(&it); bs; bs = bdrv_next(&it)) {

        AioContext *ctx = bdrv_get_aio_context(bs);



        aio_context_acquire(ctx);

        if (bdrv_is_inserted(bs) && !bdrv_is_read_only(bs)) {

            ok = bdrv_can_snapshot(bs);

        }

        aio_context_release(ctx);

        if (!ok) {


            goto fail;

        }

    }



fail:

    *first_bad_bs = bs;

    return ok;

}","bool bdrv_all_can_snapshot(BlockDriverState **first_bad_bs) {
bool ok = true; BlockDriverState *bs; BdrvNextIterator it; for (bs = bdrv_first(&it); bs; bs = bdrv_next(&it)) {
AioContext *ctx = bdrv_get_aio_context(bs); aio_context_acquire(ctx); if (bdrv_is_inserted(bs) && !bdrv_is_read_only(bs)) {
ok = bdrv_can_snapshot(bs); } aio_context_release(ctx); if (!ok) {
goto fail; } } fail: *first_bad_bs = bs; return ok; }",qemu,1
"static void gen_sse(DisasContext *s, int b, target_ulong pc_start, int rex_r)

{

    int b1, op1_offset, op2_offset, is_xmm, val, ot;

    int modrm, mod, rm, reg, reg_addr, offset_addr;

    void *sse_op2;



    b &= 0xff;

    if (s->prefix & PREFIX_DATA)

        b1 = 1;

    else if (s->prefix & PREFIX_REPZ)

        b1 = 2;

    else if (s->prefix & PREFIX_REPNZ)

        b1 = 3;

    else

        b1 = 0;

    sse_op2 = sse_op_table1[b][b1];

    if (!sse_op2)

        goto illegal_op;

    if ((b <= 0x5f && b >= 0x10) || b == 0xc6 || b == 0xc2) {

        is_xmm = 1;

    } else {

        if (b1 == 0) {

            /* MMX case */

            is_xmm = 0;

        } else {

            is_xmm = 1;

        }

    }

    /* simple MMX/SSE operation */

    if (s->flags & HF_TS_MASK) {

        gen_exception(s, EXCP07_PREX, pc_start - s->cs_base);

        return;

    }

    if (s->flags & HF_EM_MASK) {

    illegal_op:

        gen_exception(s, EXCP06_ILLOP, pc_start - s->cs_base);

        return;

    }

    if (is_xmm && !(s->flags & HF_OSFXSR_MASK))

        if ((b != 0x38 && b != 0x3a) || (s->prefix & PREFIX_DATA))

            goto illegal_op;

    if (b == 0x0e) {

        if (!(s->cpuid_ext2_features & CPUID_EXT2_3DNOW))

            goto illegal_op;

        /* femms */

        tcg_gen_helper_0_0(helper_emms);

        return;

    }

    if (b == 0x77) {

        /* emms */

        tcg_gen_helper_0_0(helper_emms);

        return;

    }

    /* prepare MMX state (XXX: optimize by storing fptt and fptags in

       the static cpu state) */

    if (!is_xmm) {

        tcg_gen_helper_0_0(helper_enter_mmx);

    }



    modrm = ldub_code(s->pc++);

    reg = ((modrm >> 3) & 7);

    if (is_xmm)

        reg |= rex_r;

    mod = (modrm >> 6) & 3;

    if (sse_op2 == SSE_SPECIAL) {

        b |= (b1 << 8);

        switch(b) {

        case 0x0e7: /* movntq */

            if (mod == 3)

                goto illegal_op;

            gen_lea_modrm(s, modrm, &reg_addr, &offset_addr);

            gen_stq_env_A0(s->mem_index, offsetof(CPUX86State,fpregs[reg].mmx));

            break;

        case 0x1e7: /* movntdq */

        case 0x02b: /* movntps */

        case 0x12b: /* movntps */

        case 0x3f0: /* lddqu */

            if (mod == 3)

                goto illegal_op;

            gen_lea_modrm(s, modrm, &reg_addr, &offset_addr);

            gen_sto_env_A0(s->mem_index, offsetof(CPUX86State,xmm_regs[reg]));

            break;

        case 0x6e: /* movd mm, ea */

#ifdef TARGET_X86_64

            if (s->dflag == 2) {

                gen_ldst_modrm(s, modrm, OT_QUAD, OR_TMP0, 0);

                tcg_gen_st_tl(cpu_T[0], cpu_env, offsetof(CPUX86State,fpregs[reg].mmx));

            } else

#endif

            {

                gen_ldst_modrm(s, modrm, OT_LONG, OR_TMP0, 0);

                tcg_gen_addi_ptr(cpu_ptr0, cpu_env, 

                                 offsetof(CPUX86State,fpregs[reg].mmx));

                tcg_gen_helper_0_2(helper_movl_mm_T0_mmx, cpu_ptr0, cpu_T[0]);

            }

            break;

        case 0x16e: /* movd xmm, ea */

#ifdef TARGET_X86_64

            if (s->dflag == 2) {

                gen_ldst_modrm(s, modrm, OT_QUAD, OR_TMP0, 0);

                tcg_gen_addi_ptr(cpu_ptr0, cpu_env, 

                                 offsetof(CPUX86State,xmm_regs[reg]));

                tcg_gen_helper_0_2(helper_movq_mm_T0_xmm, cpu_ptr0, cpu_T[0]);

            } else

#endif

            {

                gen_ldst_modrm(s, modrm, OT_LONG, OR_TMP0, 0);

                tcg_gen_addi_ptr(cpu_ptr0, cpu_env, 

                                 offsetof(CPUX86State,xmm_regs[reg]));

                tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T[0]);

                tcg_gen_helper_0_2(helper_movl_mm_T0_xmm, cpu_ptr0, cpu_tmp2_i32);

            }

            break;

        case 0x6f: /* movq mm, ea */

            if (mod != 3) {

                gen_lea_modrm(s, modrm, &reg_addr, &offset_addr);

                gen_ldq_env_A0(s->mem_index, offsetof(CPUX86State,fpregs[reg].mmx));

            } else {

                rm = (modrm & 7);

                tcg_gen_ld_i64(cpu_tmp1_i64, cpu_env,

                               offsetof(CPUX86State,fpregs[rm].mmx));

                tcg_gen_st_i64(cpu_tmp1_i64, cpu_env,

                               offsetof(CPUX86State,fpregs[reg].mmx));

            }

            break;

        case 0x010: /* movups */

        case 0x110: /* movupd */

        case 0x028: /* movaps */

        case 0x128: /* movapd */

        case 0x16f: /* movdqa xmm, ea */

        case 0x26f: /* movdqu xmm, ea */

            if (mod != 3) {

                gen_lea_modrm(s, modrm, &reg_addr, &offset_addr);

                gen_ldo_env_A0(s->mem_index, offsetof(CPUX86State,xmm_regs[reg]));

            } else {

                rm = (modrm & 7) | REX_B(s);

                gen_op_movo(offsetof(CPUX86State,xmm_regs[reg]),

                            offsetof(CPUX86State,xmm_regs[rm]));

            }

            break;

        case 0x210: /* movss xmm, ea */

            if (mod != 3) {

                gen_lea_modrm(s, modrm, &reg_addr, &offset_addr);

                gen_op_ld_T0_A0(OT_LONG + s->mem_index);

                tcg_gen_st32_tl(cpu_T[0], cpu_env, offsetof(CPUX86State,xmm_regs[reg].XMM_L(0)));

                gen_op_movl_T0_0();

                tcg_gen_st32_tl(cpu_T[0], cpu_env, offsetof(CPUX86State,xmm_regs[reg].XMM_L(1)));

                tcg_gen_st32_tl(cpu_T[0], cpu_env, offsetof(CPUX86State,xmm_regs[reg].XMM_L(2)));

                tcg_gen_st32_tl(cpu_T[0], cpu_env, offsetof(CPUX86State,xmm_regs[reg].XMM_L(3)));

            } else {

                rm = (modrm & 7) | REX_B(s);

                gen_op_movl(offsetof(CPUX86State,xmm_regs[reg].XMM_L(0)),

                            offsetof(CPUX86State,xmm_regs[rm].XMM_L(0)));

            }

            break;

        case 0x310: /* movsd xmm, ea */

            if (mod != 3) {

                gen_lea_modrm(s, modrm, &reg_addr, &offset_addr);

                gen_ldq_env_A0(s->mem_index, offsetof(CPUX86State,xmm_regs[reg].XMM_Q(0)));

                gen_op_movl_T0_0();

                tcg_gen_st32_tl(cpu_T[0], cpu_env, offsetof(CPUX86State,xmm_regs[reg].XMM_L(2)));

                tcg_gen_st32_tl(cpu_T[0], cpu_env, offsetof(CPUX86State,xmm_regs[reg].XMM_L(3)));

            } else {

                rm = (modrm & 7) | REX_B(s);

                gen_op_movq(offsetof(CPUX86State,xmm_regs[reg].XMM_Q(0)),

                            offsetof(CPUX86State,xmm_regs[rm].XMM_Q(0)));

            }

            break;

        case 0x012: /* movlps */

        case 0x112: /* movlpd */

            if (mod != 3) {

                gen_lea_modrm(s, modrm, &reg_addr, &offset_addr);

                gen_ldq_env_A0(s->mem_index, offsetof(CPUX86State,xmm_regs[reg].XMM_Q(0)));

            } else {

                /* movhlps */

                rm = (modrm & 7) | REX_B(s);

                gen_op_movq(offsetof(CPUX86State,xmm_regs[reg].XMM_Q(0)),

                            offsetof(CPUX86State,xmm_regs[rm].XMM_Q(1)));

            }

            break;

        case 0x212: /* movsldup */

            if (mod != 3) {

                gen_lea_modrm(s, modrm, &reg_addr, &offset_addr);

                gen_ldo_env_A0(s->mem_index, offsetof(CPUX86State,xmm_regs[reg]));

            } else {

                rm = (modrm & 7) | REX_B(s);

                gen_op_movl(offsetof(CPUX86State,xmm_regs[reg].XMM_L(0)),

                            offsetof(CPUX86State,xmm_regs[rm].XMM_L(0)));

                gen_op_movl(offsetof(CPUX86State,xmm_regs[reg].XMM_L(2)),

                            offsetof(CPUX86State,xmm_regs[rm].XMM_L(2)));

            }

            gen_op_movl(offsetof(CPUX86State,xmm_regs[reg].XMM_L(1)),

                        offsetof(CPUX86State,xmm_regs[reg].XMM_L(0)));

            gen_op_movl(offsetof(CPUX86State,xmm_regs[reg].XMM_L(3)),

                        offsetof(CPUX86State,xmm_regs[reg].XMM_L(2)));

            break;

        case 0x312: /* movddup */

            if (mod != 3) {

                gen_lea_modrm(s, modrm, &reg_addr, &offset_addr);

                gen_ldq_env_A0(s->mem_index, offsetof(CPUX86State,xmm_regs[reg].XMM_Q(0)));

            } else {

                rm = (modrm & 7) | REX_B(s);

                gen_op_movq(offsetof(CPUX86State,xmm_regs[reg].XMM_Q(0)),

                            offsetof(CPUX86State,xmm_regs[rm].XMM_Q(0)));

            }

            gen_op_movq(offsetof(CPUX86State,xmm_regs[reg].XMM_Q(1)),

                        offsetof(CPUX86State,xmm_regs[reg].XMM_Q(0)));

            break;

        case 0x016: /* movhps */

        case 0x116: /* movhpd */

            if (mod != 3) {

                gen_lea_modrm(s, modrm, &reg_addr, &offset_addr);

                gen_ldq_env_A0(s->mem_index, offsetof(CPUX86State,xmm_regs[reg].XMM_Q(1)));

            } else {

                /* movlhps */

                rm = (modrm & 7) | REX_B(s);

                gen_op_movq(offsetof(CPUX86State,xmm_regs[reg].XMM_Q(1)),

                            offsetof(CPUX86State,xmm_regs[rm].XMM_Q(0)));

            }

            break;

        case 0x216: /* movshdup */

            if (mod != 3) {

                gen_lea_modrm(s, modrm, &reg_addr, &offset_addr);

                gen_ldo_env_A0(s->mem_index, offsetof(CPUX86State,xmm_regs[reg]));

            } else {

                rm = (modrm & 7) | REX_B(s);

                gen_op_movl(offsetof(CPUX86State,xmm_regs[reg].XMM_L(1)),

                            offsetof(CPUX86State,xmm_regs[rm].XMM_L(1)));

                gen_op_movl(offsetof(CPUX86State,xmm_regs[reg].XMM_L(3)),

                            offsetof(CPUX86State,xmm_regs[rm].XMM_L(3)));

            }

            gen_op_movl(offsetof(CPUX86State,xmm_regs[reg].XMM_L(0)),

                        offsetof(CPUX86State,xmm_regs[reg].XMM_L(1)));

            gen_op_movl(offsetof(CPUX86State,xmm_regs[reg].XMM_L(2)),

                        offsetof(CPUX86State,xmm_regs[reg].XMM_L(3)));

            break;

        case 0x7e: /* movd ea, mm */

#ifdef TARGET_X86_64

            if (s->dflag == 2) {

                tcg_gen_ld_i64(cpu_T[0], cpu_env, 

                               offsetof(CPUX86State,fpregs[reg].mmx));

                gen_ldst_modrm(s, modrm, OT_QUAD, OR_TMP0, 1);

            } else

#endif

            {

                tcg_gen_ld32u_tl(cpu_T[0], cpu_env, 

                                 offsetof(CPUX86State,fpregs[reg].mmx.MMX_L(0)));

                gen_ldst_modrm(s, modrm, OT_LONG, OR_TMP0, 1);

            }

            break;

        case 0x17e: /* movd ea, xmm */

#ifdef TARGET_X86_64

            if (s->dflag == 2) {

                tcg_gen_ld_i64(cpu_T[0], cpu_env, 

                               offsetof(CPUX86State,xmm_regs[reg].XMM_Q(0)));

                gen_ldst_modrm(s, modrm, OT_QUAD, OR_TMP0, 1);

            } else

#endif

            {

                tcg_gen_ld32u_tl(cpu_T[0], cpu_env, 

                                 offsetof(CPUX86State,xmm_regs[reg].XMM_L(0)));

                gen_ldst_modrm(s, modrm, OT_LONG, OR_TMP0, 1);

            }

            break;

        case 0x27e: /* movq xmm, ea */

            if (mod != 3) {

                gen_lea_modrm(s, modrm, &reg_addr, &offset_addr);

                gen_ldq_env_A0(s->mem_index, offsetof(CPUX86State,xmm_regs[reg].XMM_Q(0)));

            } else {

                rm = (modrm & 7) | REX_B(s);

                gen_op_movq(offsetof(CPUX86State,xmm_regs[reg].XMM_Q(0)),

                            offsetof(CPUX86State,xmm_regs[rm].XMM_Q(0)));

            }

            gen_op_movq_env_0(offsetof(CPUX86State,xmm_regs[reg].XMM_Q(1)));

            break;

        case 0x7f: /* movq ea, mm */

            if (mod != 3) {

                gen_lea_modrm(s, modrm, &reg_addr, &offset_addr);

                gen_stq_env_A0(s->mem_index, offsetof(CPUX86State,fpregs[reg].mmx));

            } else {

                rm = (modrm & 7);

                gen_op_movq(offsetof(CPUX86State,fpregs[rm].mmx),

                            offsetof(CPUX86State,fpregs[reg].mmx));

            }

            break;

        case 0x011: /* movups */

        case 0x111: /* movupd */

        case 0x029: /* movaps */

        case 0x129: /* movapd */

        case 0x17f: /* movdqa ea, xmm */

        case 0x27f: /* movdqu ea, xmm */

            if (mod != 3) {

                gen_lea_modrm(s, modrm, &reg_addr, &offset_addr);

                gen_sto_env_A0(s->mem_index, offsetof(CPUX86State,xmm_regs[reg]));

            } else {

                rm = (modrm & 7) | REX_B(s);

                gen_op_movo(offsetof(CPUX86State,xmm_regs[rm]),

                            offsetof(CPUX86State,xmm_regs[reg]));

            }

            break;

        case 0x211: /* movss ea, xmm */

            if (mod != 3) {

                gen_lea_modrm(s, modrm, &reg_addr, &offset_addr);

                tcg_gen_ld32u_tl(cpu_T[0], cpu_env, offsetof(CPUX86State,xmm_regs[reg].XMM_L(0)));

                gen_op_st_T0_A0(OT_LONG + s->mem_index);

            } else {

                rm = (modrm & 7) | REX_B(s);

                gen_op_movl(offsetof(CPUX86State,xmm_regs[rm].XMM_L(0)),

                            offsetof(CPUX86State,xmm_regs[reg].XMM_L(0)));

            }

            break;

        case 0x311: /* movsd ea, xmm */

            if (mod != 3) {

                gen_lea_modrm(s, modrm, &reg_addr, &offset_addr);

                gen_stq_env_A0(s->mem_index, offsetof(CPUX86State,xmm_regs[reg].XMM_Q(0)));

            } else {

                rm = (modrm & 7) | REX_B(s);

                gen_op_movq(offsetof(CPUX86State,xmm_regs[rm].XMM_Q(0)),

                            offsetof(CPUX86State,xmm_regs[reg].XMM_Q(0)));

            }

            break;

        case 0x013: /* movlps */

        case 0x113: /* movlpd */

            if (mod != 3) {

                gen_lea_modrm(s, modrm, &reg_addr, &offset_addr);

                gen_stq_env_A0(s->mem_index, offsetof(CPUX86State,xmm_regs[reg].XMM_Q(0)));

            } else {

                goto illegal_op;

            }

            break;

        case 0x017: /* movhps */

        case 0x117: /* movhpd */

            if (mod != 3) {

                gen_lea_modrm(s, modrm, &reg_addr, &offset_addr);

                gen_stq_env_A0(s->mem_index, offsetof(CPUX86State,xmm_regs[reg].XMM_Q(1)));

            } else {

                goto illegal_op;

            }

            break;

        case 0x71: /* shift mm, im */

        case 0x72:

        case 0x73:

        case 0x171: /* shift xmm, im */

        case 0x172:

        case 0x173:

            val = ldub_code(s->pc++);

            if (is_xmm) {

                gen_op_movl_T0_im(val);

                tcg_gen_st32_tl(cpu_T[0], cpu_env, offsetof(CPUX86State,xmm_t0.XMM_L(0)));

                gen_op_movl_T0_0();

                tcg_gen_st32_tl(cpu_T[0], cpu_env, offsetof(CPUX86State,xmm_t0.XMM_L(1)));

                op1_offset = offsetof(CPUX86State,xmm_t0);

            } else {

                gen_op_movl_T0_im(val);

                tcg_gen_st32_tl(cpu_T[0], cpu_env, offsetof(CPUX86State,mmx_t0.MMX_L(0)));

                gen_op_movl_T0_0();

                tcg_gen_st32_tl(cpu_T[0], cpu_env, offsetof(CPUX86State,mmx_t0.MMX_L(1)));

                op1_offset = offsetof(CPUX86State,mmx_t0);

            }

            sse_op2 = sse_op_table2[((b - 1) & 3) * 8 + (((modrm >> 3)) & 7)][b1];

            if (!sse_op2)

                goto illegal_op;

            if (is_xmm) {

                rm = (modrm & 7) | REX_B(s);

                op2_offset = offsetof(CPUX86State,xmm_regs[rm]);

            } else {

                rm = (modrm & 7);

                op2_offset = offsetof(CPUX86State,fpregs[rm].mmx);

            }

            tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op2_offset);

            tcg_gen_addi_ptr(cpu_ptr1, cpu_env, op1_offset);

            tcg_gen_helper_0_2(sse_op2, cpu_ptr0, cpu_ptr1);

            break;

        case 0x050: /* movmskps */

            rm = (modrm & 7) | REX_B(s);

            tcg_gen_addi_ptr(cpu_ptr0, cpu_env, 

                             offsetof(CPUX86State,xmm_regs[rm]));

            tcg_gen_helper_1_1(helper_movmskps, cpu_tmp2_i32, cpu_ptr0);

            tcg_gen_extu_i32_tl(cpu_T[0], cpu_tmp2_i32);

            gen_op_mov_reg_T0(OT_LONG, reg);

            break;

        case 0x150: /* movmskpd */

            rm = (modrm & 7) | REX_B(s);

            tcg_gen_addi_ptr(cpu_ptr0, cpu_env, 

                             offsetof(CPUX86State,xmm_regs[rm]));

            tcg_gen_helper_1_1(helper_movmskpd, cpu_tmp2_i32, cpu_ptr0);

            tcg_gen_extu_i32_tl(cpu_T[0], cpu_tmp2_i32);

            gen_op_mov_reg_T0(OT_LONG, reg);

            break;

        case 0x02a: /* cvtpi2ps */

        case 0x12a: /* cvtpi2pd */

            tcg_gen_helper_0_0(helper_enter_mmx);

            if (mod != 3) {

                gen_lea_modrm(s, modrm, &reg_addr, &offset_addr);

                op2_offset = offsetof(CPUX86State,mmx_t0);

                gen_ldq_env_A0(s->mem_index, op2_offset);

            } else {

                rm = (modrm & 7);

                op2_offset = offsetof(CPUX86State,fpregs[rm].mmx);

            }

            op1_offset = offsetof(CPUX86State,xmm_regs[reg]);

            tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op1_offset);

            tcg_gen_addi_ptr(cpu_ptr1, cpu_env, op2_offset);

            switch(b >> 8) {

            case 0x0:

                tcg_gen_helper_0_2(helper_cvtpi2ps, cpu_ptr0, cpu_ptr1);

                break;

            default:

            case 0x1:

                tcg_gen_helper_0_2(helper_cvtpi2pd, cpu_ptr0, cpu_ptr1);

                break;

            }

            break;

        case 0x22a: /* cvtsi2ss */

        case 0x32a: /* cvtsi2sd */

            ot = (s->dflag == 2) ? OT_QUAD : OT_LONG;

            gen_ldst_modrm(s, modrm, ot, OR_TMP0, 0);

            op1_offset = offsetof(CPUX86State,xmm_regs[reg]);

            tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op1_offset);

            sse_op2 = sse_op_table3[(s->dflag == 2) * 2 + ((b >> 8) - 2)];

            if (ot == OT_LONG) {

                tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T[0]);

                tcg_gen_helper_0_2(sse_op2, cpu_ptr0, cpu_tmp2_i32);

            } else {

                tcg_gen_helper_0_2(sse_op2, cpu_ptr0, cpu_T[0]);

            }

            break;

        case 0x02c: /* cvttps2pi */

        case 0x12c: /* cvttpd2pi */

        case 0x02d: /* cvtps2pi */

        case 0x12d: /* cvtpd2pi */

            tcg_gen_helper_0_0(helper_enter_mmx);

            if (mod != 3) {

                gen_lea_modrm(s, modrm, &reg_addr, &offset_addr);

                op2_offset = offsetof(CPUX86State,xmm_t0);

                gen_ldo_env_A0(s->mem_index, op2_offset);

            } else {

                rm = (modrm & 7) | REX_B(s);

                op2_offset = offsetof(CPUX86State,xmm_regs[rm]);

            }

            op1_offset = offsetof(CPUX86State,fpregs[reg & 7].mmx);

            tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op1_offset);

            tcg_gen_addi_ptr(cpu_ptr1, cpu_env, op2_offset);

            switch(b) {

            case 0x02c:

                tcg_gen_helper_0_2(helper_cvttps2pi, cpu_ptr0, cpu_ptr1);

                break;

            case 0x12c:

                tcg_gen_helper_0_2(helper_cvttpd2pi, cpu_ptr0, cpu_ptr1);

                break;

            case 0x02d:

                tcg_gen_helper_0_2(helper_cvtps2pi, cpu_ptr0, cpu_ptr1);

                break;

            case 0x12d:

                tcg_gen_helper_0_2(helper_cvtpd2pi, cpu_ptr0, cpu_ptr1);

                break;

            }

            break;

        case 0x22c: /* cvttss2si */

        case 0x32c: /* cvttsd2si */

        case 0x22d: /* cvtss2si */

        case 0x32d: /* cvtsd2si */

            ot = (s->dflag == 2) ? OT_QUAD : OT_LONG;

            if (mod != 3) {

                gen_lea_modrm(s, modrm, &reg_addr, &offset_addr);

                if ((b >> 8) & 1) {

                    gen_ldq_env_A0(s->mem_index, offsetof(CPUX86State,xmm_t0.XMM_Q(0)));

                } else {

                    gen_op_ld_T0_A0(OT_LONG + s->mem_index);

                    tcg_gen_st32_tl(cpu_T[0], cpu_env, offsetof(CPUX86State,xmm_t0.XMM_L(0)));

                }

                op2_offset = offsetof(CPUX86State,xmm_t0);

            } else {

                rm = (modrm & 7) | REX_B(s);

                op2_offset = offsetof(CPUX86State,xmm_regs[rm]);

            }

            sse_op2 = sse_op_table3[(s->dflag == 2) * 2 + ((b >> 8) - 2) + 4 +

                                    (b & 1) * 4];

            tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op2_offset);

            if (ot == OT_LONG) {

                tcg_gen_helper_1_1(sse_op2, cpu_tmp2_i32, cpu_ptr0);

                tcg_gen_extu_i32_tl(cpu_T[0], cpu_tmp2_i32);

            } else {

                tcg_gen_helper_1_1(sse_op2, cpu_T[0], cpu_ptr0);

            }

            gen_op_mov_reg_T0(ot, reg);

            break;

        case 0xc4: /* pinsrw */

        case 0x1c4:

            s->rip_offset = 1;

            gen_ldst_modrm(s, modrm, OT_WORD, OR_TMP0, 0);

            val = ldub_code(s->pc++);

            if (b1) {

                val &= 7;

                tcg_gen_st16_tl(cpu_T[0], cpu_env,

                                offsetof(CPUX86State,xmm_regs[reg].XMM_W(val)));

            } else {

                val &= 3;

                tcg_gen_st16_tl(cpu_T[0], cpu_env,

                                offsetof(CPUX86State,fpregs[reg].mmx.MMX_W(val)));

            }

            break;

        case 0xc5: /* pextrw */

        case 0x1c5:

            if (mod != 3)

                goto illegal_op;

            ot = (s->dflag == 2) ? OT_QUAD : OT_LONG;

            val = ldub_code(s->pc++);

            if (b1) {

                val &= 7;

                rm = (modrm & 7) | REX_B(s);

                tcg_gen_ld16u_tl(cpu_T[0], cpu_env,

                                 offsetof(CPUX86State,xmm_regs[rm].XMM_W(val)));

            } else {

                val &= 3;

                rm = (modrm & 7);

                tcg_gen_ld16u_tl(cpu_T[0], cpu_env,

                                offsetof(CPUX86State,fpregs[rm].mmx.MMX_W(val)));

            }

            reg = ((modrm >> 3) & 7) | rex_r;

            gen_op_mov_reg_T0(ot, reg);

            break;

        case 0x1d6: /* movq ea, xmm */

            if (mod != 3) {

                gen_lea_modrm(s, modrm, &reg_addr, &offset_addr);

                gen_stq_env_A0(s->mem_index, offsetof(CPUX86State,xmm_regs[reg].XMM_Q(0)));

            } else {

                rm = (modrm & 7) | REX_B(s);

                gen_op_movq(offsetof(CPUX86State,xmm_regs[rm].XMM_Q(0)),

                            offsetof(CPUX86State,xmm_regs[reg].XMM_Q(0)));

                gen_op_movq_env_0(offsetof(CPUX86State,xmm_regs[rm].XMM_Q(1)));

            }

            break;

        case 0x2d6: /* movq2dq */

            tcg_gen_helper_0_0(helper_enter_mmx);

            rm = (modrm & 7);

            gen_op_movq(offsetof(CPUX86State,xmm_regs[reg].XMM_Q(0)),

                        offsetof(CPUX86State,fpregs[rm].mmx));

            gen_op_movq_env_0(offsetof(CPUX86State,xmm_regs[reg].XMM_Q(1)));

            break;

        case 0x3d6: /* movdq2q */

            tcg_gen_helper_0_0(helper_enter_mmx);

            rm = (modrm & 7) | REX_B(s);

            gen_op_movq(offsetof(CPUX86State,fpregs[reg & 7].mmx),

                        offsetof(CPUX86State,xmm_regs[rm].XMM_Q(0)));

            break;

        case 0xd7: /* pmovmskb */

        case 0x1d7:

            if (mod != 3)

                goto illegal_op;

            if (b1) {

                rm = (modrm & 7) | REX_B(s);

                tcg_gen_addi_ptr(cpu_ptr0, cpu_env, offsetof(CPUX86State,xmm_regs[rm]));

                tcg_gen_helper_1_1(helper_pmovmskb_xmm, cpu_tmp2_i32, cpu_ptr0);

            } else {

                rm = (modrm & 7);

                tcg_gen_addi_ptr(cpu_ptr0, cpu_env, offsetof(CPUX86State,fpregs[rm].mmx));

                tcg_gen_helper_1_1(helper_pmovmskb_mmx, cpu_tmp2_i32, cpu_ptr0);

            }

            tcg_gen_extu_i32_tl(cpu_T[0], cpu_tmp2_i32);

            reg = ((modrm >> 3) & 7) | rex_r;

            gen_op_mov_reg_T0(OT_LONG, reg);

            break;

        case 0x038:

        case 0x138:

            b = modrm;

            modrm = ldub_code(s->pc++);

            rm = modrm & 7;

            reg = ((modrm >> 3) & 7) | rex_r;

            mod = (modrm >> 6) & 3;



            if (s->prefix & PREFIX_REPNZ)

                goto crc32;



            sse_op2 = sse_op_table6[b].op[b1];

            if (!sse_op2)

                goto illegal_op;

            if (!(s->cpuid_ext_features & sse_op_table6[b].ext_mask))

                goto illegal_op;



            if (b1) {

                op1_offset = offsetof(CPUX86State,xmm_regs[reg]);

                if (mod == 3) {

                    op2_offset = offsetof(CPUX86State,xmm_regs[rm | REX_B(s)]);

                } else {

                    op2_offset = offsetof(CPUX86State,xmm_t0);

                    gen_lea_modrm(s, modrm, &reg_addr, &offset_addr);

                    switch (b) {

                    case 0x20: case 0x30: /* pmovsxbw, pmovzxbw */

                    case 0x23: case 0x33: /* pmovsxwd, pmovzxwd */

                    case 0x25: case 0x35: /* pmovsxdq, pmovzxdq */

                        gen_ldq_env_A0(s->mem_index, op2_offset +

                                        offsetof(XMMReg, XMM_Q(0)));

                        break;

                    case 0x21: case 0x31: /* pmovsxbd, pmovzxbd */

                    case 0x24: case 0x34: /* pmovsxwq, pmovzxwq */

                        tcg_gen_qemu_ld32u(cpu_tmp2_i32, cpu_A0,

                                          (s->mem_index >> 2) - 1);

                        tcg_gen_st_i32(cpu_tmp2_i32, cpu_env, op2_offset +

                                        offsetof(XMMReg, XMM_L(0)));

                        break;

                    case 0x22: case 0x32: /* pmovsxbq, pmovzxbq */

                        tcg_gen_qemu_ld16u(cpu_tmp0, cpu_A0,

                                          (s->mem_index >> 2) - 1);

                        tcg_gen_st16_tl(cpu_tmp0, cpu_env, op2_offset +

                                        offsetof(XMMReg, XMM_W(0)));

                        break;

                    case 0x2a:            /* movntqda */

                        gen_ldo_env_A0(s->mem_index, op1_offset);

                        return;

                    default:

                        gen_ldo_env_A0(s->mem_index, op2_offset);

                    }

                }

            } else {

                op1_offset = offsetof(CPUX86State,fpregs[reg].mmx);

                if (mod == 3) {

                    op2_offset = offsetof(CPUX86State,fpregs[rm].mmx);

                } else {

                    op2_offset = offsetof(CPUX86State,mmx_t0);

                    gen_lea_modrm(s, modrm, &reg_addr, &offset_addr);

                    gen_ldq_env_A0(s->mem_index, op2_offset);

                }

            }

            if (sse_op2 == SSE_SPECIAL)

                goto illegal_op;



            tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op1_offset);

            tcg_gen_addi_ptr(cpu_ptr1, cpu_env, op2_offset);

            tcg_gen_helper_0_2(sse_op2, cpu_ptr0, cpu_ptr1);



            if (b == 0x17)

                s->cc_op = CC_OP_EFLAGS;

            break;

        case 0x338: /* crc32 */

        crc32:

            b = modrm;

            modrm = ldub_code(s->pc++);

            reg = ((modrm >> 3) & 7) | rex_r;



            if (b != 0xf0 && b != 0xf1)

                goto illegal_op;

            if (!(s->cpuid_ext_features & CPUID_EXT_SSE42))

                goto illegal_op;



            if (b == 0xf0)

                ot = OT_BYTE;

            else if (b == 0xf1 && s->dflag != 2)

                if (s->prefix & PREFIX_DATA)

                    ot = OT_WORD;

                else

                    ot = OT_LONG;

            else

                ot = OT_QUAD;



            gen_op_mov_TN_reg(OT_LONG, 0, reg);

            tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T[0]);

            gen_ldst_modrm(s, modrm, ot, OR_TMP0, 0);

            tcg_gen_helper_1_3(helper_crc32, cpu_T[0], cpu_tmp2_i32,

                            cpu_T[0], tcg_const_i32(8 << ot));



            ot = (s->dflag == 2) ? OT_QUAD : OT_LONG;

            gen_op_mov_reg_T0(ot, reg);

            break;

        case 0x03a:

        case 0x13a:

            b = modrm;

            modrm = ldub_code(s->pc++);

            rm = modrm & 7;

            reg = ((modrm >> 3) & 7) | rex_r;

            mod = (modrm >> 6) & 3;



            sse_op2 = sse_op_table7[b].op[b1];

            if (!sse_op2)

                goto illegal_op;

            if (!(s->cpuid_ext_features & sse_op_table7[b].ext_mask))

                goto illegal_op;



            if (sse_op2 == SSE_SPECIAL) {

                ot = (s->dflag == 2) ? OT_QUAD : OT_LONG;

                rm = (modrm & 7) | REX_B(s);

                if (mod != 3)

                    gen_lea_modrm(s, modrm, &reg_addr, &offset_addr);

                reg = ((modrm >> 3) & 7) | rex_r;

                val = ldub_code(s->pc++);

                switch (b) {

                case 0x14: /* pextrb */

                    tcg_gen_ld8u_tl(cpu_T[0], cpu_env, offsetof(CPUX86State,

                                            xmm_regs[reg].XMM_B(val & 15)));

                    if (mod == 3)

                        gen_op_mov_reg_T0(ot, rm);

                    else

                        tcg_gen_qemu_st8(cpu_T[0], cpu_A0,

                                        (s->mem_index >> 2) - 1);

                    break;

                case 0x15: /* pextrw */

                    tcg_gen_ld16u_tl(cpu_T[0], cpu_env, offsetof(CPUX86State,

                                            xmm_regs[reg].XMM_W(val & 7)));

                    if (mod == 3)

                        gen_op_mov_reg_T0(ot, rm);

                    else

                        tcg_gen_qemu_st16(cpu_T[0], cpu_A0,

                                        (s->mem_index >> 2) - 1);

                    break;

                case 0x16:

                    if (ot == OT_LONG) { /* pextrd */

                        tcg_gen_ld_i32(cpu_tmp2_i32, cpu_env,

                                        offsetof(CPUX86State,

                                                xmm_regs[reg].XMM_L(val & 3)));

                        if (mod == 3)

                            gen_op_mov_reg_v(ot, rm, cpu_tmp2_i32);

                        else

                            tcg_gen_qemu_st32(cpu_tmp2_i32, cpu_A0,

                                            (s->mem_index >> 2) - 1);

                    } else { /* pextrq */

                        tcg_gen_ld_i64(cpu_tmp1_i64, cpu_env,

                                        offsetof(CPUX86State,

                                                xmm_regs[reg].XMM_Q(val & 1)));

                        if (mod == 3)

                            gen_op_mov_reg_v(ot, rm, cpu_tmp1_i64);

                        else

                            tcg_gen_qemu_st64(cpu_tmp1_i64, cpu_A0,

                                            (s->mem_index >> 2) - 1);

                    }

                    break;

                case 0x17: /* extractps */

                    tcg_gen_ld32u_tl(cpu_T[0], cpu_env, offsetof(CPUX86State,

                                            xmm_regs[reg].XMM_L(val & 3)));

                    if (mod == 3)

                        gen_op_mov_reg_T0(ot, rm);

                    else

                        tcg_gen_qemu_st32(cpu_T[0], cpu_A0,

                                        (s->mem_index >> 2) - 1);

                    break;

                case 0x20: /* pinsrb */

                    if (mod == 3)

                        gen_op_mov_TN_reg(OT_LONG, 0, rm);

                    else

                        tcg_gen_qemu_ld8u(cpu_T[0], cpu_A0,

                                        (s->mem_index >> 2) - 1);

                    tcg_gen_st8_tl(cpu_T[0], cpu_env, offsetof(CPUX86State,

                                            xmm_regs[reg].XMM_B(val & 15)));

                    break;

                case 0x21: /* insertps */

                    if (mod == 3)

                        tcg_gen_ld_i32(cpu_tmp2_i32, cpu_env,

                                        offsetof(CPUX86State,xmm_regs[rm]

                                                .XMM_L((val >> 6) & 3)));

                    else

                        tcg_gen_qemu_ld32u(cpu_tmp2_i32, cpu_A0,

                                        (s->mem_index >> 2) - 1);

                    tcg_gen_st_i32(cpu_tmp2_i32, cpu_env,

                                    offsetof(CPUX86State,xmm_regs[reg]

                                            .XMM_L((val >> 4) & 3)));

                    if ((val >> 0) & 1)

                        tcg_gen_st_i32(tcg_const_i32(0 /*float32_zero*/),

                                        cpu_env, offsetof(CPUX86State,

                                                xmm_regs[reg].XMM_L(0)));

                    if ((val >> 1) & 1)

                        tcg_gen_st_i32(tcg_const_i32(0 /*float32_zero*/),

                                        cpu_env, offsetof(CPUX86State,

                                                xmm_regs[reg].XMM_L(1)));

                    if ((val >> 2) & 1)

                        tcg_gen_st_i32(tcg_const_i32(0 /*float32_zero*/),

                                        cpu_env, offsetof(CPUX86State,

                                                xmm_regs[reg].XMM_L(2)));

                    if ((val >> 3) & 1)

                        tcg_gen_st_i32(tcg_const_i32(0 /*float32_zero*/),

                                        cpu_env, offsetof(CPUX86State,

                                                xmm_regs[reg].XMM_L(3)));

                    break;

                case 0x22:

                    if (ot == OT_LONG) { /* pinsrd */

                        if (mod == 3)

                            gen_op_mov_v_reg(ot, cpu_tmp2_i32, rm);

                        else

                            tcg_gen_qemu_ld32u(cpu_tmp2_i32, cpu_A0,

                                            (s->mem_index >> 2) - 1);

                        tcg_gen_st_i32(cpu_tmp2_i32, cpu_env,

                                        offsetof(CPUX86State,

                                                xmm_regs[reg].XMM_L(val & 3)));

                    } else { /* pinsrq */

                        if (mod == 3)

                            gen_op_mov_v_reg(ot, cpu_tmp1_i64, rm);

                        else

                            tcg_gen_qemu_ld64(cpu_tmp1_i64, cpu_A0,

                                            (s->mem_index >> 2) - 1);

                        tcg_gen_st_i64(cpu_tmp1_i64, cpu_env,

                                        offsetof(CPUX86State,

                                                xmm_regs[reg].XMM_Q(val & 1)));

                    }

                    break;

                }

                return;

            }



            if (b1) {

                op1_offset = offsetof(CPUX86State,xmm_regs[reg]);

                if (mod == 3) {

                    op2_offset = offsetof(CPUX86State,xmm_regs[rm | REX_B(s)]);

                } else {

                    op2_offset = offsetof(CPUX86State,xmm_t0);

                    gen_lea_modrm(s, modrm, &reg_addr, &offset_addr);

                    gen_ldo_env_A0(s->mem_index, op2_offset);

                }

            } else {

                op1_offset = offsetof(CPUX86State,fpregs[reg].mmx);

                if (mod == 3) {

                    op2_offset = offsetof(CPUX86State,fpregs[rm].mmx);

                } else {

                    op2_offset = offsetof(CPUX86State,mmx_t0);

                    gen_lea_modrm(s, modrm, &reg_addr, &offset_addr);

                    gen_ldq_env_A0(s->mem_index, op2_offset);

                }

            }

            val = ldub_code(s->pc++);



            if ((b & 0xfc) == 0x60) { /* pcmpXstrX */

                s->cc_op = CC_OP_EFLAGS;



                if (s->dflag == 2)

                    /* The helper must use entire 64-bit gp registers */

                    val |= 1 << 8;

            }



            tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op1_offset);

            tcg_gen_addi_ptr(cpu_ptr1, cpu_env, op2_offset);

            tcg_gen_helper_0_3(sse_op2, cpu_ptr0, cpu_ptr1, tcg_const_i32(val));

            break;

        default:

            goto illegal_op;

        }

    } else {

        /* generic MMX or SSE operation */

        switch(b) {

        case 0x70: /* pshufx insn */

        case 0xc6: /* pshufx insn */

        case 0xc2: /* compare insns */

            s->rip_offset = 1;

            break;

        default:

            break;

        }

        if (is_xmm) {

            op1_offset = offsetof(CPUX86State,xmm_regs[reg]);

            if (mod != 3) {

                gen_lea_modrm(s, modrm, &reg_addr, &offset_addr);

                op2_offset = offsetof(CPUX86State,xmm_t0);

                if (b1 >= 2 && ((b >= 0x50 && b <= 0x5f && b != 0x5b) ||

                                b == 0xc2)) {

                    /* specific case for SSE single instructions */

                    if (b1 == 2) {

                        /* 32 bit access */

                        gen_op_ld_T0_A0(OT_LONG + s->mem_index);

                        tcg_gen_st32_tl(cpu_T[0], cpu_env, offsetof(CPUX86State,xmm_t0.XMM_L(0)));

                    } else {

                        /* 64 bit access */

                        gen_ldq_env_A0(s->mem_index, offsetof(CPUX86State,xmm_t0.XMM_D(0)));

                    }

                } else {

                    gen_ldo_env_A0(s->mem_index, op2_offset);

                }

            } else {

                rm = (modrm & 7) | REX_B(s);

                op2_offset = offsetof(CPUX86State,xmm_regs[rm]);

            }

        } else {

            op1_offset = offsetof(CPUX86State,fpregs[reg].mmx);

            if (mod != 3) {

                gen_lea_modrm(s, modrm, &reg_addr, &offset_addr);

                op2_offset = offsetof(CPUX86State,mmx_t0);

                gen_ldq_env_A0(s->mem_index, op2_offset);

            } else {

                rm = (modrm & 7);

                op2_offset = offsetof(CPUX86State,fpregs[rm].mmx);

            }

        }

        switch(b) {

        case 0x0f: /* 3DNow! data insns */

            if (!(s->cpuid_ext2_features & CPUID_EXT2_3DNOW))

                goto illegal_op;

            val = ldub_code(s->pc++);

            sse_op2 = sse_op_table5[val];

            if (!sse_op2)

                goto illegal_op;

            tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op1_offset);

            tcg_gen_addi_ptr(cpu_ptr1, cpu_env, op2_offset);

            tcg_gen_helper_0_2(sse_op2, cpu_ptr0, cpu_ptr1);

            break;

        case 0x70: /* pshufx insn */

        case 0xc6: /* pshufx insn */

            val = ldub_code(s->pc++);

            tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op1_offset);

            tcg_gen_addi_ptr(cpu_ptr1, cpu_env, op2_offset);

            tcg_gen_helper_0_3(sse_op2, cpu_ptr0, cpu_ptr1, tcg_const_i32(val));

            break;

        case 0xc2:

            /* compare insns */

            val = ldub_code(s->pc++);

            if (val >= 8)

                goto illegal_op;

            sse_op2 = sse_op_table4[val][b1];

            tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op1_offset);

            tcg_gen_addi_ptr(cpu_ptr1, cpu_env, op2_offset);

            tcg_gen_helper_0_2(sse_op2, cpu_ptr0, cpu_ptr1);

            break;

        case 0xf7:

            /* maskmov : we must prepare A0 */

            if (mod != 3)

                goto illegal_op;

#ifdef TARGET_X86_64

            if (s->aflag == 2) {

                gen_op_movq_A0_reg(R_EDI);

            } else

#endif

            {

                gen_op_movl_A0_reg(R_EDI);

                if (s->aflag == 0)

                    gen_op_andl_A0_ffff();

            }

            gen_add_A0_ds_seg(s);



            tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op1_offset);

            tcg_gen_addi_ptr(cpu_ptr1, cpu_env, op2_offset);

            tcg_gen_helper_0_3(sse_op2, cpu_ptr0, cpu_ptr1, cpu_A0);

            break;

        default:

            tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op1_offset);

            tcg_gen_addi_ptr(cpu_ptr1, cpu_env, op2_offset);

            tcg_gen_helper_0_2(sse_op2, cpu_ptr0, cpu_ptr1);

            break;

        }

        if (b == 0x2e || b == 0x2f) {

            s->cc_op = CC_OP_EFLAGS;

        }

    }

}
","static void gen_sse(DisasContext *s, int b, target_ulong pc_start, int rex_r) {
int b1, op1_offset, op2_offset, is_xmm, val, ot; int modrm, mod, rm, reg, reg_addr, offset_addr; void *sse_op2; b &= 0xff; if (s->prefix & PREFIX_DATA) b1 = 1; else if (s->prefix & PREFIX_REPZ) b1 = 2; else if (s->prefix & PREFIX_REPNZ) b1 = 3; else b1 = 0; sse_op2 = sse_op_table1[b][b1]; if (!sse_op2) goto illegal_op; if ((b <= 0x5f && b >= 0x10) || b == 0xc6 || b == 0xc2) {
is_xmm = 1; } else {
if (b1 == 0) {
/* MMX case */ is_xmm = 0; } else {
is_xmm = 1; } } /* simple MMX/SSE operation */ if (s->flags & HF_TS_MASK) {
gen_exception(s, EXCP07_PREX, pc_start - s->cs_base); return; } if (s->flags & HF_EM_MASK) {
illegal_op: gen_exception(s, EXCP06_ILLOP, pc_start - s->cs_base); return; } if (is_xmm && !(s->flags & HF_OSFXSR_MASK)) if ((b != 0x38 && b != 0x3a) || (s->prefix & PREFIX_DATA)) goto illegal_op; if (b == 0x0e) {
if (!(s->cpuid_ext2_features & CPUID_EXT2_3DNOW)) goto illegal_op; /* femms */ tcg_gen_helper_0_0(helper_emms); return; } if (b == 0x77) {
/* emms */ tcg_gen_helper_0_0(helper_emms); return; } /* prepare MMX state (XXX: optimize by storing fptt and fptags in the static cpu state) */ if (!is_xmm) {
tcg_gen_helper_0_0(helper_enter_mmx); } modrm = ldub_code(s->pc++); reg = ((modrm >> 3) & 7); if (is_xmm) reg |= rex_r; mod = (modrm >> 6) & 3; if (sse_op2 == SSE_SPECIAL) {
b |= (b1 << 8); switch(b) {
case 0x0e7: /* movntq */ if (mod == 3) goto illegal_op; gen_lea_modrm(s, modrm, &reg_addr, &offset_addr); gen_stq_env_A0(s->mem_index, offsetof(CPUX86State,fpregs[reg].mmx)); break; case 0x1e7: /* movntdq */ case 0x02b: /* movntps */ case 0x12b: /* movntps */ case 0x3f0: /* lddqu */ if (mod == 3) goto illegal_op; gen_lea_modrm(s, modrm, &reg_addr, &offset_addr); gen_sto_env_A0(s->mem_index, offsetof(CPUX86State,xmm_regs[reg])); break; case 0x6e: /* movd mm, ea */ #ifdef TARGET_X86_64 if (s->dflag == 2) {
gen_ldst_modrm(s, modrm, OT_QUAD, OR_TMP0, 0); tcg_gen_st_tl(cpu_T[0], cpu_env, offsetof(CPUX86State,fpregs[reg].mmx)); } else #endif {
gen_ldst_modrm(s, modrm, OT_LONG, OR_TMP0, 0); tcg_gen_addi_ptr(cpu_ptr0, cpu_env, offsetof(CPUX86State,fpregs[reg].mmx)); tcg_gen_helper_0_2(helper_movl_mm_T0_mmx, cpu_ptr0, cpu_T[0]); } break; case 0x16e: /* movd xmm, ea */ #ifdef TARGET_X86_64 if (s->dflag == 2) {
gen_ldst_modrm(s, modrm, OT_QUAD, OR_TMP0, 0); tcg_gen_addi_ptr(cpu_ptr0, cpu_env, offsetof(CPUX86State,xmm_regs[reg])); tcg_gen_helper_0_2(helper_movq_mm_T0_xmm, cpu_ptr0, cpu_T[0]); } else #endif {
gen_ldst_modrm(s, modrm, OT_LONG, OR_TMP0, 0); tcg_gen_addi_ptr(cpu_ptr0, cpu_env, offsetof(CPUX86State,xmm_regs[reg])); tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T[0]); tcg_gen_helper_0_2(helper_movl_mm_T0_xmm, cpu_ptr0, cpu_tmp2_i32); } break; case 0x6f: /* movq mm, ea */ if (mod != 3) {
gen_lea_modrm(s, modrm, &reg_addr, &offset_addr); gen_ldq_env_A0(s->mem_index, offsetof(CPUX86State,fpregs[reg].mmx)); } else {
rm = (modrm & 7); tcg_gen_ld_i64(cpu_tmp1_i64, cpu_env, offsetof(CPUX86State,fpregs[rm].mmx)); tcg_gen_st_i64(cpu_tmp1_i64, cpu_env, offsetof(CPUX86State,fpregs[reg].mmx)); } break; case 0x010: /* movups */ case 0x110: /* movupd */ case 0x028: /* movaps */ case 0x128: /* movapd */ case 0x16f: /* movdqa xmm, ea */ case 0x26f: /* movdqu xmm, ea */ if (mod != 3) {
gen_lea_modrm(s, modrm, &reg_addr, &offset_addr); gen_ldo_env_A0(s->mem_index, offsetof(CPUX86State,xmm_regs[reg])); } else {
rm = (modrm & 7) | REX_B(s); gen_op_movo(offsetof(CPUX86State,xmm_regs[reg]), offsetof(CPUX86State,xmm_regs[rm])); } break; case 0x210: /* movss xmm, ea */ if (mod != 3) {
gen_lea_modrm(s, modrm, &reg_addr, &offset_addr); gen_op_ld_T0_A0(OT_LONG + s->mem_index); tcg_gen_st32_tl(cpu_T[0], cpu_env, offsetof(CPUX86State,xmm_regs[reg].XMM_L(0))); gen_op_movl_T0_0(); tcg_gen_st32_tl(cpu_T[0], cpu_env, offsetof(CPUX86State,xmm_regs[reg].XMM_L(1))); tcg_gen_st32_tl(cpu_T[0], cpu_env, offsetof(CPUX86State,xmm_regs[reg].XMM_L(2))); tcg_gen_st32_tl(cpu_T[0], cpu_env, offsetof(CPUX86State,xmm_regs[reg].XMM_L(3))); } else {
rm = (modrm & 7) | REX_B(s); gen_op_movl(offsetof(CPUX86State,xmm_regs[reg].XMM_L(0)), offsetof(CPUX86State,xmm_regs[rm].XMM_L(0))); } break; case 0x310: /* movsd xmm, ea */ if (mod != 3) {
gen_lea_modrm(s, modrm, &reg_addr, &offset_addr); gen_ldq_env_A0(s->mem_index, offsetof(CPUX86State,xmm_regs[reg].XMM_Q(0))); gen_op_movl_T0_0(); tcg_gen_st32_tl(cpu_T[0], cpu_env, offsetof(CPUX86State,xmm_regs[reg].XMM_L(2))); tcg_gen_st32_tl(cpu_T[0], cpu_env, offsetof(CPUX86State,xmm_regs[reg].XMM_L(3))); } else {
rm = (modrm & 7) | REX_B(s); gen_op_movq(offsetof(CPUX86State,xmm_regs[reg].XMM_Q(0)), offsetof(CPUX86State,xmm_regs[rm].XMM_Q(0))); } break; case 0x012: /* movlps */ case 0x112: /* movlpd */ if (mod != 3) {
gen_lea_modrm(s, modrm, &reg_addr, &offset_addr); gen_ldq_env_A0(s->mem_index, offsetof(CPUX86State,xmm_regs[reg].XMM_Q(0))); } else {
/* movhlps */ rm = (modrm & 7) | REX_B(s); gen_op_movq(offsetof(CPUX86State,xmm_regs[reg].XMM_Q(0)), offsetof(CPUX86State,xmm_regs[rm].XMM_Q(1))); } break; case 0x212: /* movsldup */ if (mod != 3) {
gen_lea_modrm(s, modrm, &reg_addr, &offset_addr); gen_ldo_env_A0(s->mem_index, offsetof(CPUX86State,xmm_regs[reg])); } else {
rm = (modrm & 7) | REX_B(s); gen_op_movl(offsetof(CPUX86State,xmm_regs[reg].XMM_L(0)), offsetof(CPUX86State,xmm_regs[rm].XMM_L(0))); gen_op_movl(offsetof(CPUX86State,xmm_regs[reg].XMM_L(2)), offsetof(CPUX86State,xmm_regs[rm].XMM_L(2))); } gen_op_movl(offsetof(CPUX86State,xmm_regs[reg].XMM_L(1)), offsetof(CPUX86State,xmm_regs[reg].XMM_L(0))); gen_op_movl(offsetof(CPUX86State,xmm_regs[reg].XMM_L(3)), offsetof(CPUX86State,xmm_regs[reg].XMM_L(2))); break; case 0x312: /* movddup */ if (mod != 3) {
gen_lea_modrm(s, modrm, &reg_addr, &offset_addr); gen_ldq_env_A0(s->mem_index, offsetof(CPUX86State,xmm_regs[reg].XMM_Q(0))); } else {
rm = (modrm & 7) | REX_B(s); gen_op_movq(offsetof(CPUX86State,xmm_regs[reg].XMM_Q(0)), offsetof(CPUX86State,xmm_regs[rm].XMM_Q(0))); } gen_op_movq(offsetof(CPUX86State,xmm_regs[reg].XMM_Q(1)), offsetof(CPUX86State,xmm_regs[reg].XMM_Q(0))); break; case 0x016: /* movhps */ case 0x116: /* movhpd */ if (mod != 3) {
gen_lea_modrm(s, modrm, &reg_addr, &offset_addr); gen_ldq_env_A0(s->mem_index, offsetof(CPUX86State,xmm_regs[reg].XMM_Q(1))); } else {
/* movlhps */ rm = (modrm & 7) | REX_B(s); gen_op_movq(offsetof(CPUX86State,xmm_regs[reg].XMM_Q(1)), offsetof(CPUX86State,xmm_regs[rm].XMM_Q(0))); } break; case 0x216: /* movshdup */ if (mod != 3) {
gen_lea_modrm(s, modrm, &reg_addr, &offset_addr); gen_ldo_env_A0(s->mem_index, offsetof(CPUX86State,xmm_regs[reg])); } else {
rm = (modrm & 7) | REX_B(s); gen_op_movl(offsetof(CPUX86State,xmm_regs[reg].XMM_L(1)), offsetof(CPUX86State,xmm_regs[rm].XMM_L(1))); gen_op_movl(offsetof(CPUX86State,xmm_regs[reg].XMM_L(3)), offsetof(CPUX86State,xmm_regs[rm].XMM_L(3))); } gen_op_movl(offsetof(CPUX86State,xmm_regs[reg].XMM_L(0)), offsetof(CPUX86State,xmm_regs[reg].XMM_L(1))); gen_op_movl(offsetof(CPUX86State,xmm_regs[reg].XMM_L(2)), offsetof(CPUX86State,xmm_regs[reg].XMM_L(3))); break; case 0x7e: /* movd ea, mm */ #ifdef TARGET_X86_64 if (s->dflag == 2) {
tcg_gen_ld_i64(cpu_T[0], cpu_env, offsetof(CPUX86State,fpregs[reg].mmx)); gen_ldst_modrm(s, modrm, OT_QUAD, OR_TMP0, 1); } else #endif {
tcg_gen_ld32u_tl(cpu_T[0], cpu_env, offsetof(CPUX86State,fpregs[reg].mmx.MMX_L(0))); gen_ldst_modrm(s, modrm, OT_LONG, OR_TMP0, 1); } break; case 0x17e: /* movd ea, xmm */ #ifdef TARGET_X86_64 if (s->dflag == 2) {
tcg_gen_ld_i64(cpu_T[0], cpu_env, offsetof(CPUX86State,xmm_regs[reg].XMM_Q(0))); gen_ldst_modrm(s, modrm, OT_QUAD, OR_TMP0, 1); } else #endif {
tcg_gen_ld32u_tl(cpu_T[0], cpu_env, offsetof(CPUX86State,xmm_regs[reg].XMM_L(0))); gen_ldst_modrm(s, modrm, OT_LONG, OR_TMP0, 1); } break; case 0x27e: /* movq xmm, ea */ if (mod != 3) {
gen_lea_modrm(s, modrm, &reg_addr, &offset_addr); gen_ldq_env_A0(s->mem_index, offsetof(CPUX86State,xmm_regs[reg].XMM_Q(0))); } else {
rm = (modrm & 7) | REX_B(s); gen_op_movq(offsetof(CPUX86State,xmm_regs[reg].XMM_Q(0)), offsetof(CPUX86State,xmm_regs[rm].XMM_Q(0))); } gen_op_movq_env_0(offsetof(CPUX86State,xmm_regs[reg].XMM_Q(1))); break; case 0x7f: /* movq ea, mm */ if (mod != 3) {
gen_lea_modrm(s, modrm, &reg_addr, &offset_addr); gen_stq_env_A0(s->mem_index, offsetof(CPUX86State,fpregs[reg].mmx)); } else {
rm = (modrm & 7); gen_op_movq(offsetof(CPUX86State,fpregs[rm].mmx), offsetof(CPUX86State,fpregs[reg].mmx)); } break; case 0x011: /* movups */ case 0x111: /* movupd */ case 0x029: /* movaps */ case 0x129: /* movapd */ case 0x17f: /* movdqa ea, xmm */ case 0x27f: /* movdqu ea, xmm */ if (mod != 3) {
gen_lea_modrm(s, modrm, &reg_addr, &offset_addr); gen_sto_env_A0(s->mem_index, offsetof(CPUX86State,xmm_regs[reg])); } else {
rm = (modrm & 7) | REX_B(s); gen_op_movo(offsetof(CPUX86State,xmm_regs[rm]), offsetof(CPUX86State,xmm_regs[reg])); } break; case 0x211: /* movss ea, xmm */ if (mod != 3) {
gen_lea_modrm(s, modrm, &reg_addr, &offset_addr); tcg_gen_ld32u_tl(cpu_T[0], cpu_env, offsetof(CPUX86State,xmm_regs[reg].XMM_L(0))); gen_op_st_T0_A0(OT_LONG + s->mem_index); } else {
rm = (modrm & 7) | REX_B(s); gen_op_movl(offsetof(CPUX86State,xmm_regs[rm].XMM_L(0)), offsetof(CPUX86State,xmm_regs[reg].XMM_L(0))); } break; case 0x311: /* movsd ea, xmm */ if (mod != 3) {
gen_lea_modrm(s, modrm, &reg_addr, &offset_addr); gen_stq_env_A0(s->mem_index, offsetof(CPUX86State,xmm_regs[reg].XMM_Q(0))); } else {
rm = (modrm & 7) | REX_B(s); gen_op_movq(offsetof(CPUX86State,xmm_regs[rm].XMM_Q(0)), offsetof(CPUX86State,xmm_regs[reg].XMM_Q(0))); } break; case 0x013: /* movlps */ case 0x113: /* movlpd */ if (mod != 3) {
gen_lea_modrm(s, modrm, &reg_addr, &offset_addr); gen_stq_env_A0(s->mem_index, offsetof(CPUX86State,xmm_regs[reg].XMM_Q(0))); } else {
goto illegal_op; } break; case 0x017: /* movhps */ case 0x117: /* movhpd */ if (mod != 3) {
gen_lea_modrm(s, modrm, &reg_addr, &offset_addr); gen_stq_env_A0(s->mem_index, offsetof(CPUX86State,xmm_regs[reg].XMM_Q(1))); } else {
goto illegal_op; } break; case 0x71: /* shift mm, im */ case 0x72: case 0x73: case 0x171: /* shift xmm, im */ case 0x172: case 0x173: val = ldub_code(s->pc++); if (is_xmm) {
gen_op_movl_T0_im(val); tcg_gen_st32_tl(cpu_T[0], cpu_env, offsetof(CPUX86State,xmm_t0.XMM_L(0))); gen_op_movl_T0_0(); tcg_gen_st32_tl(cpu_T[0], cpu_env, offsetof(CPUX86State,xmm_t0.XMM_L(1))); op1_offset = offsetof(CPUX86State,xmm_t0); } else {
gen_op_movl_T0_im(val); tcg_gen_st32_tl(cpu_T[0], cpu_env, offsetof(CPUX86State,mmx_t0.MMX_L(0))); gen_op_movl_T0_0(); tcg_gen_st32_tl(cpu_T[0], cpu_env, offsetof(CPUX86State,mmx_t0.MMX_L(1))); op1_offset = offsetof(CPUX86State,mmx_t0); } sse_op2 = sse_op_table2[((b - 1) & 3) * 8 + (((modrm >> 3)) & 7)][b1]; if (!sse_op2) goto illegal_op; if (is_xmm) {
rm = (modrm & 7) | REX_B(s); op2_offset = offsetof(CPUX86State,xmm_regs[rm]); } else {
rm = (modrm & 7); op2_offset = offsetof(CPUX86State,fpregs[rm].mmx); } tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op2_offset); tcg_gen_addi_ptr(cpu_ptr1, cpu_env, op1_offset); tcg_gen_helper_0_2(sse_op2, cpu_ptr0, cpu_ptr1); break; case 0x050: /* movmskps */ rm = (modrm & 7) | REX_B(s); tcg_gen_addi_ptr(cpu_ptr0, cpu_env, offsetof(CPUX86State,xmm_regs[rm])); tcg_gen_helper_1_1(helper_movmskps, cpu_tmp2_i32, cpu_ptr0); tcg_gen_extu_i32_tl(cpu_T[0], cpu_tmp2_i32); gen_op_mov_reg_T0(OT_LONG, reg); break; case 0x150: /* movmskpd */ rm = (modrm & 7) | REX_B(s); tcg_gen_addi_ptr(cpu_ptr0, cpu_env, offsetof(CPUX86State,xmm_regs[rm])); tcg_gen_helper_1_1(helper_movmskpd, cpu_tmp2_i32, cpu_ptr0); tcg_gen_extu_i32_tl(cpu_T[0], cpu_tmp2_i32); gen_op_mov_reg_T0(OT_LONG, reg); break; case 0x02a: /* cvtpi2ps */ case 0x12a: /* cvtpi2pd */ tcg_gen_helper_0_0(helper_enter_mmx); if (mod != 3) {
gen_lea_modrm(s, modrm, &reg_addr, &offset_addr); op2_offset = offsetof(CPUX86State,mmx_t0); gen_ldq_env_A0(s->mem_index, op2_offset); } else {
rm = (modrm & 7); op2_offset = offsetof(CPUX86State,fpregs[rm].mmx); } op1_offset = offsetof(CPUX86State,xmm_regs[reg]); tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op1_offset); tcg_gen_addi_ptr(cpu_ptr1, cpu_env, op2_offset); switch(b >> 8) {
case 0x0: tcg_gen_helper_0_2(helper_cvtpi2ps, cpu_ptr0, cpu_ptr1); break; default: case 0x1: tcg_gen_helper_0_2(helper_cvtpi2pd, cpu_ptr0, cpu_ptr1); break; } break; case 0x22a: /* cvtsi2ss */ case 0x32a: /* cvtsi2sd */ ot = (s->dflag == 2) ? OT_QUAD : OT_LONG; gen_ldst_modrm(s, modrm, ot, OR_TMP0, 0); op1_offset = offsetof(CPUX86State,xmm_regs[reg]); tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op1_offset); sse_op2 = sse_op_table3[(s->dflag == 2) * 2 + ((b >> 8) - 2)]; if (ot == OT_LONG) {
tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T[0]); tcg_gen_helper_0_2(sse_op2, cpu_ptr0, cpu_tmp2_i32); } else {
tcg_gen_helper_0_2(sse_op2, cpu_ptr0, cpu_T[0]); } break; case 0x02c: /* cvttps2pi */ case 0x12c: /* cvttpd2pi */ case 0x02d: /* cvtps2pi */ case 0x12d: /* cvtpd2pi */ tcg_gen_helper_0_0(helper_enter_mmx); if (mod != 3) {
gen_lea_modrm(s, modrm, &reg_addr, &offset_addr); op2_offset = offsetof(CPUX86State,xmm_t0); gen_ldo_env_A0(s->mem_index, op2_offset); } else {
rm = (modrm & 7) | REX_B(s); op2_offset = offsetof(CPUX86State,xmm_regs[rm]); } op1_offset = offsetof(CPUX86State,fpregs[reg & 7].mmx); tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op1_offset); tcg_gen_addi_ptr(cpu_ptr1, cpu_env, op2_offset); switch(b) {
case 0x02c: tcg_gen_helper_0_2(helper_cvttps2pi, cpu_ptr0, cpu_ptr1); break; case 0x12c: tcg_gen_helper_0_2(helper_cvttpd2pi, cpu_ptr0, cpu_ptr1); break; case 0x02d: tcg_gen_helper_0_2(helper_cvtps2pi, cpu_ptr0, cpu_ptr1); break; case 0x12d: tcg_gen_helper_0_2(helper_cvtpd2pi, cpu_ptr0, cpu_ptr1); break; } break; case 0x22c: /* cvttss2si */ case 0x32c: /* cvttsd2si */ case 0x22d: /* cvtss2si */ case 0x32d: /* cvtsd2si */ ot = (s->dflag == 2) ? OT_QUAD : OT_LONG; if (mod != 3) {
gen_lea_modrm(s, modrm, &reg_addr, &offset_addr); if ((b >> 8) & 1) {
gen_ldq_env_A0(s->mem_index, offsetof(CPUX86State,xmm_t0.XMM_Q(0))); } else {
gen_op_ld_T0_A0(OT_LONG + s->mem_index); tcg_gen_st32_tl(cpu_T[0], cpu_env, offsetof(CPUX86State,xmm_t0.XMM_L(0))); } op2_offset = offsetof(CPUX86State,xmm_t0); } else {
rm = (modrm & 7) | REX_B(s); op2_offset = offsetof(CPUX86State,xmm_regs[rm]); } sse_op2 = sse_op_table3[(s->dflag == 2) * 2 + ((b >> 8) - 2) + 4 + (b & 1) * 4]; tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op2_offset); if (ot == OT_LONG) {
tcg_gen_helper_1_1(sse_op2, cpu_tmp2_i32, cpu_ptr0); tcg_gen_extu_i32_tl(cpu_T[0], cpu_tmp2_i32); } else {
tcg_gen_helper_1_1(sse_op2, cpu_T[0], cpu_ptr0); } gen_op_mov_reg_T0(ot, reg); break; case 0xc4: /* pinsrw */ case 0x1c4: s->rip_offset = 1; gen_ldst_modrm(s, modrm, OT_WORD, OR_TMP0, 0); val = ldub_code(s->pc++); if (b1) {
val &= 7; tcg_gen_st16_tl(cpu_T[0], cpu_env, offsetof(CPUX86State,xmm_regs[reg].XMM_W(val))); } else {
val &= 3; tcg_gen_st16_tl(cpu_T[0], cpu_env, offsetof(CPUX86State,fpregs[reg].mmx.MMX_W(val))); } break; case 0xc5: /* pextrw */ case 0x1c5: if (mod != 3) goto illegal_op; ot = (s->dflag == 2) ? OT_QUAD : OT_LONG; val = ldub_code(s->pc++); if (b1) {
val &= 7; rm = (modrm & 7) | REX_B(s); tcg_gen_ld16u_tl(cpu_T[0], cpu_env, offsetof(CPUX86State,xmm_regs[rm].XMM_W(val))); } else {
val &= 3; rm = (modrm & 7); tcg_gen_ld16u_tl(cpu_T[0], cpu_env, offsetof(CPUX86State,fpregs[rm].mmx.MMX_W(val))); } reg = ((modrm >> 3) & 7) | rex_r; gen_op_mov_reg_T0(ot, reg); break; case 0x1d6: /* movq ea, xmm */ if (mod != 3) {
gen_lea_modrm(s, modrm, &reg_addr, &offset_addr); gen_stq_env_A0(s->mem_index, offsetof(CPUX86State,xmm_regs[reg].XMM_Q(0))); } else {
rm = (modrm & 7) | REX_B(s); gen_op_movq(offsetof(CPUX86State,xmm_regs[rm].XMM_Q(0)), offsetof(CPUX86State,xmm_regs[reg].XMM_Q(0))); gen_op_movq_env_0(offsetof(CPUX86State,xmm_regs[rm].XMM_Q(1))); } break; case 0x2d6: /* movq2dq */ tcg_gen_helper_0_0(helper_enter_mmx); rm = (modrm & 7); gen_op_movq(offsetof(CPUX86State,xmm_regs[reg].XMM_Q(0)), offsetof(CPUX86State,fpregs[rm].mmx)); gen_op_movq_env_0(offsetof(CPUX86State,xmm_regs[reg].XMM_Q(1))); break; case 0x3d6: /* movdq2q */ tcg_gen_helper_0_0(helper_enter_mmx); rm = (modrm & 7) | REX_B(s); gen_op_movq(offsetof(CPUX86State,fpregs[reg & 7].mmx), offsetof(CPUX86State,xmm_regs[rm].XMM_Q(0))); break; case 0xd7: /* pmovmskb */ case 0x1d7: if (mod != 3) goto illegal_op; if (b1) {
rm = (modrm & 7) | REX_B(s); tcg_gen_addi_ptr(cpu_ptr0, cpu_env, offsetof(CPUX86State,xmm_regs[rm])); tcg_gen_helper_1_1(helper_pmovmskb_xmm, cpu_tmp2_i32, cpu_ptr0); } else {
rm = (modrm & 7); tcg_gen_addi_ptr(cpu_ptr0, cpu_env, offsetof(CPUX86State,fpregs[rm].mmx)); tcg_gen_helper_1_1(helper_pmovmskb_mmx, cpu_tmp2_i32, cpu_ptr0); } tcg_gen_extu_i32_tl(cpu_T[0], cpu_tmp2_i32); reg = ((modrm >> 3) & 7) | rex_r; gen_op_mov_reg_T0(OT_LONG, reg); break; case 0x038: case 0x138: b = modrm; modrm = ldub_code(s->pc++); rm = modrm & 7; reg = ((modrm >> 3) & 7) | rex_r; mod = (modrm >> 6) & 3; if (s->prefix & PREFIX_REPNZ) goto crc32; sse_op2 = sse_op_table6[b].op[b1]; if (!sse_op2) goto illegal_op; if (!(s->cpuid_ext_features & sse_op_table6[b].ext_mask)) goto illegal_op; if (b1) {
op1_offset = offsetof(CPUX86State,xmm_regs[reg]); if (mod == 3) {
op2_offset = offsetof(CPUX86State,xmm_regs[rm | REX_B(s)]); } else {
op2_offset = offsetof(CPUX86State,xmm_t0); gen_lea_modrm(s, modrm, &reg_addr, &offset_addr); switch (b) {
case 0x20: case 0x30: /* pmovsxbw, pmovzxbw */ case 0x23: case 0x33: /* pmovsxwd, pmovzxwd */ case 0x25: case 0x35: /* pmovsxdq, pmovzxdq */ gen_ldq_env_A0(s->mem_index, op2_offset + offsetof(XMMReg, XMM_Q(0))); break; case 0x21: case 0x31: /* pmovsxbd, pmovzxbd */ case 0x24: case 0x34: /* pmovsxwq, pmovzxwq */ tcg_gen_qemu_ld32u(cpu_tmp2_i32, cpu_A0, (s->mem_index >> 2) - 1); tcg_gen_st_i32(cpu_tmp2_i32, cpu_env, op2_offset + offsetof(XMMReg, XMM_L(0))); break; case 0x22: case 0x32: /* pmovsxbq, pmovzxbq */ tcg_gen_qemu_ld16u(cpu_tmp0, cpu_A0, (s->mem_index >> 2) - 1); tcg_gen_st16_tl(cpu_tmp0, cpu_env, op2_offset + offsetof(XMMReg, XMM_W(0))); break; case 0x2a: /* movntqda */ gen_ldo_env_A0(s->mem_index, op1_offset); return; default: gen_ldo_env_A0(s->mem_index, op2_offset); } } } else {
op1_offset = offsetof(CPUX86State,fpregs[reg].mmx); if (mod == 3) {
op2_offset = offsetof(CPUX86State,fpregs[rm].mmx); } else {
op2_offset = offsetof(CPUX86State,mmx_t0); gen_lea_modrm(s, modrm, &reg_addr, &offset_addr); gen_ldq_env_A0(s->mem_index, op2_offset); } } if (sse_op2 == SSE_SPECIAL) goto illegal_op; tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op1_offset); tcg_gen_addi_ptr(cpu_ptr1, cpu_env, op2_offset); tcg_gen_helper_0_2(sse_op2, cpu_ptr0, cpu_ptr1); if (b == 0x17) s->cc_op = CC_OP_EFLAGS; break; case 0x338: /* crc32 */ crc32: b = modrm; modrm = ldub_code(s->pc++); reg = ((modrm >> 3) & 7) | rex_r; if (b != 0xf0 && b != 0xf1) goto illegal_op; if (!(s->cpuid_ext_features & CPUID_EXT_SSE42)) goto illegal_op; if (b == 0xf0) ot = OT_BYTE; else if (b == 0xf1 && s->dflag != 2) if (s->prefix & PREFIX_DATA) ot = OT_WORD; else ot = OT_LONG; else ot = OT_QUAD; gen_op_mov_TN_reg(OT_LONG, 0, reg); tcg_gen_trunc_tl_i32(cpu_tmp2_i32, cpu_T[0]); gen_ldst_modrm(s, modrm, ot, OR_TMP0, 0); tcg_gen_helper_1_3(helper_crc32, cpu_T[0], cpu_tmp2_i32, cpu_T[0], tcg_const_i32(8 << ot)); ot = (s->dflag == 2) ? OT_QUAD : OT_LONG; gen_op_mov_reg_T0(ot, reg); break; case 0x03a: case 0x13a: b = modrm; modrm = ldub_code(s->pc++); rm = modrm & 7; reg = ((modrm >> 3) & 7) | rex_r; mod = (modrm >> 6) & 3; sse_op2 = sse_op_table7[b].op[b1]; if (!sse_op2) goto illegal_op; if (!(s->cpuid_ext_features & sse_op_table7[b].ext_mask)) goto illegal_op; if (sse_op2 == SSE_SPECIAL) {
ot = (s->dflag == 2) ? OT_QUAD : OT_LONG; rm = (modrm & 7) | REX_B(s); if (mod != 3) gen_lea_modrm(s, modrm, &reg_addr, &offset_addr); reg = ((modrm >> 3) & 7) | rex_r; val = ldub_code(s->pc++); switch (b) {
case 0x14: /* pextrb */ tcg_gen_ld8u_tl(cpu_T[0], cpu_env, offsetof(CPUX86State, xmm_regs[reg].XMM_B(val & 15))); if (mod == 3) gen_op_mov_reg_T0(ot, rm); else tcg_gen_qemu_st8(cpu_T[0], cpu_A0, (s->mem_index >> 2) - 1); break; case 0x15: /* pextrw */ tcg_gen_ld16u_tl(cpu_T[0], cpu_env, offsetof(CPUX86State, xmm_regs[reg].XMM_W(val & 7))); if (mod == 3) gen_op_mov_reg_T0(ot, rm); else tcg_gen_qemu_st16(cpu_T[0], cpu_A0, (s->mem_index >> 2) - 1); break; case 0x16: if (ot == OT_LONG) {
/* pextrd */ tcg_gen_ld_i32(cpu_tmp2_i32, cpu_env, offsetof(CPUX86State, xmm_regs[reg].XMM_L(val & 3))); if (mod == 3) gen_op_mov_reg_v(ot, rm, cpu_tmp2_i32); else tcg_gen_qemu_st32(cpu_tmp2_i32, cpu_A0, (s->mem_index >> 2) - 1); } else {
/* pextrq */ tcg_gen_ld_i64(cpu_tmp1_i64, cpu_env, offsetof(CPUX86State, xmm_regs[reg].XMM_Q(val & 1))); if (mod == 3) gen_op_mov_reg_v(ot, rm, cpu_tmp1_i64); else tcg_gen_qemu_st64(cpu_tmp1_i64, cpu_A0, (s->mem_index >> 2) - 1); } break; case 0x17: /* extractps */ tcg_gen_ld32u_tl(cpu_T[0], cpu_env, offsetof(CPUX86State, xmm_regs[reg].XMM_L(val & 3))); if (mod == 3) gen_op_mov_reg_T0(ot, rm); else tcg_gen_qemu_st32(cpu_T[0], cpu_A0, (s->mem_index >> 2) - 1); break; case 0x20: /* pinsrb */ if (mod == 3) gen_op_mov_TN_reg(OT_LONG, 0, rm); else tcg_gen_qemu_ld8u(cpu_T[0], cpu_A0, (s->mem_index >> 2) - 1); tcg_gen_st8_tl(cpu_T[0], cpu_env, offsetof(CPUX86State, xmm_regs[reg].XMM_B(val & 15))); break; case 0x21: /* insertps */ if (mod == 3) tcg_gen_ld_i32(cpu_tmp2_i32, cpu_env, offsetof(CPUX86State,xmm_regs[rm] .XMM_L((val >> 6) & 3))); else tcg_gen_qemu_ld32u(cpu_tmp2_i32, cpu_A0, (s->mem_index >> 2) - 1); tcg_gen_st_i32(cpu_tmp2_i32, cpu_env, offsetof(CPUX86State,xmm_regs[reg] .XMM_L((val >> 4) & 3))); if ((val >> 0) & 1) tcg_gen_st_i32(tcg_const_i32(0 /*float32_zero*/), cpu_env, offsetof(CPUX86State, xmm_regs[reg].XMM_L(0))); if ((val >> 1) & 1) tcg_gen_st_i32(tcg_const_i32(0 /*float32_zero*/), cpu_env, offsetof(CPUX86State, xmm_regs[reg].XMM_L(1))); if ((val >> 2) & 1) tcg_gen_st_i32(tcg_const_i32(0 /*float32_zero*/), cpu_env, offsetof(CPUX86State, xmm_regs[reg].XMM_L(2))); if ((val >> 3) & 1) tcg_gen_st_i32(tcg_const_i32(0 /*float32_zero*/), cpu_env, offsetof(CPUX86State, xmm_regs[reg].XMM_L(3))); break; case 0x22: if (ot == OT_LONG) {
/* pinsrd */ if (mod == 3) gen_op_mov_v_reg(ot, cpu_tmp2_i32, rm); else tcg_gen_qemu_ld32u(cpu_tmp2_i32, cpu_A0, (s->mem_index >> 2) - 1); tcg_gen_st_i32(cpu_tmp2_i32, cpu_env, offsetof(CPUX86State, xmm_regs[reg].XMM_L(val & 3))); } else {
/* pinsrq */ if (mod == 3) gen_op_mov_v_reg(ot, cpu_tmp1_i64, rm); else tcg_gen_qemu_ld64(cpu_tmp1_i64, cpu_A0, (s->mem_index >> 2) - 1); tcg_gen_st_i64(cpu_tmp1_i64, cpu_env, offsetof(CPUX86State, xmm_regs[reg].XMM_Q(val & 1))); } break; } return; } if (b1) {
op1_offset = offsetof(CPUX86State,xmm_regs[reg]); if (mod == 3) {
op2_offset = offsetof(CPUX86State,xmm_regs[rm | REX_B(s)]); } else {
op2_offset = offsetof(CPUX86State,xmm_t0); gen_lea_modrm(s, modrm, &reg_addr, &offset_addr); gen_ldo_env_A0(s->mem_index, op2_offset); } } else {
op1_offset = offsetof(CPUX86State,fpregs[reg].mmx); if (mod == 3) {
op2_offset = offsetof(CPUX86State,fpregs[rm].mmx); } else {
op2_offset = offsetof(CPUX86State,mmx_t0); gen_lea_modrm(s, modrm, &reg_addr, &offset_addr); gen_ldq_env_A0(s->mem_index, op2_offset); } } val = ldub_code(s->pc++); if ((b & 0xfc) == 0x60) {
/* pcmpXstrX */ s->cc_op = CC_OP_EFLAGS; if (s->dflag == 2) /* The helper must use entire 64-bit gp registers */ val |= 1 << 8; } tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op1_offset); tcg_gen_addi_ptr(cpu_ptr1, cpu_env, op2_offset); tcg_gen_helper_0_3(sse_op2, cpu_ptr0, cpu_ptr1, tcg_const_i32(val)); break; default: goto illegal_op; } } else {
/* generic MMX or SSE operation */ switch(b) {
case 0x70: /* pshufx insn */ case 0xc6: /* pshufx insn */ case 0xc2: /* compare insns */ s->rip_offset = 1; break; default: break; } if (is_xmm) {
op1_offset = offsetof(CPUX86State,xmm_regs[reg]); if (mod != 3) {
gen_lea_modrm(s, modrm, &reg_addr, &offset_addr); op2_offset = offsetof(CPUX86State,xmm_t0); if (b1 >= 2 && ((b >= 0x50 && b <= 0x5f && b != 0x5b) || b == 0xc2)) {
/* specific case for SSE single instructions */ if (b1 == 2) {
/* 32 bit access */ gen_op_ld_T0_A0(OT_LONG + s->mem_index); tcg_gen_st32_tl(cpu_T[0], cpu_env, offsetof(CPUX86State,xmm_t0.XMM_L(0))); } else {
/* 64 bit access */ gen_ldq_env_A0(s->mem_index, offsetof(CPUX86State,xmm_t0.XMM_D(0))); } } else {
gen_ldo_env_A0(s->mem_index, op2_offset); } } else {
rm = (modrm & 7) | REX_B(s); op2_offset = offsetof(CPUX86State,xmm_regs[rm]); } } else {
op1_offset = offsetof(CPUX86State,fpregs[reg].mmx); if (mod != 3) {
gen_lea_modrm(s, modrm, &reg_addr, &offset_addr); op2_offset = offsetof(CPUX86State,mmx_t0); gen_ldq_env_A0(s->mem_index, op2_offset); } else {
rm = (modrm & 7); op2_offset = offsetof(CPUX86State,fpregs[rm].mmx); } } switch(b) {
case 0x0f: /* 3DNow! data insns */ if (!(s->cpuid_ext2_features & CPUID_EXT2_3DNOW)) goto illegal_op; val = ldub_code(s->pc++); sse_op2 = sse_op_table5[val]; if (!sse_op2) goto illegal_op; tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op1_offset); tcg_gen_addi_ptr(cpu_ptr1, cpu_env, op2_offset); tcg_gen_helper_0_2(sse_op2, cpu_ptr0, cpu_ptr1); break; case 0x70: /* pshufx insn */ case 0xc6: /* pshufx insn */ val = ldub_code(s->pc++); tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op1_offset); tcg_gen_addi_ptr(cpu_ptr1, cpu_env, op2_offset); tcg_gen_helper_0_3(sse_op2, cpu_ptr0, cpu_ptr1, tcg_const_i32(val)); break; case 0xc2: /* compare insns */ val = ldub_code(s->pc++); if (val >= 8) goto illegal_op; sse_op2 = sse_op_table4[val][b1]; tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op1_offset); tcg_gen_addi_ptr(cpu_ptr1, cpu_env, op2_offset); tcg_gen_helper_0_2(sse_op2, cpu_ptr0, cpu_ptr1); break; case 0xf7: /* maskmov : we must prepare A0 */ if (mod != 3) goto illegal_op; #ifdef TARGET_X86_64 if (s->aflag == 2) {
gen_op_movq_A0_reg(R_EDI); } else #endif {
gen_op_movl_A0_reg(R_EDI); if (s->aflag == 0) gen_op_andl_A0_ffff(); } gen_add_A0_ds_seg(s); tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op1_offset); tcg_gen_addi_ptr(cpu_ptr1, cpu_env, op2_offset); tcg_gen_helper_0_3(sse_op2, cpu_ptr0, cpu_ptr1, cpu_A0); break; default: tcg_gen_addi_ptr(cpu_ptr0, cpu_env, op1_offset); tcg_gen_addi_ptr(cpu_ptr1, cpu_env, op2_offset); tcg_gen_helper_0_2(sse_op2, cpu_ptr0, cpu_ptr1); break; } if (b == 0x2e || b == 0x2f) {
s->cc_op = CC_OP_EFLAGS; } } } ",qemu,1
"static int vmdk_create_extent(const char *filename, int64_t filesize,

                              bool flat, bool compress, bool zeroed_grain,

                              Error **errp)

{

    int ret, i;

    BlockDriverState *bs = NULL;

    VMDK4Header header;

    Error *local_err;

    uint32_t tmp, magic, grains, gd_sectors, gt_size, gt_count;

    uint32_t *gd_buf = NULL;

    int gd_buf_size;



    ret = bdrv_create_file(filename, NULL, &local_err);

    if (ret < 0) {

        error_propagate(errp, local_err);

        goto exit;

    }



    ret = bdrv_file_open(&bs, filename, NULL, NULL, BDRV_O_RDWR, &local_err);

    if (ret < 0) {

        error_propagate(errp, local_err);

        goto exit;

    }



    if (flat) {

        ret = bdrv_truncate(bs, filesize);

        if (ret < 0) {

            error_setg(errp, ""Could not truncate file"");

        }

        goto exit;

    }

    magic = cpu_to_be32(VMDK4_MAGIC);

    memset(&header, 0, sizeof(header));

    header.version = zeroed_grain ? 2 : 1;

    header.flags = VMDK4_FLAG_RGD | VMDK4_FLAG_NL_DETECT

                   | (compress ? VMDK4_FLAG_COMPRESS | VMDK4_FLAG_MARKER : 0)

                   | (zeroed_grain ? VMDK4_FLAG_ZERO_GRAIN : 0);

    header.compressAlgorithm = compress ? VMDK4_COMPRESSION_DEFLATE : 0;

    header.capacity = filesize / BDRV_SECTOR_SIZE;

    header.granularity = 128;

    header.num_gtes_per_gt = BDRV_SECTOR_SIZE;



    grains = DIV_ROUND_UP(filesize / BDRV_SECTOR_SIZE, header.granularity);

    gt_size = DIV_ROUND_UP(header.num_gtes_per_gt * sizeof(uint32_t),

                           BDRV_SECTOR_SIZE);

    gt_count = DIV_ROUND_UP(grains, header.num_gtes_per_gt);

    gd_sectors = DIV_ROUND_UP(gt_count * sizeof(uint32_t), BDRV_SECTOR_SIZE);



    header.desc_offset = 1;

    header.desc_size = 20;

    header.rgd_offset = header.desc_offset + header.desc_size;

    header.gd_offset = header.rgd_offset + gd_sectors + (gt_size * gt_count);

    header.grain_offset =

        ROUND_UP(header.gd_offset + gd_sectors + (gt_size * gt_count),

                 header.granularity);

    /* swap endianness for all header fields */

    header.version = cpu_to_le32(header.version);

    header.flags = cpu_to_le32(header.flags);

    header.capacity = cpu_to_le64(header.capacity);

    header.granularity = cpu_to_le64(header.granularity);

    header.num_gtes_per_gt = cpu_to_le32(header.num_gtes_per_gt);

    header.desc_offset = cpu_to_le64(header.desc_offset);

    header.desc_size = cpu_to_le64(header.desc_size);

    header.rgd_offset = cpu_to_le64(header.rgd_offset);

    header.gd_offset = cpu_to_le64(header.gd_offset);

    header.grain_offset = cpu_to_le64(header.grain_offset);

    header.compressAlgorithm = cpu_to_le16(header.compressAlgorithm);



    header.check_bytes[0] = 0xa;

    header.check_bytes[1] = 0x20;

    header.check_bytes[2] = 0xd;

    header.check_bytes[3] = 0xa;



    /* write all the data */

    ret = bdrv_pwrite(bs, 0, &magic, sizeof(magic));

    if (ret < 0) {

        error_set(errp, QERR_IO_ERROR);

        goto exit;

    }

    ret = bdrv_pwrite(bs, sizeof(magic), &header, sizeof(header));

    if (ret < 0) {

        error_set(errp, QERR_IO_ERROR);

        goto exit;

    }



    ret = bdrv_truncate(bs, le64_to_cpu(header.grain_offset) << 9);

    if (ret < 0) {

        error_setg(errp, ""Could not truncate file"");

        goto exit;

    }



    /* write grain directory */

    gd_buf_size = gd_sectors * BDRV_SECTOR_SIZE;

    gd_buf = g_malloc0(gd_buf_size);

    for (i = 0, tmp = le64_to_cpu(header.rgd_offset) + gd_sectors;

         i < gt_count; i++, tmp += gt_size) {

        gd_buf[i] = cpu_to_le32(tmp);

    }

    ret = bdrv_pwrite(bs, le64_to_cpu(header.rgd_offset) * BDRV_SECTOR_SIZE,

                      gd_buf, gd_buf_size);

    if (ret < 0) {

        error_set(errp, QERR_IO_ERROR);

        goto exit;

    }



    /* write backup grain directory */

    for (i = 0, tmp = le64_to_cpu(header.gd_offset) + gd_sectors;

         i < gt_count; i++, tmp += gt_size) {

        gd_buf[i] = cpu_to_le32(tmp);

    }

    ret = bdrv_pwrite(bs, le64_to_cpu(header.gd_offset) * BDRV_SECTOR_SIZE,

                      gd_buf, gd_buf_size);

    if (ret < 0) {

        error_set(errp, QERR_IO_ERROR);

        goto exit;

    }



    ret = 0;

exit:

    if (bs) {

        bdrv_unref(bs);

    }

    g_free(gd_buf);

    return ret;

}
","static int vmdk_create_extent(const char *filename, int64_t filesize, bool flat, bool compress, bool zeroed_grain, Error **errp) {
int ret, i; BlockDriverState *bs = NULL; VMDK4Header header; Error *local_err; uint32_t tmp, magic, grains, gd_sectors, gt_size, gt_count; uint32_t *gd_buf = NULL; int gd_buf_size; ret = bdrv_create_file(filename, NULL, &local_err); if (ret < 0) {
error_propagate(errp, local_err); goto exit; } ret = bdrv_file_open(&bs, filename, NULL, NULL, BDRV_O_RDWR, &local_err); if (ret < 0) {
error_propagate(errp, local_err); goto exit; } if (flat) {
ret = bdrv_truncate(bs, filesize); if (ret < 0) {
error_setg(errp, ""Could not truncate file""); } goto exit; } magic = cpu_to_be32(VMDK4_MAGIC); memset(&header, 0, sizeof(header)); header.version = zeroed_grain ? 2 : 1; header.flags = VMDK4_FLAG_RGD | VMDK4_FLAG_NL_DETECT | (compress ? VMDK4_FLAG_COMPRESS | VMDK4_FLAG_MARKER : 0) | (zeroed_grain ? VMDK4_FLAG_ZERO_GRAIN : 0); header.compressAlgorithm = compress ? VMDK4_COMPRESSION_DEFLATE : 0; header.capacity = filesize / BDRV_SECTOR_SIZE; header.granularity = 128; header.num_gtes_per_gt = BDRV_SECTOR_SIZE; grains = DIV_ROUND_UP(filesize / BDRV_SECTOR_SIZE, header.granularity); gt_size = DIV_ROUND_UP(header.num_gtes_per_gt * sizeof(uint32_t), BDRV_SECTOR_SIZE); gt_count = DIV_ROUND_UP(grains, header.num_gtes_per_gt); gd_sectors = DIV_ROUND_UP(gt_count * sizeof(uint32_t), BDRV_SECTOR_SIZE); header.desc_offset = 1; header.desc_size = 20; header.rgd_offset = header.desc_offset + header.desc_size; header.gd_offset = header.rgd_offset + gd_sectors + (gt_size * gt_count); header.grain_offset = ROUND_UP(header.gd_offset + gd_sectors + (gt_size * gt_count), header.granularity); /* swap endianness for all header fields */ header.version = cpu_to_le32(header.version); header.flags = cpu_to_le32(header.flags); header.capacity = cpu_to_le64(header.capacity); header.granularity = cpu_to_le64(header.granularity); header.num_gtes_per_gt = cpu_to_le32(header.num_gtes_per_gt); header.desc_offset = cpu_to_le64(header.desc_offset); header.desc_size = cpu_to_le64(header.desc_size); header.rgd_offset = cpu_to_le64(header.rgd_offset); header.gd_offset = cpu_to_le64(header.gd_offset); header.grain_offset = cpu_to_le64(header.grain_offset); header.compressAlgorithm = cpu_to_le16(header.compressAlgorithm); header.check_bytes[0] = 0xa; header.check_bytes[1] = 0x20; header.check_bytes[2] = 0xd; header.check_bytes[3] = 0xa; /* write all the data */ ret = bdrv_pwrite(bs, 0, &magic, sizeof(magic)); if (ret < 0) {
error_set(errp, QERR_IO_ERROR); goto exit; } ret = bdrv_pwrite(bs, sizeof(magic), &header, sizeof(header)); if (ret < 0) {
error_set(errp, QERR_IO_ERROR); goto exit; } ret = bdrv_truncate(bs, le64_to_cpu(header.grain_offset) << 9); if (ret < 0) {
error_setg(errp, ""Could not truncate file""); goto exit; } /* write grain directory */ gd_buf_size = gd_sectors * BDRV_SECTOR_SIZE; gd_buf = g_malloc0(gd_buf_size); for (i = 0, tmp = le64_to_cpu(header.rgd_offset) + gd_sectors; i < gt_count; i++, tmp += gt_size) {
gd_buf[i] = cpu_to_le32(tmp); } ret = bdrv_pwrite(bs, le64_to_cpu(header.rgd_offset) * BDRV_SECTOR_SIZE, gd_buf, gd_buf_size); if (ret < 0) {
error_set(errp, QERR_IO_ERROR); goto exit; } /* write backup grain directory */ for (i = 0, tmp = le64_to_cpu(header.gd_offset) + gd_sectors; i < gt_count; i++, tmp += gt_size) {
gd_buf[i] = cpu_to_le32(tmp); } ret = bdrv_pwrite(bs, le64_to_cpu(header.gd_offset) * BDRV_SECTOR_SIZE, gd_buf, gd_buf_size); if (ret < 0) {
error_set(errp, QERR_IO_ERROR); goto exit; } ret = 0; exit: if (bs) {
bdrv_unref(bs); } g_free(gd_buf); return ret; } ",qemu,1
"static int64_t seek_to_sector(BlockDriverState *bs, int64_t sector_num)

{

    BDRVBochsState *s = bs->opaque;

    uint64_t offset = sector_num * 512;

    uint64_t extent_index, extent_offset, bitmap_offset;

    char bitmap_entry;



    // seek to sector

    extent_index = offset / s->extent_size;

    extent_offset = (offset % s->extent_size) / 512;



    if (s->catalog_bitmap[extent_index] == 0xffffffff) {

	return -1; /* not allocated */

    }



    bitmap_offset = s->data_offset +

        (512 * (uint64_t) s->catalog_bitmap[extent_index] *

        (s->extent_blocks + s->bitmap_blocks));



    /* read in bitmap for current extent */

    if (bdrv_pread(bs->file, bitmap_offset + (extent_offset / 8),

                   &bitmap_entry, 1) != 1) {

        return -1;

    }



    if (!((bitmap_entry >> (extent_offset % 8)) & 1)) {

	return -1; /* not allocated */

    }



    return bitmap_offset + (512 * (s->bitmap_blocks + extent_offset));

}
","static int64_t seek_to_sector(BlockDriverState *bs, int64_t sector_num) {
BDRVBochsState *s = bs->opaque; uint64_t offset = sector_num * 512; uint64_t extent_index, extent_offset, bitmap_offset; char bitmap_entry; // seek to sector extent_index = offset / s->extent_size; extent_offset = (offset % s->extent_size) / 512; if (s->catalog_bitmap[extent_index] == 0xffffffff) {
return -1; /* not allocated */ } bitmap_offset = s->data_offset + (512 * (uint64_t) s->catalog_bitmap[extent_index] * (s->extent_blocks + s->bitmap_blocks)); /* read in bitmap for current extent */ if (bdrv_pread(bs->file, bitmap_offset + (extent_offset / 8), &bitmap_entry, 1) != 1) {
return -1; } if (!((bitmap_entry >> (extent_offset % 8)) & 1)) {
return -1; /* not allocated */ } return bitmap_offset + (512 * (s->bitmap_blocks + extent_offset)); } ",qemu,0
"void do_blockdev_backup(BlockdevBackup *backup, BlockJobTxn *txn, Error **errp)

{

    BlockDriverState *bs;

    BlockDriverState *target_bs;

    Error *local_err = NULL;

    AioContext *aio_context;



    if (!backup->has_speed) {

        backup->speed = 0;

    }

    if (!backup->has_on_source_error) {

        backup->on_source_error = BLOCKDEV_ON_ERROR_REPORT;

    }

    if (!backup->has_on_target_error) {

        backup->on_target_error = BLOCKDEV_ON_ERROR_REPORT;

    }

    if (!backup->has_job_id) {

        backup->job_id = NULL;

    }



    bs = qmp_get_root_bs(backup->device, errp);

    if (!bs) {

        return;

    }



    aio_context = bdrv_get_aio_context(bs);

    aio_context_acquire(aio_context);



    target_bs = bdrv_lookup_bs(backup->target, backup->target, errp);

    if (!target_bs) {

        goto out;

    }



    if (bdrv_get_aio_context(target_bs) != aio_context) {

        if (!bdrv_has_blk(target_bs)) {

            /* The target BDS is not attached, we can safely move it to another

             * AioContext. */

            bdrv_set_aio_context(target_bs, aio_context);

        } else {

            error_setg(errp, ""Target is attached to a different thread from ""

                             ""source."");

            goto out;

        }

    }

    backup_start(backup->job_id, bs, target_bs, backup->speed, backup->sync,

                 NULL, backup->on_source_error, backup->on_target_error,

                 block_job_cb, bs, txn, &local_err);

    if (local_err != NULL) {

        error_propagate(errp, local_err);

    }

out:

    aio_context_release(aio_context);

}
","void do_blockdev_backup(BlockdevBackup *backup, BlockJobTxn *txn, Error **errp) {
BlockDriverState *bs; BlockDriverState *target_bs; Error *local_err = NULL; AioContext *aio_context; if (!backup->has_speed) {
backup->speed = 0; } if (!backup->has_on_source_error) {
backup->on_source_error = BLOCKDEV_ON_ERROR_REPORT; } if (!backup->has_on_target_error) {
backup->on_target_error = BLOCKDEV_ON_ERROR_REPORT; } if (!backup->has_job_id) {
backup->job_id = NULL; } bs = qmp_get_root_bs(backup->device, errp); if (!bs) {
return; } aio_context = bdrv_get_aio_context(bs); aio_context_acquire(aio_context); target_bs = bdrv_lookup_bs(backup->target, backup->target, errp); if (!target_bs) {
goto out; } if (bdrv_get_aio_context(target_bs) != aio_context) {
if (!bdrv_has_blk(target_bs)) {
/* The target BDS is not attached, we can safely move it to another * AioContext. */ bdrv_set_aio_context(target_bs, aio_context); } else {
error_setg(errp, ""Target is attached to a different thread from "" ""source.""); goto out; } } backup_start(backup->job_id, bs, target_bs, backup->speed, backup->sync, NULL, backup->on_source_error, backup->on_target_error, block_job_cb, bs, txn, &local_err); if (local_err != NULL) {
error_propagate(errp, local_err); } out: aio_context_release(aio_context); } ",qemu,0
"uint64_t helper_fctiw(CPUPPCState *env, uint64_t arg)

{

    CPU_DoubleU farg;



    farg.ll = arg;



    if (unlikely(float64_is_signaling_nan(farg.d))) {

        /* sNaN conversion */

        farg.ll = fload_invalid_op_excp(env, POWERPC_EXCP_FP_VXSNAN |

                                        POWERPC_EXCP_FP_VXCVI);

    } else if (unlikely(float64_is_quiet_nan(farg.d) ||

                        float64_is_infinity(farg.d))) {

        /* qNan / infinity conversion */

        farg.ll = fload_invalid_op_excp(env, POWERPC_EXCP_FP_VXCVI);

    } else {

        farg.ll = float64_to_int32(farg.d, &env->fp_status);

        /* XXX: higher bits are not supposed to be significant.

         *     to make tests easier, return the same as a real PowerPC 750

         */

        farg.ll |= 0xFFF80000ULL << 32;

    }

    return farg.ll;

}
","uint64_t helper_fctiw(CPUPPCState *env, uint64_t arg) {
CPU_DoubleU farg; farg.ll = arg; if (unlikely(float64_is_signaling_nan(farg.d))) {
/* sNaN conversion */ farg.ll = fload_invalid_op_excp(env, POWERPC_EXCP_FP_VXSNAN | POWERPC_EXCP_FP_VXCVI); } else if (unlikely(float64_is_quiet_nan(farg.d) || float64_is_infinity(farg.d))) {
/* qNan / infinity conversion */ farg.ll = fload_invalid_op_excp(env, POWERPC_EXCP_FP_VXCVI); } else {
farg.ll = float64_to_int32(farg.d, &env->fp_status); /* XXX: higher bits are not supposed to be significant. * to make tests easier, return the same as a real PowerPC 750 */ farg.ll |= 0xFFF80000ULL << 32; } return farg.ll; } ",qemu,0
"static inline void gen_op_clear_ieee_excp_and_FTT(void)

{

    tcg_gen_andi_tl(cpu_fsr, cpu_fsr, ~(FSR_FTT_MASK | FSR_CEXC_MASK));

}
","static inline void gen_op_clear_ieee_excp_and_FTT(void) {
tcg_gen_andi_tl(cpu_fsr, cpu_fsr, ~(FSR_FTT_MASK | FSR_CEXC_MASK)); } ",qemu,1
"void timer_del(QEMUTimer *ts)

{

    QEMUTimerList *timer_list = ts->timer_list;



    qemu_mutex_lock(&timer_list->active_timers_lock);

    timer_del_locked(timer_list, ts);

    qemu_mutex_unlock(&timer_list->active_timers_lock);

}
","void timer_del(QEMUTimer *ts) {
QEMUTimerList *timer_list = ts->timer_list; qemu_mutex_lock(&timer_list->active_timers_lock); timer_del_locked(timer_list, ts); qemu_mutex_unlock(&timer_list->active_timers_lock); } ",qemu,1
"Visitor *string_output_get_visitor(StringOutputVisitor *sov)

{

    return &sov->visitor;

}
","Visitor *string_output_get_visitor(StringOutputVisitor *sov) {
return &sov->visitor; } ",qemu,0
"int main(int argc, char **argv, char **envp)

{

    const char *gdbstub_dev = NULL;

    int i;

    int snapshot, linux_boot;

    const char *icount_option = NULL;

    const char *initrd_filename;

    const char *kernel_filename, *kernel_cmdline;

    char boot_devices[33] = ""cad""; /* default to HD->floppy->CD-ROM */

    DisplayState *ds;

    DisplayChangeListener *dcl;

    int cyls, heads, secs, translation;

    QemuOpts *hda_opts = NULL, *opts;

    QemuOptsList *olist;

    int optind;

    const char *optarg;

    const char *loadvm = NULL;

    QEMUMachine *machine;

    const char *cpu_model;

    const char *pid_file = NULL;

    const char *incoming = NULL;

#ifdef CONFIG_VNC

    int show_vnc_port = 0;

#endif

    int defconfig = 1;

    const char *log_mask = NULL;

    const char *log_file = NULL;

    GMemVTable mem_trace = {

        .malloc = malloc_and_trace,

        .realloc = realloc_and_trace,

        .free = free_and_trace,

    };

    const char *trace_events = NULL;

    const char *trace_file = NULL;



    atexit(qemu_run_exit_notifiers);

    error_set_progname(argv[0]);



    g_mem_set_vtable(&mem_trace);

    if (!g_thread_supported()) {

#if !GLIB_CHECK_VERSION(2, 31, 0)

        g_thread_init(NULL);

#else

        fprintf(stderr, ""glib threading failed to initialize.\n"");

        exit(1);

#endif

    }



    runstate_init();



    init_clocks();

    rtc_clock = host_clock;



    qemu_cache_utils_init(envp);



    QLIST_INIT (&vm_change_state_head);

    os_setup_early_signal_handling();



    module_call_init(MODULE_INIT_MACHINE);

    machine = find_default_machine();

    cpu_model = NULL;

    initrd_filename = NULL;

    ram_size = 0;

    snapshot = 0;

    kernel_filename = NULL;

    kernel_cmdline = """";

    cyls = heads = secs = 0;

    translation = BIOS_ATA_TRANSLATION_AUTO;



    for (i = 0; i < MAX_NODES; i++) {

        node_mem[i] = 0;

        node_cpumask[i] = 0;

    }



    nb_numa_nodes = 0;

    nb_nics = 0;



    autostart= 1;



    /* first pass of option parsing */

    optind = 1;

    while (optind < argc) {

        if (argv[optind][0] != '-') {

            /* disk image */

            optind++;

            continue;

        } else {

            const QEMUOption *popt;



            popt = lookup_opt(argc, argv, &optarg, &optind);

            switch (popt->index) {

            case QEMU_OPTION_nodefconfig:

                defconfig=0;

                break;

            }

        }

    }



    if (defconfig) {

        int ret;



        ret = qemu_read_config_file(CONFIG_QEMU_CONFDIR ""/qemu.conf"");

        if (ret < 0 && ret != -ENOENT) {

            exit(1);

        }



        ret = qemu_read_config_file(arch_config_name);

        if (ret < 0 && ret != -ENOENT) {

            exit(1);

        }

    }

    cpudef_init();



    /* second pass of option parsing */

    optind = 1;

    for(;;) {

        if (optind >= argc)

            break;

        if (argv[optind][0] != '-') {

	    hda_opts = drive_add(IF_DEFAULT, 0, argv[optind++], HD_OPTS);

        } else {

            const QEMUOption *popt;



            popt = lookup_opt(argc, argv, &optarg, &optind);

            if (!(popt->arch_mask & arch_type)) {

                printf(""Option %s not supported for this target\n"", popt->name);

                exit(1);

            }

            switch(popt->index) {

            case QEMU_OPTION_M:

                machine = machine_parse(optarg);

                break;

            case QEMU_OPTION_cpu:

                /* hw initialization will check this */

                if (*optarg == '?') {

                    list_cpus(stdout, &fprintf, optarg);

                    exit(0);

                } else {

                    cpu_model = optarg;

                }

                break;

            case QEMU_OPTION_initrd:

                initrd_filename = optarg;

                break;

            case QEMU_OPTION_hda:

                {

                    char buf[256];

                    if (cyls == 0)

                        snprintf(buf, sizeof(buf), ""%s"", HD_OPTS);

                    else

                        snprintf(buf, sizeof(buf),

                                 ""%s,cyls=%d,heads=%d,secs=%d%s"",

                                 HD_OPTS , cyls, heads, secs,

                                 translation == BIOS_ATA_TRANSLATION_LBA ?

                                 "",trans=lba"" :

                                 translation == BIOS_ATA_TRANSLATION_NONE ?

                                 "",trans=none"" : """");

                    drive_add(IF_DEFAULT, 0, optarg, buf);

                    break;

                }

            case QEMU_OPTION_hdb:

            case QEMU_OPTION_hdc:

            case QEMU_OPTION_hdd:

                drive_add(IF_DEFAULT, popt->index - QEMU_OPTION_hda, optarg,

                          HD_OPTS);

                break;

            case QEMU_OPTION_drive:

                if (drive_def(optarg) == NULL) {

                    exit(1);

                }

	        break;

            case QEMU_OPTION_set:

                if (qemu_set_option(optarg) != 0)

                    exit(1);

	        break;

            case QEMU_OPTION_global:

                if (qemu_global_option(optarg) != 0)

                    exit(1);

	        break;

            case QEMU_OPTION_mtdblock:

                drive_add(IF_MTD, -1, optarg, MTD_OPTS);

                break;

            case QEMU_OPTION_sd:

                drive_add(IF_SD, 0, optarg, SD_OPTS);

                break;

            case QEMU_OPTION_pflash:

                drive_add(IF_PFLASH, -1, optarg, PFLASH_OPTS);

                break;

            case QEMU_OPTION_snapshot:

                snapshot = 1;

                break;

            case QEMU_OPTION_hdachs:

                {

                    const char *p;

                    p = optarg;

                    cyls = strtol(p, (char **)&p, 0);

                    if (cyls < 1 || cyls > 16383)

                        goto chs_fail;

                    if (*p != ',')

                        goto chs_fail;

                    p++;

                    heads = strtol(p, (char **)&p, 0);

                    if (heads < 1 || heads > 16)

                        goto chs_fail;

                    if (*p != ',')

                        goto chs_fail;

                    p++;

                    secs = strtol(p, (char **)&p, 0);

                    if (secs < 1 || secs > 63)

                        goto chs_fail;

                    if (*p == ',') {

                        p++;

                        if (!strcmp(p, ""none""))

                            translation = BIOS_ATA_TRANSLATION_NONE;

                        else if (!strcmp(p, ""lba""))

                            translation = BIOS_ATA_TRANSLATION_LBA;

                        else if (!strcmp(p, ""auto""))

                            translation = BIOS_ATA_TRANSLATION_AUTO;

                        else

                            goto chs_fail;

                    } else if (*p != '\0') {

                    chs_fail:

                        fprintf(stderr, ""qemu: invalid physical CHS format\n"");

                        exit(1);

                    }

		    if (hda_opts != NULL) {

                        char num[16];

                        snprintf(num, sizeof(num), ""%d"", cyls);

                        qemu_opt_set(hda_opts, ""cyls"", num);

                        snprintf(num, sizeof(num), ""%d"", heads);

                        qemu_opt_set(hda_opts, ""heads"", num);

                        snprintf(num, sizeof(num), ""%d"", secs);

                        qemu_opt_set(hda_opts, ""secs"", num);

                        if (translation == BIOS_ATA_TRANSLATION_LBA)

                            qemu_opt_set(hda_opts, ""trans"", ""lba"");

                        if (translation == BIOS_ATA_TRANSLATION_NONE)

                            qemu_opt_set(hda_opts, ""trans"", ""none"");

                    }

                }

                break;

            case QEMU_OPTION_numa:

                if (nb_numa_nodes >= MAX_NODES) {

                    fprintf(stderr, ""qemu: too many NUMA nodes\n"");

                    exit(1);

                }

                numa_add(optarg);

                break;

            case QEMU_OPTION_display:

                display_type = select_display(optarg);

                break;

            case QEMU_OPTION_nographic:

                display_type = DT_NOGRAPHIC;

                break;

            case QEMU_OPTION_curses:

#ifdef CONFIG_CURSES

                display_type = DT_CURSES;

#else

                fprintf(stderr, ""Curses support is disabled\n"");

                exit(1);

#endif

                break;

            case QEMU_OPTION_portrait:

                graphic_rotate = 90;

                break;

            case QEMU_OPTION_rotate:

                graphic_rotate = strtol(optarg, (char **) &optarg, 10);

                if (graphic_rotate != 0 && graphic_rotate != 90 &&

                    graphic_rotate != 180 && graphic_rotate != 270) {

                    fprintf(stderr,

                        ""qemu: only 90, 180, 270 deg rotation is available\n"");

                    exit(1);

                }

                break;

            case QEMU_OPTION_kernel:

                kernel_filename = optarg;

                break;

            case QEMU_OPTION_append:

                kernel_cmdline = optarg;

                break;

            case QEMU_OPTION_cdrom:

                drive_add(IF_DEFAULT, 2, optarg, CDROM_OPTS);

                break;

            case QEMU_OPTION_boot:

                {

                    static const char * const params[] = {

                        ""order"", ""once"", ""menu"",

                        ""splash"", ""splash-time"", NULL

                    };

                    char buf[sizeof(boot_devices)];

                    char *standard_boot_devices;

                    int legacy = 0;



                    if (!strchr(optarg, '=')) {

                        legacy = 1;

                        pstrcpy(buf, sizeof(buf), optarg);

                    } else if (check_params(buf, sizeof(buf), params, optarg) < 0) {

                        fprintf(stderr,

                                ""qemu: unknown boot parameter '%s' in '%s'\n"",

                                buf, optarg);

                        exit(1);

                    }



                    if (legacy ||

                        get_param_value(buf, sizeof(buf), ""order"", optarg)) {

                        validate_bootdevices(buf);

                        pstrcpy(boot_devices, sizeof(boot_devices), buf);

                    }

                    if (!legacy) {

                        if (get_param_value(buf, sizeof(buf),

                                            ""once"", optarg)) {

                            validate_bootdevices(buf);

                            standard_boot_devices = g_strdup(boot_devices);

                            pstrcpy(boot_devices, sizeof(boot_devices), buf);

                            qemu_register_reset(restore_boot_devices,

                                                standard_boot_devices);

                        }

                        if (get_param_value(buf, sizeof(buf),

                                            ""menu"", optarg)) {

                            if (!strcmp(buf, ""on"")) {

                                boot_menu = 1;

                            } else if (!strcmp(buf, ""off"")) {

                                boot_menu = 0;

                            } else {

                                fprintf(stderr,

                                        ""qemu: invalid option value '%s'\n"",

                                        buf);

                                exit(1);

                            }

                        }

                        qemu_opts_parse(qemu_find_opts(""boot-opts""),

                                        optarg, 0);

                    }

                }

                break;

            case QEMU_OPTION_fda:

            case QEMU_OPTION_fdb:

                drive_add(IF_FLOPPY, popt->index - QEMU_OPTION_fda,

                          optarg, FD_OPTS);

                break;

            case QEMU_OPTION_no_fd_bootchk:

                fd_bootchk = 0;

                break;

            case QEMU_OPTION_netdev:

                if (net_client_parse(qemu_find_opts(""netdev""), optarg) == -1) {

                    exit(1);

                }

                break;

            case QEMU_OPTION_net:

                if (net_client_parse(qemu_find_opts(""net""), optarg) == -1) {

                    exit(1);

                }

                break;

#ifdef CONFIG_SLIRP

            case QEMU_OPTION_tftp:

                legacy_tftp_prefix = optarg;

                break;

            case QEMU_OPTION_bootp:

                legacy_bootp_filename = optarg;

                break;

            case QEMU_OPTION_redir:

                if (net_slirp_redir(optarg) < 0)

                    exit(1);

                break;

#endif

            case QEMU_OPTION_bt:

                add_device_config(DEV_BT, optarg);

                break;

            case QEMU_OPTION_audio_help:

                if (!(audio_available())) {

                    printf(""Option %s not supported for this target\n"", popt->name);

                    exit(1);

                }

                AUD_help ();

                exit (0);

                break;

            case QEMU_OPTION_soundhw:

                if (!(audio_available())) {

                    printf(""Option %s not supported for this target\n"", popt->name);

                    exit(1);

                }

                select_soundhw (optarg);

                break;

            case QEMU_OPTION_h:

                help(0);

                break;

            case QEMU_OPTION_version:

                version();

                exit(0);

                break;

            case QEMU_OPTION_m: {

                int64_t value;

                char *end;



                value = strtosz(optarg, &end);

                if (value < 0 || *end) {

                    fprintf(stderr, ""qemu: invalid ram size: %s\n"", optarg);

                    exit(1);

                }



                if (value != (uint64_t)(ram_addr_t)value) {

                    fprintf(stderr, ""qemu: ram size too large\n"");

                    exit(1);

                }

                ram_size = value;

                break;

            }

            case QEMU_OPTION_mempath:

                mem_path = optarg;

                break;

#ifdef MAP_POPULATE

            case QEMU_OPTION_mem_prealloc:

                mem_prealloc = 1;

                break;

#endif

            case QEMU_OPTION_d:

                log_mask = optarg;

                break;

            case QEMU_OPTION_D:

                log_file = optarg;

                break;

            case QEMU_OPTION_s:

                gdbstub_dev = ""tcp::"" DEFAULT_GDBSTUB_PORT;

                break;

            case QEMU_OPTION_gdb:

                gdbstub_dev = optarg;

                break;

            case QEMU_OPTION_L:

                data_dir = optarg;

                break;

            case QEMU_OPTION_bios:

                bios_name = optarg;

                break;

            case QEMU_OPTION_singlestep:

                singlestep = 1;

                break;

            case QEMU_OPTION_S:

                autostart = 0;

                break;

	    case QEMU_OPTION_k:

		keyboard_layout = optarg;

		break;

            case QEMU_OPTION_localtime:

                rtc_utc = 0;

                break;

            case QEMU_OPTION_vga:

                select_vgahw (optarg);

                break;

            case QEMU_OPTION_g:

                {

                    const char *p;

                    int w, h, depth;

                    p = optarg;

                    w = strtol(p, (char **)&p, 10);

                    if (w <= 0) {

                    graphic_error:

                        fprintf(stderr, ""qemu: invalid resolution or depth\n"");

                        exit(1);

                    }

                    if (*p != 'x')

                        goto graphic_error;

                    p++;

                    h = strtol(p, (char **)&p, 10);

                    if (h <= 0)

                        goto graphic_error;

                    if (*p == 'x') {

                        p++;

                        depth = strtol(p, (char **)&p, 10);

                        if (depth != 8 && depth != 15 && depth != 16 &&

                            depth != 24 && depth != 32)

                            goto graphic_error;

                    } else if (*p == '\0') {

                        depth = graphic_depth;

                    } else {

                        goto graphic_error;

                    }



                    graphic_width = w;

                    graphic_height = h;

                    graphic_depth = depth;

                }

                break;

            case QEMU_OPTION_echr:

                {

                    char *r;

                    term_escape_char = strtol(optarg, &r, 0);

                    if (r == optarg)

                        printf(""Bad argument to echr\n"");

                    break;

                }

            case QEMU_OPTION_monitor:

                monitor_parse(optarg, ""readline"");

                default_monitor = 0;

                break;

            case QEMU_OPTION_qmp:

                monitor_parse(optarg, ""control"");

                default_monitor = 0;

                break;

            case QEMU_OPTION_mon:

                opts = qemu_opts_parse(qemu_find_opts(""mon""), optarg, 1);

                if (!opts) {

                    exit(1);

                }

                default_monitor = 0;

                break;

            case QEMU_OPTION_chardev:

                opts = qemu_opts_parse(qemu_find_opts(""chardev""), optarg, 1);

                if (!opts) {

                    exit(1);

                }

                break;

            case QEMU_OPTION_fsdev:

                olist = qemu_find_opts(""fsdev"");

                if (!olist) {

                    fprintf(stderr, ""fsdev is not supported by this qemu build.\n"");

                    exit(1);

                }

                opts = qemu_opts_parse(olist, optarg, 1);

                if (!opts) {

                    fprintf(stderr, ""parse error: %s\n"", optarg);

                    exit(1);

                }

                break;

            case QEMU_OPTION_virtfs: {

                QemuOpts *fsdev;

                QemuOpts *device;

                const char *writeout;



                olist = qemu_find_opts(""virtfs"");

                if (!olist) {

                    fprintf(stderr, ""virtfs is not supported by this qemu build.\n"");

                    exit(1);

                }

                opts = qemu_opts_parse(olist, optarg, 1);

                if (!opts) {

                    fprintf(stderr, ""parse error: %s\n"", optarg);

                    exit(1);

                }



                if (qemu_opt_get(opts, ""fsdriver"") == NULL ||

                        qemu_opt_get(opts, ""mount_tag"") == NULL ||

                        qemu_opt_get(opts, ""path"") == NULL) {

                    fprintf(stderr, ""Usage: -virtfs fsdriver,path=/share_path/,""

                            ""[security_model={mapped|passthrough|none}],""

                            ""mount_tag=tag.\n"");

                    exit(1);

                }

                fsdev = qemu_opts_create(qemu_find_opts(""fsdev""),

                                         qemu_opt_get(opts, ""mount_tag""), 1);

                if (!fsdev) {

                    fprintf(stderr, ""duplicate fsdev id: %s\n"",

                            qemu_opt_get(opts, ""mount_tag""));

                    exit(1);

                }



                writeout = qemu_opt_get(opts, ""writeout"");

                if (writeout) {

#ifdef CONFIG_SYNC_FILE_RANGE

                    qemu_opt_set(fsdev, ""writeout"", writeout);

#else

                    fprintf(stderr, ""writeout=immediate not supported on ""

                            ""this platform\n"");

                    exit(1);

#endif

                }

                qemu_opt_set(fsdev, ""fsdriver"", qemu_opt_get(opts, ""fsdriver""));

                qemu_opt_set(fsdev, ""path"", qemu_opt_get(opts, ""path""));

                qemu_opt_set(fsdev, ""security_model"",

                             qemu_opt_get(opts, ""security_model""));



                qemu_opt_set_bool(fsdev, ""readonly"",

                                qemu_opt_get_bool(opts, ""readonly"", 0));

                device = qemu_opts_create(qemu_find_opts(""device""), NULL, 0);

                qemu_opt_set(device, ""driver"", ""virtio-9p-pci"");

                qemu_opt_set(device, ""fsdev"",

                             qemu_opt_get(opts, ""mount_tag""));

                qemu_opt_set(device, ""mount_tag"",

                             qemu_opt_get(opts, ""mount_tag""));

                break;

            }

            case QEMU_OPTION_virtfs_synth: {

                QemuOpts *fsdev;

                QemuOpts *device;



                fsdev = qemu_opts_create(qemu_find_opts(""fsdev""), ""v_synth"", 1);

                if (!fsdev) {

                    fprintf(stderr, ""duplicate option: %s\n"", ""virtfs_synth"");

                    exit(1);

                }

                qemu_opt_set(fsdev, ""fsdriver"", ""synth"");

                qemu_opt_set(fsdev, ""path"", ""/""); /* ignored */



                device = qemu_opts_create(qemu_find_opts(""device""), NULL, 0);

                qemu_opt_set(device, ""driver"", ""virtio-9p-pci"");

                qemu_opt_set(device, ""fsdev"", ""v_synth"");

                qemu_opt_set(device, ""mount_tag"", ""v_synth"");

                break;

            }

            case QEMU_OPTION_serial:

                add_device_config(DEV_SERIAL, optarg);

                default_serial = 0;

                if (strncmp(optarg, ""mon:"", 4) == 0) {

                    default_monitor = 0;

                }

                break;

            case QEMU_OPTION_watchdog:

                if (watchdog) {

                    fprintf(stderr,

                            ""qemu: only one watchdog option may be given\n"");

                    return 1;

                }

                watchdog = optarg;

                break;

            case QEMU_OPTION_watchdog_action:

                if (select_watchdog_action(optarg) == -1) {

                    fprintf(stderr, ""Unknown -watchdog-action parameter\n"");

                    exit(1);

                }

                break;

            case QEMU_OPTION_virtiocon:

                add_device_config(DEV_VIRTCON, optarg);

                default_virtcon = 0;

                if (strncmp(optarg, ""mon:"", 4) == 0) {

                    default_monitor = 0;

                }

                break;

            case QEMU_OPTION_parallel:

                add_device_config(DEV_PARALLEL, optarg);

                default_parallel = 0;

                if (strncmp(optarg, ""mon:"", 4) == 0) {

                    default_monitor = 0;

                }

                break;

            case QEMU_OPTION_debugcon:

                add_device_config(DEV_DEBUGCON, optarg);

                break;

	    case QEMU_OPTION_loadvm:

		loadvm = optarg;

		break;

            case QEMU_OPTION_full_screen:

                full_screen = 1;

                break;

#ifdef CONFIG_SDL

            case QEMU_OPTION_no_frame:

                no_frame = 1;

                break;

            case QEMU_OPTION_alt_grab:

                alt_grab = 1;

                break;

            case QEMU_OPTION_ctrl_grab:

                ctrl_grab = 1;

                break;

            case QEMU_OPTION_no_quit:

                no_quit = 1;

                break;

            case QEMU_OPTION_sdl:

                display_type = DT_SDL;

                break;

#else

            case QEMU_OPTION_no_frame:

            case QEMU_OPTION_alt_grab:

            case QEMU_OPTION_ctrl_grab:

            case QEMU_OPTION_no_quit:

            case QEMU_OPTION_sdl:

                fprintf(stderr, ""SDL support is disabled\n"");

                exit(1);

#endif

            case QEMU_OPTION_pidfile:

                pid_file = optarg;

                break;

            case QEMU_OPTION_win2k_hack:

                win2k_install_hack = 1;

                break;

            case QEMU_OPTION_rtc_td_hack:

                rtc_td_hack = 1;

                break;

            case QEMU_OPTION_acpitable:

                do_acpitable_option(optarg);

                break;

            case QEMU_OPTION_smbios:

                do_smbios_option(optarg);

                break;

            case QEMU_OPTION_enable_kvm:

                olist = qemu_find_opts(""machine"");

                qemu_opts_reset(olist);

                qemu_opts_parse(olist, ""accel=kvm"", 0);

                break;

            case QEMU_OPTION_machine:

                olist = qemu_find_opts(""machine"");

                qemu_opts_reset(olist);

                opts = qemu_opts_parse(olist, optarg, 1);

                if (!opts) {

                    fprintf(stderr, ""parse error: %s\n"", optarg);

                    exit(1);

                }

                optarg = qemu_opt_get(opts, ""type"");

                if (optarg) {

                    machine = machine_parse(optarg);

                }

                break;

            case QEMU_OPTION_usb:

                usb_enabled = 1;

                break;

            case QEMU_OPTION_usbdevice:

                usb_enabled = 1;

                add_device_config(DEV_USB, optarg);

                break;

            case QEMU_OPTION_device:

                if (!qemu_opts_parse(qemu_find_opts(""device""), optarg, 1)) {

                    exit(1);

                }

                break;

            case QEMU_OPTION_smp:

                smp_parse(optarg);

                if (smp_cpus < 1) {

                    fprintf(stderr, ""Invalid number of CPUs\n"");

                    exit(1);

                }

                if (max_cpus < smp_cpus) {

                    fprintf(stderr, ""maxcpus must be equal to or greater than ""

                            ""smp\n"");

                    exit(1);

                }

                if (max_cpus > 255) {

                    fprintf(stderr, ""Unsupported number of maxcpus\n"");

                    exit(1);

                }

                break;

	    case QEMU_OPTION_vnc:

#ifdef CONFIG_VNC

                display_remote++;

                vnc_display = optarg;

#else

                fprintf(stderr, ""VNC support is disabled\n"");

                exit(1);

#endif

                break;

            case QEMU_OPTION_no_acpi:

                acpi_enabled = 0;

                break;

            case QEMU_OPTION_no_hpet:

                no_hpet = 1;

                break;

            case QEMU_OPTION_balloon:

                if (balloon_parse(optarg) < 0) {

                    fprintf(stderr, ""Unknown -balloon argument %s\n"", optarg);

                    exit(1);

                }

                break;

            case QEMU_OPTION_no_reboot:

                no_reboot = 1;

                break;

            case QEMU_OPTION_no_shutdown:

                no_shutdown = 1;

                break;

            case QEMU_OPTION_show_cursor:

                cursor_hide = 0;

                break;

            case QEMU_OPTION_uuid:

                if(qemu_uuid_parse(optarg, qemu_uuid) < 0) {

                    fprintf(stderr, ""Fail to parse UUID string.""

                            "" Wrong format.\n"");

                    exit(1);

                }

                break;

	    case QEMU_OPTION_option_rom:

		if (nb_option_roms >= MAX_OPTION_ROMS) {

		    fprintf(stderr, ""Too many option ROMs\n"");

		    exit(1);

		}

                opts = qemu_opts_parse(qemu_find_opts(""option-rom""), optarg, 1);

                option_rom[nb_option_roms].name = qemu_opt_get(opts, ""romfile"");

                option_rom[nb_option_roms].bootindex =

                    qemu_opt_get_number(opts, ""bootindex"", -1);

                if (!option_rom[nb_option_roms].name) {

                    fprintf(stderr, ""Option ROM file is not specified\n"");

                    exit(1);

                }

		nb_option_roms++;

		break;

            case QEMU_OPTION_semihosting:

                semihosting_enabled = 1;

                break;

            case QEMU_OPTION_name:

                qemu_name = g_strdup(optarg);

		 {

		     char *p = strchr(qemu_name, ',');

		     if (p != NULL) {

		        *p++ = 0;

			if (strncmp(p, ""process="", 8)) {

			    fprintf(stderr, ""Unknown subargument %s to -name\n"", p);

			    exit(1);

			}

			p += 8;

			os_set_proc_name(p);

		     }	

		 }	

                break;

            case QEMU_OPTION_prom_env:

                if (nb_prom_envs >= MAX_PROM_ENVS) {

                    fprintf(stderr, ""Too many prom variables\n"");

                    exit(1);

                }

                prom_envs[nb_prom_envs] = optarg;

                nb_prom_envs++;

                break;

            case QEMU_OPTION_old_param:

                old_param = 1;

                break;

            case QEMU_OPTION_clock:

                configure_alarms(optarg);

                break;

            case QEMU_OPTION_startdate:

                configure_rtc_date_offset(optarg, 1);

                break;

            case QEMU_OPTION_rtc:

                opts = qemu_opts_parse(qemu_find_opts(""rtc""), optarg, 0);

                if (!opts) {

                    exit(1);

                }

                configure_rtc(opts);

                break;

            case QEMU_OPTION_tb_size:

                tcg_tb_size = strtol(optarg, NULL, 0);

                if (tcg_tb_size < 0) {

                    tcg_tb_size = 0;

                }

                break;

            case QEMU_OPTION_icount:

                icount_option = optarg;

                break;

            case QEMU_OPTION_incoming:

                incoming = optarg;

                break;

            case QEMU_OPTION_nodefaults:

                default_serial = 0;

                default_parallel = 0;

                default_virtcon = 0;

                default_monitor = 0;

                default_vga = 0;

                default_net = 0;

                default_floppy = 0;

                default_cdrom = 0;

                default_sdcard = 0;

                break;

            case QEMU_OPTION_xen_domid:

                if (!(xen_available())) {

                    printf(""Option %s not supported for this target\n"", popt->name);

                    exit(1);

                }

                xen_domid = atoi(optarg);

                break;

            case QEMU_OPTION_xen_create:

                if (!(xen_available())) {

                    printf(""Option %s not supported for this target\n"", popt->name);

                    exit(1);

                }

                xen_mode = XEN_CREATE;

                break;

            case QEMU_OPTION_xen_attach:

                if (!(xen_available())) {

                    printf(""Option %s not supported for this target\n"", popt->name);

                    exit(1);

                }

                xen_mode = XEN_ATTACH;

                break;

            case QEMU_OPTION_trace:

            {

                opts = qemu_opts_parse(qemu_find_opts(""trace""), optarg, 0);

                if (!opts) {

                    exit(1);

                }

                trace_events = qemu_opt_get(opts, ""events"");

                trace_file = qemu_opt_get(opts, ""file"");

                break;

            }

            case QEMU_OPTION_readconfig:

                {

                    int ret = qemu_read_config_file(optarg);

                    if (ret < 0) {

                        fprintf(stderr, ""read config %s: %s\n"", optarg,

                            strerror(-ret));

                        exit(1);

                    }

                    break;

                }

            case QEMU_OPTION_spice:

                olist = qemu_find_opts(""spice"");

                if (!olist) {

                    fprintf(stderr, ""spice is not supported by this qemu build.\n"");

                    exit(1);

                }

                opts = qemu_opts_parse(olist, optarg, 0);

                if (!opts) {

                    fprintf(stderr, ""parse error: %s\n"", optarg);

                    exit(1);

                }

                break;

            case QEMU_OPTION_writeconfig:

                {

                    FILE *fp;

                    if (strcmp(optarg, ""-"") == 0) {

                        fp = stdout;

                    } else {

                        fp = fopen(optarg, ""w"");

                        if (fp == NULL) {

                            fprintf(stderr, ""open %s: %s\n"", optarg, strerror(errno));

                            exit(1);

                        }

                    }

                    qemu_config_write(fp);

                    fclose(fp);

                    break;

                }

            default:

                os_parse_cmd_args(popt->index, optarg);

            }

        }

    }

    loc_set_none();



    /* Open the logfile at this point, if necessary. We can't open the logfile

     * when encountering either of the logging options (-d or -D) because the

     * other one may be encountered later on the command line, changing the

     * location or level of logging.

     */

    if (log_mask) {

        if (log_file) {

            set_cpu_log_filename(log_file);

        }

        set_cpu_log(log_mask);

    }



    if (!trace_backend_init(trace_events, trace_file)) {

        exit(1);

    }



    /* If no data_dir is specified then try to find it relative to the

       executable path.  */

    if (!data_dir) {

        data_dir = os_find_datadir(argv[0]);

    }

    /* If all else fails use the install path specified when building. */

    if (!data_dir) {

        data_dir = CONFIG_QEMU_DATADIR;

    }



    if (machine == NULL) {

        fprintf(stderr, ""No machine found.\n"");

        exit(1);

    }



    /*

     * Default to max_cpus = smp_cpus, in case the user doesn't

     * specify a max_cpus value.

     */

    if (!max_cpus)

        max_cpus = smp_cpus;



    machine->max_cpus = machine->max_cpus ?: 1; /* Default to UP */

    if (smp_cpus > machine->max_cpus) {

        fprintf(stderr, ""Number of SMP cpus requested (%d), exceeds max cpus ""

                ""supported by machine `%s' (%d)\n"", smp_cpus,  machine->name,

                machine->max_cpus);

        exit(1);

    }



    /*

     * Get the default machine options from the machine if it is not already

     * specified either by the configuration file or by the command line.

     */

    if (machine->default_machine_opts) {

        QemuOptsList *list = qemu_find_opts(""machine"");

        const char *p = NULL;



        if (!QTAILQ_EMPTY(&list->head)) {

            p = qemu_opt_get(QTAILQ_FIRST(&list->head), ""accel"");

        }

        if (p == NULL) {

            qemu_opts_reset(list);

            opts = qemu_opts_parse(list, machine->default_machine_opts, 0);

            if (!opts) {

                fprintf(stderr, ""parse error for machine %s: %s\n"",

                        machine->name, machine->default_machine_opts);

                exit(1);

            }

        }

    }



    qemu_opts_foreach(qemu_find_opts(""device""), default_driver_check, NULL, 0);

    qemu_opts_foreach(qemu_find_opts(""global""), default_driver_check, NULL, 0);



    if (machine->no_serial) {

        default_serial = 0;

    }

    if (machine->no_parallel) {

        default_parallel = 0;

    }

    if (!machine->use_virtcon) {

        default_virtcon = 0;

    }

    if (machine->no_vga) {

        default_vga = 0;

    }

    if (machine->no_floppy) {

        default_floppy = 0;

    }

    if (machine->no_cdrom) {

        default_cdrom = 0;

    }

    if (machine->no_sdcard) {

        default_sdcard = 0;

    }



    if (display_type == DT_NOGRAPHIC) {

        if (default_parallel)

            add_device_config(DEV_PARALLEL, ""null"");

        if (default_serial && default_monitor) {

            add_device_config(DEV_SERIAL, ""mon:stdio"");

        } else if (default_virtcon && default_monitor) {

            add_device_config(DEV_VIRTCON, ""mon:stdio"");

        } else {

            if (default_serial)

                add_device_config(DEV_SERIAL, ""stdio"");

            if (default_virtcon)

                add_device_config(DEV_VIRTCON, ""stdio"");

            if (default_monitor)

                monitor_parse(""stdio"", ""readline"");

        }

    } else {

        if (default_serial)

            add_device_config(DEV_SERIAL, ""vc:80Cx24C"");

        if (default_parallel)

            add_device_config(DEV_PARALLEL, ""vc:80Cx24C"");

        if (default_monitor)

            monitor_parse(""vc:80Cx24C"", ""readline"");

        if (default_virtcon)

            add_device_config(DEV_VIRTCON, ""vc:80Cx24C"");

    }

    if (default_vga)

        vga_interface_type = VGA_CIRRUS;



    socket_init();



    if (qemu_opts_foreach(qemu_find_opts(""chardev""), chardev_init_func, NULL, 1) != 0)

        exit(1);

#ifdef CONFIG_VIRTFS

    if (qemu_opts_foreach(qemu_find_opts(""fsdev""), fsdev_init_func, NULL, 1) != 0) {

        exit(1);

    }

#endif



    os_daemonize();



    if (pid_file && qemu_create_pidfile(pid_file) != 0) {

        os_pidfile_error();

        exit(1);

    }



    /* init the memory */

    if (ram_size == 0) {

        ram_size = DEFAULT_RAM_SIZE * 1024 * 1024;

    }



    configure_accelerator();



    qemu_init_cpu_loop();

    if (qemu_init_main_loop()) {

        fprintf(stderr, ""qemu_init_main_loop failed\n"");

        exit(1);

    }

    linux_boot = (kernel_filename != NULL);



    if (!linux_boot && *kernel_cmdline != '\0') {

        fprintf(stderr, ""-append only allowed with -kernel option\n"");

        exit(1);

    }



    if (!linux_boot && initrd_filename != NULL) {

        fprintf(stderr, ""-initrd only allowed with -kernel option\n"");

        exit(1);

    }



    os_set_line_buffering();



    if (init_timer_alarm() < 0) {

        fprintf(stderr, ""could not initialize alarm timer\n"");

        exit(1);

    }



    if (icount_option && (kvm_enabled() || xen_enabled())) {

        fprintf(stderr, ""-icount is not allowed with kvm or xen\n"");

        exit(1);

    }

    configure_icount(icount_option);



    if (net_init_clients() < 0) {

        exit(1);

    }



    /* init the bluetooth world */

    if (foreach_device_config(DEV_BT, bt_parse))

        exit(1);



    if (!xen_enabled()) {

        /* On 32-bit hosts, QEMU is limited by virtual address space */

        if (ram_size > (2047 << 20) && HOST_LONG_BITS == 32) {

            fprintf(stderr, ""qemu: at most 2047 MB RAM can be simulated\n"");

            exit(1);

        }

    }



    cpu_exec_init_all();



    bdrv_init_with_whitelist();



    blk_mig_init();



    /* open the virtual block devices */

    if (snapshot)

        qemu_opts_foreach(qemu_find_opts(""drive""), drive_enable_snapshot, NULL, 0);

    if (qemu_opts_foreach(qemu_find_opts(""drive""), drive_init_func, &machine->use_scsi, 1) != 0)

        exit(1);



    default_drive(default_cdrom, snapshot, machine->use_scsi,

                  IF_DEFAULT, 2, CDROM_OPTS);

    default_drive(default_floppy, snapshot, machine->use_scsi,

                  IF_FLOPPY, 0, FD_OPTS);

    default_drive(default_sdcard, snapshot, machine->use_scsi,

                  IF_SD, 0, SD_OPTS);



    register_savevm_live(NULL, ""ram"", 0, 4, NULL, ram_save_live, NULL,

                         ram_load, NULL);



    if (nb_numa_nodes > 0) {

        int i;



        if (nb_numa_nodes > MAX_NODES) {

            nb_numa_nodes = MAX_NODES;

        }



        /* If no memory size if given for any node, assume the default case

         * and distribute the available memory equally across all nodes

         */

        for (i = 0; i < nb_numa_nodes; i++) {

            if (node_mem[i] != 0)

                break;

        }

        if (i == nb_numa_nodes) {

            uint64_t usedmem = 0;



            /* On Linux, the each node's border has to be 8MB aligned,

             * the final node gets the rest.

             */

            for (i = 0; i < nb_numa_nodes - 1; i++) {

                node_mem[i] = (ram_size / nb_numa_nodes) & ~((1 << 23UL) - 1);

                usedmem += node_mem[i];

            }

            node_mem[i] = ram_size - usedmem;

        }



        for (i = 0; i < nb_numa_nodes; i++) {

            if (node_cpumask[i] != 0)

                break;

        }

        /* assigning the VCPUs round-robin is easier to implement, guest OSes

         * must cope with this anyway, because there are BIOSes out there in

         * real machines which also use this scheme.

         */

        if (i == nb_numa_nodes) {

            for (i = 0; i < max_cpus; i++) {

                node_cpumask[i % nb_numa_nodes] |= 1 << i;

            }

        }

    }



    if (qemu_opts_foreach(qemu_find_opts(""mon""), mon_init_func, NULL, 1) != 0) {

        exit(1);

    }



    if (foreach_device_config(DEV_SERIAL, serial_parse) < 0)

        exit(1);

    if (foreach_device_config(DEV_PARALLEL, parallel_parse) < 0)

        exit(1);

    if (foreach_device_config(DEV_VIRTCON, virtcon_parse) < 0)

        exit(1);

    if (foreach_device_config(DEV_DEBUGCON, debugcon_parse) < 0)

        exit(1);



    module_call_init(MODULE_INIT_DEVICE);



    if (qemu_opts_foreach(qemu_find_opts(""device""), device_help_func, NULL, 0) != 0)

        exit(0);



    if (watchdog) {

        i = select_watchdog(watchdog);

        if (i > 0)

            exit (i == 1 ? 1 : 0);

    }



    if (machine->compat_props) {

        qdev_prop_register_global_list(machine->compat_props);

    }

    qemu_add_globals();



    qdev_machine_init();



    machine->init(ram_size, boot_devices,

                  kernel_filename, kernel_cmdline, initrd_filename, cpu_model);



    cpu_synchronize_all_post_init();



    set_numa_modes();



    current_machine = machine;



    /* init USB devices */

    if (usb_enabled) {

        if (foreach_device_config(DEV_USB, usb_parse) < 0)

            exit(1);

    }



    /* init generic devices */

    if (qemu_opts_foreach(qemu_find_opts(""device""), device_init_func, NULL, 1) != 0)

        exit(1);



    net_check_clients();



    /* just use the first displaystate for the moment */

    ds = get_displaystate();



    if (using_spice)

        display_remote++;

    if (display_type == DT_DEFAULT && !display_remote) {

#if defined(CONFIG_SDL) || defined(CONFIG_COCOA)

        display_type = DT_SDL;

#elif defined(CONFIG_VNC)

        vnc_display = ""localhost:0,to=99"";

        show_vnc_port = 1;

#else

        display_type = DT_NONE;

#endif

    }





    /* init local displays */

    switch (display_type) {

    case DT_NOGRAPHIC:

        break;

#if defined(CONFIG_CURSES)

    case DT_CURSES:

        curses_display_init(ds, full_screen);

        break;

#endif

#if defined(CONFIG_SDL)

    case DT_SDL:

        sdl_display_init(ds, full_screen, no_frame);

        break;

#elif defined(CONFIG_COCOA)

    case DT_SDL:

        cocoa_display_init(ds, full_screen);

        break;

#endif

    default:

        break;

    }



    /* must be after terminal init, SDL library changes signal handlers */

    os_setup_signal_handling();



#ifdef CONFIG_VNC

    /* init remote displays */

    if (vnc_display) {

        vnc_display_init(ds);

        if (vnc_display_open(ds, vnc_display) < 0)

            exit(1);



        if (show_vnc_port) {

            printf(""VNC server running on `%s'\n"", vnc_display_local_addr(ds));

        }

    }

#endif

#ifdef CONFIG_SPICE

    if (using_spice && !qxl_enabled) {

        qemu_spice_display_init(ds);

    }

#endif



    /* display setup */

    dpy_resize(ds);

    dcl = ds->listeners;

    while (dcl != NULL) {

        if (dcl->dpy_refresh != NULL) {

            ds->gui_timer = qemu_new_timer_ms(rt_clock, gui_update, ds);

            qemu_mod_timer(ds->gui_timer, qemu_get_clock_ms(rt_clock));

            break;

        }

        dcl = dcl->next;

    }

    text_consoles_set_display(ds);



    if (gdbstub_dev && gdbserver_start(gdbstub_dev) < 0) {

        fprintf(stderr, ""qemu: could not open gdbserver on device '%s'\n"",

                gdbstub_dev);

        exit(1);

    }



    qdev_machine_creation_done();



    if (rom_load_all() != 0) {

        fprintf(stderr, ""rom loading failed\n"");

        exit(1);

    }



    /* TODO: once all bus devices are qdevified, this should be done

     * when bus is created by qdev.c */

    qemu_register_reset(qbus_reset_all_fn, sysbus_get_default());

    qemu_run_machine_init_done_notifiers();



    qemu_system_reset(VMRESET_SILENT);

    if (loadvm) {

        if (load_vmstate(loadvm) < 0) {

            autostart = 0;

        }

    }



    if (incoming) {

        runstate_set(RUN_STATE_INMIGRATE);

        int ret = qemu_start_incoming_migration(incoming);

        if (ret < 0) {

            fprintf(stderr, ""Migration failed. Exit code %s(%d), exiting.\n"",

                    incoming, ret);

            exit(ret);

        }

    } else if (autostart) {

        vm_start();

    }



    os_setup_post();



    resume_all_vcpus();

    main_loop();

    bdrv_close_all();

    pause_all_vcpus();

    net_cleanup();

    res_free();



    return 0;

}
","int main(int argc, char **argv, char **envp) {
const char *gdbstub_dev = NULL; int i; int snapshot, linux_boot; const char *icount_option = NULL; const char *initrd_filename; const char *kernel_filename, *kernel_cmdline; char boot_devices[33] = ""cad""; /* default to HD->floppy->CD-ROM */ DisplayState *ds; DisplayChangeListener *dcl; int cyls, heads, secs, translation; QemuOpts *hda_opts = NULL, *opts; QemuOptsList *olist; int optind; const char *optarg; const char *loadvm = NULL; QEMUMachine *machine; const char *cpu_model; const char *pid_file = NULL; const char *incoming = NULL; #ifdef CONFIG_VNC int show_vnc_port = 0; #endif int defconfig = 1; const char *log_mask = NULL; const char *log_file = NULL; GMemVTable mem_trace = {
.malloc = malloc_and_trace, .realloc = realloc_and_trace, .free = free_and_trace, }; const char *trace_events = NULL; const char *trace_file = NULL; atexit(qemu_run_exit_notifiers); error_set_progname(argv[0]); g_mem_set_vtable(&mem_trace); if (!g_thread_supported()) {
#if !GLIB_CHECK_VERSION(2, 31, 0) g_thread_init(NULL); #else fprintf(stderr, ""glib threading failed to initialize.\n""); exit(1); #endif } runstate_init(); init_clocks(); rtc_clock = host_clock; qemu_cache_utils_init(envp); QLIST_INIT (&vm_change_state_head); os_setup_early_signal_handling(); module_call_init(MODULE_INIT_MACHINE); machine = find_default_machine(); cpu_model = NULL; initrd_filename = NULL; ram_size = 0; snapshot = 0; kernel_filename = NULL; kernel_cmdline = """"; cyls = heads = secs = 0; translation = BIOS_ATA_TRANSLATION_AUTO; for (i = 0; i < MAX_NODES; i++) {
node_mem[i] = 0; node_cpumask[i] = 0; } nb_numa_nodes = 0; nb_nics = 0; autostart= 1; /* first pass of option parsing */ optind = 1; while (optind < argc) {
if (argv[optind][0] != '-') {
/* disk image */ optind++; continue; } else {
const QEMUOption *popt; popt = lookup_opt(argc, argv, &optarg, &optind); switch (popt->index) {
case QEMU_OPTION_nodefconfig: defconfig=0; break; } } } if (defconfig) {
int ret; ret = qemu_read_config_file(CONFIG_QEMU_CONFDIR ""/qemu.conf""); if (ret < 0 && ret != -ENOENT) {
exit(1); } ret = qemu_read_config_file(arch_config_name); if (ret < 0 && ret != -ENOENT) {
exit(1); } } cpudef_init(); /* second pass of option parsing */ optind = 1; for(;;) {
if (optind >= argc) break; if (argv[optind][0] != '-') {
hda_opts = drive_add(IF_DEFAULT, 0, argv[optind++], HD_OPTS); } else {
const QEMUOption *popt; popt = lookup_opt(argc, argv, &optarg, &optind); if (!(popt->arch_mask & arch_type)) {
printf(""Option %s not supported for this target\n"", popt->name); exit(1); } switch(popt->index) {
case QEMU_OPTION_M: machine = machine_parse(optarg); break; case QEMU_OPTION_cpu: /* hw initialization will check this */ if (*optarg == '?') {
list_cpus(stdout, &fprintf, optarg); exit(0); } else {
cpu_model = optarg; } break; case QEMU_OPTION_initrd: initrd_filename = optarg; break; case QEMU_OPTION_hda: {
char buf[256]; if (cyls == 0) snprintf(buf, sizeof(buf), ""%s"", HD_OPTS); else snprintf(buf, sizeof(buf), ""%s,cyls=%d,heads=%d,secs=%d%s"", HD_OPTS , cyls, heads, secs, translation == BIOS_ATA_TRANSLATION_LBA ? "",trans=lba"" : translation == BIOS_ATA_TRANSLATION_NONE ? "",trans=none"" : """"); drive_add(IF_DEFAULT, 0, optarg, buf); break; } case QEMU_OPTION_hdb: case QEMU_OPTION_hdc: case QEMU_OPTION_hdd: drive_add(IF_DEFAULT, popt->index - QEMU_OPTION_hda, optarg, HD_OPTS); break; case QEMU_OPTION_drive: if (drive_def(optarg) == NULL) {
exit(1); } break; case QEMU_OPTION_set: if (qemu_set_option(optarg) != 0) exit(1); break; case QEMU_OPTION_global: if (qemu_global_option(optarg) != 0) exit(1); break; case QEMU_OPTION_mtdblock: drive_add(IF_MTD, -1, optarg, MTD_OPTS); break; case QEMU_OPTION_sd: drive_add(IF_SD, 0, optarg, SD_OPTS); break; case QEMU_OPTION_pflash: drive_add(IF_PFLASH, -1, optarg, PFLASH_OPTS); break; case QEMU_OPTION_snapshot: snapshot = 1; break; case QEMU_OPTION_hdachs: {
const char *p; p = optarg; cyls = strtol(p, (char **)&p, 0); if (cyls < 1 || cyls > 16383) goto chs_fail; if (*p != ',') goto chs_fail; p++; heads = strtol(p, (char **)&p, 0); if (heads < 1 || heads > 16) goto chs_fail; if (*p != ',') goto chs_fail; p++; secs = strtol(p, (char **)&p, 0); if (secs < 1 || secs > 63) goto chs_fail; if (*p == ',') {
p++; if (!strcmp(p, ""none"")) translation = BIOS_ATA_TRANSLATION_NONE; else if (!strcmp(p, ""lba"")) translation = BIOS_ATA_TRANSLATION_LBA; else if (!strcmp(p, ""auto"")) translation = BIOS_ATA_TRANSLATION_AUTO; else goto chs_fail; } else if (*p != '\0') {
chs_fail: fprintf(stderr, ""qemu: invalid physical CHS format\n""); exit(1); } if (hda_opts != NULL) {
char num[16]; snprintf(num, sizeof(num), ""%d"", cyls); qemu_opt_set(hda_opts, ""cyls"", num); snprintf(num, sizeof(num), ""%d"", heads); qemu_opt_set(hda_opts, ""heads"", num); snprintf(num, sizeof(num), ""%d"", secs); qemu_opt_set(hda_opts, ""secs"", num); if (translation == BIOS_ATA_TRANSLATION_LBA) qemu_opt_set(hda_opts, ""trans"", ""lba""); if (translation == BIOS_ATA_TRANSLATION_NONE) qemu_opt_set(hda_opts, ""trans"", ""none""); } } break; case QEMU_OPTION_numa: if (nb_numa_nodes >= MAX_NODES) {
fprintf(stderr, ""qemu: too many NUMA nodes\n""); exit(1); } numa_add(optarg); break; case QEMU_OPTION_display: display_type = select_display(optarg); break; case QEMU_OPTION_nographic: display_type = DT_NOGRAPHIC; break; case QEMU_OPTION_curses: #ifdef CONFIG_CURSES display_type = DT_CURSES; #else fprintf(stderr, ""Curses support is disabled\n""); exit(1); #endif break; case QEMU_OPTION_portrait: graphic_rotate = 90; break; case QEMU_OPTION_rotate: graphic_rotate = strtol(optarg, (char **) &optarg, 10); if (graphic_rotate != 0 && graphic_rotate != 90 && graphic_rotate != 180 && graphic_rotate != 270) {
fprintf(stderr, ""qemu: only 90, 180, 270 deg rotation is available\n""); exit(1); } break; case QEMU_OPTION_kernel: kernel_filename = optarg; break; case QEMU_OPTION_append: kernel_cmdline = optarg; break; case QEMU_OPTION_cdrom: drive_add(IF_DEFAULT, 2, optarg, CDROM_OPTS); break; case QEMU_OPTION_boot: {
static const char * const params[] = {
""order"", ""once"", ""menu"", ""splash"", ""splash-time"", NULL }; char buf[sizeof(boot_devices)]; char *standard_boot_devices; int legacy = 0; if (!strchr(optarg, '=')) {
legacy = 1; pstrcpy(buf, sizeof(buf), optarg); } else if (check_params(buf, sizeof(buf), params, optarg) < 0) {
fprintf(stderr, ""qemu: unknown boot parameter '%s' in '%s'\n"", buf, optarg); exit(1); } if (legacy || get_param_value(buf, sizeof(buf), ""order"", optarg)) {
validate_bootdevices(buf); pstrcpy(boot_devices, sizeof(boot_devices), buf); } if (!legacy) {
if (get_param_value(buf, sizeof(buf), ""once"", optarg)) {
validate_bootdevices(buf); standard_boot_devices = g_strdup(boot_devices); pstrcpy(boot_devices, sizeof(boot_devices), buf); qemu_register_reset(restore_boot_devices, standard_boot_devices); } if (get_param_value(buf, sizeof(buf), ""menu"", optarg)) {
if (!strcmp(buf, ""on"")) {
boot_menu = 1; } else if (!strcmp(buf, ""off"")) {
boot_menu = 0; } else {
fprintf(stderr, ""qemu: invalid option value '%s'\n"", buf); exit(1); } } qemu_opts_parse(qemu_find_opts(""boot-opts""), optarg, 0); } } break; case QEMU_OPTION_fda: case QEMU_OPTION_fdb: drive_add(IF_FLOPPY, popt->index - QEMU_OPTION_fda, optarg, FD_OPTS); break; case QEMU_OPTION_no_fd_bootchk: fd_bootchk = 0; break; case QEMU_OPTION_netdev: if (net_client_parse(qemu_find_opts(""netdev""), optarg) == -1) {
exit(1); } break; case QEMU_OPTION_net: if (net_client_parse(qemu_find_opts(""net""), optarg) == -1) {
exit(1); } break; #ifdef CONFIG_SLIRP case QEMU_OPTION_tftp: legacy_tftp_prefix = optarg; break; case QEMU_OPTION_bootp: legacy_bootp_filename = optarg; break; case QEMU_OPTION_redir: if (net_slirp_redir(optarg) < 0) exit(1); break; #endif case QEMU_OPTION_bt: add_device_config(DEV_BT, optarg); break; case QEMU_OPTION_audio_help: if (!(audio_available())) {
printf(""Option %s not supported for this target\n"", popt->name); exit(1); } AUD_help (); exit (0); break; case QEMU_OPTION_soundhw: if (!(audio_available())) {
printf(""Option %s not supported for this target\n"", popt->name); exit(1); } select_soundhw (optarg); break; case QEMU_OPTION_h: help(0); break; case QEMU_OPTION_version: version(); exit(0); break; case QEMU_OPTION_m: {
int64_t value; char *end; value = strtosz(optarg, &end); if (value < 0 || *end) {
fprintf(stderr, ""qemu: invalid ram size: %s\n"", optarg); exit(1); } if (value != (uint64_t)(ram_addr_t)value) {
fprintf(stderr, ""qemu: ram size too large\n""); exit(1); } ram_size = value; break; } case QEMU_OPTION_mempath: mem_path = optarg; break; #ifdef MAP_POPULATE case QEMU_OPTION_mem_prealloc: mem_prealloc = 1; break; #endif case QEMU_OPTION_d: log_mask = optarg; break; case QEMU_OPTION_D: log_file = optarg; break; case QEMU_OPTION_s: gdbstub_dev = ""tcp::"" DEFAULT_GDBSTUB_PORT; break; case QEMU_OPTION_gdb: gdbstub_dev = optarg; break; case QEMU_OPTION_L: data_dir = optarg; break; case QEMU_OPTION_bios: bios_name = optarg; break; case QEMU_OPTION_singlestep: singlestep = 1; break; case QEMU_OPTION_S: autostart = 0; break; case QEMU_OPTION_k: keyboard_layout = optarg; break; case QEMU_OPTION_localtime: rtc_utc = 0; break; case QEMU_OPTION_vga: select_vgahw (optarg); break; case QEMU_OPTION_g: {
const char *p; int w, h, depth; p = optarg; w = strtol(p, (char **)&p, 10); if (w <= 0) {
graphic_error: fprintf(stderr, ""qemu: invalid resolution or depth\n""); exit(1); } if (*p != 'x') goto graphic_error; p++; h = strtol(p, (char **)&p, 10); if (h <= 0) goto graphic_error; if (*p == 'x') {
p++; depth = strtol(p, (char **)&p, 10); if (depth != 8 && depth != 15 && depth != 16 && depth != 24 && depth != 32) goto graphic_error; } else if (*p == '\0') {
depth = graphic_depth; } else {
goto graphic_error; } graphic_width = w; graphic_height = h; graphic_depth = depth; } break; case QEMU_OPTION_echr: {
char *r; term_escape_char = strtol(optarg, &r, 0); if (r == optarg) printf(""Bad argument to echr\n""); break; } case QEMU_OPTION_monitor: monitor_parse(optarg, ""readline""); default_monitor = 0; break; case QEMU_OPTION_qmp: monitor_parse(optarg, ""control""); default_monitor = 0; break; case QEMU_OPTION_mon: opts = qemu_opts_parse(qemu_find_opts(""mon""), optarg, 1); if (!opts) {
exit(1); } default_monitor = 0; break; case QEMU_OPTION_chardev: opts = qemu_opts_parse(qemu_find_opts(""chardev""), optarg, 1); if (!opts) {
exit(1); } break; case QEMU_OPTION_fsdev: olist = qemu_find_opts(""fsdev""); if (!olist) {
fprintf(stderr, ""fsdev is not supported by this qemu build.\n""); exit(1); } opts = qemu_opts_parse(olist, optarg, 1); if (!opts) {
fprintf(stderr, ""parse error: %s\n"", optarg); exit(1); } break; case QEMU_OPTION_virtfs: {
QemuOpts *fsdev; QemuOpts *device; const char *writeout; olist = qemu_find_opts(""virtfs""); if (!olist) {
fprintf(stderr, ""virtfs is not supported by this qemu build.\n""); exit(1); } opts = qemu_opts_parse(olist, optarg, 1); if (!opts) {
fprintf(stderr, ""parse error: %s\n"", optarg); exit(1); } if (qemu_opt_get(opts, ""fsdriver"") == NULL || qemu_opt_get(opts, ""mount_tag"") == NULL || qemu_opt_get(opts, ""path"") == NULL) {
fprintf(stderr, ""Usage: -virtfs fsdriver,path=/share_path/,"" ""[security_model= {
mapped|passthrough|none}],"" ""mount_tag=tag.\n""); exit(1); } fsdev = qemu_opts_create(qemu_find_opts(""fsdev""), qemu_opt_get(opts, ""mount_tag""), 1); if (!fsdev) {
fprintf(stderr, ""duplicate fsdev id: %s\n"", qemu_opt_get(opts, ""mount_tag"")); exit(1); } writeout = qemu_opt_get(opts, ""writeout""); if (writeout) {
#ifdef CONFIG_SYNC_FILE_RANGE qemu_opt_set(fsdev, ""writeout"", writeout); #else fprintf(stderr, ""writeout=immediate not supported on "" ""this platform\n""); exit(1); #endif } qemu_opt_set(fsdev, ""fsdriver"", qemu_opt_get(opts, ""fsdriver"")); qemu_opt_set(fsdev, ""path"", qemu_opt_get(opts, ""path"")); qemu_opt_set(fsdev, ""security_model"", qemu_opt_get(opts, ""security_model"")); qemu_opt_set_bool(fsdev, ""readonly"", qemu_opt_get_bool(opts, ""readonly"", 0)); device = qemu_opts_create(qemu_find_opts(""device""), NULL, 0); qemu_opt_set(device, ""driver"", ""virtio-9p-pci""); qemu_opt_set(device, ""fsdev"", qemu_opt_get(opts, ""mount_tag"")); qemu_opt_set(device, ""mount_tag"", qemu_opt_get(opts, ""mount_tag"")); break; } case QEMU_OPTION_virtfs_synth: {
QemuOpts *fsdev; QemuOpts *device; fsdev = qemu_opts_create(qemu_find_opts(""fsdev""), ""v_synth"", 1); if (!fsdev) {
fprintf(stderr, ""duplicate option: %s\n"", ""virtfs_synth""); exit(1); } qemu_opt_set(fsdev, ""fsdriver"", ""synth""); qemu_opt_set(fsdev, ""path"", ""/""); /* ignored */ device = qemu_opts_create(qemu_find_opts(""device""), NULL, 0); qemu_opt_set(device, ""driver"", ""virtio-9p-pci""); qemu_opt_set(device, ""fsdev"", ""v_synth""); qemu_opt_set(device, ""mount_tag"", ""v_synth""); break; } case QEMU_OPTION_serial: add_device_config(DEV_SERIAL, optarg); default_serial = 0; if (strncmp(optarg, ""mon:"", 4) == 0) {
default_monitor = 0; } break; case QEMU_OPTION_watchdog: if (watchdog) {
fprintf(stderr, ""qemu: only one watchdog option may be given\n""); return 1; } watchdog = optarg; break; case QEMU_OPTION_watchdog_action: if (select_watchdog_action(optarg) == -1) {
fprintf(stderr, ""Unknown -watchdog-action parameter\n""); exit(1); } break; case QEMU_OPTION_virtiocon: add_device_config(DEV_VIRTCON, optarg); default_virtcon = 0; if (strncmp(optarg, ""mon:"", 4) == 0) {
default_monitor = 0; } break; case QEMU_OPTION_parallel: add_device_config(DEV_PARALLEL, optarg); default_parallel = 0; if (strncmp(optarg, ""mon:"", 4) == 0) {
default_monitor = 0; } break; case QEMU_OPTION_debugcon: add_device_config(DEV_DEBUGCON, optarg); break; case QEMU_OPTION_loadvm: loadvm = optarg; break; case QEMU_OPTION_full_screen: full_screen = 1; break; #ifdef CONFIG_SDL case QEMU_OPTION_no_frame: no_frame = 1; break; case QEMU_OPTION_alt_grab: alt_grab = 1; break; case QEMU_OPTION_ctrl_grab: ctrl_grab = 1; break; case QEMU_OPTION_no_quit: no_quit = 1; break; case QEMU_OPTION_sdl: display_type = DT_SDL; break; #else case QEMU_OPTION_no_frame: case QEMU_OPTION_alt_grab: case QEMU_OPTION_ctrl_grab: case QEMU_OPTION_no_quit: case QEMU_OPTION_sdl: fprintf(stderr, ""SDL support is disabled\n""); exit(1); #endif case QEMU_OPTION_pidfile: pid_file = optarg; break; case QEMU_OPTION_win2k_hack: win2k_install_hack = 1; break; case QEMU_OPTION_rtc_td_hack: rtc_td_hack = 1; break; case QEMU_OPTION_acpitable: do_acpitable_option(optarg); break; case QEMU_OPTION_smbios: do_smbios_option(optarg); break; case QEMU_OPTION_enable_kvm: olist = qemu_find_opts(""machine""); qemu_opts_reset(olist); qemu_opts_parse(olist, ""accel=kvm"", 0); break; case QEMU_OPTION_machine: olist = qemu_find_opts(""machine""); qemu_opts_reset(olist); opts = qemu_opts_parse(olist, optarg, 1); if (!opts) {
fprintf(stderr, ""parse error: %s\n"", optarg); exit(1); } optarg = qemu_opt_get(opts, ""type""); if (optarg) {
machine = machine_parse(optarg); } break; case QEMU_OPTION_usb: usb_enabled = 1; break; case QEMU_OPTION_usbdevice: usb_enabled = 1; add_device_config(DEV_USB, optarg); break; case QEMU_OPTION_device: if (!qemu_opts_parse(qemu_find_opts(""device""), optarg, 1)) {
exit(1); } break; case QEMU_OPTION_smp: smp_parse(optarg); if (smp_cpus < 1) {
fprintf(stderr, ""Invalid number of CPUs\n""); exit(1); } if (max_cpus < smp_cpus) {
fprintf(stderr, ""maxcpus must be equal to or greater than "" ""smp\n""); exit(1); } if (max_cpus > 255) {
fprintf(stderr, ""Unsupported number of maxcpus\n""); exit(1); } break; case QEMU_OPTION_vnc: #ifdef CONFIG_VNC display_remote++; vnc_display = optarg; #else fprintf(stderr, ""VNC support is disabled\n""); exit(1); #endif break; case QEMU_OPTION_no_acpi: acpi_enabled = 0; break; case QEMU_OPTION_no_hpet: no_hpet = 1; break; case QEMU_OPTION_balloon: if (balloon_parse(optarg) < 0) {
fprintf(stderr, ""Unknown -balloon argument %s\n"", optarg); exit(1); } break; case QEMU_OPTION_no_reboot: no_reboot = 1; break; case QEMU_OPTION_no_shutdown: no_shutdown = 1; break; case QEMU_OPTION_show_cursor: cursor_hide = 0; break; case QEMU_OPTION_uuid: if(qemu_uuid_parse(optarg, qemu_uuid) < 0) {
fprintf(stderr, ""Fail to parse UUID string."" "" Wrong format.\n""); exit(1); } break; case QEMU_OPTION_option_rom: if (nb_option_roms >= MAX_OPTION_ROMS) {
fprintf(stderr, ""Too many option ROMs\n""); exit(1); } opts = qemu_opts_parse(qemu_find_opts(""option-rom""), optarg, 1); option_rom[nb_option_roms].name = qemu_opt_get(opts, ""romfile""); option_rom[nb_option_roms].bootindex = qemu_opt_get_number(opts, ""bootindex"", -1); if (!option_rom[nb_option_roms].name) {
fprintf(stderr, ""Option ROM file is not specified\n""); exit(1); } nb_option_roms++; break; case QEMU_OPTION_semihosting: semihosting_enabled = 1; break; case QEMU_OPTION_name: qemu_name = g_strdup(optarg); {
char *p = strchr(qemu_name, ','); if (p != NULL) {
*p++ = 0; if (strncmp(p, ""process="", 8)) {
fprintf(stderr, ""Unknown subargument %s to -name\n"", p); exit(1); } p += 8; os_set_proc_name(p); } } break; case QEMU_OPTION_prom_env: if (nb_prom_envs >= MAX_PROM_ENVS) {
fprintf(stderr, ""Too many prom variables\n""); exit(1); } prom_envs[nb_prom_envs] = optarg; nb_prom_envs++; break; case QEMU_OPTION_old_param: old_param = 1; break; case QEMU_OPTION_clock: configure_alarms(optarg); break; case QEMU_OPTION_startdate: configure_rtc_date_offset(optarg, 1); break; case QEMU_OPTION_rtc: opts = qemu_opts_parse(qemu_find_opts(""rtc""), optarg, 0); if (!opts) {
exit(1); } configure_rtc(opts); break; case QEMU_OPTION_tb_size: tcg_tb_size = strtol(optarg, NULL, 0); if (tcg_tb_size < 0) {
tcg_tb_size = 0; } break; case QEMU_OPTION_icount: icount_option = optarg; break; case QEMU_OPTION_incoming: incoming = optarg; break; case QEMU_OPTION_nodefaults: default_serial = 0; default_parallel = 0; default_virtcon = 0; default_monitor = 0; default_vga = 0; default_net = 0; default_floppy = 0; default_cdrom = 0; default_sdcard = 0; break; case QEMU_OPTION_xen_domid: if (!(xen_available())) {
printf(""Option %s not supported for this target\n"", popt->name); exit(1); } xen_domid = atoi(optarg); break; case QEMU_OPTION_xen_create: if (!(xen_available())) {
printf(""Option %s not supported for this target\n"", popt->name); exit(1); } xen_mode = XEN_CREATE; break; case QEMU_OPTION_xen_attach: if (!(xen_available())) {
printf(""Option %s not supported for this target\n"", popt->name); exit(1); } xen_mode = XEN_ATTACH; break; case QEMU_OPTION_trace: {
opts = qemu_opts_parse(qemu_find_opts(""trace""), optarg, 0); if (!opts) {
exit(1); } trace_events = qemu_opt_get(opts, ""events""); trace_file = qemu_opt_get(opts, ""file""); break; } case QEMU_OPTION_readconfig: {
int ret = qemu_read_config_file(optarg); if (ret < 0) {
fprintf(stderr, ""read config %s: %s\n"", optarg, strerror(-ret)); exit(1); } break; } case QEMU_OPTION_spice: olist = qemu_find_opts(""spice""); if (!olist) {
fprintf(stderr, ""spice is not supported by this qemu build.\n""); exit(1); } opts = qemu_opts_parse(olist, optarg, 0); if (!opts) {
fprintf(stderr, ""parse error: %s\n"", optarg); exit(1); } break; case QEMU_OPTION_writeconfig: {
FILE *fp; if (strcmp(optarg, ""-"") == 0) {
fp = stdout; } else {
fp = fopen(optarg, ""w""); if (fp == NULL) {
fprintf(stderr, ""open %s: %s\n"", optarg, strerror(errno)); exit(1); } } qemu_config_write(fp); fclose(fp); break; } default: os_parse_cmd_args(popt->index, optarg); } } } loc_set_none(); /* Open the logfile at this point, if necessary. We can't open the logfile * when encountering either of the logging options (-d or -D) because the * other one may be encountered later on the command line, changing the * location or level of logging. */ if (log_mask) {
if (log_file) {
set_cpu_log_filename(log_file); } set_cpu_log(log_mask); } if (!trace_backend_init(trace_events, trace_file)) {
exit(1); } /* If no data_dir is specified then try to find it relative to the executable path. */ if (!data_dir) {
data_dir = os_find_datadir(argv[0]); } /* If all else fails use the install path specified when building. */ if (!data_dir) {
data_dir = CONFIG_QEMU_DATADIR; } if (machine == NULL) {
fprintf(stderr, ""No machine found.\n""); exit(1); } /* * Default to max_cpus = smp_cpus, in case the user doesn't * specify a max_cpus value. */ if (!max_cpus) max_cpus = smp_cpus; machine->max_cpus = machine->max_cpus ?: 1; /* Default to UP */ if (smp_cpus > machine->max_cpus) {
fprintf(stderr, ""Number of SMP cpus requested (%d), exceeds max cpus "" ""supported by machine `%s' (%d)\n"", smp_cpus, machine->name, machine->max_cpus); exit(1); } /* * Get the default machine options from the machine if it is not already * specified either by the configuration file or by the command line. */ if (machine->default_machine_opts) {
QemuOptsList *list = qemu_find_opts(""machine""); const char *p = NULL; if (!QTAILQ_EMPTY(&list->head)) {
p = qemu_opt_get(QTAILQ_FIRST(&list->head), ""accel""); } if (p == NULL) {
qemu_opts_reset(list); opts = qemu_opts_parse(list, machine->default_machine_opts, 0); if (!opts) {
fprintf(stderr, ""parse error for machine %s: %s\n"", machine->name, machine->default_machine_opts); exit(1); } } } qemu_opts_foreach(qemu_find_opts(""device""), default_driver_check, NULL, 0); qemu_opts_foreach(qemu_find_opts(""global""), default_driver_check, NULL, 0); if (machine->no_serial) {
default_serial = 0; } if (machine->no_parallel) {
default_parallel = 0; } if (!machine->use_virtcon) {
default_virtcon = 0; } if (machine->no_vga) {
default_vga = 0; } if (machine->no_floppy) {
default_floppy = 0; } if (machine->no_cdrom) {
default_cdrom = 0; } if (machine->no_sdcard) {
default_sdcard = 0; } if (display_type == DT_NOGRAPHIC) {
if (default_parallel) add_device_config(DEV_PARALLEL, ""null""); if (default_serial && default_monitor) {
add_device_config(DEV_SERIAL, ""mon:stdio""); } else if (default_virtcon && default_monitor) {
add_device_config(DEV_VIRTCON, ""mon:stdio""); } else {
if (default_serial) add_device_config(DEV_SERIAL, ""stdio""); if (default_virtcon) add_device_config(DEV_VIRTCON, ""stdio""); if (default_monitor) monitor_parse(""stdio"", ""readline""); } } else {
if (default_serial) add_device_config(DEV_SERIAL, ""vc:80Cx24C""); if (default_parallel) add_device_config(DEV_PARALLEL, ""vc:80Cx24C""); if (default_monitor) monitor_parse(""vc:80Cx24C"", ""readline""); if (default_virtcon) add_device_config(DEV_VIRTCON, ""vc:80Cx24C""); } if (default_vga) vga_interface_type = VGA_CIRRUS; socket_init(); if (qemu_opts_foreach(qemu_find_opts(""chardev""), chardev_init_func, NULL, 1) != 0) exit(1); #ifdef CONFIG_VIRTFS if (qemu_opts_foreach(qemu_find_opts(""fsdev""), fsdev_init_func, NULL, 1) != 0) {
exit(1); } #endif os_daemonize(); if (pid_file && qemu_create_pidfile(pid_file) != 0) {
os_pidfile_error(); exit(1); } /* init the memory */ if (ram_size == 0) {
ram_size = DEFAULT_RAM_SIZE * 1024 * 1024; } configure_accelerator(); qemu_init_cpu_loop(); if (qemu_init_main_loop()) {
fprintf(stderr, ""qemu_init_main_loop failed\n""); exit(1); } linux_boot = (kernel_filename != NULL); if (!linux_boot && *kernel_cmdline != '\0') {
fprintf(stderr, ""-append only allowed with -kernel option\n""); exit(1); } if (!linux_boot && initrd_filename != NULL) {
fprintf(stderr, ""-initrd only allowed with -kernel option\n""); exit(1); } os_set_line_buffering(); if (init_timer_alarm() < 0) {
fprintf(stderr, ""could not initialize alarm timer\n""); exit(1); } if (icount_option && (kvm_enabled() || xen_enabled())) {
fprintf(stderr, ""-icount is not allowed with kvm or xen\n""); exit(1); } configure_icount(icount_option); if (net_init_clients() < 0) {
exit(1); } /* init the bluetooth world */ if (foreach_device_config(DEV_BT, bt_parse)) exit(1); if (!xen_enabled()) {
/* On 32-bit hosts, QEMU is limited by virtual address space */ if (ram_size > (2047 << 20) && HOST_LONG_BITS == 32) {
fprintf(stderr, ""qemu: at most 2047 MB RAM can be simulated\n""); exit(1); } } cpu_exec_init_all(); bdrv_init_with_whitelist(); blk_mig_init(); /* open the virtual block devices */ if (snapshot) qemu_opts_foreach(qemu_find_opts(""drive""), drive_enable_snapshot, NULL, 0); if (qemu_opts_foreach(qemu_find_opts(""drive""), drive_init_func, &machine->use_scsi, 1) != 0) exit(1); default_drive(default_cdrom, snapshot, machine->use_scsi, IF_DEFAULT, 2, CDROM_OPTS); default_drive(default_floppy, snapshot, machine->use_scsi, IF_FLOPPY, 0, FD_OPTS); default_drive(default_sdcard, snapshot, machine->use_scsi, IF_SD, 0, SD_OPTS); register_savevm_live(NULL, ""ram"", 0, 4, NULL, ram_save_live, NULL, ram_load, NULL); if (nb_numa_nodes > 0) {
int i; if (nb_numa_nodes > MAX_NODES) {
nb_numa_nodes = MAX_NODES; } /* If no memory size if given for any node, assume the default case * and distribute the available memory equally across all nodes */ for (i = 0; i < nb_numa_nodes; i++) {
if (node_mem[i] != 0) break; } if (i == nb_numa_nodes) {
uint64_t usedmem = 0; /* On Linux, the each node's border has to be 8MB aligned, * the final node gets the rest. */ for (i = 0; i < nb_numa_nodes - 1; i++) {
node_mem[i] = (ram_size / nb_numa_nodes) & ~((1 << 23UL) - 1); usedmem += node_mem[i]; } node_mem[i] = ram_size - usedmem; } for (i = 0; i < nb_numa_nodes; i++) {
if (node_cpumask[i] != 0) break; } /* assigning the VCPUs round-robin is easier to implement, guest OSes * must cope with this anyway, because there are BIOSes out there in * real machines which also use this scheme. */ if (i == nb_numa_nodes) {
for (i = 0; i < max_cpus; i++) {
node_cpumask[i % nb_numa_nodes] |= 1 << i; } } } if (qemu_opts_foreach(qemu_find_opts(""mon""), mon_init_func, NULL, 1) != 0) {
exit(1); } if (foreach_device_config(DEV_SERIAL, serial_parse) < 0) exit(1); if (foreach_device_config(DEV_PARALLEL, parallel_parse) < 0) exit(1); if (foreach_device_config(DEV_VIRTCON, virtcon_parse) < 0) exit(1); if (foreach_device_config(DEV_DEBUGCON, debugcon_parse) < 0) exit(1); module_call_init(MODULE_INIT_DEVICE); if (qemu_opts_foreach(qemu_find_opts(""device""), device_help_func, NULL, 0) != 0) exit(0); if (watchdog) {
i = select_watchdog(watchdog); if (i > 0) exit (i == 1 ? 1 : 0); } if (machine->compat_props) {
qdev_prop_register_global_list(machine->compat_props); } qemu_add_globals(); qdev_machine_init(); machine->init(ram_size, boot_devices, kernel_filename, kernel_cmdline, initrd_filename, cpu_model); cpu_synchronize_all_post_init(); set_numa_modes(); current_machine = machine; /* init USB devices */ if (usb_enabled) {
if (foreach_device_config(DEV_USB, usb_parse) < 0) exit(1); } /* init generic devices */ if (qemu_opts_foreach(qemu_find_opts(""device""), device_init_func, NULL, 1) != 0) exit(1); net_check_clients(); /* just use the first displaystate for the moment */ ds = get_displaystate(); if (using_spice) display_remote++; if (display_type == DT_DEFAULT && !display_remote) {
#if defined(CONFIG_SDL) || defined(CONFIG_COCOA) display_type = DT_SDL; #elif defined(CONFIG_VNC) vnc_display = ""localhost:0,to=99""; show_vnc_port = 1; #else display_type = DT_NONE; #endif } /* init local displays */ switch (display_type) {
case DT_NOGRAPHIC: break; #if defined(CONFIG_CURSES) case DT_CURSES: curses_display_init(ds, full_screen); break; #endif #if defined(CONFIG_SDL) case DT_SDL: sdl_display_init(ds, full_screen, no_frame); break; #elif defined(CONFIG_COCOA) case DT_SDL: cocoa_display_init(ds, full_screen); break; #endif default: break; } /* must be after terminal init, SDL library changes signal handlers */ os_setup_signal_handling(); #ifdef CONFIG_VNC /* init remote displays */ if (vnc_display) {
vnc_display_init(ds); if (vnc_display_open(ds, vnc_display) < 0) exit(1); if (show_vnc_port) {
printf(""VNC server running on `%s'\n"", vnc_display_local_addr(ds)); } } #endif #ifdef CONFIG_SPICE if (using_spice && !qxl_enabled) {
qemu_spice_display_init(ds); } #endif /* display setup */ dpy_resize(ds); dcl = ds->listeners; while (dcl != NULL) {
if (dcl->dpy_refresh != NULL) {
ds->gui_timer = qemu_new_timer_ms(rt_clock, gui_update, ds); qemu_mod_timer(ds->gui_timer, qemu_get_clock_ms(rt_clock)); break; } dcl = dcl->next; } text_consoles_set_display(ds); if (gdbstub_dev && gdbserver_start(gdbstub_dev) < 0) {
fprintf(stderr, ""qemu: could not open gdbserver on device '%s'\n"", gdbstub_dev); exit(1); } qdev_machine_creation_done(); if (rom_load_all() != 0) {
fprintf(stderr, ""rom loading failed\n""); exit(1); } /* TODO: once all bus devices are qdevified, this should be done * when bus is created by qdev.c */ qemu_register_reset(qbus_reset_all_fn, sysbus_get_default()); qemu_run_machine_init_done_notifiers(); qemu_system_reset(VMRESET_SILENT); if (loadvm) {
if (load_vmstate(loadvm) < 0) {
autostart = 0; } } if (incoming) {
runstate_set(RUN_STATE_INMIGRATE); int ret = qemu_start_incoming_migration(incoming); if (ret < 0) {
fprintf(stderr, ""Migration failed. Exit code %s(%d), exiting.\n"", incoming, ret); exit(ret); } } else if (autostart) {
vm_start(); } os_setup_post(); resume_all_vcpus(); main_loop(); bdrv_close_all(); pause_all_vcpus(); net_cleanup(); res_free(); return 0; } ",qemu,0
"static void qxl_blit(PCIQXLDevice *qxl, QXLRect *rect)

{

    DisplaySurface *surface = qemu_console_surface(qxl->vga.con);

    uint8_t *dst = surface_data(surface);

    uint8_t *src;

    int len, i;



    if (is_buffer_shared(surface)) {

        return;

    }

    if (!qxl->guest_primary.data) {

        trace_qxl_render_blit_guest_primary_initialized();

        qxl->guest_primary.data = memory_region_get_ram_ptr(&qxl->vga.vram);

    }

    trace_qxl_render_blit(qxl->guest_primary.qxl_stride,

            rect->left, rect->right, rect->top, rect->bottom);

    src = qxl->guest_primary.data;

    if (qxl->guest_primary.qxl_stride < 0) {

        /* qxl surface is upside down, walk src scanlines

         * in reverse order to flip it */

        src += (qxl->guest_primary.surface.height - rect->top - 1) *

            qxl->guest_primary.abs_stride;

    } else {

        src += rect->top * qxl->guest_primary.abs_stride;

    }

    dst += rect->top  * qxl->guest_primary.abs_stride;

    src += rect->left * qxl->guest_primary.bytes_pp;

    dst += rect->left * qxl->guest_primary.bytes_pp;

    len  = (rect->right - rect->left) * qxl->guest_primary.bytes_pp;



    for (i = rect->top; i < rect->bottom; i++) {

        memcpy(dst, src, len);

        dst += qxl->guest_primary.abs_stride;

        src += qxl->guest_primary.qxl_stride;

    }

}
","static void qxl_blit(PCIQXLDevice *qxl, QXLRect *rect) {
DisplaySurface *surface = qemu_console_surface(qxl->vga.con); uint8_t *dst = surface_data(surface); uint8_t *src; int len, i; if (is_buffer_shared(surface)) {
return; } if (!qxl->guest_primary.data) {
trace_qxl_render_blit_guest_primary_initialized(); qxl->guest_primary.data = memory_region_get_ram_ptr(&qxl->vga.vram); } trace_qxl_render_blit(qxl->guest_primary.qxl_stride, rect->left, rect->right, rect->top, rect->bottom); src = qxl->guest_primary.data; if (qxl->guest_primary.qxl_stride < 0) {
/* qxl surface is upside down, walk src scanlines * in reverse order to flip it */ src += (qxl->guest_primary.surface.height - rect->top - 1) * qxl->guest_primary.abs_stride; } else {
src += rect->top * qxl->guest_primary.abs_stride; } dst += rect->top * qxl->guest_primary.abs_stride; src += rect->left * qxl->guest_primary.bytes_pp; dst += rect->left * qxl->guest_primary.bytes_pp; len = (rect->right - rect->left) * qxl->guest_primary.bytes_pp; for (i = rect->top; i < rect->bottom; i++) {
memcpy(dst, src, len); dst += qxl->guest_primary.abs_stride; src += qxl->guest_primary.qxl_stride; } } ",qemu,0
"void net_host_device_remove(Monitor *mon, int vlan_id, const char *device)

{

    VLANState *vlan;

    VLANClientState *vc;



    vlan = qemu_find_vlan(vlan_id);

    if (!vlan) {

        monitor_printf(mon, ""can't find vlan %d\n"", vlan_id);

        return;

    }



   for(vc = vlan->first_client; vc != NULL; vc = vc->next)

        if (!strcmp(vc->name, device))

            break;



    if (!vc) {

        monitor_printf(mon, ""can't find device %s\n"", device);

        return;

    }

    qemu_del_vlan_client(vc);

}
","void net_host_device_remove(Monitor *mon, int vlan_id, const char *device) {
VLANState *vlan; VLANClientState *vc; vlan = qemu_find_vlan(vlan_id); if (!vlan) {
monitor_printf(mon, ""can't find vlan %d\n"", vlan_id); return; } for(vc = vlan->first_client; vc != NULL; vc = vc->next) if (!strcmp(vc->name, device)) break; if (!vc) {
monitor_printf(mon, ""can't find device %s\n"", device); return; } qemu_del_vlan_client(vc); } ",qemu,1
"static void set_acpi_power_state(IPMIBmcSim *ibs,

                          uint8_t *cmd, unsigned int cmd_len,

                          uint8_t *rsp, unsigned int *rsp_len,

                          unsigned int max_rsp_len)

{

    IPMI_CHECK_CMD_LEN(4);

    ibs->acpi_power_state[0] = cmd[2];

    ibs->acpi_power_state[1] = cmd[3];

}
","static void set_acpi_power_state(IPMIBmcSim *ibs, uint8_t *cmd, unsigned int cmd_len, uint8_t *rsp, unsigned int *rsp_len, unsigned int max_rsp_len) {
IPMI_CHECK_CMD_LEN(4); ibs->acpi_power_state[0] = cmd[2]; ibs->acpi_power_state[1] = cmd[3]; } ",qemu,1
"static void mcf_fec_write(void *opaque, hwaddr addr,

                          uint64_t value, unsigned size)

{

    mcf_fec_state *s = (mcf_fec_state *)opaque;

    switch (addr & 0x3ff) {

    case 0x004:

        s->eir &= ~value;

        break;

    case 0x008:

        s->eimr = value;

        break;

    case 0x010: /* RDAR */

        if ((s->ecr & FEC_EN) && !s->rx_enabled) {

            DPRINTF(""RX enable\n"");

            mcf_fec_enable_rx(s);

        }

        break;

    case 0x014: /* TDAR */

        if (s->ecr & FEC_EN) {

            mcf_fec_do_tx(s);

        }

        break;

    case 0x024:

        s->ecr = value;

        if (value & FEC_RESET) {

            DPRINTF(""Reset\n"");

            mcf_fec_reset(s);

        }

        if ((s->ecr & FEC_EN) == 0) {

            s->rx_enabled = 0;

        }

        break;

    case 0x040:

        s->mmfr = value;

        s->eir |= FEC_INT_MII;

        break;

    case 0x044:

        s->mscr = value & 0xfe;

        break;

    case 0x064:

        /* TODO: Implement MIB.  */

        break;

    case 0x084:

        s->rcr = value & 0x07ff003f;

        /* TODO: Implement LOOP mode.  */

        break;

    case 0x0c4: /* TCR */

        /* We transmit immediately, so raise GRA immediately.  */

        s->tcr = value;

        if (value & 1)

            s->eir |= FEC_INT_GRA;

        break;

    case 0x0e4: /* PALR */

        s->conf.macaddr.a[0] = value >> 24;

        s->conf.macaddr.a[1] = value >> 16;

        s->conf.macaddr.a[2] = value >> 8;

        s->conf.macaddr.a[3] = value;

        break;

    case 0x0e8: /* PAUR */

        s->conf.macaddr.a[4] = value >> 24;

        s->conf.macaddr.a[5] = value >> 16;

        break;

    case 0x0ec:

        /* OPD */

        break;

    case 0x118:

    case 0x11c:

    case 0x120:

    case 0x124:

        /* TODO: implement MAC hash filtering.  */

        break;

    case 0x144:

        s->tfwr = value & 3;

        break;

    case 0x14c:

        /* FRBR writes ignored.  */

        break;

    case 0x150:

        s->rfsr = (value & 0x3fc) | 0x400;

        break;

    case 0x180:

        s->erdsr = value & ~3;

        s->rx_descriptor = s->erdsr;

        break;

    case 0x184:

        s->etdsr = value & ~3;

        s->tx_descriptor = s->etdsr;

        break;

    case 0x188:

        s->emrbr = value & 0x7f0;

        break;

    default:

        hw_error(""mcf_fec_write Bad address 0x%x\n"", (int)addr);

    }

    mcf_fec_update(s);

}
","static void mcf_fec_write(void *opaque, hwaddr addr, uint64_t value, unsigned size) {
mcf_fec_state *s = (mcf_fec_state *)opaque; switch (addr & 0x3ff) {
case 0x004: s->eir &= ~value; break; case 0x008: s->eimr = value; break; case 0x010: /* RDAR */ if ((s->ecr & FEC_EN) && !s->rx_enabled) {
DPRINTF(""RX enable\n""); mcf_fec_enable_rx(s); } break; case 0x014: /* TDAR */ if (s->ecr & FEC_EN) {
mcf_fec_do_tx(s); } break; case 0x024: s->ecr = value; if (value & FEC_RESET) {
DPRINTF(""Reset\n""); mcf_fec_reset(s); } if ((s->ecr & FEC_EN) == 0) {
s->rx_enabled = 0; } break; case 0x040: s->mmfr = value; s->eir |= FEC_INT_MII; break; case 0x044: s->mscr = value & 0xfe; break; case 0x064: /* TODO: Implement MIB. */ break; case 0x084: s->rcr = value & 0x07ff003f; /* TODO: Implement LOOP mode. */ break; case 0x0c4: /* TCR */ /* We transmit immediately, so raise GRA immediately. */ s->tcr = value; if (value & 1) s->eir |= FEC_INT_GRA; break; case 0x0e4: /* PALR */ s->conf.macaddr.a[0] = value >> 24; s->conf.macaddr.a[1] = value >> 16; s->conf.macaddr.a[2] = value >> 8; s->conf.macaddr.a[3] = value; break; case 0x0e8: /* PAUR */ s->conf.macaddr.a[4] = value >> 24; s->conf.macaddr.a[5] = value >> 16; break; case 0x0ec: /* OPD */ break; case 0x118: case 0x11c: case 0x120: case 0x124: /* TODO: implement MAC hash filtering. */ break; case 0x144: s->tfwr = value & 3; break; case 0x14c: /* FRBR writes ignored. */ break; case 0x150: s->rfsr = (value & 0x3fc) | 0x400; break; case 0x180: s->erdsr = value & ~3; s->rx_descriptor = s->erdsr; break; case 0x184: s->etdsr = value & ~3; s->tx_descriptor = s->etdsr; break; case 0x188: s->emrbr = value & 0x7f0; break; default: hw_error(""mcf_fec_write Bad address 0x%x\n"", (int)addr); } mcf_fec_update(s); } ",qemu,0
"void scsi_req_cancel(SCSIRequest *req)

{

    trace_scsi_req_cancel(req->dev->id, req->lun, req->tag);

    if (!req->enqueued) {

        return;

    }

    scsi_req_ref(req);

    scsi_req_dequeue(req);

    req->io_canceled = true;

    if (req->aiocb) {

        bdrv_aio_cancel(req->aiocb);

    }

}
","void scsi_req_cancel(SCSIRequest *req) {
trace_scsi_req_cancel(req->dev->id, req->lun, req->tag); if (!req->enqueued) {
return; } scsi_req_ref(req); scsi_req_dequeue(req); req->io_canceled = true; if (req->aiocb) {
bdrv_aio_cancel(req->aiocb); } } ",qemu,0
"static void coroutine_fn mirror_run(void *opaque)

{

    MirrorBlockJob *s = opaque;

    MirrorExitData *data;

    BlockDriverState *bs = blk_bs(s->common.blk);

    BlockDriverState *target_bs = blk_bs(s->target);

    int64_t length;

    BlockDriverInfo bdi;

    char backing_filename[2]; /* we only need 2 characters because we are only

                                 checking for a NULL string */

    int ret = 0;

    int target_cluster_size = BDRV_SECTOR_SIZE;



    if (block_job_is_cancelled(&s->common)) {

        goto immediate_exit;

    }



    s->bdev_length = bdrv_getlength(bs);

    if (s->bdev_length < 0) {

        ret = s->bdev_length;

        goto immediate_exit;

    } else if (s->bdev_length == 0) {

        /* Report BLOCK_JOB_READY and wait for complete. */

        block_job_event_ready(&s->common);

        s->synced = true;

        while (!block_job_is_cancelled(&s->common) && !s->should_complete) {

            block_job_yield(&s->common);

        }

        s->common.cancelled = false;

        goto immediate_exit;

    }



    length = DIV_ROUND_UP(s->bdev_length, s->granularity);

    s->in_flight_bitmap = bitmap_new(length);



    /* If we have no backing file yet in the destination, we cannot let

     * the destination do COW.  Instead, we copy sectors around the

     * dirty data if needed.  We need a bitmap to do that.

     */

    bdrv_get_backing_filename(target_bs, backing_filename,

                              sizeof(backing_filename));

    if (!bdrv_get_info(target_bs, &bdi) && bdi.cluster_size) {

        target_cluster_size = bdi.cluster_size;

    }

    if (backing_filename[0] && !target_bs->backing

        && s->granularity < target_cluster_size) {

        s->buf_size = MAX(s->buf_size, target_cluster_size);

        s->cow_bitmap = bitmap_new(length);

    }

    s->target_cluster_sectors = target_cluster_size >> BDRV_SECTOR_BITS;

    s->max_iov = MIN(bs->bl.max_iov, target_bs->bl.max_iov);



    s->buf = qemu_try_blockalign(bs, s->buf_size);

    if (s->buf == NULL) {

        ret = -ENOMEM;

        goto immediate_exit;

    }



    mirror_free_init(s);



    s->last_pause_ns = qemu_clock_get_ns(QEMU_CLOCK_REALTIME);

    if (!s->is_none_mode) {

        ret = mirror_dirty_init(s);

        if (ret < 0 || block_job_is_cancelled(&s->common)) {

            goto immediate_exit;

        }

    }



    assert(!s->dbi);

    s->dbi = bdrv_dirty_iter_new(s->dirty_bitmap, 0);

    for (;;) {

        uint64_t delay_ns = 0;

        int64_t cnt, delta;

        bool should_complete;



        if (s->ret < 0) {

            ret = s->ret;

            goto immediate_exit;

        }



        block_job_pause_point(&s->common);



        cnt = bdrv_get_dirty_count(s->dirty_bitmap);

        /* s->common.offset contains the number of bytes already processed so

         * far, cnt is the number of dirty sectors remaining and

         * s->sectors_in_flight is the number of sectors currently being

         * processed; together those are the current total operation length */

        s->common.len = s->common.offset +

                        (cnt + s->sectors_in_flight) * BDRV_SECTOR_SIZE;



        /* Note that even when no rate limit is applied we need to yield

         * periodically with no pending I/O so that bdrv_drain_all() returns.

         * We do so every SLICE_TIME nanoseconds, or when there is an error,

         * or when the source is clean, whichever comes first.

         */

        delta = qemu_clock_get_ns(QEMU_CLOCK_REALTIME) - s->last_pause_ns;

        if (delta < SLICE_TIME &&

            s->common.iostatus == BLOCK_DEVICE_IO_STATUS_OK) {

            if (s->in_flight >= MAX_IN_FLIGHT || s->buf_free_count == 0 ||

                (cnt == 0 && s->in_flight > 0)) {

                trace_mirror_yield(s, s->in_flight, s->buf_free_count, cnt);

                mirror_wait_for_io(s);

                continue;

            } else if (cnt != 0) {

                delay_ns = mirror_iteration(s);

            }

        }



        should_complete = false;

        if (s->in_flight == 0 && cnt == 0) {

            trace_mirror_before_flush(s);

            ret = blk_flush(s->target);

            if (ret < 0) {

                if (mirror_error_action(s, false, -ret) ==

                    BLOCK_ERROR_ACTION_REPORT) {

                    goto immediate_exit;

                }

            } else {

                /* We're out of the streaming phase.  From now on, if the job

                 * is cancelled we will actually complete all pending I/O and

                 * report completion.  This way, block-job-cancel will leave

                 * the target in a consistent state.

                 */

                if (!s->synced) {

                    block_job_event_ready(&s->common);

                    s->synced = true;

                }



                should_complete = s->should_complete ||

                    block_job_is_cancelled(&s->common);

                cnt = bdrv_get_dirty_count(s->dirty_bitmap);

            }

        }



        if (cnt == 0 && should_complete) {

            /* The dirty bitmap is not updated while operations are pending.

             * If we're about to exit, wait for pending operations before

             * calling bdrv_get_dirty_count(bs), or we may exit while the

             * source has dirty data to copy!

             *

             * Note that I/O can be submitted by the guest while

             * mirror_populate runs.

             */

            trace_mirror_before_drain(s, cnt);

            bdrv_co_drain(bs);

            cnt = bdrv_get_dirty_count(s->dirty_bitmap);

        }



        ret = 0;

        trace_mirror_before_sleep(s, cnt, s->synced, delay_ns);

        if (!s->synced) {

            block_job_sleep_ns(&s->common, QEMU_CLOCK_REALTIME, delay_ns);

            if (block_job_is_cancelled(&s->common)) {

                break;

            }

        } else if (!should_complete) {

            delay_ns = (s->in_flight == 0 && cnt == 0 ? SLICE_TIME : 0);

            block_job_sleep_ns(&s->common, QEMU_CLOCK_REALTIME, delay_ns);

        } else if (cnt == 0) {

            /* The two disks are in sync.  Exit and report successful

             * completion.

             */

            assert(QLIST_EMPTY(&bs->tracked_requests));

            s->common.cancelled = false;

            break;

        }

        s->last_pause_ns = qemu_clock_get_ns(QEMU_CLOCK_REALTIME);

    }



immediate_exit:

    if (s->in_flight > 0) {

        /* We get here only if something went wrong.  Either the job failed,

         * or it was cancelled prematurely so that we do not guarantee that

         * the target is a copy of the source.

         */

        assert(ret < 0 || (!s->synced && block_job_is_cancelled(&s->common)));

        mirror_drain(s);

    }



    assert(s->in_flight == 0);

    qemu_vfree(s->buf);

    g_free(s->cow_bitmap);

    g_free(s->in_flight_bitmap);

    bdrv_dirty_iter_free(s->dbi);

    bdrv_release_dirty_bitmap(bs, s->dirty_bitmap);



    data = g_malloc(sizeof(*data));

    data->ret = ret;

    /* Before we switch to target in mirror_exit, make sure data doesn't

     * change. */

    bdrv_drained_begin(bs);

    block_job_defer_to_main_loop(&s->common, mirror_exit, data);

}
","static void coroutine_fn mirror_run(void *opaque) {
MirrorBlockJob *s = opaque; MirrorExitData *data; BlockDriverState *bs = blk_bs(s->common.blk); BlockDriverState *target_bs = blk_bs(s->target); int64_t length; BlockDriverInfo bdi; char backing_filename[2]; /* we only need 2 characters because we are only checking for a NULL string */ int ret = 0; int target_cluster_size = BDRV_SECTOR_SIZE; if (block_job_is_cancelled(&s->common)) {
goto immediate_exit; } s->bdev_length = bdrv_getlength(bs); if (s->bdev_length < 0) {
ret = s->bdev_length; goto immediate_exit; } else if (s->bdev_length == 0) {
/* Report BLOCK_JOB_READY and wait for complete. */ block_job_event_ready(&s->common); s->synced = true; while (!block_job_is_cancelled(&s->common) && !s->should_complete) {
block_job_yield(&s->common); } s->common.cancelled = false; goto immediate_exit; } length = DIV_ROUND_UP(s->bdev_length, s->granularity); s->in_flight_bitmap = bitmap_new(length); /* If we have no backing file yet in the destination, we cannot let * the destination do COW. Instead, we copy sectors around the * dirty data if needed. We need a bitmap to do that. */ bdrv_get_backing_filename(target_bs, backing_filename, sizeof(backing_filename)); if (!bdrv_get_info(target_bs, &bdi) && bdi.cluster_size) {
target_cluster_size = bdi.cluster_size; } if (backing_filename[0] && !target_bs->backing && s->granularity < target_cluster_size) {
s->buf_size = MAX(s->buf_size, target_cluster_size); s->cow_bitmap = bitmap_new(length); } s->target_cluster_sectors = target_cluster_size >> BDRV_SECTOR_BITS; s->max_iov = MIN(bs->bl.max_iov, target_bs->bl.max_iov); s->buf = qemu_try_blockalign(bs, s->buf_size); if (s->buf == NULL) {
ret = -ENOMEM; goto immediate_exit; } mirror_free_init(s); s->last_pause_ns = qemu_clock_get_ns(QEMU_CLOCK_REALTIME); if (!s->is_none_mode) {
ret = mirror_dirty_init(s); if (ret < 0 || block_job_is_cancelled(&s->common)) {
goto immediate_exit; } } assert(!s->dbi); s->dbi = bdrv_dirty_iter_new(s->dirty_bitmap, 0); for (;;) {
uint64_t delay_ns = 0; int64_t cnt, delta; bool should_complete; if (s->ret < 0) {
ret = s->ret; goto immediate_exit; } block_job_pause_point(&s->common); cnt = bdrv_get_dirty_count(s->dirty_bitmap); /* s->common.offset contains the number of bytes already processed so * far, cnt is the number of dirty sectors remaining and * s->sectors_in_flight is the number of sectors currently being * processed; together those are the current total operation length */ s->common.len = s->common.offset + (cnt + s->sectors_in_flight) * BDRV_SECTOR_SIZE; /* Note that even when no rate limit is applied we need to yield * periodically with no pending I/O so that bdrv_drain_all() returns. * We do so every SLICE_TIME nanoseconds, or when there is an error, * or when the source is clean, whichever comes first. */ delta = qemu_clock_get_ns(QEMU_CLOCK_REALTIME) - s->last_pause_ns; if (delta < SLICE_TIME && s->common.iostatus == BLOCK_DEVICE_IO_STATUS_OK) {
if (s->in_flight >= MAX_IN_FLIGHT || s->buf_free_count == 0 || (cnt == 0 && s->in_flight > 0)) {
trace_mirror_yield(s, s->in_flight, s->buf_free_count, cnt); mirror_wait_for_io(s); continue; } else if (cnt != 0) {
delay_ns = mirror_iteration(s); } } should_complete = false; if (s->in_flight == 0 && cnt == 0) {
trace_mirror_before_flush(s); ret = blk_flush(s->target); if (ret < 0) {
if (mirror_error_action(s, false, -ret) == BLOCK_ERROR_ACTION_REPORT) {
goto immediate_exit; } } else {
/* We're out of the streaming phase. From now on, if the job * is cancelled we will actually complete all pending I/O and * report completion. This way, block-job-cancel will leave * the target in a consistent state. */ if (!s->synced) {
block_job_event_ready(&s->common); s->synced = true; } should_complete = s->should_complete || block_job_is_cancelled(&s->common); cnt = bdrv_get_dirty_count(s->dirty_bitmap); } } if (cnt == 0 && should_complete) {
/* The dirty bitmap is not updated while operations are pending. * If we're about to exit, wait for pending operations before * calling bdrv_get_dirty_count(bs), or we may exit while the * source has dirty data to copy! * * Note that I/O can be submitted by the guest while * mirror_populate runs. */ trace_mirror_before_drain(s, cnt); bdrv_co_drain(bs); cnt = bdrv_get_dirty_count(s->dirty_bitmap); } ret = 0; trace_mirror_before_sleep(s, cnt, s->synced, delay_ns); if (!s->synced) {
block_job_sleep_ns(&s->common, QEMU_CLOCK_REALTIME, delay_ns); if (block_job_is_cancelled(&s->common)) {
break; } } else if (!should_complete) {
delay_ns = (s->in_flight == 0 && cnt == 0 ? SLICE_TIME : 0); block_job_sleep_ns(&s->common, QEMU_CLOCK_REALTIME, delay_ns); } else if (cnt == 0) {
/* The two disks are in sync. Exit and report successful * completion. */ assert(QLIST_EMPTY(&bs->tracked_requests)); s->common.cancelled = false; break; } s->last_pause_ns = qemu_clock_get_ns(QEMU_CLOCK_REALTIME); } immediate_exit: if (s->in_flight > 0) {
/* We get here only if something went wrong. Either the job failed, * or it was cancelled prematurely so that we do not guarantee that * the target is a copy of the source. */ assert(ret < 0 || (!s->synced && block_job_is_cancelled(&s->common))); mirror_drain(s); } assert(s->in_flight == 0); qemu_vfree(s->buf); g_free(s->cow_bitmap); g_free(s->in_flight_bitmap); bdrv_dirty_iter_free(s->dbi); bdrv_release_dirty_bitmap(bs, s->dirty_bitmap); data = g_malloc(sizeof(*data)); data->ret = ret; /* Before we switch to target in mirror_exit, make sure data doesn't * change. */ bdrv_drained_begin(bs); block_job_defer_to_main_loop(&s->common, mirror_exit, data); } ",qemu,0
"static void coroutine_fn bdrv_flush_co_entry(void *opaque)

{

    RwCo *rwco = opaque;



    rwco->ret = bdrv_co_flush(rwco->bs);

}
","static void coroutine_fn bdrv_flush_co_entry(void *opaque) {
RwCo *rwco = opaque; rwco->ret = bdrv_co_flush(rwco->bs); } ",qemu,0
"static void mips_cpu_realizefn(DeviceState *dev, Error **errp)

{

    MIPSCPU *cpu = MIPS_CPU(dev);

    MIPSCPUClass *mcc = MIPS_CPU_GET_CLASS(dev);



    cpu_reset(CPU(cpu));



    mcc->parent_realize(dev, errp);

}
","static void mips_cpu_realizefn(DeviceState *dev, Error **errp) {
MIPSCPU *cpu = MIPS_CPU(dev); MIPSCPUClass *mcc = MIPS_CPU_GET_CLASS(dev); cpu_reset(CPU(cpu)); mcc->parent_realize(dev, errp); } ",qemu,1
"bool postcopy_ram_supported_by_host(void)

{

    error_report(""%s: No OS support"", __func__);

    return false;

}
","bool postcopy_ram_supported_by_host(void) {
error_report(""%s: No OS support"", __func__); return false; } ",qemu,0
"static uint64_t mv88w8618_eth_read(void *opaque, target_phys_addr_t offset,

                                   unsigned size)

{

    mv88w8618_eth_state *s = opaque;



    switch (offset) {

    case MP_ETH_SMIR:

        if (s->smir & MP_ETH_SMIR_OPCODE) {

            switch (s->smir & MP_ETH_SMIR_ADDR) {

            case MP_ETH_PHY1_BMSR:

                return MP_PHY_BMSR_LINK | MP_PHY_BMSR_AUTONEG |

                       MP_ETH_SMIR_RDVALID;

            case MP_ETH_PHY1_PHYSID1:

                return (MP_PHY_88E3015 >> 16) | MP_ETH_SMIR_RDVALID;

            case MP_ETH_PHY1_PHYSID2:

                return (MP_PHY_88E3015 & 0xFFFF) | MP_ETH_SMIR_RDVALID;

            default:

                return MP_ETH_SMIR_RDVALID;

            }

        }

        return 0;



    case MP_ETH_ICR:

        return s->icr;



    case MP_ETH_IMR:

        return s->imr;



    case MP_ETH_FRDP0 ... MP_ETH_FRDP3:

        return s->frx_queue[(offset - MP_ETH_FRDP0)/4];



    case MP_ETH_CRDP0 ... MP_ETH_CRDP3:

        return s->rx_queue[(offset - MP_ETH_CRDP0)/4];



    case MP_ETH_CTDP0 ... MP_ETH_CTDP3:

        return s->tx_queue[(offset - MP_ETH_CTDP0)/4];



    default:

        return 0;

    }

}
","static uint64_t mv88w8618_eth_read(void *opaque, target_phys_addr_t offset, unsigned size) {
mv88w8618_eth_state *s = opaque; switch (offset) {
case MP_ETH_SMIR: if (s->smir & MP_ETH_SMIR_OPCODE) {
switch (s->smir & MP_ETH_SMIR_ADDR) {
case MP_ETH_PHY1_BMSR: return MP_PHY_BMSR_LINK | MP_PHY_BMSR_AUTONEG | MP_ETH_SMIR_RDVALID; case MP_ETH_PHY1_PHYSID1: return (MP_PHY_88E3015 >> 16) | MP_ETH_SMIR_RDVALID; case MP_ETH_PHY1_PHYSID2: return (MP_PHY_88E3015 & 0xFFFF) | MP_ETH_SMIR_RDVALID; default: return MP_ETH_SMIR_RDVALID; } } return 0; case MP_ETH_ICR: return s->icr; case MP_ETH_IMR: return s->imr; case MP_ETH_FRDP0 ... MP_ETH_FRDP3: return s->frx_queue[(offset - MP_ETH_FRDP0)/4]; case MP_ETH_CRDP0 ... MP_ETH_CRDP3: return s->rx_queue[(offset - MP_ETH_CRDP0)/4]; case MP_ETH_CTDP0 ... MP_ETH_CTDP3: return s->tx_queue[(offset - MP_ETH_CTDP0)/4]; default: return 0; } } ",qemu,0
"static void x86_cpu_parse_featurestr(CPUState *cs, char *features,

                                     Error **errp)

{

    X86CPU *cpu = X86_CPU(cs);

    char *featurestr; /* Single 'key=value"" string being parsed */

    /* Features to be added */

    FeatureWordArray plus_features = { 0 };

    /* Features to be removed */

    FeatureWordArray minus_features = { 0 };

    uint32_t numvalue;

    CPUX86State *env = &cpu->env;

    Error *local_err = NULL;



    featurestr = features ? strtok(features, "","") : NULL;



    while (featurestr) {

        char *val;

        if (featurestr[0] == '+') {

            add_flagname_to_bitmaps(featurestr + 1, plus_features);

        } else if (featurestr[0] == '-') {

            add_flagname_to_bitmaps(featurestr + 1, minus_features);

        } else if ((val = strchr(featurestr, '='))) {

            *val = 0; val++;

            feat2prop(featurestr);

            if (!strcmp(featurestr, ""xlevel"")) {

                char *err;

                char num[32];



                numvalue = strtoul(val, &err, 0);

                if (!*val || *err) {

                    error_setg(&local_err, ""bad numerical value %s"", val);

                    goto out;

                }

                if (numvalue < 0x80000000) {

                    error_report(""xlevel value shall always be >= 0x80000000""

                                 "", fixup will be removed in future versions"");

                    numvalue += 0x80000000;

                }

                snprintf(num, sizeof(num), ""%"" PRIu32, numvalue);

                object_property_parse(OBJECT(cpu), num, featurestr, &local_err);

            } else if (!strcmp(featurestr, ""tsc-freq"")) {

                int64_t tsc_freq;

                char *err;

                char num[32];



                tsc_freq = strtosz_suffix_unit(val, &err,

                                               STRTOSZ_DEFSUFFIX_B, 1000);

                if (tsc_freq < 0 || *err) {

                    error_setg(&local_err, ""bad numerical value %s"", val);

                    goto out;

                }

                snprintf(num, sizeof(num), ""%"" PRId64, tsc_freq);

                object_property_parse(OBJECT(cpu), num, ""tsc-frequency"",

                                      &local_err);

            } else if (!strcmp(featurestr, ""hv-spinlocks"")) {

                char *err;

                const int min = 0xFFF;

                char num[32];

                numvalue = strtoul(val, &err, 0);

                if (!*val || *err) {

                    error_setg(&local_err, ""bad numerical value %s"", val);

                    goto out;

                }

                if (numvalue < min) {

                    error_report(""hv-spinlocks value shall always be >= 0x%x""

                            "", fixup will be removed in future versions"",

                            min);

                    numvalue = min;

                }

                snprintf(num, sizeof(num), ""%"" PRId32, numvalue);

                object_property_parse(OBJECT(cpu), num, featurestr, &local_err);

            } else {

                object_property_parse(OBJECT(cpu), val, featurestr, &local_err);

            }

        } else {

            feat2prop(featurestr);

            object_property_parse(OBJECT(cpu), ""on"", featurestr, &local_err);

        }

        if (local_err) {

            error_propagate(errp, local_err);

            goto out;

        }

        featurestr = strtok(NULL, "","");

    }

    env->features[FEAT_1_EDX] |= plus_features[FEAT_1_EDX];

    env->features[FEAT_1_ECX] |= plus_features[FEAT_1_ECX];

    env->features[FEAT_8000_0001_EDX] |= plus_features[FEAT_8000_0001_EDX];

    env->features[FEAT_8000_0001_ECX] |= plus_features[FEAT_8000_0001_ECX];

    env->features[FEAT_C000_0001_EDX] |= plus_features[FEAT_C000_0001_EDX];

    env->features[FEAT_KVM] |= plus_features[FEAT_KVM];

    env->features[FEAT_SVM] |= plus_features[FEAT_SVM];

    env->features[FEAT_7_0_EBX] |= plus_features[FEAT_7_0_EBX];

    env->features[FEAT_1_EDX] &= ~minus_features[FEAT_1_EDX];

    env->features[FEAT_1_ECX] &= ~minus_features[FEAT_1_ECX];

    env->features[FEAT_8000_0001_EDX] &= ~minus_features[FEAT_8000_0001_EDX];

    env->features[FEAT_8000_0001_ECX] &= ~minus_features[FEAT_8000_0001_ECX];

    env->features[FEAT_C000_0001_EDX] &= ~minus_features[FEAT_C000_0001_EDX];

    env->features[FEAT_KVM] &= ~minus_features[FEAT_KVM];

    env->features[FEAT_SVM] &= ~minus_features[FEAT_SVM];

    env->features[FEAT_7_0_EBX] &= ~minus_features[FEAT_7_0_EBX];



out:

    return;

}
","static void x86_cpu_parse_featurestr(CPUState *cs, char *features, Error **errp) {
X86CPU *cpu = X86_CPU(cs); char *featurestr; /* Single 'key=value"" string being parsed */ /* Features to be added */ FeatureWordArray plus_features = {
0 }; /* Features to be removed */ FeatureWordArray minus_features = {
0 }; uint32_t numvalue; CPUX86State *env = &cpu->env; Error *local_err = NULL; featurestr = features ? strtok(features, "","") : NULL; while (featurestr) {
char *val; if (featurestr[0] == '+') {
add_flagname_to_bitmaps(featurestr + 1, plus_features); } else if (featurestr[0] == '-') {
add_flagname_to_bitmaps(featurestr + 1, minus_features); } else if ((val = strchr(featurestr, '='))) {
*val = 0; val++; feat2prop(featurestr); if (!strcmp(featurestr, ""xlevel"")) {
char *err; char num[32]; numvalue = strtoul(val, &err, 0); if (!*val || *err) {
error_setg(&local_err, ""bad numerical value %s"", val); goto out; } if (numvalue < 0x80000000) {
error_report(""xlevel value shall always be >= 0x80000000"" "", fixup will be removed in future versions""); numvalue += 0x80000000; } snprintf(num, sizeof(num), ""%"" PRIu32, numvalue); object_property_parse(OBJECT(cpu), num, featurestr, &local_err); } else if (!strcmp(featurestr, ""tsc-freq"")) {
int64_t tsc_freq; char *err; char num[32]; tsc_freq = strtosz_suffix_unit(val, &err, STRTOSZ_DEFSUFFIX_B, 1000); if (tsc_freq < 0 || *err) {
error_setg(&local_err, ""bad numerical value %s"", val); goto out; } snprintf(num, sizeof(num), ""%"" PRId64, tsc_freq); object_property_parse(OBJECT(cpu), num, ""tsc-frequency"", &local_err); } else if (!strcmp(featurestr, ""hv-spinlocks"")) {
char *err; const int min = 0xFFF; char num[32]; numvalue = strtoul(val, &err, 0); if (!*val || *err) {
error_setg(&local_err, ""bad numerical value %s"", val); goto out; } if (numvalue < min) {
error_report(""hv-spinlocks value shall always be >= 0x%x"" "", fixup will be removed in future versions"", min); numvalue = min; } snprintf(num, sizeof(num), ""%"" PRId32, numvalue); object_property_parse(OBJECT(cpu), num, featurestr, &local_err); } else {
object_property_parse(OBJECT(cpu), val, featurestr, &local_err); } } else {
feat2prop(featurestr); object_property_parse(OBJECT(cpu), ""on"", featurestr, &local_err); } if (local_err) {
error_propagate(errp, local_err); goto out; } featurestr = strtok(NULL, "",""); } env->features[FEAT_1_EDX] |= plus_features[FEAT_1_EDX]; env->features[FEAT_1_ECX] |= plus_features[FEAT_1_ECX]; env->features[FEAT_8000_0001_EDX] |= plus_features[FEAT_8000_0001_EDX]; env->features[FEAT_8000_0001_ECX] |= plus_features[FEAT_8000_0001_ECX]; env->features[FEAT_C000_0001_EDX] |= plus_features[FEAT_C000_0001_EDX]; env->features[FEAT_KVM] |= plus_features[FEAT_KVM]; env->features[FEAT_SVM] |= plus_features[FEAT_SVM]; env->features[FEAT_7_0_EBX] |= plus_features[FEAT_7_0_EBX]; env->features[FEAT_1_EDX] &= ~minus_features[FEAT_1_EDX]; env->features[FEAT_1_ECX] &= ~minus_features[FEAT_1_ECX]; env->features[FEAT_8000_0001_EDX] &= ~minus_features[FEAT_8000_0001_EDX]; env->features[FEAT_8000_0001_ECX] &= ~minus_features[FEAT_8000_0001_ECX]; env->features[FEAT_C000_0001_EDX] &= ~minus_features[FEAT_C000_0001_EDX]; env->features[FEAT_KVM] &= ~minus_features[FEAT_KVM]; env->features[FEAT_SVM] &= ~minus_features[FEAT_SVM]; env->features[FEAT_7_0_EBX] &= ~minus_features[FEAT_7_0_EBX]; out: return; } ",qemu,1
"static void test_cancel(void)

{

    WorkerTestData data[100];

    int num_canceled;

    int i;



    /* Start more work items than there will be threads, to ensure

     * the pool is full.

     */

    test_submit_many();



    /* Start long running jobs, to ensure we can cancel some.  */

    for (i = 0; i < 100; i++) {

        data[i].n = 0;

        data[i].ret = -EINPROGRESS;

        data[i].aiocb = thread_pool_submit_aio(long_cb, &data[i],

                                               done_cb, &data[i]);

    }



    /* Starting the threads may be left to a bottom half.  Let it

     * run, but do not waste too much time...

     */

    active = 100;

    qemu_aio_wait_nonblocking();



    /* Wait some time for the threads to start, with some sanity

     * testing on the behavior of the scheduler...

     */

    g_assert_cmpint(active, ==, 100);

    g_usleep(1000000);

    g_assert_cmpint(active, >, 50);



    /* Cancel the jobs that haven't been started yet.  */

    num_canceled = 0;

    for (i = 0; i < 100; i++) {

        if (__sync_val_compare_and_swap(&data[i].n, 0, 3) == 0) {

            data[i].ret = -ECANCELED;

            bdrv_aio_cancel(data[i].aiocb);

            active--;

            num_canceled++;

        }

    }

    g_assert_cmpint(active, >, 0);

    g_assert_cmpint(num_canceled, <, 100);



    /* Canceling the others will be a blocking operation.  */

    for (i = 0; i < 100; i++) {

        if (data[i].n != 3) {

            bdrv_aio_cancel(data[i].aiocb);

        }

    }



    /* Finish execution and execute any remaining callbacks.  */

    qemu_aio_wait_all();

    g_assert_cmpint(active, ==, 0);

    for (i = 0; i < 100; i++) {

        if (data[i].n == 3) {

            g_assert_cmpint(data[i].ret, ==, -ECANCELED);

            g_assert(data[i].aiocb != NULL);

        } else {

            g_assert_cmpint(data[i].n, ==, 2);

            g_assert_cmpint(data[i].ret, ==, 0);

            g_assert(data[i].aiocb == NULL);

        }

    }

}
","static void test_cancel(void) {
WorkerTestData data[100]; int num_canceled; int i; /* Start more work items than there will be threads, to ensure * the pool is full. */ test_submit_many(); /* Start long running jobs, to ensure we can cancel some. */ for (i = 0; i < 100; i++) {
data[i].n = 0; data[i].ret = -EINPROGRESS; data[i].aiocb = thread_pool_submit_aio(long_cb, &data[i], done_cb, &data[i]); } /* Starting the threads may be left to a bottom half. Let it * run, but do not waste too much time... */ active = 100; qemu_aio_wait_nonblocking(); /* Wait some time for the threads to start, with some sanity * testing on the behavior of the scheduler... */ g_assert_cmpint(active, ==, 100); g_usleep(1000000); g_assert_cmpint(active, >, 50); /* Cancel the jobs that haven't been started yet. */ num_canceled = 0; for (i = 0; i < 100; i++) {
if (__sync_val_compare_and_swap(&data[i].n, 0, 3) == 0) {
data[i].ret = -ECANCELED; bdrv_aio_cancel(data[i].aiocb); active--; num_canceled++; } } g_assert_cmpint(active, >, 0); g_assert_cmpint(num_canceled, <, 100); /* Canceling the others will be a blocking operation. */ for (i = 0; i < 100; i++) {
if (data[i].n != 3) {
bdrv_aio_cancel(data[i].aiocb); } } /* Finish execution and execute any remaining callbacks. */ qemu_aio_wait_all(); g_assert_cmpint(active, ==, 0); for (i = 0; i < 100; i++) {
if (data[i].n == 3) {
g_assert_cmpint(data[i].ret, ==, -ECANCELED); g_assert(data[i].aiocb != NULL); } else {
g_assert_cmpint(data[i].n, ==, 2); g_assert_cmpint(data[i].ret, ==, 0); g_assert(data[i].aiocb == NULL); } } } ",qemu,0
"static void mcf_fec_do_tx(mcf_fec_state *s)

{

    uint32_t addr;

    mcf_fec_bd bd;

    int frame_size;

    int len;

    uint8_t frame[FEC_MAX_FRAME_SIZE];

    uint8_t *ptr;



    DPRINTF(""do_tx\n"");

    ptr = frame;

    frame_size = 0;

    addr = s->tx_descriptor;

    while (1) {

        mcf_fec_read_bd(&bd, addr);

        DPRINTF(""tx_bd %x flags %04x len %d data %08x\n"",

                addr, bd.flags, bd.length, bd.data);

        if ((bd.flags & FEC_BD_R) == 0) {

            /* Run out of descriptors to transmit.  */

            break;

        }

        len = bd.length;

        if (frame_size + len > FEC_MAX_FRAME_SIZE) {

            len = FEC_MAX_FRAME_SIZE - frame_size;

            s->eir |= FEC_INT_BABT;

        }

        cpu_physical_memory_read(bd.data, ptr, len);

        ptr += len;

        frame_size += len;

        if (bd.flags & FEC_BD_L) {

            /* Last buffer in frame.  */

            DPRINTF(""Sending packet\n"");

            qemu_send_packet(qemu_get_queue(s->nic), frame, len);

            ptr = frame;

            frame_size = 0;

            s->eir |= FEC_INT_TXF;

        }

        s->eir |= FEC_INT_TXB;

        bd.flags &= ~FEC_BD_R;

        /* Write back the modified descriptor.  */

        mcf_fec_write_bd(&bd, addr);

        /* Advance to the next descriptor.  */

        if ((bd.flags & FEC_BD_W) != 0) {

            addr = s->etdsr;

        } else {

            addr += 8;

        }

    }

    s->tx_descriptor = addr;

}
","static void mcf_fec_do_tx(mcf_fec_state *s) {
uint32_t addr; mcf_fec_bd bd; int frame_size; int len; uint8_t frame[FEC_MAX_FRAME_SIZE]; uint8_t *ptr; DPRINTF(""do_tx\n""); ptr = frame; frame_size = 0; addr = s->tx_descriptor; while (1) {
mcf_fec_read_bd(&bd, addr); DPRINTF(""tx_bd %x flags %04x len %d data %08x\n"", addr, bd.flags, bd.length, bd.data); if ((bd.flags & FEC_BD_R) == 0) {
/* Run out of descriptors to transmit. */ break; } len = bd.length; if (frame_size + len > FEC_MAX_FRAME_SIZE) {
len = FEC_MAX_FRAME_SIZE - frame_size; s->eir |= FEC_INT_BABT; } cpu_physical_memory_read(bd.data, ptr, len); ptr += len; frame_size += len; if (bd.flags & FEC_BD_L) {
/* Last buffer in frame. */ DPRINTF(""Sending packet\n""); qemu_send_packet(qemu_get_queue(s->nic), frame, len); ptr = frame; frame_size = 0; s->eir |= FEC_INT_TXF; } s->eir |= FEC_INT_TXB; bd.flags &= ~FEC_BD_R; /* Write back the modified descriptor. */ mcf_fec_write_bd(&bd, addr); /* Advance to the next descriptor. */ if ((bd.flags & FEC_BD_W) != 0) {
addr = s->etdsr; } else {
addr += 8; } } s->tx_descriptor = addr; } ",qemu,0
"int audio_available(void)

{

#ifdef HAS_AUDIO

    return 1;

#else

    return 0;

#endif

}
","int audio_available(void) {
#ifdef HAS_AUDIO return 1; #else return 0; #endif } ",qemu,0
"static int emulated_initfn(CCIDCardState *base)

{

    EmulatedState *card = DO_UPCAST(EmulatedState, base, base);

    VCardEmulError ret;

    const EnumTable *ptable;



    QSIMPLEQ_INIT(&card->event_list);

    QSIMPLEQ_INIT(&card->guest_apdu_list);

    qemu_mutex_init(&card->event_list_mutex);

    qemu_mutex_init(&card->vreader_mutex);

    qemu_mutex_init(&card->handle_apdu_mutex);

    qemu_cond_init(&card->handle_apdu_cond);

    card->reader = NULL;

    card->quit_apdu_thread = 0;

    if (init_pipe_signaling(card) < 0) {

        return -1;

    }

    card->backend = parse_enumeration(card->backend_str, backend_enum_table, 0);

    if (card->backend == 0) {

        printf(""unknown backend, must be one of:\n"");

        for (ptable = backend_enum_table; ptable->name != NULL; ++ptable) {

            printf(""%s\n"", ptable->name);

        }

        return -1;

    }



    /* TODO: a passthru backened that works on local machine. third card type?*/

    if (card->backend == BACKEND_CERTIFICATES) {

        if (card->cert1 != NULL && card->cert2 != NULL && card->cert3 != NULL) {

            ret = emulated_initialize_vcard_from_certificates(card);

        } else {

            printf(""%s: you must provide all three certs for""

                   "" certificates backend\n"", EMULATED_DEV_NAME);

            return -1;

        }

    } else {

        if (card->backend != BACKEND_NSS_EMULATED) {

            printf(""%s: bad backend specified. The options are:\n%s (default),""

                "" %s.\n"", EMULATED_DEV_NAME, BACKEND_NSS_EMULATED_NAME,

                BACKEND_CERTIFICATES_NAME);

            return -1;

        }

        if (card->cert1 != NULL || card->cert2 != NULL || card->cert3 != NULL) {

            printf(""%s: unexpected cert parameters to nss emulated backend\n"",

                   EMULATED_DEV_NAME);

            return -1;

        }

        /* default to mirroring the local hardware readers */

        ret = wrap_vcard_emul_init(NULL);

    }

    if (ret != VCARD_EMUL_OK) {

        printf(""%s: failed to initialize vcard\n"", EMULATED_DEV_NAME);

        return -1;

    }

    qemu_thread_create(&card->event_thread_id, event_thread, card,

                       QEMU_THREAD_JOINABLE);

    qemu_thread_create(&card->apdu_thread_id, handle_apdu_thread, card,

                       QEMU_THREAD_JOINABLE);

    return 0;

}
","static int emulated_initfn(CCIDCardState *base) {
EmulatedState *card = DO_UPCAST(EmulatedState, base, base); VCardEmulError ret; const EnumTable *ptable; QSIMPLEQ_INIT(&card->event_list); QSIMPLEQ_INIT(&card->guest_apdu_list); qemu_mutex_init(&card->event_list_mutex); qemu_mutex_init(&card->vreader_mutex); qemu_mutex_init(&card->handle_apdu_mutex); qemu_cond_init(&card->handle_apdu_cond); card->reader = NULL; card->quit_apdu_thread = 0; if (init_pipe_signaling(card) < 0) {
return -1; } card->backend = parse_enumeration(card->backend_str, backend_enum_table, 0); if (card->backend == 0) {
printf(""unknown backend, must be one of:\n""); for (ptable = backend_enum_table; ptable->name != NULL; ++ptable) {
printf(""%s\n"", ptable->name); } return -1; } /* TODO: a passthru backened that works on local machine. third card type?*/ if (card->backend == BACKEND_CERTIFICATES) {
if (card->cert1 != NULL && card->cert2 != NULL && card->cert3 != NULL) {
ret = emulated_initialize_vcard_from_certificates(card); } else {
printf(""%s: you must provide all three certs for"" "" certificates backend\n"", EMULATED_DEV_NAME); return -1; } } else {
if (card->backend != BACKEND_NSS_EMULATED) {
printf(""%s: bad backend specified. The options are:\n%s (default),"" "" %s.\n"", EMULATED_DEV_NAME, BACKEND_NSS_EMULATED_NAME, BACKEND_CERTIFICATES_NAME); return -1; } if (card->cert1 != NULL || card->cert2 != NULL || card->cert3 != NULL) {
printf(""%s: unexpected cert parameters to nss emulated backend\n"", EMULATED_DEV_NAME); return -1; } /* default to mirroring the local hardware readers */ ret = wrap_vcard_emul_init(NULL); } if (ret != VCARD_EMUL_OK) {
printf(""%s: failed to initialize vcard\n"", EMULATED_DEV_NAME); return -1; } qemu_thread_create(&card->event_thread_id, event_thread, card, QEMU_THREAD_JOINABLE); qemu_thread_create(&card->apdu_thread_id, handle_apdu_thread, card, QEMU_THREAD_JOINABLE); return 0; } ",qemu,1
"static int qemu_rdma_registration_handle(QEMUFile *f, void *opaque,

                                         uint64_t flags)

{

    RDMAControlHeader reg_resp = { .len = sizeof(RDMARegisterResult),

                               .type = RDMA_CONTROL_REGISTER_RESULT,

                               .repeat = 0,

                             };

    RDMAControlHeader unreg_resp = { .len = 0,

                               .type = RDMA_CONTROL_UNREGISTER_FINISHED,

                               .repeat = 0,

                             };

    RDMAControlHeader blocks = { .type = RDMA_CONTROL_RAM_BLOCKS_RESULT,

                                 .repeat = 1 };

    QEMUFileRDMA *rfile = opaque;

    RDMAContext *rdma = rfile->rdma;

    RDMALocalBlocks *local = &rdma->local_ram_blocks;

    RDMAControlHeader head;

    RDMARegister *reg, *registers;

    RDMACompress *comp;

    RDMARegisterResult *reg_result;

    static RDMARegisterResult results[RDMA_CONTROL_MAX_COMMANDS_PER_MESSAGE];

    RDMALocalBlock *block;

    void *host_addr;

    int ret = 0;

    int idx = 0;

    int count = 0;

    int i = 0;



    CHECK_ERROR_STATE();



    do {

        DDDPRINTF(""Waiting for next request %"" PRIu64 ""...\n"", flags);



        ret = qemu_rdma_exchange_recv(rdma, &head, RDMA_CONTROL_NONE);



        if (ret < 0) {

            break;

        }



        if (head.repeat > RDMA_CONTROL_MAX_COMMANDS_PER_MESSAGE) {

            fprintf(stderr, ""rdma: Too many requests in this message (%d).""

                            ""Bailing.\n"", head.repeat);

            ret = -EIO;

            break;

        }



        switch (head.type) {

        case RDMA_CONTROL_COMPRESS:

            comp = (RDMACompress *) rdma->wr_data[idx].control_curr;

            network_to_compress(comp);



            DDPRINTF(""Zapping zero chunk: %"" PRId64

                    "" bytes, index %d, offset %"" PRId64 ""\n"",

                    comp->length, comp->block_idx, comp->offset);

            block = &(rdma->local_ram_blocks.block[comp->block_idx]);



            host_addr = block->local_host_addr +

                            (comp->offset - block->offset);



            ram_handle_compressed(host_addr, comp->value, comp->length);

            break;



        case RDMA_CONTROL_REGISTER_FINISHED:

            DDDPRINTF(""Current registrations complete.\n"");

            goto out;



        case RDMA_CONTROL_RAM_BLOCKS_REQUEST:

            DPRINTF(""Initial setup info requested.\n"");



            if (rdma->pin_all) {

                ret = qemu_rdma_reg_whole_ram_blocks(rdma);

                if (ret) {

                    fprintf(stderr, ""rdma migration: error dest ""

                                    ""registering ram blocks!\n"");

                    goto out;

                }

            }



            /*

             * Dest uses this to prepare to transmit the RAMBlock descriptions

             * to the source VM after connection setup.

             * Both sides use the ""remote"" structure to communicate and update

             * their ""local"" descriptions with what was sent.

             */

            for (i = 0; i < local->nb_blocks; i++) {

                rdma->block[i].remote_host_addr =

                    (uint64_t)(local->block[i].local_host_addr);



                if (rdma->pin_all) {

                    rdma->block[i].remote_rkey = local->block[i].mr->rkey;

                }



                rdma->block[i].offset = local->block[i].offset;

                rdma->block[i].length = local->block[i].length;



                remote_block_to_network(&rdma->block[i]);

            }



            blocks.len = rdma->local_ram_blocks.nb_blocks

                                                * sizeof(RDMARemoteBlock);





            ret = qemu_rdma_post_send_control(rdma,

                                        (uint8_t *) rdma->block, &blocks);



            if (ret < 0) {

                fprintf(stderr, ""rdma migration: error sending remote info!\n"");

                goto out;

            }



            break;

        case RDMA_CONTROL_REGISTER_REQUEST:

            DDPRINTF(""There are %d registration requests\n"", head.repeat);



            reg_resp.repeat = head.repeat;

            registers = (RDMARegister *) rdma->wr_data[idx].control_curr;



            for (count = 0; count < head.repeat; count++) {

                uint64_t chunk;

                uint8_t *chunk_start, *chunk_end;



                reg = &registers[count];

                network_to_register(reg);



                reg_result = &results[count];



                DDPRINTF(""Registration request (%d): index %d, current_addr %""

                         PRIu64 "" chunks: %"" PRIu64 ""\n"", count,

                         reg->current_index, reg->key.current_addr, reg->chunks);



                block = &(rdma->local_ram_blocks.block[reg->current_index]);

                if (block->is_ram_block) {

                    host_addr = (block->local_host_addr +

                                (reg->key.current_addr - block->offset));

                    chunk = ram_chunk_index(block->local_host_addr,

                                            (uint8_t *) host_addr);

                } else {

                    chunk = reg->key.chunk;

                    host_addr = block->local_host_addr +

                        (reg->key.chunk * (1UL << RDMA_REG_CHUNK_SHIFT));

                }

                chunk_start = ram_chunk_start(block, chunk);

                chunk_end = ram_chunk_end(block, chunk + reg->chunks);

                if (qemu_rdma_register_and_get_keys(rdma, block,

                            (uint8_t *)host_addr, NULL, &reg_result->rkey,

                            chunk, chunk_start, chunk_end)) {

                    fprintf(stderr, ""cannot get rkey!\n"");

                    ret = -EINVAL;

                    goto out;

                }



                reg_result->host_addr = (uint64_t) block->local_host_addr;



                DDPRINTF(""Registered rkey for this request: %x\n"",

                                reg_result->rkey);



                result_to_network(reg_result);

            }



            ret = qemu_rdma_post_send_control(rdma,

                            (uint8_t *) results, &reg_resp);



            if (ret < 0) {

                fprintf(stderr, ""Failed to send control buffer!\n"");

                goto out;

            }

            break;

        case RDMA_CONTROL_UNREGISTER_REQUEST:

            DDPRINTF(""There are %d unregistration requests\n"", head.repeat);

            unreg_resp.repeat = head.repeat;

            registers = (RDMARegister *) rdma->wr_data[idx].control_curr;



            for (count = 0; count < head.repeat; count++) {

                reg = &registers[count];

                network_to_register(reg);



                DDPRINTF(""Unregistration request (%d): ""

                         "" index %d, chunk %"" PRIu64 ""\n"",

                         count, reg->current_index, reg->key.chunk);



                block = &(rdma->local_ram_blocks.block[reg->current_index]);



                ret = ibv_dereg_mr(block->pmr[reg->key.chunk]);

                block->pmr[reg->key.chunk] = NULL;



                if (ret != 0) {

                    perror(""rdma unregistration chunk failed"");

                    ret = -ret;

                    goto out;

                }



                rdma->total_registrations--;



                DDPRINTF(""Unregistered chunk %"" PRIu64 "" successfully.\n"",

                            reg->key.chunk);

            }



            ret = qemu_rdma_post_send_control(rdma, NULL, &unreg_resp);



            if (ret < 0) {

                fprintf(stderr, ""Failed to send control buffer!\n"");

                goto out;

            }

            break;

        case RDMA_CONTROL_REGISTER_RESULT:

            fprintf(stderr, ""Invalid RESULT message at dest.\n"");

            ret = -EIO;

            goto out;

        default:

            fprintf(stderr, ""Unknown control message %s\n"",

                                control_desc[head.type]);

            ret = -EIO;

            goto out;

        }

    } while (1);

out:

    if (ret < 0) {

        rdma->error_state = ret;

    }

    return ret;

}
","static int qemu_rdma_registration_handle(QEMUFile *f, void *opaque, uint64_t flags) {
RDMAControlHeader reg_resp = {
.len = sizeof(RDMARegisterResult), .type = RDMA_CONTROL_REGISTER_RESULT, .repeat = 0, }; RDMAControlHeader unreg_resp = {
.len = 0, .type = RDMA_CONTROL_UNREGISTER_FINISHED, .repeat = 0, }; RDMAControlHeader blocks = {
.type = RDMA_CONTROL_RAM_BLOCKS_RESULT, .repeat = 1 }; QEMUFileRDMA *rfile = opaque; RDMAContext *rdma = rfile->rdma; RDMALocalBlocks *local = &rdma->local_ram_blocks; RDMAControlHeader head; RDMARegister *reg, *registers; RDMACompress *comp; RDMARegisterResult *reg_result; static RDMARegisterResult results[RDMA_CONTROL_MAX_COMMANDS_PER_MESSAGE]; RDMALocalBlock *block; void *host_addr; int ret = 0; int idx = 0; int count = 0; int i = 0; CHECK_ERROR_STATE(); do {
DDDPRINTF(""Waiting for next request %"" PRIu64 ""...\n"", flags); ret = qemu_rdma_exchange_recv(rdma, &head, RDMA_CONTROL_NONE); if (ret < 0) {
break; } if (head.repeat > RDMA_CONTROL_MAX_COMMANDS_PER_MESSAGE) {
fprintf(stderr, ""rdma: Too many requests in this message (%d)."" ""Bailing.\n"", head.repeat); ret = -EIO; break; } switch (head.type) {
case RDMA_CONTROL_COMPRESS: comp = (RDMACompress *) rdma->wr_data[idx].control_curr; network_to_compress(comp); DDPRINTF(""Zapping zero chunk: %"" PRId64 "" bytes, index %d, offset %"" PRId64 ""\n"", comp->length, comp->block_idx, comp->offset); block = &(rdma->local_ram_blocks.block[comp->block_idx]); host_addr = block->local_host_addr + (comp->offset - block->offset); ram_handle_compressed(host_addr, comp->value, comp->length); break; case RDMA_CONTROL_REGISTER_FINISHED: DDDPRINTF(""Current registrations complete.\n""); goto out; case RDMA_CONTROL_RAM_BLOCKS_REQUEST: DPRINTF(""Initial setup info requested.\n""); if (rdma->pin_all) {
ret = qemu_rdma_reg_whole_ram_blocks(rdma); if (ret) {
fprintf(stderr, ""rdma migration: error dest "" ""registering ram blocks!\n""); goto out; } } /* * Dest uses this to prepare to transmit the RAMBlock descriptions * to the source VM after connection setup. * Both sides use the ""remote"" structure to communicate and update * their ""local"" descriptions with what was sent. */ for (i = 0; i < local->nb_blocks; i++) {
rdma->block[i].remote_host_addr = (uint64_t)(local->block[i].local_host_addr); if (rdma->pin_all) {
rdma->block[i].remote_rkey = local->block[i].mr->rkey; } rdma->block[i].offset = local->block[i].offset; rdma->block[i].length = local->block[i].length; remote_block_to_network(&rdma->block[i]); } blocks.len = rdma->local_ram_blocks.nb_blocks * sizeof(RDMARemoteBlock); ret = qemu_rdma_post_send_control(rdma, (uint8_t *) rdma->block, &blocks); if (ret < 0) {
fprintf(stderr, ""rdma migration: error sending remote info!\n""); goto out; } break; case RDMA_CONTROL_REGISTER_REQUEST: DDPRINTF(""There are %d registration requests\n"", head.repeat); reg_resp.repeat = head.repeat; registers = (RDMARegister *) rdma->wr_data[idx].control_curr; for (count = 0; count < head.repeat; count++) {
uint64_t chunk; uint8_t *chunk_start, *chunk_end; reg = &registers[count]; network_to_register(reg); reg_result = &results[count]; DDPRINTF(""Registration request (%d): index %d, current_addr %"" PRIu64 "" chunks: %"" PRIu64 ""\n"", count, reg->current_index, reg->key.current_addr, reg->chunks); block = &(rdma->local_ram_blocks.block[reg->current_index]); if (block->is_ram_block) {
host_addr = (block->local_host_addr + (reg->key.current_addr - block->offset)); chunk = ram_chunk_index(block->local_host_addr, (uint8_t *) host_addr); } else {
chunk = reg->key.chunk; host_addr = block->local_host_addr + (reg->key.chunk * (1UL << RDMA_REG_CHUNK_SHIFT)); } chunk_start = ram_chunk_start(block, chunk); chunk_end = ram_chunk_end(block, chunk + reg->chunks); if (qemu_rdma_register_and_get_keys(rdma, block, (uint8_t *)host_addr, NULL, &reg_result->rkey, chunk, chunk_start, chunk_end)) {
fprintf(stderr, ""cannot get rkey!\n""); ret = -EINVAL; goto out; } reg_result->host_addr = (uint64_t) block->local_host_addr; DDPRINTF(""Registered rkey for this request: %x\n"", reg_result->rkey); result_to_network(reg_result); } ret = qemu_rdma_post_send_control(rdma, (uint8_t *) results, &reg_resp); if (ret < 0) {
fprintf(stderr, ""Failed to send control buffer!\n""); goto out; } break; case RDMA_CONTROL_UNREGISTER_REQUEST: DDPRINTF(""There are %d unregistration requests\n"", head.repeat); unreg_resp.repeat = head.repeat; registers = (RDMARegister *) rdma->wr_data[idx].control_curr; for (count = 0; count < head.repeat; count++) {
reg = &registers[count]; network_to_register(reg); DDPRINTF(""Unregistration request (%d): "" "" index %d, chunk %"" PRIu64 ""\n"", count, reg->current_index, reg->key.chunk); block = &(rdma->local_ram_blocks.block[reg->current_index]); ret = ibv_dereg_mr(block->pmr[reg->key.chunk]); block->pmr[reg->key.chunk] = NULL; if (ret != 0) {
perror(""rdma unregistration chunk failed""); ret = -ret; goto out; } rdma->total_registrations--; DDPRINTF(""Unregistered chunk %"" PRIu64 "" successfully.\n"", reg->key.chunk); } ret = qemu_rdma_post_send_control(rdma, NULL, &unreg_resp); if (ret < 0) {
fprintf(stderr, ""Failed to send control buffer!\n""); goto out; } break; case RDMA_CONTROL_REGISTER_RESULT: fprintf(stderr, ""Invalid RESULT message at dest.\n""); ret = -EIO; goto out; default: fprintf(stderr, ""Unknown control message %s\n"", control_desc[head.type]); ret = -EIO; goto out; } } while (1); out: if (ret < 0) {
rdma->error_state = ret; } return ret; } ",qemu,1
"VirtIODevice *virtio_serial_init(DeviceState *dev, uint32_t max_nr_ports)

{

    VirtIOSerial *vser;

    VirtIODevice *vdev;

    uint32_t i;



    if (!max_nr_ports)

        return NULL;



    vdev = virtio_common_init(""virtio-serial"", VIRTIO_ID_CONSOLE,

                              sizeof(struct virtio_console_config),

                              sizeof(VirtIOSerial));



    vser = DO_UPCAST(VirtIOSerial, vdev, vdev);



    /* Spawn a new virtio-serial bus on which the ports will ride as devices */

    vser->bus = virtser_bus_new(dev);

    vser->bus->vser = vser;

    QTAILQ_INIT(&vser->ports);



    vser->bus->max_nr_ports = max_nr_ports;

    vser->ivqs = qemu_malloc(max_nr_ports * sizeof(VirtQueue *));

    vser->ovqs = qemu_malloc(max_nr_ports * sizeof(VirtQueue *));



    /* Add a queue for host to guest transfers for port 0 (backward compat) */

    vser->ivqs[0] = virtio_add_queue(vdev, 128, handle_input);

    /* Add a queue for guest to host transfers for port 0 (backward compat) */

    vser->ovqs[0] = virtio_add_queue(vdev, 128, handle_output);



    /* control queue: host to guest */

    vser->c_ivq = virtio_add_queue(vdev, 16, control_in);

    /* control queue: guest to host */

    vser->c_ovq = virtio_add_queue(vdev, 16, control_out);



    for (i = 1; i < vser->bus->max_nr_ports; i++) {

        /* Add a per-port queue for host to guest transfers */

        vser->ivqs[i] = virtio_add_queue(vdev, 128, handle_input);

        /* Add a per-per queue for guest to host transfers */

        vser->ovqs[i] = virtio_add_queue(vdev, 128, handle_output);

    }



    vser->config.max_nr_ports = max_nr_ports;

    vser->ports_map = qemu_mallocz(((max_nr_ports + 31) / 32)

        * sizeof(vser->ports_map[0]));

    /*

     * Reserve location 0 for a console port for backward compat

     * (old kernel, new qemu)

     */

    mark_port_added(vser, 0);



    vser->vdev.get_features = get_features;

    vser->vdev.get_config = get_config;

    vser->vdev.set_config = set_config;



    /*

     * Register for the savevm section with the virtio-console name

     * to preserve backward compat

     */

    register_savevm(dev, ""virtio-console"", -1, 2, virtio_serial_save,

                    virtio_serial_load, vser);



    return vdev;

}
","VirtIODevice *virtio_serial_init(DeviceState *dev, uint32_t max_nr_ports) {
VirtIOSerial *vser; VirtIODevice *vdev; uint32_t i; if (!max_nr_ports) return NULL; vdev = virtio_common_init(""virtio-serial"", VIRTIO_ID_CONSOLE, sizeof(struct virtio_console_config), sizeof(VirtIOSerial)); vser = DO_UPCAST(VirtIOSerial, vdev, vdev); /* Spawn a new virtio-serial bus on which the ports will ride as devices */ vser->bus = virtser_bus_new(dev); vser->bus->vser = vser; QTAILQ_INIT(&vser->ports); vser->bus->max_nr_ports = max_nr_ports; vser->ivqs = qemu_malloc(max_nr_ports * sizeof(VirtQueue *)); vser->ovqs = qemu_malloc(max_nr_ports * sizeof(VirtQueue *)); /* Add a queue for host to guest transfers for port 0 (backward compat) */ vser->ivqs[0] = virtio_add_queue(vdev, 128, handle_input); /* Add a queue for guest to host transfers for port 0 (backward compat) */ vser->ovqs[0] = virtio_add_queue(vdev, 128, handle_output); /* control queue: host to guest */ vser->c_ivq = virtio_add_queue(vdev, 16, control_in); /* control queue: guest to host */ vser->c_ovq = virtio_add_queue(vdev, 16, control_out); for (i = 1; i < vser->bus->max_nr_ports; i++) {
/* Add a per-port queue for host to guest transfers */ vser->ivqs[i] = virtio_add_queue(vdev, 128, handle_input); /* Add a per-per queue for guest to host transfers */ vser->ovqs[i] = virtio_add_queue(vdev, 128, handle_output); } vser->config.max_nr_ports = max_nr_ports; vser->ports_map = qemu_mallocz(((max_nr_ports + 31) / 32) * sizeof(vser->ports_map[0])); /* * Reserve location 0 for a console port for backward compat * (old kernel, new qemu) */ mark_port_added(vser, 0); vser->vdev.get_features = get_features; vser->vdev.get_config = get_config; vser->vdev.set_config = set_config; /* * Register for the savevm section with the virtio-console name * to preserve backward compat */ register_savevm(dev, ""virtio-console"", -1, 2, virtio_serial_save, virtio_serial_load, vser); return vdev; } ",qemu,0
"static void gen_cli(DisasContext *ctx)

{

    /* Cache line invalidate: privileged and treated as no-op */

#if defined(CONFIG_USER_ONLY)

    gen_inval_exception(ctx, POWERPC_EXCP_PRIV_OPC);

#else

    if (unlikely(ctx->pr)) {

        gen_inval_exception(ctx, POWERPC_EXCP_PRIV_OPC);

        return;

    }

#endif

}
","static void gen_cli(DisasContext *ctx) {
/* Cache line invalidate: privileged and treated as no-op */ #if defined(CONFIG_USER_ONLY) gen_inval_exception(ctx, POWERPC_EXCP_PRIV_OPC); #else if (unlikely(ctx->pr)) {
gen_inval_exception(ctx, POWERPC_EXCP_PRIV_OPC); return; } #endif } ",qemu,1
"static int ipmi_register_netfn(IPMIBmcSim *s, unsigned int netfn,

                               const IPMINetfn *netfnd)

{

    if ((netfn & 1) || (netfn > MAX_NETFNS) || (s->netfns[netfn / 2])) {

        return -1;

    }

    s->netfns[netfn / 2] = netfnd;

    return 0;

}
","static int ipmi_register_netfn(IPMIBmcSim *s, unsigned int netfn, const IPMINetfn *netfnd) {
if ((netfn & 1) || (netfn > MAX_NETFNS) || (s->netfns[netfn / 2])) {
return -1; } s->netfns[netfn / 2] = netfnd; return 0; } ",qemu,1
"static void boston_flash_write(void *opaque, hwaddr addr,

                               uint64_t val, unsigned size)

{

}
","static void boston_flash_write(void *opaque, hwaddr addr, uint64_t val, unsigned size) {
} ",qemu,1
"sPAPRTCETable *spapr_tce_new_table(DeviceState *owner, uint32_t liobn, size_t window_size)

{

    sPAPRTCETable *tcet;



    if (spapr_tce_find_by_liobn(liobn)) {

        fprintf(stderr, ""Attempted to create TCE table with duplicate""

                "" LIOBN 0x%x\n"", liobn);

        return NULL;

    }



    if (!window_size) {

        return NULL;

    }



    tcet = g_malloc0(sizeof(*tcet));

    tcet->liobn = liobn;

    tcet->window_size = window_size;



    if (kvm_enabled()) {

        tcet->table = kvmppc_create_spapr_tce(liobn,

                                              window_size,

                                              &tcet->fd);

    }



    if (!tcet->table) {

        size_t table_size = (window_size >> SPAPR_TCE_PAGE_SHIFT)

            * sizeof(sPAPRTCE);

        tcet->table = g_malloc0(table_size);

    }



#ifdef DEBUG_TCE

    fprintf(stderr, ""spapr_iommu: New TCE table @ %p, liobn=0x%x, ""

            ""table @ %p, fd=%d\n"", tcet, liobn, tcet->table, tcet->fd);

#endif



    memory_region_init_iommu(&tcet->iommu, OBJECT(owner), &spapr_iommu_ops,

                             ""iommu-spapr"", UINT64_MAX);



    QLIST_INSERT_HEAD(&spapr_tce_tables, tcet, list);



    return tcet;

}
","sPAPRTCETable *spapr_tce_new_table(DeviceState *owner, uint32_t liobn, size_t window_size) {
sPAPRTCETable *tcet; if (spapr_tce_find_by_liobn(liobn)) {
fprintf(stderr, ""Attempted to create TCE table with duplicate"" "" LIOBN 0x%x\n"", liobn); return NULL; } if (!window_size) {
return NULL; } tcet = g_malloc0(sizeof(*tcet)); tcet->liobn = liobn; tcet->window_size = window_size; if (kvm_enabled()) {
tcet->table = kvmppc_create_spapr_tce(liobn, window_size, &tcet->fd); } if (!tcet->table) {
size_t table_size = (window_size >> SPAPR_TCE_PAGE_SHIFT) * sizeof(sPAPRTCE); tcet->table = g_malloc0(table_size); } #ifdef DEBUG_TCE fprintf(stderr, ""spapr_iommu: New TCE table @ %p, liobn=0x%x, "" ""table @ %p, fd=%d\n"", tcet, liobn, tcet->table, tcet->fd); #endif memory_region_init_iommu(&tcet->iommu, OBJECT(owner), &spapr_iommu_ops, ""iommu-spapr"", UINT64_MAX); QLIST_INSERT_HEAD(&spapr_tce_tables, tcet, list); return tcet; } ",qemu,0
"START_TEST(qint_from_int_test)

{

    QInt *qi;

    const int value = -42;



    qi = qint_from_int(value);

    fail_unless(qi != NULL);

    fail_unless(qi->value == value);

    fail_unless(qi->base.refcnt == 1);

    fail_unless(qobject_type(QOBJECT(qi)) == QTYPE_QINT);



    // destroy doesn't exit yet

    g_free(qi);

}
","START_TEST(qint_from_int_test) {
QInt *qi; const int value = -42; qi = qint_from_int(value); fail_unless(qi != NULL); fail_unless(qi->value == value); fail_unless(qi->base.refcnt == 1); fail_unless(qobject_type(QOBJECT(qi)) == QTYPE_QINT); // destroy doesn't exit yet g_free(qi); } ",qemu,0
"void acpi_pm_tmr_init(ACPIREGS *ar, acpi_update_sci_fn update_sci,

                      MemoryRegion *parent)

{

    ar->tmr.update_sci = update_sci;

    ar->tmr.timer = timer_new_ns(QEMU_CLOCK_VIRTUAL, acpi_pm_tmr_timer, ar);

    memory_region_init_io(&ar->tmr.io, memory_region_owner(parent),

                          &acpi_pm_tmr_ops, ar, ""acpi-tmr"", 4);

    memory_region_clear_global_locking(&ar->tmr.io);

    memory_region_add_subregion(parent, 8, &ar->tmr.io);

}
","void acpi_pm_tmr_init(ACPIREGS *ar, acpi_update_sci_fn update_sci, MemoryRegion *parent) {
ar->tmr.update_sci = update_sci; ar->tmr.timer = timer_new_ns(QEMU_CLOCK_VIRTUAL, acpi_pm_tmr_timer, ar); memory_region_init_io(&ar->tmr.io, memory_region_owner(parent), &acpi_pm_tmr_ops, ar, ""acpi-tmr"", 4); memory_region_clear_global_locking(&ar->tmr.io); memory_region_add_subregion(parent, 8, &ar->tmr.io); } ",qemu,0
"static void virtser_port_device_realize(DeviceState *dev, Error **errp)

{

    VirtIOSerialPort *port = VIRTIO_SERIAL_PORT(dev);

    VirtIOSerialPortClass *vsc = VIRTIO_SERIAL_PORT_GET_CLASS(port);

    VirtIOSerialBus *bus = VIRTIO_SERIAL_BUS(qdev_get_parent_bus(dev));

    VirtIODevice *vdev = VIRTIO_DEVICE(bus->vser);

    int max_nr_ports;

    bool plugging_port0;

    Error *err = NULL;



    port->vser = bus->vser;

    port->bh = qemu_bh_new(flush_queued_data_bh, port);



    assert(vsc->have_data);



    /*

     * Is the first console port we're seeing? If so, put it up at

     * location 0. This is done for backward compatibility (old

     * kernel, new qemu).

     */

    plugging_port0 = vsc->is_console && !find_port_by_id(port->vser, 0);



    if (find_port_by_id(port->vser, port->id)) {

        error_setg(errp, ""virtio-serial-bus: A port already exists at id %u"",

                   port->id);

        return;

    }



    if (find_port_by_name(port->name)) {

        error_setg(errp, ""virtio-serial-bus: A port already exists by name %s"",

                   port->name);

        return;

    }



    if (port->id == VIRTIO_CONSOLE_BAD_ID) {

        if (plugging_port0) {

            port->id = 0;

        } else {

            port->id = find_free_port_id(port->vser);

            if (port->id == VIRTIO_CONSOLE_BAD_ID) {

                error_setg(errp, ""virtio-serial-bus: Maximum port limit for ""

                                 ""this device reached"");

                return;

            }

        }

    }



    max_nr_ports = virtio_tswap32(vdev, port->vser->config.max_nr_ports);

    if (port->id >= max_nr_ports) {

        error_setg(errp, ""virtio-serial-bus: Out-of-range port id specified, ""

                         ""max. allowed: %u"", max_nr_ports - 1);

        return;

    }



    vsc->realize(dev, &err);

    if (err != NULL) {

        error_propagate(errp, err);

        return;

    }



    port->elem.out_num = 0;

}
","static void virtser_port_device_realize(DeviceState *dev, Error **errp) {
VirtIOSerialPort *port = VIRTIO_SERIAL_PORT(dev); VirtIOSerialPortClass *vsc = VIRTIO_SERIAL_PORT_GET_CLASS(port); VirtIOSerialBus *bus = VIRTIO_SERIAL_BUS(qdev_get_parent_bus(dev)); VirtIODevice *vdev = VIRTIO_DEVICE(bus->vser); int max_nr_ports; bool plugging_port0; Error *err = NULL; port->vser = bus->vser; port->bh = qemu_bh_new(flush_queued_data_bh, port); assert(vsc->have_data); /* * Is the first console port we're seeing? If so, put it up at * location 0. This is done for backward compatibility (old * kernel, new qemu). */ plugging_port0 = vsc->is_console && !find_port_by_id(port->vser, 0); if (find_port_by_id(port->vser, port->id)) {
error_setg(errp, ""virtio-serial-bus: A port already exists at id %u"", port->id); return; } if (find_port_by_name(port->name)) {
error_setg(errp, ""virtio-serial-bus: A port already exists by name %s"", port->name); return; } if (port->id == VIRTIO_CONSOLE_BAD_ID) {
if (plugging_port0) {
port->id = 0; } else {
port->id = find_free_port_id(port->vser); if (port->id == VIRTIO_CONSOLE_BAD_ID) {
error_setg(errp, ""virtio-serial-bus: Maximum port limit for "" ""this device reached""); return; } } } max_nr_ports = virtio_tswap32(vdev, port->vser->config.max_nr_ports); if (port->id >= max_nr_ports) {
error_setg(errp, ""virtio-serial-bus: Out-of-range port id specified, "" ""max. allowed: %u"", max_nr_ports - 1); return; } vsc->realize(dev, &err); if (err != NULL) {
error_propagate(errp, err); return; } port->elem.out_num = 0; } ",qemu,1
"static uint64_t ac97_read(void *opaque, target_phys_addr_t addr,

                          unsigned size)

{

    MilkymistAC97State *s = opaque;

    uint32_t r = 0;



    addr >>= 2;

    switch (addr) {

    case R_AC97_CTRL:

    case R_AC97_ADDR:

    case R_AC97_DATAOUT:

    case R_AC97_DATAIN:

    case R_D_CTRL:

    case R_D_ADDR:

    case R_D_REMAINING:

    case R_U_CTRL:

    case R_U_ADDR:

    case R_U_REMAINING:

        r = s->regs[addr];

        break;



    default:

        error_report(""milkymist_ac97: read access to unknown register 0x""

                TARGET_FMT_plx, addr << 2);

        break;

    }



    trace_milkymist_ac97_memory_read(addr << 2, r);



    return r;

}
","static uint64_t ac97_read(void *opaque, target_phys_addr_t addr, unsigned size) {
MilkymistAC97State *s = opaque; uint32_t r = 0; addr >>= 2; switch (addr) {
case R_AC97_CTRL: case R_AC97_ADDR: case R_AC97_DATAOUT: case R_AC97_DATAIN: case R_D_CTRL: case R_D_ADDR: case R_D_REMAINING: case R_U_CTRL: case R_U_ADDR: case R_U_REMAINING: r = s->regs[addr]; break; default: error_report(""milkymist_ac97: read access to unknown register 0x"" TARGET_FMT_plx, addr << 2); break; } trace_milkymist_ac97_memory_read(addr << 2, r); return r; } ",qemu,0
"void nbd_client_new(NBDExport *exp, int csock, void (*close_fn)(NBDClient *))

{

    NBDClient *client;

    client = g_malloc0(sizeof(NBDClient));

    client->refcount = 1;

    client->exp = exp;

    client->sock = csock;

    client->can_read = true;

    if (nbd_send_negotiate(client)) {

        shutdown(client->sock, 2);

        close_fn(client);

        return;

    }

    client->close = close_fn;

    qemu_co_mutex_init(&client->send_lock);

    nbd_set_handlers(client);



    if (exp) {

        QTAILQ_INSERT_TAIL(&exp->clients, client, next);

        nbd_export_get(exp);

    }

}
","void nbd_client_new(NBDExport *exp, int csock, void (*close_fn)(NBDClient *)) {
NBDClient *client; client = g_malloc0(sizeof(NBDClient)); client->refcount = 1; client->exp = exp; client->sock = csock; client->can_read = true; if (nbd_send_negotiate(client)) {
shutdown(client->sock, 2); close_fn(client); return; } client->close = close_fn; qemu_co_mutex_init(&client->send_lock); nbd_set_handlers(client); if (exp) {
QTAILQ_INSERT_TAIL(&exp->clients, client, next); nbd_export_get(exp); } } ",qemu,0
"static void uart_write(void *opaque, target_phys_addr_t addr, uint64_t value,

                       unsigned size)

{

    MilkymistUartState *s = opaque;

    unsigned char ch = value;



    trace_milkymist_uart_memory_write(addr, value);



    addr >>= 2;

    switch (addr) {

    case R_RXTX:

        if (s->chr) {

            qemu_chr_fe_write(s->chr, &ch, 1);

        }

        s->regs[R_STAT] |= STAT_TX_EVT;

        break;

    case R_DIV:

    case R_CTRL:

    case R_DBG:

        s->regs[addr] = value;

        break;



    case R_STAT:

        /* write one to clear bits */

        s->regs[addr] &= ~(value & (STAT_RX_EVT | STAT_TX_EVT));

        break;



    default:

        error_report(""milkymist_uart: write access to unknown register 0x""

                TARGET_FMT_plx, addr << 2);

        break;

    }



    uart_update_irq(s);

}
","static void uart_write(void *opaque, target_phys_addr_t addr, uint64_t value, unsigned size) {
MilkymistUartState *s = opaque; unsigned char ch = value; trace_milkymist_uart_memory_write(addr, value); addr >>= 2; switch (addr) {
case R_RXTX: if (s->chr) {
qemu_chr_fe_write(s->chr, &ch, 1); } s->regs[R_STAT] |= STAT_TX_EVT; break; case R_DIV: case R_CTRL: case R_DBG: s->regs[addr] = value; break; case R_STAT: /* write one to clear bits */ s->regs[addr] &= ~(value & (STAT_RX_EVT | STAT_TX_EVT)); break; default: error_report(""milkymist_uart: write access to unknown register 0x"" TARGET_FMT_plx, addr << 2); break; } uart_update_irq(s); } ",qemu,0
"long do_sigreturn(CPUMIPSState *regs)

{

    struct sigframe *frame;

    abi_ulong frame_addr;

    sigset_t blocked;

    target_sigset_t target_set;

    int i;



#if defined(DEBUG_SIGNAL)

    fprintf(stderr, ""do_sigreturn\n"");

#endif

    frame_addr = regs->active_tc.gpr[29];

    if (!lock_user_struct(VERIFY_READ, frame, frame_addr, 1))

   	goto badframe;



    for(i = 0; i < TARGET_NSIG_WORDS; i++) {

   	if(__get_user(target_set.sig[i], &frame->sf_mask.sig[i]))

	    goto badframe;

    }



    target_to_host_sigset_internal(&blocked, &target_set);

    sigprocmask(SIG_SETMASK, &blocked, NULL);



    if (restore_sigcontext(regs, &frame->sf_sc))

   	goto badframe;



#if 0

    /*

     * Don't let your children do this ...

     */

    __asm__ __volatile__(

   	""move\t$29, %0\n\t""

   	""j\tsyscall_exit""

   	:/* no outputs */

   	:""r"" (&regs));

    /* Unreached */

#endif



    regs->active_tc.PC = regs->CP0_EPC;

    mips_set_hflags_isa_mode_from_pc(regs);

    /* I am not sure this is right, but it seems to work

    * maybe a problem with nested signals ? */

    regs->CP0_EPC = 0;

    return -TARGET_QEMU_ESIGRETURN;



badframe:

    force_sig(TARGET_SIGSEGV/*, current*/);

    return 0;

}
","long do_sigreturn(CPUMIPSState *regs) {
struct sigframe *frame; abi_ulong frame_addr; sigset_t blocked; target_sigset_t target_set; int i; #if defined(DEBUG_SIGNAL) fprintf(stderr, ""do_sigreturn\n""); #endif frame_addr = regs->active_tc.gpr[29]; if (!lock_user_struct(VERIFY_READ, frame, frame_addr, 1)) goto badframe; for(i = 0; i < TARGET_NSIG_WORDS; i++) {
if(__get_user(target_set.sig[i], &frame->sf_mask.sig[i])) goto badframe; } target_to_host_sigset_internal(&blocked, &target_set); sigprocmask(SIG_SETMASK, &blocked, NULL); if (restore_sigcontext(regs, &frame->sf_sc)) goto badframe; #if 0 /* * Don't let your children do this ... */ __asm__ __volatile__( ""move\t$29, %0\n\t"" ""j\tsyscall_exit"" :/* no outputs */ :""r"" (&regs)); /* Unreached */ #endif regs->active_tc.PC = regs->CP0_EPC; mips_set_hflags_isa_mode_from_pc(regs); /* I am not sure this is right, but it seems to work * maybe a problem with nested signals ? */ regs->CP0_EPC = 0; return -TARGET_QEMU_ESIGRETURN; badframe: force_sig(TARGET_SIGSEGV/*, current*/); return 0; } ",qemu,0
"void do_info_vnc(void)

{

    if (vnc_state == NULL)

	term_printf(""VNC server disabled\n"");

    else {

	term_printf(""VNC server active on: "");

	term_print_filename(vnc_state->display);

	term_printf(""\n"");



	if (vnc_state->csock == -1)

	    term_printf(""No client connected\n"");

	else

	    term_printf(""Client connected\n"");

    }

}
","void do_info_vnc(void) {
if (vnc_state == NULL) term_printf(""VNC server disabled\n""); else {
term_printf(""VNC server active on: ""); term_print_filename(vnc_state->display); term_printf(""\n""); if (vnc_state->csock == -1) term_printf(""No client connected\n""); else term_printf(""Client connected\n""); } } ",qemu,1
"void helper_ldq_kernel(uint64_t t0, uint64_t t1)

{

    ldq_kernel(t1, t0);

}
","void helper_ldq_kernel(uint64_t t0, uint64_t t1) {
ldq_kernel(t1, t0); } ",qemu,0
"static void mb_cpu_class_init(ObjectClass *oc, void *data)
{
    DeviceClass *dc = DEVICE_CLASS(oc);
    CPUClass *cc = CPU_CLASS(oc);
    MicroBlazeCPUClass *mcc = MICROBLAZE_CPU_CLASS(oc);
    mcc->parent_realize = dc->realize;
    dc->realize = mb_cpu_realizefn;
    mcc->parent_reset = cc->reset;
    cc->reset = mb_cpu_reset;
    cc->has_work = mb_cpu_has_work;
    cc->do_interrupt = mb_cpu_do_interrupt;
    cc->cpu_exec_interrupt = mb_cpu_exec_interrupt;
    cc->dump_state = mb_cpu_dump_state;
    cc->set_pc = mb_cpu_set_pc;
    cc->gdb_read_register = mb_cpu_gdb_read_register;
    cc->gdb_write_register = mb_cpu_gdb_write_register;
#ifdef CONFIG_USER_ONLY
    cc->handle_mmu_fault = mb_cpu_handle_mmu_fault;
#else
    cc->do_unassigned_access = mb_cpu_unassigned_access;
    cc->get_phys_page_debug = mb_cpu_get_phys_page_debug;
#endif
    dc->vmsd = &vmstate_mb_cpu;
    dc->props = mb_properties;
    cc->gdb_num_core_regs = 32 + 5;
    cc->disas_set_info = mb_disas_set_info;
}","static void mb_cpu_class_init(ObjectClass *oc, void *data) {
DeviceClass *dc = DEVICE_CLASS(oc); CPUClass *cc = CPU_CLASS(oc); MicroBlazeCPUClass *mcc = MICROBLAZE_CPU_CLASS(oc); mcc->parent_realize = dc->realize; dc->realize = mb_cpu_realizefn; mcc->parent_reset = cc->reset; cc->reset = mb_cpu_reset; cc->has_work = mb_cpu_has_work; cc->do_interrupt = mb_cpu_do_interrupt; cc->cpu_exec_interrupt = mb_cpu_exec_interrupt; cc->dump_state = mb_cpu_dump_state; cc->set_pc = mb_cpu_set_pc; cc->gdb_read_register = mb_cpu_gdb_read_register; cc->gdb_write_register = mb_cpu_gdb_write_register; #ifdef CONFIG_USER_ONLY cc->handle_mmu_fault = mb_cpu_handle_mmu_fault; #else cc->do_unassigned_access = mb_cpu_unassigned_access; cc->get_phys_page_debug = mb_cpu_get_phys_page_debug; #endif dc->vmsd = &vmstate_mb_cpu; dc->props = mb_properties; cc->gdb_num_core_regs = 32 + 5; cc->disas_set_info = mb_disas_set_info; }",qemu,1
"static void get_offset_range(hwaddr phys_addr,

                             ram_addr_t mapping_length,

                             DumpState *s,

                             hwaddr *p_offset,

                             hwaddr *p_filesz)

{

    RAMBlock *block;

    hwaddr offset = s->memory_offset;

    int64_t size_in_block, start;



    /* When the memory is not stored into vmcore, offset will be -1 */

    *p_offset = -1;

    *p_filesz = 0;



    if (s->has_filter) {

        if (phys_addr < s->begin || phys_addr >= s->begin + s->length) {

            return;

        }

    }



    QTAILQ_FOREACH(block, &ram_list.blocks, next) {

        if (s->has_filter) {

            if (block->offset >= s->begin + s->length ||

                block->offset + block->length <= s->begin) {

                /* This block is out of the range */

                continue;

            }



            if (s->begin <= block->offset) {

                start = block->offset;

            } else {

                start = s->begin;

            }



            size_in_block = block->length - (start - block->offset);

            if (s->begin + s->length < block->offset + block->length) {

                size_in_block -= block->offset + block->length -

                                 (s->begin + s->length);

            }

        } else {

            start = block->offset;

            size_in_block = block->length;

        }



        if (phys_addr >= start && phys_addr < start + size_in_block) {

            *p_offset = phys_addr - start + offset;



            /* The offset range mapped from the vmcore file must not spill over

             * the RAMBlock, clamp it. The rest of the mapping will be

             * zero-filled in memory at load time; see

             * <http://refspecs.linuxbase.org/elf/gabi4+/ch5.pheader.html>.

             */

            *p_filesz = phys_addr + mapping_length <= start + size_in_block ?

                        mapping_length :

                        size_in_block - (phys_addr - start);

            return;

        }



        offset += size_in_block;

    }

}
","static void get_offset_range(hwaddr phys_addr, ram_addr_t mapping_length, DumpState *s, hwaddr *p_offset, hwaddr *p_filesz) {
RAMBlock *block; hwaddr offset = s->memory_offset; int64_t size_in_block, start; /* When the memory is not stored into vmcore, offset will be -1 */ *p_offset = -1; *p_filesz = 0; if (s->has_filter) {
if (phys_addr < s->begin || phys_addr >= s->begin + s->length) {
return; } } QTAILQ_FOREACH(block, &ram_list.blocks, next) {
if (s->has_filter) {
if (block->offset >= s->begin + s->length || block->offset + block->length <= s->begin) {
/* This block is out of the range */ continue; } if (s->begin <= block->offset) {
start = block->offset; } else {
start = s->begin; } size_in_block = block->length - (start - block->offset); if (s->begin + s->length < block->offset + block->length) {
size_in_block -= block->offset + block->length - (s->begin + s->length); } } else {
start = block->offset; size_in_block = block->length; } if (phys_addr >= start && phys_addr < start + size_in_block) {
*p_offset = phys_addr - start + offset; /* The offset range mapped from the vmcore file must not spill over * the RAMBlock, clamp it. The rest of the mapping will be * zero-filled in memory at load time; see * <http://refspecs.linuxbase.org/elf/gabi4+/ch5.pheader.html>. */ *p_filesz = phys_addr + mapping_length <= start + size_in_block ? mapping_length : size_in_block - (phys_addr - start); return; } offset += size_in_block; } } ",qemu,1
"static inline int cris_addc(int a, const int b)

{

	asm (""addc\t%1, %0\n"" : ""+r"" (a) : ""r"" (b));

	return a;

}
","static inline int cris_addc(int a, const int b) {
asm (""addc\t%1, %0\n"" : ""+r"" (a) : ""r"" (b)); return a; } ",qemu,1
"unsigned long hbitmap_iter_skip_words(HBitmapIter *hbi)

{

    size_t pos = hbi->pos;

    const HBitmap *hb = hbi->hb;

    unsigned i = HBITMAP_LEVELS - 1;



    unsigned long cur;

    do {

        cur = hbi->cur[--i];

        pos >>= BITS_PER_LEVEL;

    } while (cur == 0);



    /* Check for end of iteration.  We always use fewer than BITS_PER_LONG

     * bits in the level 0 bitmap; thus we can repurpose the most significant

     * bit as a sentinel.  The sentinel is set in hbitmap_alloc and ensures

     * that the above loop ends even without an explicit check on i.

     */



    if (i == 0 && cur == (1UL << (BITS_PER_LONG - 1))) {

        return 0;

    }

    for (; i < HBITMAP_LEVELS - 1; i++) {

        /* Shift back pos to the left, matching the right shifts above.

         * The index of this word's least significant set bit provides

         * the low-order bits.

         */

        pos = (pos << BITS_PER_LEVEL) + ffsl(cur) - 1;

        hbi->cur[i] = cur & (cur - 1);



        /* Set up next level for iteration.  */

        cur = hb->levels[i + 1][pos];

    }



    hbi->pos = pos;

    trace_hbitmap_iter_skip_words(hbi->hb, hbi, pos, cur);



    assert(cur);

    return cur;

}
","unsigned long hbitmap_iter_skip_words(HBitmapIter *hbi) {
size_t pos = hbi->pos; const HBitmap *hb = hbi->hb; unsigned i = HBITMAP_LEVELS - 1; unsigned long cur; do {
cur = hbi->cur[--i]; pos >>= BITS_PER_LEVEL; } while (cur == 0); /* Check for end of iteration. We always use fewer than BITS_PER_LONG * bits in the level 0 bitmap; thus we can repurpose the most significant * bit as a sentinel. The sentinel is set in hbitmap_alloc and ensures * that the above loop ends even without an explicit check on i. */ if (i == 0 && cur == (1UL << (BITS_PER_LONG - 1))) {
return 0; } for (; i < HBITMAP_LEVELS - 1; i++) {
/* Shift back pos to the left, matching the right shifts above. * The index of this word's least significant set bit provides * the low-order bits. */ pos = (pos << BITS_PER_LEVEL) + ffsl(cur) - 1; hbi->cur[i] = cur & (cur - 1); /* Set up next level for iteration. */ cur = hb->levels[i + 1][pos]; } hbi->pos = pos; trace_hbitmap_iter_skip_words(hbi->hb, hbi, pos, cur); assert(cur); return cur; } ",qemu,1
"void do_mulldo (void)

{

    int64_t th;

    uint64_t tl;



    muls64(&tl, &th, T0, T1);

    if (likely(th == 0)) {

        xer_ov = 0;

    } else {

        xer_ov = 1;

        xer_so = 1;

    }

    T0 = (int64_t)tl;

}
","void do_mulldo (void) {
int64_t th; uint64_t tl; muls64(&tl, &th, T0, T1); if (likely(th == 0)) {
xer_ov = 0; } else {
xer_ov = 1; xer_so = 1; } T0 = (int64_t)tl; } ",qemu,1
"void virtio_scsi_common_realize(DeviceState *dev, Error **errp)

{

    VirtIODevice *vdev = VIRTIO_DEVICE(dev);

    VirtIOSCSICommon *s = VIRTIO_SCSI_COMMON(dev);

    int i;



    virtio_init(vdev, ""virtio-scsi"", VIRTIO_ID_SCSI,

                sizeof(VirtIOSCSIConfig));



    s->cmd_vqs = g_malloc0(s->conf.num_queues * sizeof(VirtQueue *));

    s->sense_size = VIRTIO_SCSI_SENSE_SIZE;

    s->cdb_size = VIRTIO_SCSI_CDB_SIZE;



    s->ctrl_vq = virtio_add_queue(vdev, VIRTIO_SCSI_VQ_SIZE,

                                  virtio_scsi_handle_ctrl);

    s->event_vq = virtio_add_queue(vdev, VIRTIO_SCSI_VQ_SIZE,

                                   virtio_scsi_handle_event);

    for (i = 0; i < s->conf.num_queues; i++) {

        s->cmd_vqs[i] = virtio_add_queue(vdev, VIRTIO_SCSI_VQ_SIZE,

                                         virtio_scsi_handle_cmd);

    }

}
","void virtio_scsi_common_realize(DeviceState *dev, Error **errp) {
VirtIODevice *vdev = VIRTIO_DEVICE(dev); VirtIOSCSICommon *s = VIRTIO_SCSI_COMMON(dev); int i; virtio_init(vdev, ""virtio-scsi"", VIRTIO_ID_SCSI, sizeof(VirtIOSCSIConfig)); s->cmd_vqs = g_malloc0(s->conf.num_queues * sizeof(VirtQueue *)); s->sense_size = VIRTIO_SCSI_SENSE_SIZE; s->cdb_size = VIRTIO_SCSI_CDB_SIZE; s->ctrl_vq = virtio_add_queue(vdev, VIRTIO_SCSI_VQ_SIZE, virtio_scsi_handle_ctrl); s->event_vq = virtio_add_queue(vdev, VIRTIO_SCSI_VQ_SIZE, virtio_scsi_handle_event); for (i = 0; i < s->conf.num_queues; i++) {
s->cmd_vqs[i] = virtio_add_queue(vdev, VIRTIO_SCSI_VQ_SIZE, virtio_scsi_handle_cmd); } } ",qemu,1
"static void virtio_scsi_request_cancelled(SCSIRequest *r)

{

    VirtIOSCSIReq *req = r->hba_private;



    if (!req) {

        return;

    }

    if (req->dev->resetting) {

        req->resp.cmd->response = VIRTIO_SCSI_S_RESET;

    } else {

        req->resp.cmd->response = VIRTIO_SCSI_S_ABORTED;

    }

    virtio_scsi_complete_cmd_req(req);

}
","static void virtio_scsi_request_cancelled(SCSIRequest *r) {
VirtIOSCSIReq *req = r->hba_private; if (!req) {
return; } if (req->dev->resetting) {
req->resp.cmd->response = VIRTIO_SCSI_S_RESET; } else {
req->resp.cmd->response = VIRTIO_SCSI_S_ABORTED; } virtio_scsi_complete_cmd_req(req); } ",qemu,0
"static void multiwrite_user_cb(MultiwriteCB *mcb)

{

    int i;



    for (i = 0; i < mcb->num_callbacks; i++) {

        mcb->callbacks[i].cb(mcb->callbacks[i].opaque, mcb->error);

        qemu_free(mcb->callbacks[i].free_qiov);

        qemu_free(mcb->callbacks[i].free_buf);

    }

}
","static void multiwrite_user_cb(MultiwriteCB *mcb) {
int i; for (i = 0; i < mcb->num_callbacks; i++) {
mcb->callbacks[i].cb(mcb->callbacks[i].opaque, mcb->error); qemu_free(mcb->callbacks[i].free_qiov); qemu_free(mcb->callbacks[i].free_buf); } } ",qemu,1
"static int virtio_blk_device_init(VirtIODevice *vdev)

{

    DeviceState *qdev = DEVICE(vdev);

    VirtIOBlock *s = VIRTIO_BLK(vdev);

    VirtIOBlkConf *blk = &(s->blk);

    static int virtio_blk_id;



    if (!blk->conf.bs) {

        error_report(""drive property not set"");

        return -1;

    }

    if (!bdrv_is_inserted(blk->conf.bs)) {

        error_report(""Device needs media, but drive is empty"");

        return -1;

    }



    blkconf_serial(&blk->conf, &blk->serial);

    if (blkconf_geometry(&blk->conf, NULL, 65535, 255, 255) < 0) {

        return -1;

    }



    virtio_init(vdev, ""virtio-blk"", VIRTIO_ID_BLOCK,

                sizeof(struct virtio_blk_config));



    vdev->get_config = virtio_blk_update_config;

    vdev->set_config = virtio_blk_set_config;

    vdev->get_features = virtio_blk_get_features;

    vdev->set_status = virtio_blk_set_status;

    vdev->reset = virtio_blk_reset;

    s->bs = blk->conf.bs;

    s->conf = &blk->conf;

    memcpy(&(s->blk), blk, sizeof(struct VirtIOBlkConf));

    s->rq = NULL;

    s->sector_mask = (s->conf->logical_block_size / BDRV_SECTOR_SIZE) - 1;



    s->vq = virtio_add_queue(vdev, 128, virtio_blk_handle_output);

#ifdef CONFIG_VIRTIO_BLK_DATA_PLANE

    if (!virtio_blk_data_plane_create(vdev, blk, &s->dataplane)) {

        virtio_cleanup(vdev);

        return -1;

    }

#endif



    s->change = qemu_add_vm_change_state_handler(virtio_blk_dma_restart_cb, s);

    register_savevm(qdev, ""virtio-blk"", virtio_blk_id++, 2,

                    virtio_blk_save, virtio_blk_load, s);

    bdrv_set_dev_ops(s->bs, &virtio_block_ops, s);

    bdrv_set_buffer_alignment(s->bs, s->conf->logical_block_size);



    bdrv_iostatus_enable(s->bs);



    add_boot_device_path(s->conf->bootindex, qdev, ""/disk@0,0"");

    return 0;

}
","static int virtio_blk_device_init(VirtIODevice *vdev) {
DeviceState *qdev = DEVICE(vdev); VirtIOBlock *s = VIRTIO_BLK(vdev); VirtIOBlkConf *blk = &(s->blk); static int virtio_blk_id; if (!blk->conf.bs) {
error_report(""drive property not set""); return -1; } if (!bdrv_is_inserted(blk->conf.bs)) {
error_report(""Device needs media, but drive is empty""); return -1; } blkconf_serial(&blk->conf, &blk->serial); if (blkconf_geometry(&blk->conf, NULL, 65535, 255, 255) < 0) {
return -1; } virtio_init(vdev, ""virtio-blk"", VIRTIO_ID_BLOCK, sizeof(struct virtio_blk_config)); vdev->get_config = virtio_blk_update_config; vdev->set_config = virtio_blk_set_config; vdev->get_features = virtio_blk_get_features; vdev->set_status = virtio_blk_set_status; vdev->reset = virtio_blk_reset; s->bs = blk->conf.bs; s->conf = &blk->conf; memcpy(&(s->blk), blk, sizeof(struct VirtIOBlkConf)); s->rq = NULL; s->sector_mask = (s->conf->logical_block_size / BDRV_SECTOR_SIZE) - 1; s->vq = virtio_add_queue(vdev, 128, virtio_blk_handle_output); #ifdef CONFIG_VIRTIO_BLK_DATA_PLANE if (!virtio_blk_data_plane_create(vdev, blk, &s->dataplane)) {
virtio_cleanup(vdev); return -1; } #endif s->change = qemu_add_vm_change_state_handler(virtio_blk_dma_restart_cb, s); register_savevm(qdev, ""virtio-blk"", virtio_blk_id++, 2, virtio_blk_save, virtio_blk_load, s); bdrv_set_dev_ops(s->bs, &virtio_block_ops, s); bdrv_set_buffer_alignment(s->bs, s->conf->logical_block_size); bdrv_iostatus_enable(s->bs); add_boot_device_path(s->conf->bootindex, qdev, ""/disk@0,0""); return 0; } ",qemu,1
"void qemu_run_all_timers(void)

{

    /* rearm timer, if not periodic */

    if (alarm_timer->expired) {

        alarm_timer->expired = 0;

        qemu_rearm_alarm_timer(alarm_timer);

    }



    alarm_timer->pending = 0;



    /* vm time timers */

    if (vm_running) {

        qemu_run_timers(vm_clock);

    }



    qemu_run_timers(rt_clock);

    qemu_run_timers(host_clock);

}
","void qemu_run_all_timers(void) {
/* rearm timer, if not periodic */ if (alarm_timer->expired) {
alarm_timer->expired = 0; qemu_rearm_alarm_timer(alarm_timer); } alarm_timer->pending = 0; /* vm time timers */ if (vm_running) {
qemu_run_timers(vm_clock); } qemu_run_timers(rt_clock); qemu_run_timers(host_clock); } ",qemu,1
"static void mv88w8618_pit_write(void *opaque, target_phys_addr_t offset,

                                uint32_t value)

{

    mv88w8618_pit_state *s = opaque;

    mv88w8618_timer_state *t;

    int i;



    switch (offset) {

    case MP_PIT_TIMER1_LENGTH ... MP_PIT_TIMER4_LENGTH:

        t = &s->timer[offset >> 2];

        t->limit = value;

        ptimer_set_limit(t->ptimer, t->limit, 1);

        break;



    case MP_PIT_CONTROL:

        for (i = 0; i < 4; i++) {

            if (value & 0xf) {

                t = &s->timer[i];

                ptimer_set_limit(t->ptimer, t->limit, 0);

                ptimer_set_freq(t->ptimer, t->freq);

                ptimer_run(t->ptimer, 0);

            }

            value >>= 4;

        }

        break;



    case MP_BOARD_RESET:

        if (value == MP_BOARD_RESET_MAGIC) {

            qemu_system_reset_request();

        }

        break;

    }

}
","static void mv88w8618_pit_write(void *opaque, target_phys_addr_t offset, uint32_t value) {
mv88w8618_pit_state *s = opaque; mv88w8618_timer_state *t; int i; switch (offset) {
case MP_PIT_TIMER1_LENGTH ... MP_PIT_TIMER4_LENGTH: t = &s->timer[offset >> 2]; t->limit = value; ptimer_set_limit(t->ptimer, t->limit, 1); break; case MP_PIT_CONTROL: for (i = 0; i < 4; i++) {
if (value & 0xf) {
t = &s->timer[i]; ptimer_set_limit(t->ptimer, t->limit, 0); ptimer_set_freq(t->ptimer, t->freq); ptimer_run(t->ptimer, 0); } value >>= 4; } break; case MP_BOARD_RESET: if (value == MP_BOARD_RESET_MAGIC) {
qemu_system_reset_request(); } break; } } ",qemu,0
"static int parse_drive(DeviceState *dev, Property *prop, const char *str)

{

    DriveInfo **ptr = qdev_get_prop_ptr(dev, prop);



    *ptr = drive_get_by_id(str);

    if (*ptr == NULL)

        return -ENOENT;

    return 0;

}
","static int parse_drive(DeviceState *dev, Property *prop, const char *str) {
DriveInfo **ptr = qdev_get_prop_ptr(dev, prop); *ptr = drive_get_by_id(str); if (*ptr == NULL) return -ENOENT; return 0; } ",qemu,0
"DriveInfo *drive_init(QemuOpts *opts, BlockInterfaceType block_default_type)

{

    const char *buf;

    const char *file = NULL;

    const char *serial;

    const char *mediastr = """";

    BlockInterfaceType type;

    enum { MEDIA_DISK, MEDIA_CDROM } media;

    int bus_id, unit_id;

    int cyls, heads, secs, translation;

    BlockDriver *drv = NULL;

    int max_devs;

    int index;

    int ro = 0;

    int bdrv_flags = 0;

    int on_read_error, on_write_error;

    const char *devaddr;

    DriveInfo *dinfo;

    BlockIOLimit io_limits;

    int snapshot = 0;

    bool copy_on_read;

    int ret;



    translation = BIOS_ATA_TRANSLATION_AUTO;

    media = MEDIA_DISK;



    /* extract parameters */

    bus_id  = qemu_opt_get_number(opts, ""bus"", 0);

    unit_id = qemu_opt_get_number(opts, ""unit"", -1);

    index   = qemu_opt_get_number(opts, ""index"", -1);



    cyls  = qemu_opt_get_number(opts, ""cyls"", 0);

    heads = qemu_opt_get_number(opts, ""heads"", 0);

    secs  = qemu_opt_get_number(opts, ""secs"", 0);



    snapshot = qemu_opt_get_bool(opts, ""snapshot"", 0);

    ro = qemu_opt_get_bool(opts, ""readonly"", 0);

    copy_on_read = qemu_opt_get_bool(opts, ""copy-on-read"", false);



    file = qemu_opt_get(opts, ""file"");

    serial = qemu_opt_get(opts, ""serial"");



    if ((buf = qemu_opt_get(opts, ""if"")) != NULL) {

        for (type = 0; type < IF_COUNT && strcmp(buf, if_name[type]); type++)

            ;

        if (type == IF_COUNT) {

            error_report(""unsupported bus type '%s'"", buf);

            return NULL;

	}

    } else {

        type = block_default_type;

    }



    max_devs = if_max_devs[type];



    if (cyls || heads || secs) {

        if (cyls < 1) {

            error_report(""invalid physical cyls number"");

	    return NULL;

	}

        if (heads < 1) {

            error_report(""invalid physical heads number"");

	    return NULL;

	}

        if (secs < 1) {

            error_report(""invalid physical secs number"");

	    return NULL;

	}

    }



    if ((buf = qemu_opt_get(opts, ""trans"")) != NULL) {

        if (!cyls) {

            error_report(""'%s' trans must be used with cyls, heads and secs"",

                         buf);

            return NULL;

        }

        if (!strcmp(buf, ""none""))

            translation = BIOS_ATA_TRANSLATION_NONE;

        else if (!strcmp(buf, ""lba""))

            translation = BIOS_ATA_TRANSLATION_LBA;

        else if (!strcmp(buf, ""auto""))

            translation = BIOS_ATA_TRANSLATION_AUTO;

	else {

            error_report(""'%s' invalid translation type"", buf);

	    return NULL;

	}

    }



    if ((buf = qemu_opt_get(opts, ""media"")) != NULL) {

        if (!strcmp(buf, ""disk"")) {

	    media = MEDIA_DISK;

	} else if (!strcmp(buf, ""cdrom"")) {

            if (cyls || secs || heads) {

                error_report(""CHS can't be set with media=%s"", buf);

	        return NULL;

            }

	    media = MEDIA_CDROM;

	} else {

	    error_report(""'%s' invalid media"", buf);

	    return NULL;

	}

    }



    bdrv_flags |= BDRV_O_CACHE_WB;

    if ((buf = qemu_opt_get(opts, ""cache"")) != NULL) {

        if (bdrv_parse_cache_flags(buf, &bdrv_flags) != 0) {

            error_report(""invalid cache option"");

            return NULL;

        }

    }



#ifdef CONFIG_LINUX_AIO

    if ((buf = qemu_opt_get(opts, ""aio"")) != NULL) {

        if (!strcmp(buf, ""native"")) {

            bdrv_flags |= BDRV_O_NATIVE_AIO;

        } else if (!strcmp(buf, ""threads"")) {

            /* this is the default */

        } else {

           error_report(""invalid aio option"");

           return NULL;

        }

    }

#endif



    if ((buf = qemu_opt_get(opts, ""format"")) != NULL) {

        if (is_help_option(buf)) {

            error_printf(""Supported formats:"");

            bdrv_iterate_format(bdrv_format_print, NULL);

            error_printf(""\n"");

            return NULL;

        }

        drv = bdrv_find_whitelisted_format(buf);

        if (!drv) {

            error_report(""'%s' invalid format"", buf);

            return NULL;

        }

    }



    /* disk I/O throttling */

    io_limits.bps[BLOCK_IO_LIMIT_TOTAL]  =

                           qemu_opt_get_number(opts, ""bps"", 0);

    io_limits.bps[BLOCK_IO_LIMIT_READ]   =

                           qemu_opt_get_number(opts, ""bps_rd"", 0);

    io_limits.bps[BLOCK_IO_LIMIT_WRITE]  =

                           qemu_opt_get_number(opts, ""bps_wr"", 0);

    io_limits.iops[BLOCK_IO_LIMIT_TOTAL] =

                           qemu_opt_get_number(opts, ""iops"", 0);

    io_limits.iops[BLOCK_IO_LIMIT_READ]  =

                           qemu_opt_get_number(opts, ""iops_rd"", 0);

    io_limits.iops[BLOCK_IO_LIMIT_WRITE] =

                           qemu_opt_get_number(opts, ""iops_wr"", 0);



    if (!do_check_io_limits(&io_limits)) {

        error_report(""bps(iops) and bps_rd/bps_wr(iops_rd/iops_wr) ""

                     ""cannot be used at the same time"");

        return NULL;

    }



    if (qemu_opt_get(opts, ""boot"") != NULL) {

        fprintf(stderr, ""qemu-kvm: boot=on|off is deprecated and will be ""

                ""ignored. Future versions will reject this parameter. Please ""

                ""update your scripts.\n"");

    }



    on_write_error = BLOCKDEV_ON_ERROR_ENOSPC;

    if ((buf = qemu_opt_get(opts, ""werror"")) != NULL) {

        if (type != IF_IDE && type != IF_SCSI && type != IF_VIRTIO && type != IF_NONE) {

            error_report(""werror is not supported by this bus type"");

            return NULL;

        }



        on_write_error = parse_block_error_action(buf, 0);

        if (on_write_error < 0) {

            return NULL;

        }

    }



    on_read_error = BLOCKDEV_ON_ERROR_REPORT;

    if ((buf = qemu_opt_get(opts, ""rerror"")) != NULL) {

        if (type != IF_IDE && type != IF_VIRTIO && type != IF_SCSI && type != IF_NONE) {

            error_report(""rerror is not supported by this bus type"");

            return NULL;

        }



        on_read_error = parse_block_error_action(buf, 1);

        if (on_read_error < 0) {

            return NULL;

        }

    }



    if ((devaddr = qemu_opt_get(opts, ""addr"")) != NULL) {

        if (type != IF_VIRTIO) {

            error_report(""addr is not supported by this bus type"");

            return NULL;

        }

    }



    /* compute bus and unit according index */



    if (index != -1) {

        if (bus_id != 0 || unit_id != -1) {

            error_report(""index cannot be used with bus and unit"");

            return NULL;

        }

        bus_id = drive_index_to_bus_id(type, index);

        unit_id = drive_index_to_unit_id(type, index);

    }



    /* if user doesn't specify a unit_id,

     * try to find the first free

     */



    if (unit_id == -1) {

       unit_id = 0;

       while (drive_get(type, bus_id, unit_id) != NULL) {

           unit_id++;

           if (max_devs && unit_id >= max_devs) {

               unit_id -= max_devs;

               bus_id++;

           }

       }

    }



    /* check unit id */



    if (max_devs && unit_id >= max_devs) {

        error_report(""unit %d too big (max is %d)"",

                     unit_id, max_devs - 1);

        return NULL;

    }



    /*

     * catch multiple definitions

     */



    if (drive_get(type, bus_id, unit_id) != NULL) {

        error_report(""drive with bus=%d, unit=%d (index=%d) exists"",

                     bus_id, unit_id, index);

        return NULL;

    }



    /* init */



    dinfo = g_malloc0(sizeof(*dinfo));

    if ((buf = qemu_opts_id(opts)) != NULL) {

        dinfo->id = g_strdup(buf);

    } else {

        /* no id supplied -> create one */

        dinfo->id = g_malloc0(32);

        if (type == IF_IDE || type == IF_SCSI)

            mediastr = (media == MEDIA_CDROM) ? ""-cd"" : ""-hd"";

        if (max_devs)

            snprintf(dinfo->id, 32, ""%s%i%s%i"",

                     if_name[type], bus_id, mediastr, unit_id);

        else

            snprintf(dinfo->id, 32, ""%s%s%i"",

                     if_name[type], mediastr, unit_id);

    }

    dinfo->bdrv = bdrv_new(dinfo->id);

    dinfo->bdrv->open_flags = snapshot ? BDRV_O_SNAPSHOT : 0;

    dinfo->bdrv->read_only = ro;

    dinfo->devaddr = devaddr;

    dinfo->type = type;

    dinfo->bus = bus_id;

    dinfo->unit = unit_id;

    dinfo->cyls = cyls;

    dinfo->heads = heads;

    dinfo->secs = secs;

    dinfo->trans = translation;

    dinfo->opts = opts;

    dinfo->refcount = 1;

    dinfo->serial = serial;

    QTAILQ_INSERT_TAIL(&drives, dinfo, next);



    bdrv_set_on_error(dinfo->bdrv, on_read_error, on_write_error);



    /* disk I/O throttling */

    bdrv_set_io_limits(dinfo->bdrv, &io_limits);



    switch(type) {

    case IF_IDE:

    case IF_SCSI:

    case IF_XEN:

    case IF_NONE:

        dinfo->media_cd = media == MEDIA_CDROM;

        break;

    case IF_SD:

    case IF_FLOPPY:

    case IF_PFLASH:

    case IF_MTD:

        break;

    case IF_VIRTIO:

        /* add virtio block device */

        opts = qemu_opts_create_nofail(qemu_find_opts(""device""));

        if (arch_type == QEMU_ARCH_S390X) {

            qemu_opt_set(opts, ""driver"", ""virtio-blk-s390"");

        } else {

            qemu_opt_set(opts, ""driver"", ""virtio-blk-pci"");

        }

        qemu_opt_set(opts, ""drive"", dinfo->id);

        if (devaddr)

            qemu_opt_set(opts, ""addr"", devaddr);

        break;

    default:

        abort();

    }

    if (!file || !*file) {

        return dinfo;

    }

    if (snapshot) {

        /* always use cache=unsafe with snapshot */

        bdrv_flags &= ~BDRV_O_CACHE_MASK;

        bdrv_flags |= (BDRV_O_SNAPSHOT|BDRV_O_CACHE_WB|BDRV_O_NO_FLUSH);

    }



    if (copy_on_read) {

        bdrv_flags |= BDRV_O_COPY_ON_READ;

    }



    if (runstate_check(RUN_STATE_INMIGRATE)) {

        bdrv_flags |= BDRV_O_INCOMING;

    }



    if (media == MEDIA_CDROM) {

        /* CDROM is fine for any interface, don't check.  */

        ro = 1;

    } else if (ro == 1) {

        if (type != IF_SCSI && type != IF_VIRTIO && type != IF_FLOPPY &&

            type != IF_NONE && type != IF_PFLASH) {

            error_report(""readonly not supported by this bus type"");

            goto err;

        }

    }



    bdrv_flags |= ro ? 0 : BDRV_O_RDWR;



    if (ro && copy_on_read) {

        error_report(""warning: disabling copy_on_read on readonly drive"");

    }



    ret = bdrv_open(dinfo->bdrv, file, bdrv_flags, drv);

    if (ret < 0) {

        if (ret == -EMEDIUMTYPE) {

            error_report(""could not open disk image %s: not in %s format"",

                         file, drv->format_name);

        } else {

            error_report(""could not open disk image %s: %s"",

                         file, strerror(-ret));

        }

        goto err;

    }



    if (bdrv_key_required(dinfo->bdrv))

        autostart = 0;

    return dinfo;



err:

    bdrv_delete(dinfo->bdrv);

    g_free(dinfo->id);

    QTAILQ_REMOVE(&drives, dinfo, next);

    g_free(dinfo);

    return NULL;

}
","DriveInfo *drive_init(QemuOpts *opts, BlockInterfaceType block_default_type) {
const char *buf; const char *file = NULL; const char *serial; const char *mediastr = """"; BlockInterfaceType type; enum {
MEDIA_DISK, MEDIA_CDROM } media; int bus_id, unit_id; int cyls, heads, secs, translation; BlockDriver *drv = NULL; int max_devs; int index; int ro = 0; int bdrv_flags = 0; int on_read_error, on_write_error; const char *devaddr; DriveInfo *dinfo; BlockIOLimit io_limits; int snapshot = 0; bool copy_on_read; int ret; translation = BIOS_ATA_TRANSLATION_AUTO; media = MEDIA_DISK; /* extract parameters */ bus_id = qemu_opt_get_number(opts, ""bus"", 0); unit_id = qemu_opt_get_number(opts, ""unit"", -1); index = qemu_opt_get_number(opts, ""index"", -1); cyls = qemu_opt_get_number(opts, ""cyls"", 0); heads = qemu_opt_get_number(opts, ""heads"", 0); secs = qemu_opt_get_number(opts, ""secs"", 0); snapshot = qemu_opt_get_bool(opts, ""snapshot"", 0); ro = qemu_opt_get_bool(opts, ""readonly"", 0); copy_on_read = qemu_opt_get_bool(opts, ""copy-on-read"", false); file = qemu_opt_get(opts, ""file""); serial = qemu_opt_get(opts, ""serial""); if ((buf = qemu_opt_get(opts, ""if"")) != NULL) {
for (type = 0; type < IF_COUNT && strcmp(buf, if_name[type]); type++) ; if (type == IF_COUNT) {
error_report(""unsupported bus type '%s'"", buf); return NULL; } } else {
type = block_default_type; } max_devs = if_max_devs[type]; if (cyls || heads || secs) {
if (cyls < 1) {
error_report(""invalid physical cyls number""); return NULL; } if (heads < 1) {
error_report(""invalid physical heads number""); return NULL; } if (secs < 1) {
error_report(""invalid physical secs number""); return NULL; } } if ((buf = qemu_opt_get(opts, ""trans"")) != NULL) {
if (!cyls) {
error_report(""'%s' trans must be used with cyls, heads and secs"", buf); return NULL; } if (!strcmp(buf, ""none"")) translation = BIOS_ATA_TRANSLATION_NONE; else if (!strcmp(buf, ""lba"")) translation = BIOS_ATA_TRANSLATION_LBA; else if (!strcmp(buf, ""auto"")) translation = BIOS_ATA_TRANSLATION_AUTO; else {
error_report(""'%s' invalid translation type"", buf); return NULL; } } if ((buf = qemu_opt_get(opts, ""media"")) != NULL) {
if (!strcmp(buf, ""disk"")) {
media = MEDIA_DISK; } else if (!strcmp(buf, ""cdrom"")) {
if (cyls || secs || heads) {
error_report(""CHS can't be set with media=%s"", buf); return NULL; } media = MEDIA_CDROM; } else {
error_report(""'%s' invalid media"", buf); return NULL; } } bdrv_flags |= BDRV_O_CACHE_WB; if ((buf = qemu_opt_get(opts, ""cache"")) != NULL) {
if (bdrv_parse_cache_flags(buf, &bdrv_flags) != 0) {
error_report(""invalid cache option""); return NULL; } } #ifdef CONFIG_LINUX_AIO if ((buf = qemu_opt_get(opts, ""aio"")) != NULL) {
if (!strcmp(buf, ""native"")) {
bdrv_flags |= BDRV_O_NATIVE_AIO; } else if (!strcmp(buf, ""threads"")) {
/* this is the default */ } else {
error_report(""invalid aio option""); return NULL; } } #endif if ((buf = qemu_opt_get(opts, ""format"")) != NULL) {
if (is_help_option(buf)) {
error_printf(""Supported formats:""); bdrv_iterate_format(bdrv_format_print, NULL); error_printf(""\n""); return NULL; } drv = bdrv_find_whitelisted_format(buf); if (!drv) {
error_report(""'%s' invalid format"", buf); return NULL; } } /* disk I/O throttling */ io_limits.bps[BLOCK_IO_LIMIT_TOTAL] = qemu_opt_get_number(opts, ""bps"", 0); io_limits.bps[BLOCK_IO_LIMIT_READ] = qemu_opt_get_number(opts, ""bps_rd"", 0); io_limits.bps[BLOCK_IO_LIMIT_WRITE] = qemu_opt_get_number(opts, ""bps_wr"", 0); io_limits.iops[BLOCK_IO_LIMIT_TOTAL] = qemu_opt_get_number(opts, ""iops"", 0); io_limits.iops[BLOCK_IO_LIMIT_READ] = qemu_opt_get_number(opts, ""iops_rd"", 0); io_limits.iops[BLOCK_IO_LIMIT_WRITE] = qemu_opt_get_number(opts, ""iops_wr"", 0); if (!do_check_io_limits(&io_limits)) {
error_report(""bps(iops) and bps_rd/bps_wr(iops_rd/iops_wr) "" ""cannot be used at the same time""); return NULL; } if (qemu_opt_get(opts, ""boot"") != NULL) {
fprintf(stderr, ""qemu-kvm: boot=on|off is deprecated and will be "" ""ignored. Future versions will reject this parameter. Please "" ""update your scripts.\n""); } on_write_error = BLOCKDEV_ON_ERROR_ENOSPC; if ((buf = qemu_opt_get(opts, ""werror"")) != NULL) {
if (type != IF_IDE && type != IF_SCSI && type != IF_VIRTIO && type != IF_NONE) {
error_report(""werror is not supported by this bus type""); return NULL; } on_write_error = parse_block_error_action(buf, 0); if (on_write_error < 0) {
return NULL; } } on_read_error = BLOCKDEV_ON_ERROR_REPORT; if ((buf = qemu_opt_get(opts, ""rerror"")) != NULL) {
if (type != IF_IDE && type != IF_VIRTIO && type != IF_SCSI && type != IF_NONE) {
error_report(""rerror is not supported by this bus type""); return NULL; } on_read_error = parse_block_error_action(buf, 1); if (on_read_error < 0) {
return NULL; } } if ((devaddr = qemu_opt_get(opts, ""addr"")) != NULL) {
if (type != IF_VIRTIO) {
error_report(""addr is not supported by this bus type""); return NULL; } } /* compute bus and unit according index */ if (index != -1) {
if (bus_id != 0 || unit_id != -1) {
error_report(""index cannot be used with bus and unit""); return NULL; } bus_id = drive_index_to_bus_id(type, index); unit_id = drive_index_to_unit_id(type, index); } /* if user doesn't specify a unit_id, * try to find the first free */ if (unit_id == -1) {
unit_id = 0; while (drive_get(type, bus_id, unit_id) != NULL) {
unit_id++; if (max_devs && unit_id >= max_devs) {
unit_id -= max_devs; bus_id++; } } } /* check unit id */ if (max_devs && unit_id >= max_devs) {
error_report(""unit %d too big (max is %d)"", unit_id, max_devs - 1); return NULL; } /* * catch multiple definitions */ if (drive_get(type, bus_id, unit_id) != NULL) {
error_report(""drive with bus=%d, unit=%d (index=%d) exists"", bus_id, unit_id, index); return NULL; } /* init */ dinfo = g_malloc0(sizeof(*dinfo)); if ((buf = qemu_opts_id(opts)) != NULL) {
dinfo->id = g_strdup(buf); } else {
/* no id supplied -> create one */ dinfo->id = g_malloc0(32); if (type == IF_IDE || type == IF_SCSI) mediastr = (media == MEDIA_CDROM) ? ""-cd"" : ""-hd""; if (max_devs) snprintf(dinfo->id, 32, ""%s%i%s%i"", if_name[type], bus_id, mediastr, unit_id); else snprintf(dinfo->id, 32, ""%s%s%i"", if_name[type], mediastr, unit_id); } dinfo->bdrv = bdrv_new(dinfo->id); dinfo->bdrv->open_flags = snapshot ? BDRV_O_SNAPSHOT : 0; dinfo->bdrv->read_only = ro; dinfo->devaddr = devaddr; dinfo->type = type; dinfo->bus = bus_id; dinfo->unit = unit_id; dinfo->cyls = cyls; dinfo->heads = heads; dinfo->secs = secs; dinfo->trans = translation; dinfo->opts = opts; dinfo->refcount = 1; dinfo->serial = serial; QTAILQ_INSERT_TAIL(&drives, dinfo, next); bdrv_set_on_error(dinfo->bdrv, on_read_error, on_write_error); /* disk I/O throttling */ bdrv_set_io_limits(dinfo->bdrv, &io_limits); switch(type) {
case IF_IDE: case IF_SCSI: case IF_XEN: case IF_NONE: dinfo->media_cd = media == MEDIA_CDROM; break; case IF_SD: case IF_FLOPPY: case IF_PFLASH: case IF_MTD: break; case IF_VIRTIO: /* add virtio block device */ opts = qemu_opts_create_nofail(qemu_find_opts(""device"")); if (arch_type == QEMU_ARCH_S390X) {
qemu_opt_set(opts, ""driver"", ""virtio-blk-s390""); } else {
qemu_opt_set(opts, ""driver"", ""virtio-blk-pci""); } qemu_opt_set(opts, ""drive"", dinfo->id); if (devaddr) qemu_opt_set(opts, ""addr"", devaddr); break; default: abort(); } if (!file || !*file) {
return dinfo; } if (snapshot) {
/* always use cache=unsafe with snapshot */ bdrv_flags &= ~BDRV_O_CACHE_MASK; bdrv_flags |= (BDRV_O_SNAPSHOT|BDRV_O_CACHE_WB|BDRV_O_NO_FLUSH); } if (copy_on_read) {
bdrv_flags |= BDRV_O_COPY_ON_READ; } if (runstate_check(RUN_STATE_INMIGRATE)) {
bdrv_flags |= BDRV_O_INCOMING; } if (media == MEDIA_CDROM) {
/* CDROM is fine for any interface, don't check. */ ro = 1; } else if (ro == 1) {
if (type != IF_SCSI && type != IF_VIRTIO && type != IF_FLOPPY && type != IF_NONE && type != IF_PFLASH) {
error_report(""readonly not supported by this bus type""); goto err; } } bdrv_flags |= ro ? 0 : BDRV_O_RDWR; if (ro && copy_on_read) {
error_report(""warning: disabling copy_on_read on readonly drive""); } ret = bdrv_open(dinfo->bdrv, file, bdrv_flags, drv); if (ret < 0) {
if (ret == -EMEDIUMTYPE) {
error_report(""could not open disk image %s: not in %s format"", file, drv->format_name); } else {
error_report(""could not open disk image %s: %s"", file, strerror(-ret)); } goto err; } if (bdrv_key_required(dinfo->bdrv)) autostart = 0; return dinfo; err: bdrv_delete(dinfo->bdrv); g_free(dinfo->id); QTAILQ_REMOVE(&drives, dinfo, next); g_free(dinfo); return NULL; } ",qemu,0
"static void create_pcie(const VirtBoardInfo *vbi, qemu_irq *pic,

                        bool use_highmem)

{

    hwaddr base_mmio = vbi->memmap[VIRT_PCIE_MMIO].base;

    hwaddr size_mmio = vbi->memmap[VIRT_PCIE_MMIO].size;

    hwaddr base_mmio_high = vbi->memmap[VIRT_PCIE_MMIO_HIGH].base;

    hwaddr size_mmio_high = vbi->memmap[VIRT_PCIE_MMIO_HIGH].size;

    hwaddr base_pio = vbi->memmap[VIRT_PCIE_PIO].base;

    hwaddr size_pio = vbi->memmap[VIRT_PCIE_PIO].size;

    hwaddr base_ecam = vbi->memmap[VIRT_PCIE_ECAM].base;

    hwaddr size_ecam = vbi->memmap[VIRT_PCIE_ECAM].size;

    hwaddr base = base_mmio;

    int nr_pcie_buses = size_ecam / PCIE_MMCFG_SIZE_MIN;

    int irq = vbi->irqmap[VIRT_PCIE];

    MemoryRegion *mmio_alias;

    MemoryRegion *mmio_reg;

    MemoryRegion *ecam_alias;

    MemoryRegion *ecam_reg;

    DeviceState *dev;

    char *nodename;

    int i;



    dev = qdev_create(NULL, TYPE_GPEX_HOST);

    qdev_init_nofail(dev);



    /* Map only the first size_ecam bytes of ECAM space */

    ecam_alias = g_new0(MemoryRegion, 1);

    ecam_reg = sysbus_mmio_get_region(SYS_BUS_DEVICE(dev), 0);

    memory_region_init_alias(ecam_alias, OBJECT(dev), ""pcie-ecam"",

                             ecam_reg, 0, size_ecam);

    memory_region_add_subregion(get_system_memory(), base_ecam, ecam_alias);



    /* Map the MMIO window into system address space so as to expose

     * the section of PCI MMIO space which starts at the same base address

     * (ie 1:1 mapping for that part of PCI MMIO space visible through

     * the window).

     */

    mmio_alias = g_new0(MemoryRegion, 1);

    mmio_reg = sysbus_mmio_get_region(SYS_BUS_DEVICE(dev), 1);

    memory_region_init_alias(mmio_alias, OBJECT(dev), ""pcie-mmio"",

                             mmio_reg, base_mmio, size_mmio);

    memory_region_add_subregion(get_system_memory(), base_mmio, mmio_alias);



    if (use_highmem) {

        /* Map high MMIO space */

        MemoryRegion *high_mmio_alias = g_new0(MemoryRegion, 1);



        memory_region_init_alias(high_mmio_alias, OBJECT(dev), ""pcie-mmio-high"",

                                 mmio_reg, base_mmio_high, size_mmio_high);

        memory_region_add_subregion(get_system_memory(), base_mmio_high,

                                    high_mmio_alias);

    }



    /* Map IO port space */

    sysbus_mmio_map(SYS_BUS_DEVICE(dev), 2, base_pio);



    for (i = 0; i < GPEX_NUM_IRQS; i++) {

        sysbus_connect_irq(SYS_BUS_DEVICE(dev), i, pic[irq + i]);

    }



    nodename = g_strdup_printf(""/pcie@%"" PRIx64, base);

    qemu_fdt_add_subnode(vbi->fdt, nodename);

    qemu_fdt_setprop_string(vbi->fdt, nodename,

                            ""compatible"", ""pci-host-ecam-generic"");

    qemu_fdt_setprop_string(vbi->fdt, nodename, ""device_type"", ""pci"");

    qemu_fdt_setprop_cell(vbi->fdt, nodename, ""#address-cells"", 3);

    qemu_fdt_setprop_cell(vbi->fdt, nodename, ""#size-cells"", 2);

    qemu_fdt_setprop_cells(vbi->fdt, nodename, ""bus-range"", 0,

                           nr_pcie_buses - 1);



    qemu_fdt_setprop_cells(vbi->fdt, nodename, ""msi-parent"", vbi->v2m_phandle);



    qemu_fdt_setprop_sized_cells(vbi->fdt, nodename, ""reg"",

                                 2, base_ecam, 2, size_ecam);



    if (use_highmem) {

        qemu_fdt_setprop_sized_cells(vbi->fdt, nodename, ""ranges"",

                                     1, FDT_PCI_RANGE_IOPORT, 2, 0,

                                     2, base_pio, 2, size_pio,

                                     1, FDT_PCI_RANGE_MMIO, 2, base_mmio,

                                     2, base_mmio, 2, size_mmio,

                                     1, FDT_PCI_RANGE_MMIO_64BIT,

                                     2, base_mmio_high,

                                     2, base_mmio_high, 2, size_mmio_high);

    } else {

        qemu_fdt_setprop_sized_cells(vbi->fdt, nodename, ""ranges"",

                                     1, FDT_PCI_RANGE_IOPORT, 2, 0,

                                     2, base_pio, 2, size_pio,

                                     1, FDT_PCI_RANGE_MMIO, 2, base_mmio,

                                     2, base_mmio, 2, size_mmio);

    }



    qemu_fdt_setprop_cell(vbi->fdt, nodename, ""#interrupt-cells"", 1);

    create_pcie_irq_map(vbi, vbi->gic_phandle, irq, nodename);



    g_free(nodename);

}
","static void create_pcie(const VirtBoardInfo *vbi, qemu_irq *pic, bool use_highmem) {
hwaddr base_mmio = vbi->memmap[VIRT_PCIE_MMIO].base; hwaddr size_mmio = vbi->memmap[VIRT_PCIE_MMIO].size; hwaddr base_mmio_high = vbi->memmap[VIRT_PCIE_MMIO_HIGH].base; hwaddr size_mmio_high = vbi->memmap[VIRT_PCIE_MMIO_HIGH].size; hwaddr base_pio = vbi->memmap[VIRT_PCIE_PIO].base; hwaddr size_pio = vbi->memmap[VIRT_PCIE_PIO].size; hwaddr base_ecam = vbi->memmap[VIRT_PCIE_ECAM].base; hwaddr size_ecam = vbi->memmap[VIRT_PCIE_ECAM].size; hwaddr base = base_mmio; int nr_pcie_buses = size_ecam / PCIE_MMCFG_SIZE_MIN; int irq = vbi->irqmap[VIRT_PCIE]; MemoryRegion *mmio_alias; MemoryRegion *mmio_reg; MemoryRegion *ecam_alias; MemoryRegion *ecam_reg; DeviceState *dev; char *nodename; int i; dev = qdev_create(NULL, TYPE_GPEX_HOST); qdev_init_nofail(dev); /* Map only the first size_ecam bytes of ECAM space */ ecam_alias = g_new0(MemoryRegion, 1); ecam_reg = sysbus_mmio_get_region(SYS_BUS_DEVICE(dev), 0); memory_region_init_alias(ecam_alias, OBJECT(dev), ""pcie-ecam"", ecam_reg, 0, size_ecam); memory_region_add_subregion(get_system_memory(), base_ecam, ecam_alias); /* Map the MMIO window into system address space so as to expose * the section of PCI MMIO space which starts at the same base address * (ie 1:1 mapping for that part of PCI MMIO space visible through * the window). */ mmio_alias = g_new0(MemoryRegion, 1); mmio_reg = sysbus_mmio_get_region(SYS_BUS_DEVICE(dev), 1); memory_region_init_alias(mmio_alias, OBJECT(dev), ""pcie-mmio"", mmio_reg, base_mmio, size_mmio); memory_region_add_subregion(get_system_memory(), base_mmio, mmio_alias); if (use_highmem) {
/* Map high MMIO space */ MemoryRegion *high_mmio_alias = g_new0(MemoryRegion, 1); memory_region_init_alias(high_mmio_alias, OBJECT(dev), ""pcie-mmio-high"", mmio_reg, base_mmio_high, size_mmio_high); memory_region_add_subregion(get_system_memory(), base_mmio_high, high_mmio_alias); } /* Map IO port space */ sysbus_mmio_map(SYS_BUS_DEVICE(dev), 2, base_pio); for (i = 0; i < GPEX_NUM_IRQS; i++) {
sysbus_connect_irq(SYS_BUS_DEVICE(dev), i, pic[irq + i]); } nodename = g_strdup_printf(""/pcie@%"" PRIx64, base); qemu_fdt_add_subnode(vbi->fdt, nodename); qemu_fdt_setprop_string(vbi->fdt, nodename, ""compatible"", ""pci-host-ecam-generic""); qemu_fdt_setprop_string(vbi->fdt, nodename, ""device_type"", ""pci""); qemu_fdt_setprop_cell(vbi->fdt, nodename, ""#address-cells"", 3); qemu_fdt_setprop_cell(vbi->fdt, nodename, ""#size-cells"", 2); qemu_fdt_setprop_cells(vbi->fdt, nodename, ""bus-range"", 0, nr_pcie_buses - 1); qemu_fdt_setprop_cells(vbi->fdt, nodename, ""msi-parent"", vbi->v2m_phandle); qemu_fdt_setprop_sized_cells(vbi->fdt, nodename, ""reg"", 2, base_ecam, 2, size_ecam); if (use_highmem) {
qemu_fdt_setprop_sized_cells(vbi->fdt, nodename, ""ranges"", 1, FDT_PCI_RANGE_IOPORT, 2, 0, 2, base_pio, 2, size_pio, 1, FDT_PCI_RANGE_MMIO, 2, base_mmio, 2, base_mmio, 2, size_mmio, 1, FDT_PCI_RANGE_MMIO_64BIT, 2, base_mmio_high, 2, base_mmio_high, 2, size_mmio_high); } else {
qemu_fdt_setprop_sized_cells(vbi->fdt, nodename, ""ranges"", 1, FDT_PCI_RANGE_IOPORT, 2, 0, 2, base_pio, 2, size_pio, 1, FDT_PCI_RANGE_MMIO, 2, base_mmio, 2, base_mmio, 2, size_mmio); } qemu_fdt_setprop_cell(vbi->fdt, nodename, ""#interrupt-cells"", 1); create_pcie_irq_map(vbi, vbi->gic_phandle, irq, nodename); g_free(nodename); } ",qemu,0
"void check_file_unfixed_eof_mmaps(void)

{

	char *cp;

	unsigned int *p1;

	uintptr_t p;

	int i;



	fprintf (stderr, ""%s"", __func__);

	for (i = 0; i < 0x10; i++)

	{

		p1 = mmap(NULL, pagesize, PROT_READ, 

			  MAP_PRIVATE, 

			  test_fd, 

			  (test_fsize - sizeof *p1) & ~pagemask);



		fail_unless (p1 != MAP_FAILED);



		/* Make sure we get pages aligned with the pagesize. The

		   target expects this.  */

		p = (uintptr_t) p1;

		fail_unless ((p & pagemask) == 0);

		/* Verify that the file maps was made correctly.  */

		fail_unless (p1[(test_fsize & pagemask) / sizeof *p1 - 1]

			     == ((test_fsize - sizeof *p1) / sizeof *p1));



		/* Verify that the end of page is accessable and zeroed.  */

		cp = (void *) p1;

		fail_unless (cp[pagesize - 4] == 0);

		munmap (p1, pagesize);

	}

	fprintf (stderr, "" passed\n"");

}
","void check_file_unfixed_eof_mmaps(void) {
char *cp; unsigned int *p1; uintptr_t p; int i; fprintf (stderr, ""%s"", __func__); for (i = 0; i < 0x10; i++) {
p1 = mmap(NULL, pagesize, PROT_READ, MAP_PRIVATE, test_fd, (test_fsize - sizeof *p1) & ~pagemask); fail_unless (p1 != MAP_FAILED); /* Make sure we get pages aligned with the pagesize. The target expects this. */ p = (uintptr_t) p1; fail_unless ((p & pagemask) == 0); /* Verify that the file maps was made correctly. */ fail_unless (p1[(test_fsize & pagemask) / sizeof *p1 - 1] == ((test_fsize - sizeof *p1) / sizeof *p1)); /* Verify that the end of page is accessable and zeroed. */ cp = (void *) p1; fail_unless (cp[pagesize - 4] == 0); munmap (p1, pagesize); } fprintf (stderr, "" passed\n""); } ",qemu,0
"static char *vio_format_dev_name(VIOsPAPRDevice *dev)

{

    VIOsPAPRDeviceInfo *info = (VIOsPAPRDeviceInfo *)qdev_get_info(&dev->qdev);

    char *name;



    /* Device tree style name device@reg */

    if (asprintf(&name, ""%s@%x"", info->dt_name, dev->reg) < 0) {

        return NULL;

    }



    return name;

}
","static char *vio_format_dev_name(VIOsPAPRDevice *dev) {
VIOsPAPRDeviceInfo *info = (VIOsPAPRDeviceInfo *)qdev_get_info(&dev->qdev); char *name; /* Device tree style name device@reg */ if (asprintf(&name, ""%s@%x"", info->dt_name, dev->reg) < 0) {
return NULL; } return name; } ",qemu,0
"static void term_handle_command(const char *cmdline)

{

    const char *p, *pstart, *typestr;

    char *q;

    int c, nb_args, len, i, has_arg;

    term_cmd_t *cmd;

    char cmdname[256];

    char buf[1024];

    void *str_allocated[MAX_ARGS];

    void *args[MAX_ARGS];



#ifdef DEBUG

    term_printf(""command='%s'\n"", cmdline);

#endif

    

    /* extract the command name */

    p = cmdline;

    q = cmdname;

    while (isspace(*p))

        p++;

    if (*p == '\0')

        return;

    pstart = p;

    while (*p != '\0' && *p != '/' && !isspace(*p))

        p++;

    len = p - pstart;

    if (len > sizeof(cmdname) - 1)

        len = sizeof(cmdname) - 1;

    memcpy(cmdname, pstart, len);

    cmdname[len] = '\0';

    

    /* find the command */

    for(cmd = term_cmds; cmd->name != NULL; cmd++) {

        if (compare_cmd(cmdname, cmd->name)) 

            goto found;

    }

    term_printf(""unknown command: '%s'\n"", cmdname);

    return;

 found:



    for(i = 0; i < MAX_ARGS; i++)

        str_allocated[i] = NULL;

    

    /* parse the parameters */

    typestr = cmd->args_type;

    nb_args = 0;

    for(;;) {

        c = *typestr;

        if (c == '\0')

            break;

        typestr++;

        switch(c) {

        case 'F':

        case 'B':

        case 's':

            {

                int ret;

                char *str;

                

                while (isspace(*p)) 

                    p++;

                if (*typestr == '?') {

                    typestr++;

                    if (*p == '\0') {

                        /* no optional string: NULL argument */

                        str = NULL;

                        goto add_str;

                    }

                }

                ret = get_str(buf, sizeof(buf), &p);

                if (ret < 0) {

                    switch(c) {

                    case 'F':

                        term_printf(""%s: filename expected\n"", cmdname);

                        break;

                    case 'B':

                        term_printf(""%s: block device name expected\n"", cmdname);

                        break;

                    default:

                        term_printf(""%s: string expected\n"", cmdname);

                        break;

                    }

                    goto fail;

                }

                str = qemu_malloc(strlen(buf) + 1);

                strcpy(str, buf);

                str_allocated[nb_args] = str;

            add_str:

                if (nb_args >= MAX_ARGS) {

                error_args:

                    term_printf(""%s: too many arguments\n"", cmdname);

                    goto fail;

                }

                args[nb_args++] = str;

            }

            break;

        case '/':

            {

                int count, format, size;

                

                while (isspace(*p))

                    p++;

                if (*p == '/') {

                    /* format found */

                    p++;

                    count = 1;

                    if (isdigit(*p)) {

                        count = 0;

                        while (isdigit(*p)) {

                            count = count * 10 + (*p - '0');

                            p++;

                        }

                    }

                    size = -1;

                    format = -1;

                    for(;;) {

                        switch(*p) {

                        case 'o':

                        case 'd':

                        case 'u':

                        case 'x':

                        case 'i':

                        case 'c':

                            format = *p++;

                            break;

                        case 'b':

                            size = 1;

                            p++;

                            break;

                        case 'h':

                            size = 2;

                            p++;

                            break;

                        case 'w':

                            size = 4;

                            p++;

                            break;

                        case 'g':

                        case 'L':

                            size = 8;

                            p++;

                            break;

                        default:

                            goto next;

                        }

                    }

                next:

                    if (*p != '\0' && !isspace(*p)) {

                        term_printf(""invalid char in format: '%c'\n"", *p);

                        goto fail;

                    }

                    if (format < 0)

                        format = default_fmt_format;

                    if (format != 'i') {

                        /* for 'i', not specifying a size gives -1 as size */

                        if (size < 0)

                            size = default_fmt_size;

                    }

                    default_fmt_size = size;

                    default_fmt_format = format;

                } else {

                    count = 1;

                    format = default_fmt_format;

                    if (format != 'i') {

                        size = default_fmt_size;

                    } else {

                        size = -1;

                    }

                }

                if (nb_args + 3 > MAX_ARGS)

                    goto error_args;

                args[nb_args++] = (void*)count;

                args[nb_args++] = (void*)format;

                args[nb_args++] = (void*)size;

            }

            break;

        case 'i':

            {

                int val;

                while (isspace(*p)) 

                    p++;

                if (*typestr == '?' || *typestr == '.') {

                    typestr++;

                    if (*typestr == '?') {

                        if (*p == '\0')

                            has_arg = 0;

                        else

                            has_arg = 1;

                    } else {

                        if (*p == '.') {

                            p++;

                            while (isspace(*p)) 

                                p++;

                            has_arg = 1;

                        } else {

                            has_arg = 0;

                        }

                    }

                    if (nb_args >= MAX_ARGS)

                        goto error_args;

                    args[nb_args++] = (void *)has_arg;

                    if (!has_arg) {

                        if (nb_args >= MAX_ARGS)

                            goto error_args;

                        val = -1;

                        goto add_num;

                    }

                }

                if (get_expr(&val, &p))

                    goto fail;

            add_num:

                if (nb_args >= MAX_ARGS)

                    goto error_args;

                args[nb_args++] = (void *)val;

            }

            break;

        case '-':

            {

                int has_option;

                /* option */

                

                c = *typestr++;

                if (c == '\0')

                    goto bad_type;

                while (isspace(*p)) 

                    p++;

                has_option = 0;

                if (*p == '-') {

                    p++;

                    if (*p != c) {

                        term_printf(""%s: unsupported option -%c\n"", 

                                    cmdname, *p);

                        goto fail;

                    }

                    p++;

                    has_option = 1;

                }

                if (nb_args >= MAX_ARGS)

                    goto error_args;

                args[nb_args++] = (void *)has_option;

            }

            break;

        default:

        bad_type:

            term_printf(""%s: unknown type '%c'\n"", cmdname, c);

            goto fail;

        }

    }

    /* check that all arguments were parsed */

    while (isspace(*p))

        p++;

    if (*p != '\0') {

        term_printf(""%s: extraneous characters at the end of line\n"", 

                    cmdname);

        goto fail;

    }



    switch(nb_args) {

    case 0:

        cmd->handler();

        break;

    case 1:

        cmd->handler(args[0]);

        break;

    case 2:

        cmd->handler(args[0], args[1]);

        break;

    case 3:

        cmd->handler(args[0], args[1], args[2]);

        break;

    case 4:

        cmd->handler(args[0], args[1], args[2], args[3]);

        break;

    case 5:

        cmd->handler(args[0], args[1], args[2], args[3], args[4]);

        break;

    case 6:

        cmd->handler(args[0], args[1], args[2], args[3], args[4], args[5]);

        break;

    default:

        term_printf(""unsupported number of arguments: %d\n"", nb_args);

        goto fail;

    }

 fail:

    for(i = 0; i < MAX_ARGS; i++)

        qemu_free(str_allocated[i]);

    return;

}
","static void term_handle_command(const char *cmdline) {
const char *p, *pstart, *typestr; char *q; int c, nb_args, len, i, has_arg; term_cmd_t *cmd; char cmdname[256]; char buf[1024]; void *str_allocated[MAX_ARGS]; void *args[MAX_ARGS]; #ifdef DEBUG term_printf(""command='%s'\n"", cmdline); #endif /* extract the command name */ p = cmdline; q = cmdname; while (isspace(*p)) p++; if (*p == '\0') return; pstart = p; while (*p != '\0' && *p != '/' && !isspace(*p)) p++; len = p - pstart; if (len > sizeof(cmdname) - 1) len = sizeof(cmdname) - 1; memcpy(cmdname, pstart, len); cmdname[len] = '\0'; /* find the command */ for(cmd = term_cmds; cmd->name != NULL; cmd++) {
if (compare_cmd(cmdname, cmd->name)) goto found; } term_printf(""unknown command: '%s'\n"", cmdname); return; found: for(i = 0; i < MAX_ARGS; i++) str_allocated[i] = NULL; /* parse the parameters */ typestr = cmd->args_type; nb_args = 0; for(;;) {
c = *typestr; if (c == '\0') break; typestr++; switch(c) {
case 'F': case 'B': case 's': {
int ret; char *str; while (isspace(*p)) p++; if (*typestr == '?') {
typestr++; if (*p == '\0') {
/* no optional string: NULL argument */ str = NULL; goto add_str; } } ret = get_str(buf, sizeof(buf), &p); if (ret < 0) {
switch(c) {
case 'F': term_printf(""%s: filename expected\n"", cmdname); break; case 'B': term_printf(""%s: block device name expected\n"", cmdname); break; default: term_printf(""%s: string expected\n"", cmdname); break; } goto fail; } str = qemu_malloc(strlen(buf) + 1); strcpy(str, buf); str_allocated[nb_args] = str; add_str: if (nb_args >= MAX_ARGS) {
error_args: term_printf(""%s: too many arguments\n"", cmdname); goto fail; } args[nb_args++] = str; } break; case '/': {
int count, format, size; while (isspace(*p)) p++; if (*p == '/') {
/* format found */ p++; count = 1; if (isdigit(*p)) {
count = 0; while (isdigit(*p)) {
count = count * 10 + (*p - '0'); p++; } } size = -1; format = -1; for(;;) {
switch(*p) {
case 'o': case 'd': case 'u': case 'x': case 'i': case 'c': format = *p++; break; case 'b': size = 1; p++; break; case 'h': size = 2; p++; break; case 'w': size = 4; p++; break; case 'g': case 'L': size = 8; p++; break; default: goto next; } } next: if (*p != '\0' && !isspace(*p)) {
term_printf(""invalid char in format: '%c'\n"", *p); goto fail; } if (format < 0) format = default_fmt_format; if (format != 'i') {
/* for 'i', not specifying a size gives -1 as size */ if (size < 0) size = default_fmt_size; } default_fmt_size = size; default_fmt_format = format; } else {
count = 1; format = default_fmt_format; if (format != 'i') {
size = default_fmt_size; } else {
size = -1; } } if (nb_args + 3 > MAX_ARGS) goto error_args; args[nb_args++] = (void*)count; args[nb_args++] = (void*)format; args[nb_args++] = (void*)size; } break; case 'i': {
int val; while (isspace(*p)) p++; if (*typestr == '?' || *typestr == '.') {
typestr++; if (*typestr == '?') {
if (*p == '\0') has_arg = 0; else has_arg = 1; } else {
if (*p == '.') {
p++; while (isspace(*p)) p++; has_arg = 1; } else {
has_arg = 0; } } if (nb_args >= MAX_ARGS) goto error_args; args[nb_args++] = (void *)has_arg; if (!has_arg) {
if (nb_args >= MAX_ARGS) goto error_args; val = -1; goto add_num; } } if (get_expr(&val, &p)) goto fail; add_num: if (nb_args >= MAX_ARGS) goto error_args; args[nb_args++] = (void *)val; } break; case '-': {
int has_option; /* option */ c = *typestr++; if (c == '\0') goto bad_type; while (isspace(*p)) p++; has_option = 0; if (*p == '-') {
p++; if (*p != c) {
term_printf(""%s: unsupported option -%c\n"", cmdname, *p); goto fail; } p++; has_option = 1; } if (nb_args >= MAX_ARGS) goto error_args; args[nb_args++] = (void *)has_option; } break; default: bad_type: term_printf(""%s: unknown type '%c'\n"", cmdname, c); goto fail; } } /* check that all arguments were parsed */ while (isspace(*p)) p++; if (*p != '\0') {
term_printf(""%s: extraneous characters at the end of line\n"", cmdname); goto fail; } switch(nb_args) {
case 0: cmd->handler(); break; case 1: cmd->handler(args[0]); break; case 2: cmd->handler(args[0], args[1]); break; case 3: cmd->handler(args[0], args[1], args[2]); break; case 4: cmd->handler(args[0], args[1], args[2], args[3]); break; case 5: cmd->handler(args[0], args[1], args[2], args[3], args[4]); break; case 6: cmd->handler(args[0], args[1], args[2], args[3], args[4], args[5]); break; default: term_printf(""unsupported number of arguments: %d\n"", nb_args); goto fail; } fail: for(i = 0; i < MAX_ARGS; i++) qemu_free(str_allocated[i]); return; } ",qemu,0
"void helper_restore_mode (void)

{

    env->ps = (env->ps & ~0xC) | env->saved_mode;

}
","void helper_restore_mode (void) {
env->ps = (env->ps & ~0xC) | env->saved_mode; } ",qemu,0
"static void phys_section_destroy(MemoryRegion *mr)

{

    memory_region_unref(mr);



    if (mr->subpage) {

        subpage_t *subpage = container_of(mr, subpage_t, iomem);

        object_unref(OBJECT(&subpage->iomem));

        g_free(subpage);

    }

}
","static void phys_section_destroy(MemoryRegion *mr) {
memory_region_unref(mr); if (mr->subpage) {
subpage_t *subpage = container_of(mr, subpage_t, iomem); object_unref(OBJECT(&subpage->iomem)); g_free(subpage); } } ",qemu,0
"static int do_attach(USBDevice *dev)

{

    USBBus *bus = usb_bus_from_device(dev);

    USBPort *port;



    if (dev->attached) {

        error_report(""Error: tried to attach usb device %s twice\n"",

                dev->product_desc);

        return -1;

    }

    if (bus->nfree == 0) {

        error_report(""Error: tried to attach usb device %s to a bus with no free ports\n"",

                dev->product_desc);

        return -1;

    }

    if (dev->port_path) {

        QTAILQ_FOREACH(port, &bus->free, next) {

            if (strcmp(port->path, dev->port_path) == 0) {

                break;

            }

        }

        if (port == NULL) {

            error_report(""Error: usb port %s (bus %s) not found\n"",

                    dev->port_path, bus->qbus.name);

            return -1;

        }

    } else {

        port = QTAILQ_FIRST(&bus->free);

    }

    if (!(port->speedmask & dev->speedmask)) {

        error_report(""Warning: speed mismatch trying to attach usb device %s to bus %s\n"",

                dev->product_desc, bus->qbus.name);

        return -1;

    }



    dev->attached++;

    QTAILQ_REMOVE(&bus->free, port, next);

    bus->nfree--;



    usb_attach(port, dev);



    QTAILQ_INSERT_TAIL(&bus->used, port, next);

    bus->nused++;



    return 0;

}
","static int do_attach(USBDevice *dev) {
USBBus *bus = usb_bus_from_device(dev); USBPort *port; if (dev->attached) {
error_report(""Error: tried to attach usb device %s twice\n"", dev->product_desc); return -1; } if (bus->nfree == 0) {
error_report(""Error: tried to attach usb device %s to a bus with no free ports\n"", dev->product_desc); return -1; } if (dev->port_path) {
QTAILQ_FOREACH(port, &bus->free, next) {
if (strcmp(port->path, dev->port_path) == 0) {
break; } } if (port == NULL) {
error_report(""Error: usb port %s (bus %s) not found\n"", dev->port_path, bus->qbus.name); return -1; } } else {
port = QTAILQ_FIRST(&bus->free); } if (!(port->speedmask & dev->speedmask)) {
error_report(""Warning: speed mismatch trying to attach usb device %s to bus %s\n"", dev->product_desc, bus->qbus.name); return -1; } dev->attached++; QTAILQ_REMOVE(&bus->free, port, next); bus->nfree--; usb_attach(port, dev); QTAILQ_INSERT_TAIL(&bus->used, port, next); bus->nused++; return 0; } ",qemu,0
"static int nbd_can_read(void *opaque)

{

    NBDClient *client = opaque;



    return client->recv_coroutine || client->nb_requests < MAX_NBD_REQUESTS;

}
","static int nbd_can_read(void *opaque) {
NBDClient *client = opaque; return client->recv_coroutine || client->nb_requests < MAX_NBD_REQUESTS; } ",qemu,0
"static void vmxnet3_pci_realize(PCIDevice *pci_dev, Error **errp)

{

    DeviceState *dev = DEVICE(pci_dev);

    VMXNET3State *s = VMXNET3(pci_dev);

    int ret;



    VMW_CBPRN(""Starting init..."");



    memory_region_init_io(&s->bar0, OBJECT(s), &b0_ops, s,

                          ""vmxnet3-b0"", VMXNET3_PT_REG_SIZE);

    pci_register_bar(pci_dev, VMXNET3_BAR0_IDX,

                     PCI_BASE_ADDRESS_SPACE_MEMORY, &s->bar0);



    memory_region_init_io(&s->bar1, OBJECT(s), &b1_ops, s,

                          ""vmxnet3-b1"", VMXNET3_VD_REG_SIZE);

    pci_register_bar(pci_dev, VMXNET3_BAR1_IDX,

                     PCI_BASE_ADDRESS_SPACE_MEMORY, &s->bar1);



    memory_region_init(&s->msix_bar, OBJECT(s), ""vmxnet3-msix-bar"",

                       VMXNET3_MSIX_BAR_SIZE);

    pci_register_bar(pci_dev, VMXNET3_MSIX_BAR_IDX,

                     PCI_BASE_ADDRESS_SPACE_MEMORY, &s->msix_bar);



    vmxnet3_reset_interrupt_states(s);



    /* Interrupt pin A */

    pci_dev->config[PCI_INTERRUPT_PIN] = 0x01;



    ret = msi_init(pci_dev, VMXNET3_MSI_OFFSET(s), VMXNET3_MAX_NMSIX_INTRS,

                   VMXNET3_USE_64BIT, VMXNET3_PER_VECTOR_MASK, NULL);

    /* Any error other than -ENOTSUP(board's MSI support is broken)

     * is a programming error. Fall back to INTx silently on -ENOTSUP */

    assert(!ret || ret == -ENOTSUP);



    if (!vmxnet3_init_msix(s)) {

        VMW_WRPRN(""Failed to initialize MSI-X, configuration is inconsistent."");

    }



    vmxnet3_net_init(s);



    if (pci_is_express(pci_dev)) {

        if (pci_bus_is_express(pci_dev->bus)) {

            pcie_endpoint_cap_init(pci_dev, VMXNET3_EXP_EP_OFFSET);

        }



        pcie_dev_ser_num_init(pci_dev, VMXNET3_DSN_OFFSET,

                              vmxnet3_device_serial_num(s));

    }



    register_savevm_live(dev, ""vmxnet3-msix"", -1, 1, &savevm_vmxnet3_msix, s);

}
","static void vmxnet3_pci_realize(PCIDevice *pci_dev, Error **errp) {
DeviceState *dev = DEVICE(pci_dev); VMXNET3State *s = VMXNET3(pci_dev); int ret; VMW_CBPRN(""Starting init...""); memory_region_init_io(&s->bar0, OBJECT(s), &b0_ops, s, ""vmxnet3-b0"", VMXNET3_PT_REG_SIZE); pci_register_bar(pci_dev, VMXNET3_BAR0_IDX, PCI_BASE_ADDRESS_SPACE_MEMORY, &s->bar0); memory_region_init_io(&s->bar1, OBJECT(s), &b1_ops, s, ""vmxnet3-b1"", VMXNET3_VD_REG_SIZE); pci_register_bar(pci_dev, VMXNET3_BAR1_IDX, PCI_BASE_ADDRESS_SPACE_MEMORY, &s->bar1); memory_region_init(&s->msix_bar, OBJECT(s), ""vmxnet3-msix-bar"", VMXNET3_MSIX_BAR_SIZE); pci_register_bar(pci_dev, VMXNET3_MSIX_BAR_IDX, PCI_BASE_ADDRESS_SPACE_MEMORY, &s->msix_bar); vmxnet3_reset_interrupt_states(s); /* Interrupt pin A */ pci_dev->config[PCI_INTERRUPT_PIN] = 0x01; ret = msi_init(pci_dev, VMXNET3_MSI_OFFSET(s), VMXNET3_MAX_NMSIX_INTRS, VMXNET3_USE_64BIT, VMXNET3_PER_VECTOR_MASK, NULL); /* Any error other than -ENOTSUP(board's MSI support is broken) * is a programming error. Fall back to INTx silently on -ENOTSUP */ assert(!ret || ret == -ENOTSUP); if (!vmxnet3_init_msix(s)) {
VMW_WRPRN(""Failed to initialize MSI-X, configuration is inconsistent.""); } vmxnet3_net_init(s); if (pci_is_express(pci_dev)) {
if (pci_bus_is_express(pci_dev->bus)) {
pcie_endpoint_cap_init(pci_dev, VMXNET3_EXP_EP_OFFSET); } pcie_dev_ser_num_init(pci_dev, VMXNET3_DSN_OFFSET, vmxnet3_device_serial_num(s)); } register_savevm_live(dev, ""vmxnet3-msix"", -1, 1, &savevm_vmxnet3_msix, s); } ",qemu,0
"static void vmsvga_value_write(void *opaque, uint32_t address, uint32_t value)

{

    struct vmsvga_state_s *s = opaque;

    switch (s->index) {

    case SVGA_REG_ID:

        if (value == SVGA_ID_2 || value == SVGA_ID_1 || value == SVGA_ID_0)

            s->svgaid = value;

        break;



    case SVGA_REG_ENABLE:

        s->enable = value;

        s->config &= !!value;

        s->width = -1;

        s->height = -1;

        s->invalidated = 1;

        s->vga.invalidate(&s->vga);

        if (s->enable) {

            s->fb_size = ((s->depth + 7) >> 3) * s->new_width * s->new_height;

            vga_dirty_log_stop(&s->vga);

        } else {

            vga_dirty_log_start(&s->vga);

        }

        break;



    case SVGA_REG_WIDTH:

        s->new_width = value;

        s->invalidated = 1;

        break;



    case SVGA_REG_HEIGHT:

        s->new_height = value;

        s->invalidated = 1;

        break;



    case SVGA_REG_DEPTH:

    case SVGA_REG_BITS_PER_PIXEL:

        if (value != s->depth) {

            printf(""%s: Bad colour depth: %i bits\n"", __FUNCTION__, value);

            s->config = 0;

        }

        break;



    case SVGA_REG_CONFIG_DONE:

        if (value) {

            s->fifo = (uint32_t *) s->fifo_ptr;

            /* Check range and alignment.  */

            if ((CMD(min) | CMD(max) |

                        CMD(next_cmd) | CMD(stop)) & 3)

                break;

            if (CMD(min) < (uint8_t *) s->cmd->fifo - (uint8_t *) s->fifo)

                break;

            if (CMD(max) > SVGA_FIFO_SIZE)

                break;

            if (CMD(max) < CMD(min) + 10 * 1024)

                break;

        }

        s->config = !!value;

        break;



    case SVGA_REG_SYNC:

        s->syncing = 1;

        vmsvga_fifo_run(s); /* Or should we just wait for update_display? */

        break;



    case SVGA_REG_GUEST_ID:

        s->guest = value;

#ifdef VERBOSE

        if (value >= GUEST_OS_BASE && value < GUEST_OS_BASE +

                ARRAY_SIZE(vmsvga_guest_id))

            printf(""%s: guest runs %s.\n"", __FUNCTION__,

                            vmsvga_guest_id[value - GUEST_OS_BASE]);

#endif

        break;



    case SVGA_REG_CURSOR_ID:

        s->cursor.id = value;

        break;



    case SVGA_REG_CURSOR_X:

        s->cursor.x = value;

        break;



    case SVGA_REG_CURSOR_Y:

        s->cursor.y = value;

        break;



    case SVGA_REG_CURSOR_ON:

        s->cursor.on |= (value == SVGA_CURSOR_ON_SHOW);

        s->cursor.on &= (value != SVGA_CURSOR_ON_HIDE);

#ifdef HW_MOUSE_ACCEL

        if (value <= SVGA_CURSOR_ON_SHOW) {

            dpy_mouse_set(s->vga.ds, s->cursor.x, s->cursor.y, s->cursor.on);

        }

#endif

        break;



    case SVGA_REG_MEM_REGS:

    case SVGA_REG_NUM_DISPLAYS:

    case SVGA_REG_PITCHLOCK:

    case SVGA_PALETTE_BASE ... SVGA_PALETTE_END:

        break;



    default:

        if (s->index >= SVGA_SCRATCH_BASE &&

                s->index < SVGA_SCRATCH_BASE + s->scratch_size) {

            s->scratch[s->index - SVGA_SCRATCH_BASE] = value;

            break;

        }

        printf(""%s: Bad register %02x\n"", __FUNCTION__, s->index);

    }

}
","static void vmsvga_value_write(void *opaque, uint32_t address, uint32_t value) {
struct vmsvga_state_s *s = opaque; switch (s->index) {
case SVGA_REG_ID: if (value == SVGA_ID_2 || value == SVGA_ID_1 || value == SVGA_ID_0) s->svgaid = value; break; case SVGA_REG_ENABLE: s->enable = value; s->config &= !!value; s->width = -1; s->height = -1; s->invalidated = 1; s->vga.invalidate(&s->vga); if (s->enable) {
s->fb_size = ((s->depth + 7) >> 3) * s->new_width * s->new_height; vga_dirty_log_stop(&s->vga); } else {
vga_dirty_log_start(&s->vga); } break; case SVGA_REG_WIDTH: s->new_width = value; s->invalidated = 1; break; case SVGA_REG_HEIGHT: s->new_height = value; s->invalidated = 1; break; case SVGA_REG_DEPTH: case SVGA_REG_BITS_PER_PIXEL: if (value != s->depth) {
printf(""%s: Bad colour depth: %i bits\n"", __FUNCTION__, value); s->config = 0; } break; case SVGA_REG_CONFIG_DONE: if (value) {
s->fifo = (uint32_t *) s->fifo_ptr; /* Check range and alignment. */ if ((CMD(min) | CMD(max) | CMD(next_cmd) | CMD(stop)) & 3) break; if (CMD(min) < (uint8_t *) s->cmd->fifo - (uint8_t *) s->fifo) break; if (CMD(max) > SVGA_FIFO_SIZE) break; if (CMD(max) < CMD(min) + 10 * 1024) break; } s->config = !!value; break; case SVGA_REG_SYNC: s->syncing = 1; vmsvga_fifo_run(s); /* Or should we just wait for update_display? */ break; case SVGA_REG_GUEST_ID: s->guest = value; #ifdef VERBOSE if (value >= GUEST_OS_BASE && value < GUEST_OS_BASE + ARRAY_SIZE(vmsvga_guest_id)) printf(""%s: guest runs %s.\n"", __FUNCTION__, vmsvga_guest_id[value - GUEST_OS_BASE]); #endif break; case SVGA_REG_CURSOR_ID: s->cursor.id = value; break; case SVGA_REG_CURSOR_X: s->cursor.x = value; break; case SVGA_REG_CURSOR_Y: s->cursor.y = value; break; case SVGA_REG_CURSOR_ON: s->cursor.on |= (value == SVGA_CURSOR_ON_SHOW); s->cursor.on &= (value != SVGA_CURSOR_ON_HIDE); #ifdef HW_MOUSE_ACCEL if (value <= SVGA_CURSOR_ON_SHOW) {
dpy_mouse_set(s->vga.ds, s->cursor.x, s->cursor.y, s->cursor.on); } #endif break; case SVGA_REG_MEM_REGS: case SVGA_REG_NUM_DISPLAYS: case SVGA_REG_PITCHLOCK: case SVGA_PALETTE_BASE ... SVGA_PALETTE_END: break; default: if (s->index >= SVGA_SCRATCH_BASE && s->index < SVGA_SCRATCH_BASE + s->scratch_size) {
s->scratch[s->index - SVGA_SCRATCH_BASE] = value; break; } printf(""%s: Bad register %02x\n"", __FUNCTION__, s->index); } } ",qemu,0
"set_phy_ctrl(E1000State *s, int index, uint16_t val)

{

    /*

     * QEMU 1.3 does not support link auto-negotiation emulation, so if we

     * migrate during auto negotiation, after migration the link will be

     * down.

     */

    if (!(s->compat_flags & E1000_FLAG_AUTONEG)) {

        return;

    }

    if ((val & MII_CR_AUTO_NEG_EN) && (val & MII_CR_RESTART_AUTO_NEG)) {

        e1000_link_down(s);

        DBGOUT(PHY, ""Start link auto negotiation\n"");

        timer_mod(s->autoneg_timer, qemu_clock_get_ms(QEMU_CLOCK_VIRTUAL) + 500);

    }

}
","set_phy_ctrl(E1000State *s, int index, uint16_t val) {
/* * QEMU 1.3 does not support link auto-negotiation emulation, so if we * migrate during auto negotiation, after migration the link will be * down. */ if (!(s->compat_flags & E1000_FLAG_AUTONEG)) {
return; } if ((val & MII_CR_AUTO_NEG_EN) && (val & MII_CR_RESTART_AUTO_NEG)) {
e1000_link_down(s); DBGOUT(PHY, ""Start link auto negotiation\n""); timer_mod(s->autoneg_timer, qemu_clock_get_ms(QEMU_CLOCK_VIRTUAL) + 500); } } ",qemu,0
"static void gen_mttr(CPUMIPSState *env, DisasContext *ctx, int rd, int rt,

                     int u, int sel, int h)

{

    int other_tc = env->CP0_VPEControl & (0xff << CP0VPECo_TargTC);

    TCGv t0 = tcg_temp_local_new();



    gen_load_gpr(t0, rt);

    if ((env->CP0_VPEConf0 & (1 << CP0VPEC0_MVP)) == 0 &&

        ((env->tcs[other_tc].CP0_TCBind & (0xf << CP0TCBd_CurVPE)) !=

         (env->active_tc.CP0_TCBind & (0xf << CP0TCBd_CurVPE))))

        /* NOP */ ;

    else if ((env->CP0_VPEControl & (0xff << CP0VPECo_TargTC)) >

             (env->mvp->CP0_MVPConf0 & (0xff << CP0MVPC0_PTC)))

        /* NOP */ ;

    else if (u == 0) {

        switch (rd) {

        case 1:

            switch (sel) {

            case 1:

                gen_helper_mttc0_vpecontrol(cpu_env, t0);

                break;

            case 2:

                gen_helper_mttc0_vpeconf0(cpu_env, t0);

                break;

            default:

                goto die;

                break;

            }

            break;

        case 2:

            switch (sel) {

            case 1:

                gen_helper_mttc0_tcstatus(cpu_env, t0);

                break;

            case 2:

                gen_helper_mttc0_tcbind(cpu_env, t0);

                break;

            case 3:

                gen_helper_mttc0_tcrestart(cpu_env, t0);

                break;

            case 4:

                gen_helper_mttc0_tchalt(cpu_env, t0);

                break;

            case 5:

                gen_helper_mttc0_tccontext(cpu_env, t0);

                break;

            case 6:

                gen_helper_mttc0_tcschedule(cpu_env, t0);

                break;

            case 7:

                gen_helper_mttc0_tcschefback(cpu_env, t0);

                break;

            default:

                gen_mtc0(ctx, t0, rd, sel);

                break;

            }

            break;

        case 10:

            switch (sel) {

            case 0:

                gen_helper_mttc0_entryhi(cpu_env, t0);

                break;

            default:

                gen_mtc0(ctx, t0, rd, sel);

                break;

            }

        case 12:

            switch (sel) {

            case 0:

                gen_helper_mttc0_status(cpu_env, t0);

                break;

            default:

                gen_mtc0(ctx, t0, rd, sel);

                break;

            }

        case 13:

            switch (sel) {

            case 0:

                gen_helper_mttc0_cause(cpu_env, t0);

                break;

            default:

                goto die;

                break;

            }

            break;

        case 15:

            switch (sel) {

            case 1:

                gen_helper_mttc0_ebase(cpu_env, t0);

                break;

            default:

                goto die;

                break;

            }

            break;

        case 23:

            switch (sel) {

            case 0:

                gen_helper_mttc0_debug(cpu_env, t0);

                break;

            default:

                gen_mtc0(ctx, t0, rd, sel);

                break;

            }

            break;

        default:

            gen_mtc0(ctx, t0, rd, sel);

        }

    } else switch (sel) {

    /* GPR registers. */

    case 0:

        gen_helper_0e1i(mttgpr, t0, rd);

        break;

    /* Auxiliary CPU registers */

    case 1:

        switch (rd) {

        case 0:

            gen_helper_0e1i(mttlo, t0, 0);

            break;

        case 1:

            gen_helper_0e1i(mtthi, t0, 0);

            break;

        case 2:

            gen_helper_0e1i(mttacx, t0, 0);

            break;

        case 4:

            gen_helper_0e1i(mttlo, t0, 1);

            break;

        case 5:

            gen_helper_0e1i(mtthi, t0, 1);

            break;

        case 6:

            gen_helper_0e1i(mttacx, t0, 1);

            break;

        case 8:

            gen_helper_0e1i(mttlo, t0, 2);

            break;

        case 9:

            gen_helper_0e1i(mtthi, t0, 2);

            break;

        case 10:

            gen_helper_0e1i(mttacx, t0, 2);

            break;

        case 12:

            gen_helper_0e1i(mttlo, t0, 3);

            break;

        case 13:

            gen_helper_0e1i(mtthi, t0, 3);

            break;

        case 14:

            gen_helper_0e1i(mttacx, t0, 3);

            break;

        case 16:

            gen_helper_mttdsp(cpu_env, t0);

            break;

        default:

            goto die;

        }

        break;

    /* Floating point (COP1). */

    case 2:

        /* XXX: For now we support only a single FPU context. */

        if (h == 0) {

            TCGv_i32 fp0 = tcg_temp_new_i32();



            tcg_gen_trunc_tl_i32(fp0, t0);

            gen_store_fpr32(fp0, rd);

            tcg_temp_free_i32(fp0);

        } else {

            TCGv_i32 fp0 = tcg_temp_new_i32();



            tcg_gen_trunc_tl_i32(fp0, t0);

            gen_store_fpr32h(fp0, rd);

            tcg_temp_free_i32(fp0);

        }

        break;

    case 3:

        /* XXX: For now we support only a single FPU context. */

        {

            TCGv_i32 fs_tmp = tcg_const_i32(rd);



            gen_helper_0e2i(ctc1, t0, fs_tmp, rt);

            tcg_temp_free_i32(fs_tmp);

        }

        break;

    /* COP2: Not implemented. */

    case 4:

    case 5:

        /* fall through */

    default:

        goto die;

    }

    LOG_DISAS(""mttr (reg %d u %d sel %d h %d)\n"", rd, u, sel, h);

    tcg_temp_free(t0);

    return;



die:

    tcg_temp_free(t0);

    LOG_DISAS(""mttr (reg %d u %d sel %d h %d)\n"", rd, u, sel, h);

    generate_exception(ctx, EXCP_RI);

}
","static void gen_mttr(CPUMIPSState *env, DisasContext *ctx, int rd, int rt, int u, int sel, int h) {
int other_tc = env->CP0_VPEControl & (0xff << CP0VPECo_TargTC); TCGv t0 = tcg_temp_local_new(); gen_load_gpr(t0, rt); if ((env->CP0_VPEConf0 & (1 << CP0VPEC0_MVP)) == 0 && ((env->tcs[other_tc].CP0_TCBind & (0xf << CP0TCBd_CurVPE)) != (env->active_tc.CP0_TCBind & (0xf << CP0TCBd_CurVPE)))) /* NOP */ ; else if ((env->CP0_VPEControl & (0xff << CP0VPECo_TargTC)) > (env->mvp->CP0_MVPConf0 & (0xff << CP0MVPC0_PTC))) /* NOP */ ; else if (u == 0) {
switch (rd) {
case 1: switch (sel) {
case 1: gen_helper_mttc0_vpecontrol(cpu_env, t0); break; case 2: gen_helper_mttc0_vpeconf0(cpu_env, t0); break; default: goto die; break; } break; case 2: switch (sel) {
case 1: gen_helper_mttc0_tcstatus(cpu_env, t0); break; case 2: gen_helper_mttc0_tcbind(cpu_env, t0); break; case 3: gen_helper_mttc0_tcrestart(cpu_env, t0); break; case 4: gen_helper_mttc0_tchalt(cpu_env, t0); break; case 5: gen_helper_mttc0_tccontext(cpu_env, t0); break; case 6: gen_helper_mttc0_tcschedule(cpu_env, t0); break; case 7: gen_helper_mttc0_tcschefback(cpu_env, t0); break; default: gen_mtc0(ctx, t0, rd, sel); break; } break; case 10: switch (sel) {
case 0: gen_helper_mttc0_entryhi(cpu_env, t0); break; default: gen_mtc0(ctx, t0, rd, sel); break; } case 12: switch (sel) {
case 0: gen_helper_mttc0_status(cpu_env, t0); break; default: gen_mtc0(ctx, t0, rd, sel); break; } case 13: switch (sel) {
case 0: gen_helper_mttc0_cause(cpu_env, t0); break; default: goto die; break; } break; case 15: switch (sel) {
case 1: gen_helper_mttc0_ebase(cpu_env, t0); break; default: goto die; break; } break; case 23: switch (sel) {
case 0: gen_helper_mttc0_debug(cpu_env, t0); break; default: gen_mtc0(ctx, t0, rd, sel); break; } break; default: gen_mtc0(ctx, t0, rd, sel); } } else switch (sel) {
/* GPR registers. */ case 0: gen_helper_0e1i(mttgpr, t0, rd); break; /* Auxiliary CPU registers */ case 1: switch (rd) {
case 0: gen_helper_0e1i(mttlo, t0, 0); break; case 1: gen_helper_0e1i(mtthi, t0, 0); break; case 2: gen_helper_0e1i(mttacx, t0, 0); break; case 4: gen_helper_0e1i(mttlo, t0, 1); break; case 5: gen_helper_0e1i(mtthi, t0, 1); break; case 6: gen_helper_0e1i(mttacx, t0, 1); break; case 8: gen_helper_0e1i(mttlo, t0, 2); break; case 9: gen_helper_0e1i(mtthi, t0, 2); break; case 10: gen_helper_0e1i(mttacx, t0, 2); break; case 12: gen_helper_0e1i(mttlo, t0, 3); break; case 13: gen_helper_0e1i(mtthi, t0, 3); break; case 14: gen_helper_0e1i(mttacx, t0, 3); break; case 16: gen_helper_mttdsp(cpu_env, t0); break; default: goto die; } break; /* Floating point (COP1). */ case 2: /* XXX: For now we support only a single FPU context. */ if (h == 0) {
TCGv_i32 fp0 = tcg_temp_new_i32(); tcg_gen_trunc_tl_i32(fp0, t0); gen_store_fpr32(fp0, rd); tcg_temp_free_i32(fp0); } else {
TCGv_i32 fp0 = tcg_temp_new_i32(); tcg_gen_trunc_tl_i32(fp0, t0); gen_store_fpr32h(fp0, rd); tcg_temp_free_i32(fp0); } break; case 3: /* XXX: For now we support only a single FPU context. */ {
TCGv_i32 fs_tmp = tcg_const_i32(rd); gen_helper_0e2i(ctc1, t0, fs_tmp, rt); tcg_temp_free_i32(fs_tmp); } break; /* COP2: Not implemented. */ case 4: case 5: /* fall through */ default: goto die; } LOG_DISAS(""mttr (reg %d u %d sel %d h %d)\n"", rd, u, sel, h); tcg_temp_free(t0); return; die: tcg_temp_free(t0); LOG_DISAS(""mttr (reg %d u %d sel %d h %d)\n"", rd, u, sel, h); generate_exception(ctx, EXCP_RI); } ",qemu,0
"static void pc_compat_2_1(MachineState *machine)

{

    PCMachineState *pcms = PC_MACHINE(machine);



    pc_compat_2_2(machine);

    pcms->enforce_aligned_dimm = false;

    smbios_uuid_encoded = false;

    x86_cpu_change_kvm_default(""svm"", NULL);

}
","static void pc_compat_2_1(MachineState *machine) {
PCMachineState *pcms = PC_MACHINE(machine); pc_compat_2_2(machine); pcms->enforce_aligned_dimm = false; smbios_uuid_encoded = false; x86_cpu_change_kvm_default(""svm"", NULL); } ",qemu,0
"static int32_t scsi_target_send_command(SCSIRequest *req, uint8_t *buf)

{

    SCSITargetReq *r = DO_UPCAST(SCSITargetReq, req, req);



    switch (buf[0]) {

    case REPORT_LUNS:

        if (!scsi_target_emulate_report_luns(r)) {

            goto illegal_request;

        }

        break;

    case INQUIRY:

        if (!scsi_target_emulate_inquiry(r)) {

            goto illegal_request;

        }

        break;

    case REQUEST_SENSE:

        r->len = scsi_device_get_sense(r->req.dev, r->buf,

                                       MIN(req->cmd.xfer, sizeof r->buf),

                                       (req->cmd.buf[1] & 1) == 0);

        if (r->req.dev->sense_is_ua) {

            scsi_device_unit_attention_reported(req->dev);

            r->req.dev->sense_len = 0;

            r->req.dev->sense_is_ua = false;

        }

        break;

    default:

        scsi_req_build_sense(req, SENSE_CODE(LUN_NOT_SUPPORTED));

        scsi_req_complete(req, CHECK_CONDITION);

        return 0;

    illegal_request:

        scsi_req_build_sense(req, SENSE_CODE(INVALID_FIELD));

        scsi_req_complete(req, CHECK_CONDITION);

        return 0;

    }



    if (!r->len) {

        scsi_req_complete(req, GOOD);

    }

    return r->len;

}
","static int32_t scsi_target_send_command(SCSIRequest *req, uint8_t *buf) {
SCSITargetReq *r = DO_UPCAST(SCSITargetReq, req, req); switch (buf[0]) {
case REPORT_LUNS: if (!scsi_target_emulate_report_luns(r)) {
goto illegal_request; } break; case INQUIRY: if (!scsi_target_emulate_inquiry(r)) {
goto illegal_request; } break; case REQUEST_SENSE: r->len = scsi_device_get_sense(r->req.dev, r->buf, MIN(req->cmd.xfer, sizeof r->buf), (req->cmd.buf[1] & 1) == 0); if (r->req.dev->sense_is_ua) {
scsi_device_unit_attention_reported(req->dev); r->req.dev->sense_len = 0; r->req.dev->sense_is_ua = false; } break; default: scsi_req_build_sense(req, SENSE_CODE(LUN_NOT_SUPPORTED)); scsi_req_complete(req, CHECK_CONDITION); return 0; illegal_request: scsi_req_build_sense(req, SENSE_CODE(INVALID_FIELD)); scsi_req_complete(req, CHECK_CONDITION); return 0; } if (!r->len) {
scsi_req_complete(req, GOOD); } return r->len; } ",qemu,1
"static int pci_init_multifunction(PCIBus *bus, PCIDevice *dev)

{

    uint8_t slot = PCI_SLOT(dev->devfn);

    uint8_t func;



    if (dev->cap_present & QEMU_PCI_CAP_MULTIFUNCTION) {

        dev->config[PCI_HEADER_TYPE] |= PCI_HEADER_TYPE_MULTI_FUNCTION;

    }



    /*

     * multifunction bit is interpreted in two ways as follows.

     *   - all functions must set the bit to 1.

     *     Example: Intel X53

     *   - function 0 must set the bit, but the rest function (> 0)

     *     is allowed to leave the bit to 0.

     *     Example: PIIX3(also in qemu), PIIX4(also in qemu), ICH10,

     *

     * So OS (at least Linux) checks the bit of only function 0,

     * and doesn't see the bit of function > 0.

     *

     * The below check allows both interpretation.

     */

    if (PCI_FUNC(dev->devfn)) {

        PCIDevice *f0 = bus->devices[PCI_DEVFN(slot, 0)];

        if (f0 && !(f0->cap_present & QEMU_PCI_CAP_MULTIFUNCTION)) {

            /* function 0 should set multifunction bit */

            error_report(""PCI: single function device can't be populated ""

                         ""in function %x.%x"", slot, PCI_FUNC(dev->devfn));

            return -1;

        }

        return 0;

    }



    if (dev->cap_present & QEMU_PCI_CAP_MULTIFUNCTION) {

        return 0;

    }

    /* function 0 indicates single function, so function > 0 must be NULL */

    for (func = 1; func < PCI_FUNC_MAX; ++func) {

        if (bus->devices[PCI_DEVFN(slot, func)]) {

            error_report(""PCI: %x.0 indicates single function, ""

                         ""but %x.%x is already populated."",

                         slot, slot, func);

            return -1;

        }

    }

    return 0;

}
","static int pci_init_multifunction(PCIBus *bus, PCIDevice *dev) {
uint8_t slot = PCI_SLOT(dev->devfn); uint8_t func; if (dev->cap_present & QEMU_PCI_CAP_MULTIFUNCTION) {
dev->config[PCI_HEADER_TYPE] |= PCI_HEADER_TYPE_MULTI_FUNCTION; } /* * multifunction bit is interpreted in two ways as follows. * - all functions must set the bit to 1. * Example: Intel X53 * - function 0 must set the bit, but the rest function (> 0) * is allowed to leave the bit to 0. * Example: PIIX3(also in qemu), PIIX4(also in qemu), ICH10, * * So OS (at least Linux) checks the bit of only function 0, * and doesn't see the bit of function > 0. * * The below check allows both interpretation. */ if (PCI_FUNC(dev->devfn)) {
PCIDevice *f0 = bus->devices[PCI_DEVFN(slot, 0)]; if (f0 && !(f0->cap_present & QEMU_PCI_CAP_MULTIFUNCTION)) {
/* function 0 should set multifunction bit */ error_report(""PCI: single function device can't be populated "" ""in function %x.%x"", slot, PCI_FUNC(dev->devfn)); return -1; } return 0; } if (dev->cap_present & QEMU_PCI_CAP_MULTIFUNCTION) {
return 0; } /* function 0 indicates single function, so function > 0 must be NULL */ for (func = 1; func < PCI_FUNC_MAX; ++func) {
if (bus->devices[PCI_DEVFN(slot, func)]) {
error_report(""PCI: %x.0 indicates single function, "" ""but %x.%x is already populated."", slot, slot, func); return -1; } } return 0; } ",qemu,1
"static void pc_machine_device_post_plug_cb(HotplugHandler *hotplug_dev,

                                           DeviceState *dev, Error **errp)

{

    if (object_dynamic_cast(OBJECT(dev), TYPE_PC_DIMM)) {

        pc_dimm_post_plug(hotplug_dev, dev, errp);

    }

}
","static void pc_machine_device_post_plug_cb(HotplugHandler *hotplug_dev, DeviceState *dev, Error **errp) {
if (object_dynamic_cast(OBJECT(dev), TYPE_PC_DIMM)) {
pc_dimm_post_plug(hotplug_dev, dev, errp); } } ",qemu,0
"static void megasas_mmio_write(void *opaque, hwaddr addr,

                               uint64_t val, unsigned size)

{

    MegasasState *s = opaque;

    PCIDevice *pci_dev = PCI_DEVICE(s);

    uint64_t frame_addr;

    uint32_t frame_count;

    int i;



    switch (addr) {

    case MFI_IDB:

        trace_megasas_mmio_writel(""MFI_IDB"", val);

        if (val & MFI_FWINIT_ABORT) {

            /* Abort all pending cmds */

            for (i = 0; i < s->fw_cmds; i++) {

                megasas_abort_command(&s->frames[i]);

            }

        }

        if (val & MFI_FWINIT_READY) {

            /* move to FW READY */

            megasas_soft_reset(s);

        }

        if (val & MFI_FWINIT_MFIMODE) {

            /* discard MFIs */

        }

        if (val & MFI_FWINIT_STOP_ADP) {

            /* Terminal error, stop processing */

            s->fw_state = MFI_FWSTATE_FAULT;

        }

        break;

    case MFI_OMSK:

        trace_megasas_mmio_writel(""MFI_OMSK"", val);

        s->intr_mask = val;

        if (!megasas_intr_enabled(s) &&

            !msi_enabled(pci_dev) &&

            !msix_enabled(pci_dev)) {

            trace_megasas_irq_lower();

            pci_irq_deassert(pci_dev);

        }

        if (megasas_intr_enabled(s)) {

            if (msix_enabled(pci_dev)) {

                trace_megasas_msix_enabled(0);

            } else if (msi_enabled(pci_dev)) {

                trace_megasas_msi_enabled(0);

            } else {

                trace_megasas_intr_enabled();

            }

        } else {

            trace_megasas_intr_disabled();

            megasas_soft_reset(s);

        }

        break;

    case MFI_ODCR0:

        trace_megasas_mmio_writel(""MFI_ODCR0"", val);

        s->doorbell = 0;

        if (megasas_intr_enabled(s)) {

            if (!msix_enabled(pci_dev) && !msi_enabled(pci_dev)) {

                trace_megasas_irq_lower();

                pci_irq_deassert(pci_dev);

            }

        }

        break;

    case MFI_IQPH:

        trace_megasas_mmio_writel(""MFI_IQPH"", val);

        /* Received high 32 bits of a 64 bit MFI frame address */

        s->frame_hi = val;

        break;

    case MFI_IQPL:

        trace_megasas_mmio_writel(""MFI_IQPL"", val);

        /* Received low 32 bits of a 64 bit MFI frame address */

        /* Fallthrough */

    case MFI_IQP:

        if (addr == MFI_IQP) {

            trace_megasas_mmio_writel(""MFI_IQP"", val);

            /* Received 64 bit MFI frame address */

            s->frame_hi = 0;

        }

        frame_addr = (val & ~0x1F);

        /* Add possible 64 bit offset */

        frame_addr |= ((uint64_t)s->frame_hi << 32);

        s->frame_hi = 0;

        frame_count = (val >> 1) & 0xF;

        megasas_handle_frame(s, frame_addr, frame_count);

        break;

    case MFI_SEQ:

        trace_megasas_mmio_writel(""MFI_SEQ"", val);

        /* Magic sequence to start ADP reset */

        if (adp_reset_seq[s->adp_reset] == val) {

            s->adp_reset++;

        } else {

            s->adp_reset = 0;

            s->diag = 0;

        }

        if (s->adp_reset == 6) {

            s->diag = MFI_DIAG_WRITE_ENABLE;

        }

        break;

    case MFI_DIAG:

        trace_megasas_mmio_writel(""MFI_DIAG"", val);

        /* ADP reset */

        if ((s->diag & MFI_DIAG_WRITE_ENABLE) &&

            (val & MFI_DIAG_RESET_ADP)) {

            s->diag |= MFI_DIAG_RESET_ADP;

            megasas_soft_reset(s);

            s->adp_reset = 0;

            s->diag = 0;

        }

        break;

    default:

        trace_megasas_mmio_invalid_writel(addr, val);

        break;

    }

}
","static void megasas_mmio_write(void *opaque, hwaddr addr, uint64_t val, unsigned size) {
MegasasState *s = opaque; PCIDevice *pci_dev = PCI_DEVICE(s); uint64_t frame_addr; uint32_t frame_count; int i; switch (addr) {
case MFI_IDB: trace_megasas_mmio_writel(""MFI_IDB"", val); if (val & MFI_FWINIT_ABORT) {
/* Abort all pending cmds */ for (i = 0; i < s->fw_cmds; i++) {
megasas_abort_command(&s->frames[i]); } } if (val & MFI_FWINIT_READY) {
/* move to FW READY */ megasas_soft_reset(s); } if (val & MFI_FWINIT_MFIMODE) {
/* discard MFIs */ } if (val & MFI_FWINIT_STOP_ADP) {
/* Terminal error, stop processing */ s->fw_state = MFI_FWSTATE_FAULT; } break; case MFI_OMSK: trace_megasas_mmio_writel(""MFI_OMSK"", val); s->intr_mask = val; if (!megasas_intr_enabled(s) && !msi_enabled(pci_dev) && !msix_enabled(pci_dev)) {
trace_megasas_irq_lower(); pci_irq_deassert(pci_dev); } if (megasas_intr_enabled(s)) {
if (msix_enabled(pci_dev)) {
trace_megasas_msix_enabled(0); } else if (msi_enabled(pci_dev)) {
trace_megasas_msi_enabled(0); } else {
trace_megasas_intr_enabled(); } } else {
trace_megasas_intr_disabled(); megasas_soft_reset(s); } break; case MFI_ODCR0: trace_megasas_mmio_writel(""MFI_ODCR0"", val); s->doorbell = 0; if (megasas_intr_enabled(s)) {
if (!msix_enabled(pci_dev) && !msi_enabled(pci_dev)) {
trace_megasas_irq_lower(); pci_irq_deassert(pci_dev); } } break; case MFI_IQPH: trace_megasas_mmio_writel(""MFI_IQPH"", val); /* Received high 32 bits of a 64 bit MFI frame address */ s->frame_hi = val; break; case MFI_IQPL: trace_megasas_mmio_writel(""MFI_IQPL"", val); /* Received low 32 bits of a 64 bit MFI frame address */ /* Fallthrough */ case MFI_IQP: if (addr == MFI_IQP) {
trace_megasas_mmio_writel(""MFI_IQP"", val); /* Received 64 bit MFI frame address */ s->frame_hi = 0; } frame_addr = (val & ~0x1F); /* Add possible 64 bit offset */ frame_addr |= ((uint64_t)s->frame_hi << 32); s->frame_hi = 0; frame_count = (val >> 1) & 0xF; megasas_handle_frame(s, frame_addr, frame_count); break; case MFI_SEQ: trace_megasas_mmio_writel(""MFI_SEQ"", val); /* Magic sequence to start ADP reset */ if (adp_reset_seq[s->adp_reset] == val) {
s->adp_reset++; } else {
s->adp_reset = 0; s->diag = 0; } if (s->adp_reset == 6) {
s->diag = MFI_DIAG_WRITE_ENABLE; } break; case MFI_DIAG: trace_megasas_mmio_writel(""MFI_DIAG"", val); /* ADP reset */ if ((s->diag & MFI_DIAG_WRITE_ENABLE) && (val & MFI_DIAG_RESET_ADP)) {
s->diag |= MFI_DIAG_RESET_ADP; megasas_soft_reset(s); s->adp_reset = 0; s->diag = 0; } break; default: trace_megasas_mmio_invalid_writel(addr, val); break; } } ",qemu,1
"static void lsi_do_dma(LSIState *s, int out)

{

    uint32_t count;

    target_phys_addr_t addr;



    if (!s->current_dma_len) {

        /* Wait until data is available.  */

        DPRINTF(""DMA no data available\n"");

        return;

    }



    count = s->dbc;

    if (count > s->current_dma_len)

        count = s->current_dma_len;



    addr = s->dnad;

    if (lsi_dma_40bit(s))

        addr |= ((uint64_t)s->dnad64 << 32);

    else if (s->sbms)

        addr |= ((uint64_t)s->sbms << 32);



    DPRINTF(""DMA addr=0x"" TARGET_FMT_plx "" len=%d\n"", addr, count);

    s->csbc += count;

    s->dnad += count;

    s->dbc -= count;



    if (s->dma_buf == NULL) {

        s->dma_buf = s->current_dev->get_buf(s->current_dev,

                                             s->current_tag);

    }



    /* ??? Set SFBR to first data byte.  */

    if (out) {

        cpu_physical_memory_read(addr, s->dma_buf, count);

    } else {

        cpu_physical_memory_write(addr, s->dma_buf, count);

    }

    s->current_dma_len -= count;

    if (s->current_dma_len == 0) {

        s->dma_buf = NULL;

        if (out) {

            /* Write the data.  */

            s->current_dev->write_data(s->current_dev, s->current_tag);

        } else {

            /* Request any remaining data.  */

            s->current_dev->read_data(s->current_dev, s->current_tag);

        }

    } else {

        s->dma_buf += count;

        lsi_resume_script(s);

    }

}
","static void lsi_do_dma(LSIState *s, int out) {
uint32_t count; target_phys_addr_t addr; if (!s->current_dma_len) {
/* Wait until data is available. */ DPRINTF(""DMA no data available\n""); return; } count = s->dbc; if (count > s->current_dma_len) count = s->current_dma_len; addr = s->dnad; if (lsi_dma_40bit(s)) addr |= ((uint64_t)s->dnad64 << 32); else if (s->sbms) addr |= ((uint64_t)s->sbms << 32); DPRINTF(""DMA addr=0x"" TARGET_FMT_plx "" len=%d\n"", addr, count); s->csbc += count; s->dnad += count; s->dbc -= count; if (s->dma_buf == NULL) {
s->dma_buf = s->current_dev->get_buf(s->current_dev, s->current_tag); } /* ??? Set SFBR to first data byte. */ if (out) {
cpu_physical_memory_read(addr, s->dma_buf, count); } else {
cpu_physical_memory_write(addr, s->dma_buf, count); } s->current_dma_len -= count; if (s->current_dma_len == 0) {
s->dma_buf = NULL; if (out) {
/* Write the data. */ s->current_dev->write_data(s->current_dev, s->current_tag); } else {
/* Request any remaining data. */ s->current_dev->read_data(s->current_dev, s->current_tag); } } else {
s->dma_buf += count; lsi_resume_script(s); } } ",qemu,0
"static PCIDevice *do_pci_register_device(PCIDevice *pci_dev, PCIBus *bus,

                                         const char *name, int devfn,

                                         Error **errp)

{

    PCIDeviceClass *pc = PCI_DEVICE_GET_CLASS(pci_dev);

    PCIConfigReadFunc *config_read = pc->config_read;

    PCIConfigWriteFunc *config_write = pc->config_write;

    Error *local_err = NULL;

    DeviceState *dev = DEVICE(pci_dev);



    pci_dev->bus = bus;

    /* Only pci bridges can be attached to extra PCI root buses */

    if (pci_bus_is_root(bus) && bus->parent_dev && !pc->is_bridge) {

        error_setg(errp,

                   ""PCI: Only PCI/PCIe bridges can be plugged into %s"",

                    bus->parent_dev->name);

        return NULL;

    }



    if (devfn < 0) {

        for(devfn = bus->devfn_min ; devfn < ARRAY_SIZE(bus->devices);

            devfn += PCI_FUNC_MAX) {

            if (!bus->devices[devfn])

                goto found;

        }

        error_setg(errp, ""PCI: no slot/function available for %s, all in use"",

                   name);

        return NULL;

    found: ;

    } else if (bus->devices[devfn]) {

        error_setg(errp, ""PCI: slot %d function %d not available for %s,""

                   "" in use by %s"",

                   PCI_SLOT(devfn), PCI_FUNC(devfn), name,

                   bus->devices[devfn]->name);

        return NULL;

    } else if (dev->hotplugged &&

               pci_get_function_0(pci_dev)) {

        error_setg(errp, ""PCI: slot %d function 0 already ocuppied by %s,""

                   "" new func %s cannot be exposed to guest."",

                   PCI_SLOT(pci_get_function_0(pci_dev)->devfn),

                   pci_get_function_0(pci_dev)->name,

                   name);



       return NULL;

    }



    pci_dev->devfn = devfn;

    pci_dev->requester_id_cache = pci_req_id_cache_get(pci_dev);



    memory_region_init(&pci_dev->bus_master_container_region, OBJECT(pci_dev),

                       ""bus master container"", UINT64_MAX);

    address_space_init(&pci_dev->bus_master_as,

                       &pci_dev->bus_master_container_region, pci_dev->name);



    if (qdev_hotplug) {

        pci_init_bus_master(pci_dev);

    }

    pstrcpy(pci_dev->name, sizeof(pci_dev->name), name);

    pci_dev->irq_state = 0;

    pci_config_alloc(pci_dev);



    pci_config_set_vendor_id(pci_dev->config, pc->vendor_id);

    pci_config_set_device_id(pci_dev->config, pc->device_id);

    pci_config_set_revision(pci_dev->config, pc->revision);

    pci_config_set_class(pci_dev->config, pc->class_id);



    if (!pc->is_bridge) {

        if (pc->subsystem_vendor_id || pc->subsystem_id) {

            pci_set_word(pci_dev->config + PCI_SUBSYSTEM_VENDOR_ID,

                         pc->subsystem_vendor_id);

            pci_set_word(pci_dev->config + PCI_SUBSYSTEM_ID,

                         pc->subsystem_id);

        } else {

            pci_set_default_subsystem_id(pci_dev);

        }

    } else {

        /* subsystem_vendor_id/subsystem_id are only for header type 0 */

        assert(!pc->subsystem_vendor_id);

        assert(!pc->subsystem_id);

    }

    pci_init_cmask(pci_dev);

    pci_init_wmask(pci_dev);

    pci_init_w1cmask(pci_dev);

    if (pc->is_bridge) {

        pci_init_mask_bridge(pci_dev);

    }

    pci_init_multifunction(bus, pci_dev, &local_err);

    if (local_err) {

        error_propagate(errp, local_err);

        do_pci_unregister_device(pci_dev);

        return NULL;

    }



    if (!config_read)

        config_read = pci_default_read_config;

    if (!config_write)

        config_write = pci_default_write_config;

    pci_dev->config_read = config_read;

    pci_dev->config_write = config_write;

    bus->devices[devfn] = pci_dev;

    pci_dev->version_id = 2; /* Current pci device vmstate version */

    return pci_dev;

}
","static PCIDevice *do_pci_register_device(PCIDevice *pci_dev, PCIBus *bus, const char *name, int devfn, Error **errp) {
PCIDeviceClass *pc = PCI_DEVICE_GET_CLASS(pci_dev); PCIConfigReadFunc *config_read = pc->config_read; PCIConfigWriteFunc *config_write = pc->config_write; Error *local_err = NULL; DeviceState *dev = DEVICE(pci_dev); pci_dev->bus = bus; /* Only pci bridges can be attached to extra PCI root buses */ if (pci_bus_is_root(bus) && bus->parent_dev && !pc->is_bridge) {
error_setg(errp, ""PCI: Only PCI/PCIe bridges can be plugged into %s"", bus->parent_dev->name); return NULL; } if (devfn < 0) {
for(devfn = bus->devfn_min ; devfn < ARRAY_SIZE(bus->devices); devfn += PCI_FUNC_MAX) {
if (!bus->devices[devfn]) goto found; } error_setg(errp, ""PCI: no slot/function available for %s, all in use"", name); return NULL; found: ; } else if (bus->devices[devfn]) {
error_setg(errp, ""PCI: slot %d function %d not available for %s,"" "" in use by %s"", PCI_SLOT(devfn), PCI_FUNC(devfn), name, bus->devices[devfn]->name); return NULL; } else if (dev->hotplugged && pci_get_function_0(pci_dev)) {
error_setg(errp, ""PCI: slot %d function 0 already ocuppied by %s,"" "" new func %s cannot be exposed to guest."", PCI_SLOT(pci_get_function_0(pci_dev)->devfn), pci_get_function_0(pci_dev)->name, name); return NULL; } pci_dev->devfn = devfn; pci_dev->requester_id_cache = pci_req_id_cache_get(pci_dev); memory_region_init(&pci_dev->bus_master_container_region, OBJECT(pci_dev), ""bus master container"", UINT64_MAX); address_space_init(&pci_dev->bus_master_as, &pci_dev->bus_master_container_region, pci_dev->name); if (qdev_hotplug) {
pci_init_bus_master(pci_dev); } pstrcpy(pci_dev->name, sizeof(pci_dev->name), name); pci_dev->irq_state = 0; pci_config_alloc(pci_dev); pci_config_set_vendor_id(pci_dev->config, pc->vendor_id); pci_config_set_device_id(pci_dev->config, pc->device_id); pci_config_set_revision(pci_dev->config, pc->revision); pci_config_set_class(pci_dev->config, pc->class_id); if (!pc->is_bridge) {
if (pc->subsystem_vendor_id || pc->subsystem_id) {
pci_set_word(pci_dev->config + PCI_SUBSYSTEM_VENDOR_ID, pc->subsystem_vendor_id); pci_set_word(pci_dev->config + PCI_SUBSYSTEM_ID, pc->subsystem_id); } else {
pci_set_default_subsystem_id(pci_dev); } } else {
/* subsystem_vendor_id/subsystem_id are only for header type 0 */ assert(!pc->subsystem_vendor_id); assert(!pc->subsystem_id); } pci_init_cmask(pci_dev); pci_init_wmask(pci_dev); pci_init_w1cmask(pci_dev); if (pc->is_bridge) {
pci_init_mask_bridge(pci_dev); } pci_init_multifunction(bus, pci_dev, &local_err); if (local_err) {
error_propagate(errp, local_err); do_pci_unregister_device(pci_dev); return NULL; } if (!config_read) config_read = pci_default_read_config; if (!config_write) config_write = pci_default_write_config; pci_dev->config_read = config_read; pci_dev->config_write = config_write; bus->devices[devfn] = pci_dev; pci_dev->version_id = 2; /* Current pci device vmstate version */ return pci_dev; } ",qemu,0
"static void address_space_update_topology(AddressSpace *as)

{

    FlatView old_view = as->current_map;

    FlatView new_view = generate_memory_topology(as->root);



    address_space_update_topology_pass(as, old_view, new_view, false);

    address_space_update_topology_pass(as, old_view, new_view, true);



    as->current_map = new_view;

    flatview_destroy(&old_view);

    address_space_update_ioeventfds(as);

}
","static void address_space_update_topology(AddressSpace *as) {
FlatView old_view = as->current_map; FlatView new_view = generate_memory_topology(as->root); address_space_update_topology_pass(as, old_view, new_view, false); address_space_update_topology_pass(as, old_view, new_view, true); as->current_map = new_view; flatview_destroy(&old_view); address_space_update_ioeventfds(as); } ",qemu,1
"void host_to_target_siginfo(target_siginfo_t *tinfo, const siginfo_t *info)

{

    host_to_target_siginfo_noswap(tinfo, info);

    tswap_siginfo(tinfo, tinfo);

}
","void host_to_target_siginfo(target_siginfo_t *tinfo, const siginfo_t *info) {
host_to_target_siginfo_noswap(tinfo, info); tswap_siginfo(tinfo, tinfo); } ",qemu,1
"static void qemu_rdma_init_one_block(void *host_addr,

    ram_addr_t block_offset, ram_addr_t length, void *opaque)

{

    __qemu_rdma_add_block(opaque, host_addr, block_offset, length);

}
","static void qemu_rdma_init_one_block(void *host_addr, ram_addr_t block_offset, ram_addr_t length, void *opaque) {
__qemu_rdma_add_block(opaque, host_addr, block_offset, length); } ",qemu,1
"static uint32_t pci_unin_config_readl (void *opaque,

                                       target_phys_addr_t addr)

{

    UNINState *s = opaque;



    return s->config_reg;

}
","static uint32_t pci_unin_config_readl (void *opaque, target_phys_addr_t addr) {
UNINState *s = opaque; return s->config_reg; } ",qemu,1
"void fw_cfg_add_i32(FWCfgState *s, uint16_t key, uint32_t value)

{

    uint32_t *copy;



    copy = g_malloc(sizeof(value));

    *copy = cpu_to_le32(value);

    fw_cfg_add_bytes(s, key, (uint8_t *)copy, sizeof(value));

}
","void fw_cfg_add_i32(FWCfgState *s, uint16_t key, uint32_t value) {
uint32_t *copy; copy = g_malloc(sizeof(value)); *copy = cpu_to_le32(value); fw_cfg_add_bytes(s, key, (uint8_t *)copy, sizeof(value)); } ",qemu,1
"static int vmdk_write(BlockDriverState *bs, int64_t sector_num,

                     const uint8_t *buf, int nb_sectors)

{

    BDRVVmdkState *s = bs->opaque;

    VmdkExtent *extent = NULL;

    int n;

    int64_t index_in_cluster;

    uint64_t cluster_offset;

    VmdkMetaData m_data;



    if (sector_num > bs->total_sectors) {

        fprintf(stderr,

                ""(VMDK) Wrong offset: sector_num=0x%"" PRIx64

                "" total_sectors=0x%"" PRIx64 ""\n"",

                sector_num, bs->total_sectors);

        return -1;

    }



    while (nb_sectors > 0) {

        extent = find_extent(s, sector_num, extent);

        if (!extent) {

            return -EIO;

        }

        cluster_offset = get_cluster_offset(

                                bs,

                                extent,

                                &m_data,

                                sector_num << 9, 1);

        if (!cluster_offset) {

            return -1;

        }

        index_in_cluster = sector_num % extent->cluster_sectors;

        n = extent->cluster_sectors - index_in_cluster;

        if (n > nb_sectors) {

            n = nb_sectors;

        }



        if (bdrv_pwrite(bs->file,

                        cluster_offset + index_in_cluster * 512,

                        buf, n * 512)

                != n * 512) {

            return -1;

        }

        if (m_data.valid) {

            /* update L2 tables */

            if (vmdk_L2update(extent, &m_data) == -1) {

                return -1;

            }

        }

        nb_sectors -= n;

        sector_num += n;

        buf += n * 512;



        // update CID on the first write every time the virtual disk is opened

        if (!s->cid_updated) {

            vmdk_write_cid(bs, time(NULL));

            s->cid_updated = true;

        }

    }

    return 0;

}
","static int vmdk_write(BlockDriverState *bs, int64_t sector_num, const uint8_t *buf, int nb_sectors) {
BDRVVmdkState *s = bs->opaque; VmdkExtent *extent = NULL; int n; int64_t index_in_cluster; uint64_t cluster_offset; VmdkMetaData m_data; if (sector_num > bs->total_sectors) {
fprintf(stderr, ""(VMDK) Wrong offset: sector_num=0x%"" PRIx64 "" total_sectors=0x%"" PRIx64 ""\n"", sector_num, bs->total_sectors); return -1; } while (nb_sectors > 0) {
extent = find_extent(s, sector_num, extent); if (!extent) {
return -EIO; } cluster_offset = get_cluster_offset( bs, extent, &m_data, sector_num << 9, 1); if (!cluster_offset) {
return -1; } index_in_cluster = sector_num % extent->cluster_sectors; n = extent->cluster_sectors - index_in_cluster; if (n > nb_sectors) {
n = nb_sectors; } if (bdrv_pwrite(bs->file, cluster_offset + index_in_cluster * 512, buf, n * 512) != n * 512) {
return -1; } if (m_data.valid) {
/* update L2 tables */ if (vmdk_L2update(extent, &m_data) == -1) {
return -1; } } nb_sectors -= n; sector_num += n; buf += n * 512; // update CID on the first write every time the virtual disk is opened if (!s->cid_updated) {
vmdk_write_cid(bs, time(NULL)); s->cid_updated = true; } } return 0; } ",qemu,0
"int gdbserver_start(const char *device)

{

    GDBState *s;

    char gdbstub_device_name[128];

    CharDriverState *chr = NULL;

    CharDriverState *mon_chr;



    if (!device)

        return -1;

    if (strcmp(device, ""none"") != 0) {

        if (strstart(device, ""tcp:"", NULL)) {

            /* enforce required TCP attributes */

            snprintf(gdbstub_device_name, sizeof(gdbstub_device_name),

                     ""%s,nowait,nodelay,server"", device);

            device = gdbstub_device_name;

        }

#ifndef _WIN32

        else if (strcmp(device, ""stdio"") == 0) {

            struct sigaction act;



            memset(&act, 0, sizeof(act));

            act.sa_handler = gdb_sigterm_handler;

            sigaction(SIGINT, &act, NULL);

        }

#endif

        chr = qemu_chr_new(""gdb"", device, NULL);

        if (!chr)

            return -1;




        qemu_chr_add_handlers(chr, gdb_chr_can_receive, gdb_chr_receive,

                              gdb_chr_event, NULL);

    }



    s = gdbserver_state;

    if (!s) {

        s = g_malloc0(sizeof(GDBState));

        gdbserver_state = s;



        qemu_add_vm_change_state_handler(gdb_vm_state_change, NULL);



        /* Initialize a monitor terminal for gdb */

        mon_chr = g_malloc0(sizeof(*mon_chr));

        mon_chr->chr_write = gdb_monitor_write;

        monitor_init(mon_chr, 0);

    } else {

        if (s->chr)

            qemu_chr_delete(s->chr);

        mon_chr = s->mon_chr;

        memset(s, 0, sizeof(GDBState));

    }

    s->c_cpu = first_cpu;

    s->g_cpu = first_cpu;

    s->chr = chr;

    s->state = chr ? RS_IDLE : RS_INACTIVE;

    s->mon_chr = mon_chr;

    s->current_syscall_cb = NULL;



    return 0;

}","int gdbserver_start(const char *device) {
GDBState *s; char gdbstub_device_name[128]; CharDriverState *chr = NULL; CharDriverState *mon_chr; if (!device) return -1; if (strcmp(device, ""none"") != 0) {
if (strstart(device, ""tcp:"", NULL)) {
/* enforce required TCP attributes */ snprintf(gdbstub_device_name, sizeof(gdbstub_device_name), ""%s,nowait,nodelay,server"", device); device = gdbstub_device_name; } #ifndef _WIN32 else if (strcmp(device, ""stdio"") == 0) {
struct sigaction act; memset(&act, 0, sizeof(act)); act.sa_handler = gdb_sigterm_handler; sigaction(SIGINT, &act, NULL); } #endif chr = qemu_chr_new(""gdb"", device, NULL); if (!chr) return -1; qemu_chr_add_handlers(chr, gdb_chr_can_receive, gdb_chr_receive, gdb_chr_event, NULL); } s = gdbserver_state; if (!s) {
s = g_malloc0(sizeof(GDBState)); gdbserver_state = s; qemu_add_vm_change_state_handler(gdb_vm_state_change, NULL); /* Initialize a monitor terminal for gdb */ mon_chr = g_malloc0(sizeof(*mon_chr)); mon_chr->chr_write = gdb_monitor_write; monitor_init(mon_chr, 0); } else {
if (s->chr) qemu_chr_delete(s->chr); mon_chr = s->mon_chr; memset(s, 0, sizeof(GDBState)); } s->c_cpu = first_cpu; s->g_cpu = first_cpu; s->chr = chr; s->state = chr ? RS_IDLE : RS_INACTIVE; s->mon_chr = mon_chr; s->current_syscall_cb = NULL; return 0; }",qemu,1
"int qcrypto_pbkdf2_count_iters(QCryptoHashAlgorithm hash,

                               const uint8_t *key, size_t nkey,

                               const uint8_t *salt, size_t nsalt,

                               Error **errp)

{

    uint8_t out[32];

    long long int iterations = (1 << 15);

    unsigned long long delta_ms, start_ms, end_ms;



    while (1) {

        if (qcrypto_pbkdf2_get_thread_cpu(&start_ms, errp) < 0) {

            return -1;

        }

        if (qcrypto_pbkdf2(hash,

                           key, nkey,

                           salt, nsalt,

                           iterations,

                           out, sizeof(out),

                           errp) < 0) {

            return -1;

        }

        if (qcrypto_pbkdf2_get_thread_cpu(&end_ms, errp) < 0) {

            return -1;

        }



        delta_ms = end_ms - start_ms;



        if (delta_ms > 500) {

            break;

        } else if (delta_ms < 100) {

            iterations = iterations * 10;

        } else {

            iterations = (iterations * 1000 / delta_ms);

        }

    }



    iterations = iterations * 1000 / delta_ms;



    if (iterations > INT32_MAX) {

        error_setg(errp, ""Iterations %lld too large for a 32-bit int"",

                   iterations);

        return -1;

    }



    return iterations;

}
","int qcrypto_pbkdf2_count_iters(QCryptoHashAlgorithm hash, const uint8_t *key, size_t nkey, const uint8_t *salt, size_t nsalt, Error **errp) {
uint8_t out[32]; long long int iterations = (1 << 15); unsigned long long delta_ms, start_ms, end_ms; while (1) {
if (qcrypto_pbkdf2_get_thread_cpu(&start_ms, errp) < 0) {
return -1; } if (qcrypto_pbkdf2(hash, key, nkey, salt, nsalt, iterations, out, sizeof(out), errp) < 0) {
return -1; } if (qcrypto_pbkdf2_get_thread_cpu(&end_ms, errp) < 0) {
return -1; } delta_ms = end_ms - start_ms; if (delta_ms > 500) {
break; } else if (delta_ms < 100) {
iterations = iterations * 10; } else {
iterations = (iterations * 1000 / delta_ms); } } iterations = iterations * 1000 / delta_ms; if (iterations > INT32_MAX) {
error_setg(errp, ""Iterations %lld too large for a 32-bit int"", iterations); return -1; } return iterations; } ",qemu,0
"static int qxl_init_primary(PCIDevice *dev)

{

    PCIQXLDevice *qxl = DO_UPCAST(PCIQXLDevice, pci, dev);

    VGACommonState *vga = &qxl->vga;

    PortioList *qxl_vga_port_list = g_new(PortioList, 1);

    int rc;



    qxl->id = 0;

    qxl_init_ramsize(qxl);

    vga->vram_size_mb = qxl->vga.vram_size >> 20;

    vga_common_init(vga, OBJECT(dev), true);

    vga_init(vga, OBJECT(dev),

             pci_address_space(dev), pci_address_space_io(dev), false);

    portio_list_init(qxl_vga_port_list, OBJECT(dev), qxl_vga_portio_list,

                     vga, ""vga"");

    portio_list_set_flush_coalesced(qxl_vga_port_list);

    portio_list_add(qxl_vga_port_list, pci_address_space_io(dev), 0x3b0);



    vga->con = graphic_console_init(DEVICE(dev), 0, &qxl_ops, qxl);

    qemu_spice_display_init_common(&qxl->ssd);



    rc = qxl_init_common(qxl);

    if (rc != 0) {

        return rc;

    }



    qxl->ssd.dcl.ops = &display_listener_ops;

    qxl->ssd.dcl.con = vga->con;

    register_displaychangelistener(&qxl->ssd.dcl);

    return rc;

}
","static int qxl_init_primary(PCIDevice *dev) {
PCIQXLDevice *qxl = DO_UPCAST(PCIQXLDevice, pci, dev); VGACommonState *vga = &qxl->vga; PortioList *qxl_vga_port_list = g_new(PortioList, 1); int rc; qxl->id = 0; qxl_init_ramsize(qxl); vga->vram_size_mb = qxl->vga.vram_size >> 20; vga_common_init(vga, OBJECT(dev), true); vga_init(vga, OBJECT(dev), pci_address_space(dev), pci_address_space_io(dev), false); portio_list_init(qxl_vga_port_list, OBJECT(dev), qxl_vga_portio_list, vga, ""vga""); portio_list_set_flush_coalesced(qxl_vga_port_list); portio_list_add(qxl_vga_port_list, pci_address_space_io(dev), 0x3b0); vga->con = graphic_console_init(DEVICE(dev), 0, &qxl_ops, qxl); qemu_spice_display_init_common(&qxl->ssd); rc = qxl_init_common(qxl); if (rc != 0) {
return rc; } qxl->ssd.dcl.ops = &display_listener_ops; qxl->ssd.dcl.con = vga->con; register_displaychangelistener(&qxl->ssd.dcl); return rc; } ",qemu,1
"static void scsi_destroy(SCSIDevice *s)

{

    scsi_device_purge_requests(s, SENSE_CODE(NO_SENSE));

    blockdev_mark_auto_del(s->conf.bs);

}
","static void scsi_destroy(SCSIDevice *s) {
scsi_device_purge_requests(s, SENSE_CODE(NO_SENSE)); blockdev_mark_auto_del(s->conf.bs); } ",qemu,0
"static void vnc_async_encoding_end(VncState *orig, VncState *local)
{
    orig->tight = local->tight;
    orig->zlib = local->zlib;
    orig->hextile = local->hextile;
    orig->zrle = local->zrle;
    orig->lossy_rect = local->lossy_rect;
}","static void vnc_async_encoding_end(VncState *orig, VncState *local) {
orig->tight = local->tight; orig->zlib = local->zlib; orig->hextile = local->hextile; orig->zrle = local->zrle; orig->lossy_rect = local->lossy_rect; }",qemu,1
"static const char *exynos4210_uart_regname(target_phys_addr_t  offset)

{



    int regs_number = sizeof(exynos4210_uart_regs) / sizeof(Exynos4210UartReg);

    int i;



    for (i = 0; i < regs_number; i++) {

        if (offset == exynos4210_uart_regs[i].offset) {

            return exynos4210_uart_regs[i].name;

        }

    }



    return NULL;

}
","static const char *exynos4210_uart_regname(target_phys_addr_t offset) {
int regs_number = sizeof(exynos4210_uart_regs) / sizeof(Exynos4210UartReg); int i; for (i = 0; i < regs_number; i++) {
if (offset == exynos4210_uart_regs[i].offset) {
return exynos4210_uart_regs[i].name; } } return NULL; } ",qemu,0
"static int get_physical_address_code(CPUState *env,

                                     target_phys_addr_t *physical, int *prot,

                                     target_ulong address, int is_user)

{

    target_ulong mask;

    unsigned int i;



    if ((env->lsu & IMMU_E) == 0) { /* IMMU disabled */

        *physical = address;

        *prot = PAGE_EXEC;

        return 0;

    }



    for (i = 0; i < 64; i++) {

        switch ((env->itlb_tte[i] >> 61) & 3) {

        default:

        case 0x0: // 8k

            mask = 0xffffffffffffe000ULL;

            break;

        case 0x1: // 64k

            mask = 0xffffffffffff0000ULL;

            break;

        case 0x2: // 512k

            mask = 0xfffffffffff80000ULL;

            break;

        case 0x3: // 4M

            mask = 0xffffffffffc00000ULL;

                break;

        }

        // ctx match, vaddr match, valid?

        if (env->dmmuregs[1] == (env->itlb_tag[i] & 0x1fff) &&

            (address & mask) == (env->itlb_tag[i] & mask) &&

            (env->itlb_tte[i] & 0x8000000000000000ULL)) {

            // access ok?

            if ((env->itlb_tte[i] & 0x4) && is_user) {

                if (env->immuregs[3]) /* Fault status register */

                    env->immuregs[3] = 2; /* overflow (not read before

                                             another fault) */

                env->immuregs[3] |= (is_user << 3) | 1;

                env->exception_index = TT_TFAULT;

#ifdef DEBUG_MMU

                printf(""TFAULT at 0x%"" PRIx64 ""\n"", address);

#endif

                return 1;

            }

            *physical = ((env->itlb_tte[i] & mask) | (address & ~mask)) &

                        0x1ffffffe000ULL;

            *prot = PAGE_EXEC;

            return 0;

        }

    }

#ifdef DEBUG_MMU

    printf(""TMISS at 0x%"" PRIx64 ""\n"", address);

#endif

    /* Context is stored in DMMU (dmmuregs[1]) also for IMMU */

    env->immuregs[6] = (address & ~0x1fffULL) | (env->dmmuregs[1] & 0x1fff);

    env->exception_index = TT_TMISS;

    return 1;

}
","static int get_physical_address_code(CPUState *env, target_phys_addr_t *physical, int *prot, target_ulong address, int is_user) {
target_ulong mask; unsigned int i; if ((env->lsu & IMMU_E) == 0) {
/* IMMU disabled */ *physical = address; *prot = PAGE_EXEC; return 0; } for (i = 0; i < 64; i++) {
switch ((env->itlb_tte[i] >> 61) & 3) {
default: case 0x0: // 8k mask = 0xffffffffffffe000ULL; break; case 0x1: // 64k mask = 0xffffffffffff0000ULL; break; case 0x2: // 512k mask = 0xfffffffffff80000ULL; break; case 0x3: // 4M mask = 0xffffffffffc00000ULL; break; } // ctx match, vaddr match, valid? if (env->dmmuregs[1] == (env->itlb_tag[i] & 0x1fff) && (address & mask) == (env->itlb_tag[i] & mask) && (env->itlb_tte[i] & 0x8000000000000000ULL)) {
// access ok? if ((env->itlb_tte[i] & 0x4) && is_user) {
if (env->immuregs[3]) /* Fault status register */ env->immuregs[3] = 2; /* overflow (not read before another fault) */ env->immuregs[3] |= (is_user << 3) | 1; env->exception_index = TT_TFAULT; #ifdef DEBUG_MMU printf(""TFAULT at 0x%"" PRIx64 ""\n"", address); #endif return 1; } *physical = ((env->itlb_tte[i] & mask) | (address & ~mask)) & 0x1ffffffe000ULL; *prot = PAGE_EXEC; return 0; } } #ifdef DEBUG_MMU printf(""TMISS at 0x%"" PRIx64 ""\n"", address); #endif /* Context is stored in DMMU (dmmuregs[1]) also for IMMU */ env->immuregs[6] = (address & ~0x1fffULL) | (env->dmmuregs[1] & 0x1fff); env->exception_index = TT_TMISS; return 1; } ",qemu,0
"static void s390_pci_generate_error_event(uint16_t pec, uint32_t fh,

                                          uint32_t fid, uint64_t faddr,

                                          uint32_t e)

{

    s390_pci_generate_event(1, pec, fh, fid, faddr, e);

}
","static void s390_pci_generate_error_event(uint16_t pec, uint32_t fh, uint32_t fid, uint64_t faddr, uint32_t e) {
s390_pci_generate_event(1, pec, fh, fid, faddr, e); } ",qemu,0
"static struct glfs *qemu_gluster_glfs_init(BlockdevOptionsGluster *gconf,

                                           Error **errp)

{

    struct glfs *glfs;

    int ret;

    int old_errno;

    GlusterServerList *server;



    glfs = glfs_new(gconf->volume);

    if (!glfs) {

        goto out;

    }



    for (server = gconf->server; server; server = server->next) {

        if (server->value->type  == GLUSTER_TRANSPORT_UNIX) {

            ret = glfs_set_volfile_server(glfs,

                                   GlusterTransport_lookup[server->value->type],

                                   server->value->u.q_unix.path, 0);

        } else {

            ret = glfs_set_volfile_server(glfs,

                                   GlusterTransport_lookup[server->value->type],

                                   server->value->u.tcp.host,

                                   atoi(server->value->u.tcp.port));

        }



        if (ret < 0) {

            goto out;

        }

    }



    ret = glfs_set_logging(glfs, ""-"", gconf->debug_level);

    if (ret < 0) {

        goto out;

    }



    ret = glfs_init(glfs);

    if (ret) {

        error_setg(errp, ""Gluster connection for volume %s, path %s failed""

                         "" to connect"", gconf->volume, gconf->path);

        for (server = gconf->server; server; server = server->next) {

            if (server->value->type  == GLUSTER_TRANSPORT_UNIX) {

                error_append_hint(errp, ""hint: failed on socket %s "",

                                  server->value->u.q_unix.path);

            } else {

                error_append_hint(errp, ""hint: failed on host %s and port %s "",

                                  server->value->u.tcp.host,

                                  server->value->u.tcp.port);

            }

        }



        error_append_hint(errp, ""Please refer to gluster logs for more info\n"");



        /* glfs_init sometimes doesn't set errno although docs suggest that */

        if (errno == 0) {

            errno = EINVAL;

        }



        goto out;

    }

    return glfs;



out:

    if (glfs) {

        old_errno = errno;

        glfs_fini(glfs);

        errno = old_errno;

    }

    return NULL;

}
","static struct glfs *qemu_gluster_glfs_init(BlockdevOptionsGluster *gconf, Error **errp) {
struct glfs *glfs; int ret; int old_errno; GlusterServerList *server; glfs = glfs_new(gconf->volume); if (!glfs) {
goto out; } for (server = gconf->server; server; server = server->next) {
if (server->value->type == GLUSTER_TRANSPORT_UNIX) {
ret = glfs_set_volfile_server(glfs, GlusterTransport_lookup[server->value->type], server->value->u.q_unix.path, 0); } else {
ret = glfs_set_volfile_server(glfs, GlusterTransport_lookup[server->value->type], server->value->u.tcp.host, atoi(server->value->u.tcp.port)); } if (ret < 0) {
goto out; } } ret = glfs_set_logging(glfs, ""-"", gconf->debug_level); if (ret < 0) {
goto out; } ret = glfs_init(glfs); if (ret) {
error_setg(errp, ""Gluster connection for volume %s, path %s failed"" "" to connect"", gconf->volume, gconf->path); for (server = gconf->server; server; server = server->next) {
if (server->value->type == GLUSTER_TRANSPORT_UNIX) {
error_append_hint(errp, ""hint: failed on socket %s "", server->value->u.q_unix.path); } else {
error_append_hint(errp, ""hint: failed on host %s and port %s "", server->value->u.tcp.host, server->value->u.tcp.port); } } error_append_hint(errp, ""Please refer to gluster logs for more info\n""); /* glfs_init sometimes doesn't set errno although docs suggest that */ if (errno == 0) {
errno = EINVAL; } goto out; } return glfs; out: if (glfs) {
old_errno = errno; glfs_fini(glfs); errno = old_errno; } return NULL; } ",qemu,1
"static void mtree_print_flatview(fprintf_function p, void *f,

                                 AddressSpace *as)

{

    FlatView *view = address_space_get_flatview(as);

    FlatRange *range = &view->ranges[0];

    MemoryRegion *mr;

    int n = view->nr;



    if (n <= 0) {

        p(f, MTREE_INDENT ""No rendered FlatView for ""

          ""address space '%s'\n"", as->name);

        flatview_unref(view);

        return;

    }



    while (n--) {

        mr = range->mr;

        p(f, MTREE_INDENT TARGET_FMT_plx ""-""

          TARGET_FMT_plx "" (prio %d, %s): %s\n"",

          int128_get64(range->addr.start),

          int128_get64(range->addr.start) + MR_SIZE(range->addr.size),

          mr->priority,

          memory_region_type(mr),

          memory_region_name(mr));

        range++;

    }



    flatview_unref(view);

}
","static void mtree_print_flatview(fprintf_function p, void *f, AddressSpace *as) {
FlatView *view = address_space_get_flatview(as); FlatRange *range = &view->ranges[0]; MemoryRegion *mr; int n = view->nr; if (n <= 0) {
p(f, MTREE_INDENT ""No rendered FlatView for "" ""address space '%s'\n"", as->name); flatview_unref(view); return; } while (n--) {
mr = range->mr; p(f, MTREE_INDENT TARGET_FMT_plx ""-"" TARGET_FMT_plx "" (prio %d, %s): %s\n"", int128_get64(range->addr.start), int128_get64(range->addr.start) + MR_SIZE(range->addr.size), mr->priority, memory_region_type(mr), memory_region_name(mr)); range++; } flatview_unref(view); } ",qemu,0
"static void superh_cpu_class_init(ObjectClass *oc, void *data)

{

    DeviceClass *dc = DEVICE_CLASS(oc);

    CPUClass *cc = CPU_CLASS(oc);

    SuperHCPUClass *scc = SUPERH_CPU_CLASS(oc);



    scc->parent_realize = dc->realize;

    dc->realize = superh_cpu_realizefn;



    scc->parent_reset = cc->reset;

    cc->reset = superh_cpu_reset;



    cc->class_by_name = superh_cpu_class_by_name;

    cc->has_work = superh_cpu_has_work;

    cc->do_interrupt = superh_cpu_do_interrupt;

    cc->cpu_exec_interrupt = superh_cpu_exec_interrupt;

    cc->dump_state = superh_cpu_dump_state;

    cc->set_pc = superh_cpu_set_pc;

    cc->synchronize_from_tb = superh_cpu_synchronize_from_tb;

    cc->gdb_read_register = superh_cpu_gdb_read_register;

    cc->gdb_write_register = superh_cpu_gdb_write_register;

#ifdef CONFIG_USER_ONLY

    cc->handle_mmu_fault = superh_cpu_handle_mmu_fault;

#else

    cc->get_phys_page_debug = superh_cpu_get_phys_page_debug;

#endif

    cc->disas_set_info = superh_cpu_disas_set_info;



    cc->gdb_num_core_regs = 59;



    dc->vmsd = &vmstate_sh_cpu;



    /*

     * Reason: superh_cpu_initfn() calls cpu_exec_init(), which saves

     * the object in cpus -> dangling pointer after final

     * object_unref().

     */

    dc->cannot_destroy_with_object_finalize_yet = true;

}
","static void superh_cpu_class_init(ObjectClass *oc, void *data) {
DeviceClass *dc = DEVICE_CLASS(oc); CPUClass *cc = CPU_CLASS(oc); SuperHCPUClass *scc = SUPERH_CPU_CLASS(oc); scc->parent_realize = dc->realize; dc->realize = superh_cpu_realizefn; scc->parent_reset = cc->reset; cc->reset = superh_cpu_reset; cc->class_by_name = superh_cpu_class_by_name; cc->has_work = superh_cpu_has_work; cc->do_interrupt = superh_cpu_do_interrupt; cc->cpu_exec_interrupt = superh_cpu_exec_interrupt; cc->dump_state = superh_cpu_dump_state; cc->set_pc = superh_cpu_set_pc; cc->synchronize_from_tb = superh_cpu_synchronize_from_tb; cc->gdb_read_register = superh_cpu_gdb_read_register; cc->gdb_write_register = superh_cpu_gdb_write_register; #ifdef CONFIG_USER_ONLY cc->handle_mmu_fault = superh_cpu_handle_mmu_fault; #else cc->get_phys_page_debug = superh_cpu_get_phys_page_debug; #endif cc->disas_set_info = superh_cpu_disas_set_info; cc->gdb_num_core_regs = 59; dc->vmsd = &vmstate_sh_cpu; /* * Reason: superh_cpu_initfn() calls cpu_exec_init(), which saves * the object in cpus -> dangling pointer after final * object_unref(). */ dc->cannot_destroy_with_object_finalize_yet = true; } ",qemu,1
"static inline void tcg_out_ext8u(TCGContext *s, int dest, int src)

{

    /* movzbl */

    assert(src < 4 || TCG_TARGET_REG_BITS == 64);

    tcg_out_modrm(s, OPC_MOVZBL + P_REXB_RM, dest, src);

}
","static inline void tcg_out_ext8u(TCGContext *s, int dest, int src) {
/* movzbl */ assert(src < 4 || TCG_TARGET_REG_BITS == 64); tcg_out_modrm(s, OPC_MOVZBL + P_REXB_RM, dest, src); } ",qemu,0
"static void bdrv_delete(BlockDriverState *bs)

{

    assert(!bs->dev);

    assert(!bs->job);

    assert(bdrv_op_blocker_is_empty(bs));

    assert(!bs->refcnt);

    assert(QLIST_EMPTY(&bs->dirty_bitmaps));



    bdrv_close(bs);



    /* remove from list, if necessary */

    bdrv_make_anon(bs);




    g_free(bs);

}","static void bdrv_delete(BlockDriverState *bs) {
assert(!bs->dev); assert(!bs->job); assert(bdrv_op_blocker_is_empty(bs)); assert(!bs->refcnt); assert(QLIST_EMPTY(&bs->dirty_bitmaps)); bdrv_close(bs); /* remove from list, if necessary */ bdrv_make_anon(bs); g_free(bs); }",qemu,1
"void qjson_finish(QJSON *json)

{

    json_end_object(json);

}
","void qjson_finish(QJSON *json) {
json_end_object(json); } ",qemu,0
"void xen_cmos_set_s3_resume(void *opaque, int irq, int level)

{

    pc_cmos_set_s3_resume(opaque, irq, level);

    if (level) {

        xc_set_hvm_param(xen_xc, xen_domid, HVM_PARAM_ACPI_S_STATE, 3);

    }

}
","void xen_cmos_set_s3_resume(void *opaque, int irq, int level) {
pc_cmos_set_s3_resume(opaque, irq, level); if (level) {
xc_set_hvm_param(xen_xc, xen_domid, HVM_PARAM_ACPI_S_STATE, 3); } } ",qemu,0
"static void cpu_notify_map_clients(void)

{

    MapClient *client;



    while (!LIST_EMPTY(&map_client_list)) {

        client = LIST_FIRST(&map_client_list);

        client->callback(client->opaque);

        LIST_REMOVE(client, link);

    }

}
","static void cpu_notify_map_clients(void) {
MapClient *client; while (!LIST_EMPTY(&map_client_list)) {
client = LIST_FIRST(&map_client_list); client->callback(client->opaque); LIST_REMOVE(client, link); } } ",qemu,1
"static void cond_broadcast(pthread_cond_t *cond)

{

    int ret = pthread_cond_broadcast(cond);

    if (ret) die2(ret, ""pthread_cond_broadcast"");

}
","static void cond_broadcast(pthread_cond_t *cond) {
int ret = pthread_cond_broadcast(cond); if (ret) die2(ret, ""pthread_cond_broadcast""); } ",qemu,0
"void err (const char *s)

{

  perror (s);

  abort ();

}
","void err (const char *s) {
perror (s); abort (); } ",qemu,0
"static void qpci_spapr_io_writeb(QPCIBus *bus, void *addr, uint8_t value)

{

    QPCIBusSPAPR *s = container_of(bus, QPCIBusSPAPR, bus);

    uint64_t port = (uintptr_t)addr;

    if (port < s->pio.size) {

        writeb(s->pio_cpu_base + port, value);

    } else {

        writeb(s->mmio_cpu_base + port, value);

    }

}
","static void qpci_spapr_io_writeb(QPCIBus *bus, void *addr, uint8_t value) {
QPCIBusSPAPR *s = container_of(bus, QPCIBusSPAPR, bus); uint64_t port = (uintptr_t)addr; if (port < s->pio.size) {
writeb(s->pio_cpu_base + port, value); } else {
writeb(s->mmio_cpu_base + port, value); } } ",qemu,0
"static void aio_epoll_disable(AioContext *ctx)

{

    ctx->epoll_available = false;

    if (!ctx->epoll_enabled) {

        return;

    }

    ctx->epoll_enabled = false;

    close(ctx->epollfd);

}
","static void aio_epoll_disable(AioContext *ctx) {
ctx->epoll_available = false; if (!ctx->epoll_enabled) {
return; } ctx->epoll_enabled = false; close(ctx->epollfd); } ",qemu,0
"static void scsi_remove_request(SCSIGenericReq *r)

{

    qemu_free(r->buf);

    scsi_req_free(&r->req);

}
","static void scsi_remove_request(SCSIGenericReq *r) {
qemu_free(r->buf); scsi_req_free(&r->req); } ",qemu,1
"void qmp_migrate_set_cache_size(int64_t value, Error **errp)

{

    MigrationState *s = migrate_get_current();

    int64_t new_size;



    /* Check for truncation */

    if (value != (size_t)value) {

        error_set(errp, QERR_INVALID_PARAMETER_VALUE, ""cache size"",

                  ""exceeding address space"");

        return;

    }



    /* Cache should not be larger than guest ram size */

    if (value > ram_bytes_total()) {

        error_set(errp, QERR_INVALID_PARAMETER_VALUE, ""cache size"",

                  ""exceeds guest ram size "");

        return;

    }



    new_size = xbzrle_cache_resize(value);

    if (new_size < 0) {

        error_set(errp, QERR_INVALID_PARAMETER_VALUE, ""cache size"",

                  ""is smaller than page size"");

        return;

    }



    s->xbzrle_cache_size = new_size;

}
","void qmp_migrate_set_cache_size(int64_t value, Error **errp) {
MigrationState *s = migrate_get_current(); int64_t new_size; /* Check for truncation */ if (value != (size_t)value) {
error_set(errp, QERR_INVALID_PARAMETER_VALUE, ""cache size"", ""exceeding address space""); return; } /* Cache should not be larger than guest ram size */ if (value > ram_bytes_total()) {
error_set(errp, QERR_INVALID_PARAMETER_VALUE, ""cache size"", ""exceeds guest ram size ""); return; } new_size = xbzrle_cache_resize(value); if (new_size < 0) {
error_set(errp, QERR_INVALID_PARAMETER_VALUE, ""cache size"", ""is smaller than page size""); return; } s->xbzrle_cache_size = new_size; } ",qemu,1
"static void qxl_init_ramsize(PCIQXLDevice *qxl)

{

    /* vga mode framebuffer / primary surface (bar 0, first part) */

    if (qxl->vgamem_size_mb < 8) {

        qxl->vgamem_size_mb = 8;








    qxl->vgamem_size = qxl->vgamem_size_mb * 1024 * 1024;



    /* vga ram (bar 0, total) */

    if (qxl->ram_size_mb != -1) {

        qxl->vga.vram_size = qxl->ram_size_mb * 1024 * 1024;


    if (qxl->vga.vram_size < qxl->vgamem_size * 2) {

        qxl->vga.vram_size = qxl->vgamem_size * 2;




    /* vram32 (surfaces, 32bit, bar 1) */

    if (qxl->vram32_size_mb != -1) {

        qxl->vram32_size = qxl->vram32_size_mb * 1024 * 1024;


    if (qxl->vram32_size < 4096) {

        qxl->vram32_size = 4096;




    /* vram (surfaces, 64bit, bar 4+5) */

    if (qxl->vram_size_mb != -1) {

        qxl->vram_size = qxl->vram_size_mb * 1024 * 1024;


    if (qxl->vram_size < qxl->vram32_size) {

        qxl->vram_size = qxl->vram32_size;




    if (qxl->revision == 1) {

        qxl->vram32_size = 4096;

        qxl->vram_size = 4096;


    qxl->vgamem_size = msb_mask(qxl->vgamem_size * 2 - 1);

    qxl->vga.vram_size = msb_mask(qxl->vga.vram_size * 2 - 1);

    qxl->vram32_size = msb_mask(qxl->vram32_size * 2 - 1);

    qxl->vram_size = msb_mask(qxl->vram_size * 2 - 1);
","static void qxl_init_ramsize(PCIQXLDevice *qxl) {
/* vga mode framebuffer / primary surface (bar 0, first part) */ if (qxl->vgamem_size_mb < 8) {
qxl->vgamem_size_mb = 8; qxl->vgamem_size = qxl->vgamem_size_mb * 1024 * 1024; /* vga ram (bar 0, total) */ if (qxl->ram_size_mb != -1) {
qxl->vga.vram_size = qxl->ram_size_mb * 1024 * 1024; if (qxl->vga.vram_size < qxl->vgamem_size * 2) {
qxl->vga.vram_size = qxl->vgamem_size * 2; /* vram32 (surfaces, 32bit, bar 1) */ if (qxl->vram32_size_mb != -1) {
qxl->vram32_size = qxl->vram32_size_mb * 1024 * 1024; if (qxl->vram32_size < 4096) {
qxl->vram32_size = 4096; /* vram (surfaces, 64bit, bar 4+5) */ if (qxl->vram_size_mb != -1) {
qxl->vram_size = qxl->vram_size_mb * 1024 * 1024; if (qxl->vram_size < qxl->vram32_size) {
qxl->vram_size = qxl->vram32_size; if (qxl->revision == 1) {
qxl->vram32_size = 4096; qxl->vram_size = 4096; qxl->vgamem_size = msb_mask(qxl->vgamem_size * 2 - 1); qxl->vga.vram_size = msb_mask(qxl->vga.vram_size * 2 - 1); qxl->vram32_size = msb_mask(qxl->vram32_size * 2 - 1); qxl->vram_size = msb_mask(qxl->vram_size * 2 - 1); ",qemu,1
"static void virtio_set_status(struct subchannel_id schid,

                              unsigned long dev_addr)

{

    unsigned char status = dev_addr;

    if (run_ccw(schid, CCW_CMD_WRITE_STATUS, &status, sizeof(status))) {

        virtio_panic(""Could not write status to host!\n"");

    }

}
","static void virtio_set_status(struct subchannel_id schid, unsigned long dev_addr) {
unsigned char status = dev_addr; if (run_ccw(schid, CCW_CMD_WRITE_STATUS, &status, sizeof(status))) {
virtio_panic(""Could not write status to host!\n""); } } ",qemu,1
"void ide_dma_cb(void *opaque, int ret)

{

    IDEState *s = opaque;

    int n;

    int64_t sector_num;

    bool stay_active = false;



    if (ret == -ECANCELED) {

        return;

    }

    if (ret < 0) {

        int op = IDE_RETRY_DMA;



        if (s->dma_cmd == IDE_DMA_READ)

            op |= IDE_RETRY_READ;

        else if (s->dma_cmd == IDE_DMA_TRIM)

            op |= IDE_RETRY_TRIM;



        if (ide_handle_rw_error(s, -ret, op)) {

            return;

        }

    }



    n = s->io_buffer_size >> 9;

    if (n > s->nsector) {

        /* The PRDs were longer than needed for this request. Shorten them so

         * we don't get a negative remainder. The Active bit must remain set

         * after the request completes. */

        n = s->nsector;

        stay_active = true;

    }



    sector_num = ide_get_sector(s);

    if (n > 0) {

        dma_buf_commit(s);

        sector_num += n;

        ide_set_sector(s, sector_num);

        s->nsector -= n;

    }



    /* end of transfer ? */

    if (s->nsector == 0) {

        s->status = READY_STAT | SEEK_STAT;

        ide_set_irq(s->bus);

        goto eot;

    }



    /* launch next transfer */

    n = s->nsector;

    s->io_buffer_index = 0;

    s->io_buffer_size = n * 512;

    if (s->bus->dma->ops->prepare_buf(s->bus->dma, ide_cmd_is_read(s)) == 0) {

        /* The PRDs were too short. Reset the Active bit, but don't raise an

         * interrupt. */

        s->status = READY_STAT | SEEK_STAT;

        goto eot;

    }



#ifdef DEBUG_AIO

    printf(""ide_dma_cb: sector_num=%"" PRId64 "" n=%d, cmd_cmd=%d\n"",

           sector_num, n, s->dma_cmd);

#endif



    if ((s->dma_cmd == IDE_DMA_READ || s->dma_cmd == IDE_DMA_WRITE) &&

        !ide_sect_range_ok(s, sector_num, n)) {

        dma_buf_commit(s);

        ide_dma_error(s);

        return;

    }



    switch (s->dma_cmd) {

    case IDE_DMA_READ:

        s->bus->dma->aiocb = dma_bdrv_read(s->bs, &s->sg, sector_num,

                                           ide_dma_cb, s);

        break;

    case IDE_DMA_WRITE:

        s->bus->dma->aiocb = dma_bdrv_write(s->bs, &s->sg, sector_num,

                                            ide_dma_cb, s);

        break;

    case IDE_DMA_TRIM:

        s->bus->dma->aiocb = dma_bdrv_io(s->bs, &s->sg, sector_num,

                                         ide_issue_trim, ide_dma_cb, s,

                                         DMA_DIRECTION_TO_DEVICE);

        break;

    }

    return;



eot:

    if (s->dma_cmd == IDE_DMA_READ || s->dma_cmd == IDE_DMA_WRITE) {

        block_acct_done(bdrv_get_stats(s->bs), &s->acct);

    }

    ide_set_inactive(s, stay_active);

}
","void ide_dma_cb(void *opaque, int ret) {
IDEState *s = opaque; int n; int64_t sector_num; bool stay_active = false; if (ret == -ECANCELED) {
return; } if (ret < 0) {
int op = IDE_RETRY_DMA; if (s->dma_cmd == IDE_DMA_READ) op |= IDE_RETRY_READ; else if (s->dma_cmd == IDE_DMA_TRIM) op |= IDE_RETRY_TRIM; if (ide_handle_rw_error(s, -ret, op)) {
return; } } n = s->io_buffer_size >> 9; if (n > s->nsector) {
/* The PRDs were longer than needed for this request. Shorten them so * we don't get a negative remainder. The Active bit must remain set * after the request completes. */ n = s->nsector; stay_active = true; } sector_num = ide_get_sector(s); if (n > 0) {
dma_buf_commit(s); sector_num += n; ide_set_sector(s, sector_num); s->nsector -= n; } /* end of transfer ? */ if (s->nsector == 0) {
s->status = READY_STAT | SEEK_STAT; ide_set_irq(s->bus); goto eot; } /* launch next transfer */ n = s->nsector; s->io_buffer_index = 0; s->io_buffer_size = n * 512; if (s->bus->dma->ops->prepare_buf(s->bus->dma, ide_cmd_is_read(s)) == 0) {
/* The PRDs were too short. Reset the Active bit, but don't raise an * interrupt. */ s->status = READY_STAT | SEEK_STAT; goto eot; } #ifdef DEBUG_AIO printf(""ide_dma_cb: sector_num=%"" PRId64 "" n=%d, cmd_cmd=%d\n"", sector_num, n, s->dma_cmd); #endif if ((s->dma_cmd == IDE_DMA_READ || s->dma_cmd == IDE_DMA_WRITE) && !ide_sect_range_ok(s, sector_num, n)) {
dma_buf_commit(s); ide_dma_error(s); return; } switch (s->dma_cmd) {
case IDE_DMA_READ: s->bus->dma->aiocb = dma_bdrv_read(s->bs, &s->sg, sector_num, ide_dma_cb, s); break; case IDE_DMA_WRITE: s->bus->dma->aiocb = dma_bdrv_write(s->bs, &s->sg, sector_num, ide_dma_cb, s); break; case IDE_DMA_TRIM: s->bus->dma->aiocb = dma_bdrv_io(s->bs, &s->sg, sector_num, ide_issue_trim, ide_dma_cb, s, DMA_DIRECTION_TO_DEVICE); break; } return; eot: if (s->dma_cmd == IDE_DMA_READ || s->dma_cmd == IDE_DMA_WRITE) {
block_acct_done(bdrv_get_stats(s->bs), &s->acct); } ide_set_inactive(s, stay_active); } ",qemu,0
"static inline bool use_goto_tb(DisasContext *s, int n, uint64_t dest)

{

    /* No direct tb linking with singlestep (either QEMU's or the ARM

     * debug architecture kind) or deterministic io

     */

    if (s->base.singlestep_enabled || s->ss_active || (s->base.tb->cflags & CF_LAST_IO)) {

        return false;

    }



#ifndef CONFIG_USER_ONLY

    /* Only link tbs from inside the same guest page */

    if ((s->base.tb->pc & TARGET_PAGE_MASK) != (dest & TARGET_PAGE_MASK)) {

        return false;

    }

#endif



    return true;

}
","static inline bool use_goto_tb(DisasContext *s, int n, uint64_t dest) {
/* No direct tb linking with singlestep (either QEMU's or the ARM * debug architecture kind) or deterministic io */ if (s->base.singlestep_enabled || s->ss_active || (s->base.tb->cflags & CF_LAST_IO)) {
return false; } #ifndef CONFIG_USER_ONLY /* Only link tbs from inside the same guest page */ if ((s->base.tb->pc & TARGET_PAGE_MASK) != (dest & TARGET_PAGE_MASK)) {
return false; } #endif return true; } ",qemu,1
"static void do_change_vnc(const char *target)

{

    if (strcmp(target, ""passwd"") == 0 ||

	strcmp(target, ""password"") == 0) {

	char password[9];

	monitor_readline(""Password: "", 1, password, sizeof(password)-1);

	password[sizeof(password)-1] = '\0';

	if (vnc_display_password(NULL, password) < 0)

	    term_printf(""could not set VNC server password\n"");

    } else {

	if (vnc_display_open(NULL, target) < 0)

	    term_printf(""could not start VNC server on %s\n"", target);

    }

}
","static void do_change_vnc(const char *target) {
if (strcmp(target, ""passwd"") == 0 || strcmp(target, ""password"") == 0) {
char password[9]; monitor_readline(""Password: "", 1, password, sizeof(password)-1); password[sizeof(password)-1] = '\0'; if (vnc_display_password(NULL, password) < 0) term_printf(""could not set VNC server password\n""); } else {
if (vnc_display_open(NULL, target) < 0) term_printf(""could not start VNC server on %s\n"", target); } } ",qemu,1
"static void setup_rt_frame_v1(int usig, struct target_sigaction *ka,

                              target_siginfo_t *info,

                              target_sigset_t *set, CPUARMState *env)

{

	struct rt_sigframe_v1 *frame;

	abi_ulong frame_addr = get_sigframe(ka, env, sizeof(*frame));

	struct target_sigaltstack stack;

	int i;

        abi_ulong info_addr, uc_addr;



	if (!lock_user_struct(VERIFY_WRITE, frame, frame_addr, 0))

            return /* 1 */;



        info_addr = frame_addr + offsetof(struct rt_sigframe_v1, info);

	__put_user(info_addr, &frame->pinfo);

        uc_addr = frame_addr + offsetof(struct rt_sigframe_v1, uc);

	__put_user(uc_addr, &frame->puc);

	copy_siginfo_to_user(&frame->info, info);



	/* Clear all the bits of the ucontext we don't use.  */

	memset(&frame->uc, 0, offsetof(struct target_ucontext_v1, tuc_mcontext));



        memset(&stack, 0, sizeof(stack));

        __put_user(target_sigaltstack_used.ss_sp, &stack.ss_sp);

        __put_user(target_sigaltstack_used.ss_size, &stack.ss_size);

        __put_user(sas_ss_flags(get_sp_from_cpustate(env)), &stack.ss_flags);

        memcpy(&frame->uc.tuc_stack, &stack, sizeof(stack));



	setup_sigcontext(&frame->uc.tuc_mcontext, env, set->sig[0]);

        for(i = 0; i < TARGET_NSIG_WORDS; i++) {

            if (__put_user(set->sig[i], &frame->uc.tuc_sigmask.sig[i]))

                goto end;

        }



        setup_return(env, ka, &frame->retcode, frame_addr, usig,

                     frame_addr + offsetof(struct rt_sigframe_v1, retcode));



        env->regs[1] = info_addr;

        env->regs[2] = uc_addr;



end:

	unlock_user_struct(frame, frame_addr, 1);

}
","static void setup_rt_frame_v1(int usig, struct target_sigaction *ka, target_siginfo_t *info, target_sigset_t *set, CPUARMState *env) {
struct rt_sigframe_v1 *frame; abi_ulong frame_addr = get_sigframe(ka, env, sizeof(*frame)); struct target_sigaltstack stack; int i; abi_ulong info_addr, uc_addr; if (!lock_user_struct(VERIFY_WRITE, frame, frame_addr, 0)) return /* 1 */; info_addr = frame_addr + offsetof(struct rt_sigframe_v1, info); __put_user(info_addr, &frame->pinfo); uc_addr = frame_addr + offsetof(struct rt_sigframe_v1, uc); __put_user(uc_addr, &frame->puc); copy_siginfo_to_user(&frame->info, info); /* Clear all the bits of the ucontext we don't use. */ memset(&frame->uc, 0, offsetof(struct target_ucontext_v1, tuc_mcontext)); memset(&stack, 0, sizeof(stack)); __put_user(target_sigaltstack_used.ss_sp, &stack.ss_sp); __put_user(target_sigaltstack_used.ss_size, &stack.ss_size); __put_user(sas_ss_flags(get_sp_from_cpustate(env)), &stack.ss_flags); memcpy(&frame->uc.tuc_stack, &stack, sizeof(stack)); setup_sigcontext(&frame->uc.tuc_mcontext, env, set->sig[0]); for(i = 0; i < TARGET_NSIG_WORDS; i++) {
if (__put_user(set->sig[i], &frame->uc.tuc_sigmask.sig[i])) goto end; } setup_return(env, ka, &frame->retcode, frame_addr, usig, frame_addr + offsetof(struct rt_sigframe_v1, retcode)); env->regs[1] = info_addr; env->regs[2] = uc_addr; end: unlock_user_struct(frame, frame_addr, 1); } ",qemu,1
"static void fw_cfg_data_mem_write(void *opaque, hwaddr addr,

                                  uint64_t value, unsigned size)

{

    FWCfgState *s = opaque;

    uint8_t buf[8];

    unsigned i;



    switch (size) {

    case 1:

        buf[0] = value;

        break;

    case 2:

        stw_he_p(buf, value);

        break;

    case 4:

        stl_he_p(buf, value);

        break;

    case 8:

        stq_he_p(buf, value);

        break;

    default:

        abort();

    }

    for (i = 0; i < size; ++i) {

        fw_cfg_write(s, buf[i]);

    }

}
","static void fw_cfg_data_mem_write(void *opaque, hwaddr addr, uint64_t value, unsigned size) {
FWCfgState *s = opaque; uint8_t buf[8]; unsigned i; switch (size) {
case 1: buf[0] = value; break; case 2: stw_he_p(buf, value); break; case 4: stl_he_p(buf, value); break; case 8: stq_he_p(buf, value); break; default: abort(); } for (i = 0; i < size; ++i) {
fw_cfg_write(s, buf[i]); } } ",qemu,1
"void bdrv_set_boot_sector(BlockDriverState *bs, const uint8_t *data, int size)

{

    bs->boot_sector_enabled = 1;

    if (size > 512)

        size = 512;

    memcpy(bs->boot_sector_data, data, size);

    memset(bs->boot_sector_data + size, 0, 512 - size);

}
","void bdrv_set_boot_sector(BlockDriverState *bs, const uint8_t *data, int size) {
bs->boot_sector_enabled = 1; if (size > 512) size = 512; memcpy(bs->boot_sector_data, data, size); memset(bs->boot_sector_data + size, 0, 512 - size); } ",qemu,0
"static AioHandler *find_aio_handler(int fd)

{

    AioHandler *node;



    LIST_FOREACH(node, &aio_handlers, node) {

        if (node->fd == fd)

            if (!node->deleted)

                return node;

    }



    return NULL;

}
","static AioHandler *find_aio_handler(int fd) {
AioHandler *node; LIST_FOREACH(node, &aio_handlers, node) {
if (node->fd == fd) if (!node->deleted) return node; } return NULL; } ",qemu,0
"static void spr_write_40x_dbcr0 (void *opaque, int sprn)

{

    DisasContext *ctx = opaque;



    gen_op_store_40x_dbcr0();

    /* We must stop translation as we may have rebooted */

    RET_STOP(ctx);

}
","static void spr_write_40x_dbcr0 (void *opaque, int sprn) {
DisasContext *ctx = opaque; gen_op_store_40x_dbcr0(); /* We must stop translation as we may have rebooted */ RET_STOP(ctx); } ",qemu,0
"static inline int host_to_target_errno(int err)

{

    if(host_to_target_errno_table[err])

        return host_to_target_errno_table[err];

    return err;

}
","static inline int host_to_target_errno(int err) {
if(host_to_target_errno_table[err]) return host_to_target_errno_table[err]; return err; } ",qemu,1
"BlockAIOCB *ide_issue_trim(BlockDriverState *bs,

        int64_t sector_num, QEMUIOVector *qiov, int nb_sectors,

        BlockCompletionFunc *cb, void *opaque)

{

    TrimAIOCB *iocb;



    iocb = qemu_aio_get(&trim_aiocb_info, bs, cb, opaque);

    iocb->bh = qemu_bh_new(ide_trim_bh_cb, iocb);

    iocb->ret = 0;

    iocb->qiov = qiov;

    iocb->i = -1;

    iocb->j = 0;

    ide_issue_trim_cb(iocb, 0);

    return &iocb->common;

}
","BlockAIOCB *ide_issue_trim(BlockDriverState *bs, int64_t sector_num, QEMUIOVector *qiov, int nb_sectors, BlockCompletionFunc *cb, void *opaque) {
TrimAIOCB *iocb; iocb = qemu_aio_get(&trim_aiocb_info, bs, cb, opaque); iocb->bh = qemu_bh_new(ide_trim_bh_cb, iocb); iocb->ret = 0; iocb->qiov = qiov; iocb->i = -1; iocb->j = 0; ide_issue_trim_cb(iocb, 0); return &iocb->common; } ",qemu,0
"static void arm_gic_class_init(ObjectClass *klass, void *data)

{

    DeviceClass *dc = DEVICE_CLASS(klass);

    SysBusDeviceClass *sbc = SYS_BUS_DEVICE_CLASS(klass);

    ARMGICClass *agc = ARM_GIC_CLASS(klass);

    agc->parent_init = sbc->init;

    sbc->init = arm_gic_init;

    dc->no_user = 1;

}
","static void arm_gic_class_init(ObjectClass *klass, void *data) {
DeviceClass *dc = DEVICE_CLASS(klass); SysBusDeviceClass *sbc = SYS_BUS_DEVICE_CLASS(klass); ARMGICClass *agc = ARM_GIC_CLASS(klass); agc->parent_init = sbc->init; sbc->init = arm_gic_init; dc->no_user = 1; } ",qemu,1
"static void omap_clkm_write(void *opaque, target_phys_addr_t addr,

                            uint64_t value, unsigned size)

{

    struct omap_mpu_state_s *s = (struct omap_mpu_state_s *) opaque;

    uint16_t diff;

    omap_clk clk;

    static const char *clkschemename[8] = {

        ""fully synchronous"", ""fully asynchronous"", ""synchronous scalable"",

        ""mix mode 1"", ""mix mode 2"", ""bypass mode"", ""mix mode 3"", ""mix mode 4"",

    };



    if (size != 2) {

        return omap_badwidth_write16(opaque, addr, value);

    }



    switch (addr) {

    case 0x00:	/* ARM_CKCTL */

        diff = s->clkm.arm_ckctl ^ value;

        s->clkm.arm_ckctl = value & 0x7fff;

        omap_clkm_ckctl_update(s, diff, value);

        return;



    case 0x04:	/* ARM_IDLECT1 */

        diff = s->clkm.arm_idlect1 ^ value;

        s->clkm.arm_idlect1 = value & 0x0fff;

        omap_clkm_idlect1_update(s, diff, value);

        return;



    case 0x08:	/* ARM_IDLECT2 */

        diff = s->clkm.arm_idlect2 ^ value;

        s->clkm.arm_idlect2 = value & 0x07ff;

        omap_clkm_idlect2_update(s, diff, value);

        return;



    case 0x0c:	/* ARM_EWUPCT */

        s->clkm.arm_ewupct = value & 0x003f;

        return;



    case 0x10:	/* ARM_RSTCT1 */

        diff = s->clkm.arm_rstct1 ^ value;

        s->clkm.arm_rstct1 = value & 0x0007;

        if (value & 9) {

            qemu_system_reset_request();

            s->clkm.cold_start = 0xa;

        }

        if (diff & ~value & 4) {				/* DSP_RST */

            omap_mpui_reset(s);

            omap_tipb_bridge_reset(s->private_tipb);

            omap_tipb_bridge_reset(s->public_tipb);

        }

        if (diff & 2) {						/* DSP_EN */

            clk = omap_findclk(s, ""dsp_ck"");

            omap_clk_canidle(clk, (~value >> 1) & 1);

        }

        return;



    case 0x14:	/* ARM_RSTCT2 */

        s->clkm.arm_rstct2 = value & 0x0001;

        return;



    case 0x18:	/* ARM_SYSST */

        if ((s->clkm.clocking_scheme ^ (value >> 11)) & 7) {

            s->clkm.clocking_scheme = (value >> 11) & 7;

            printf(""%s: clocking scheme set to %s\n"", __FUNCTION__,

                            clkschemename[s->clkm.clocking_scheme]);

        }

        s->clkm.cold_start &= value & 0x3f;

        return;



    case 0x1c:	/* ARM_CKOUT1 */

        diff = s->clkm.arm_ckout1 ^ value;

        s->clkm.arm_ckout1 = value & 0x003f;

        omap_clkm_ckout1_update(s, diff, value);

        return;



    case 0x20:	/* ARM_CKOUT2 */

    default:

        OMAP_BAD_REG(addr);

    }

}
","static void omap_clkm_write(void *opaque, target_phys_addr_t addr, uint64_t value, unsigned size) {
struct omap_mpu_state_s *s = (struct omap_mpu_state_s *) opaque; uint16_t diff; omap_clk clk; static const char *clkschemename[8] = {
""fully synchronous"", ""fully asynchronous"", ""synchronous scalable"", ""mix mode 1"", ""mix mode 2"", ""bypass mode"", ""mix mode 3"", ""mix mode 4"", }; if (size != 2) {
return omap_badwidth_write16(opaque, addr, value); } switch (addr) {
case 0x00: /* ARM_CKCTL */ diff = s->clkm.arm_ckctl ^ value; s->clkm.arm_ckctl = value & 0x7fff; omap_clkm_ckctl_update(s, diff, value); return; case 0x04: /* ARM_IDLECT1 */ diff = s->clkm.arm_idlect1 ^ value; s->clkm.arm_idlect1 = value & 0x0fff; omap_clkm_idlect1_update(s, diff, value); return; case 0x08: /* ARM_IDLECT2 */ diff = s->clkm.arm_idlect2 ^ value; s->clkm.arm_idlect2 = value & 0x07ff; omap_clkm_idlect2_update(s, diff, value); return; case 0x0c: /* ARM_EWUPCT */ s->clkm.arm_ewupct = value & 0x003f; return; case 0x10: /* ARM_RSTCT1 */ diff = s->clkm.arm_rstct1 ^ value; s->clkm.arm_rstct1 = value & 0x0007; if (value & 9) {
qemu_system_reset_request(); s->clkm.cold_start = 0xa; } if (diff & ~value & 4) {
/* DSP_RST */ omap_mpui_reset(s); omap_tipb_bridge_reset(s->private_tipb); omap_tipb_bridge_reset(s->public_tipb); } if (diff & 2) {
/* DSP_EN */ clk = omap_findclk(s, ""dsp_ck""); omap_clk_canidle(clk, (~value >> 1) & 1); } return; case 0x14: /* ARM_RSTCT2 */ s->clkm.arm_rstct2 = value & 0x0001; return; case 0x18: /* ARM_SYSST */ if ((s->clkm.clocking_scheme ^ (value >> 11)) & 7) {
s->clkm.clocking_scheme = (value >> 11) & 7; printf(""%s: clocking scheme set to %s\n"", __FUNCTION__, clkschemename[s->clkm.clocking_scheme]); } s->clkm.cold_start &= value & 0x3f; return; case 0x1c: /* ARM_CKOUT1 */ diff = s->clkm.arm_ckout1 ^ value; s->clkm.arm_ckout1 = value & 0x003f; omap_clkm_ckout1_update(s, diff, value); return; case 0x20: /* ARM_CKOUT2 */ default: OMAP_BAD_REG(addr); } } ",qemu,0
"static bool vring_notify(VirtIODevice *vdev, VirtQueue *vq)

{

    uint16_t old, new;

    bool v;



    /* Always notify when queue is empty (when feature acknowledge) */

    if (((vdev->guest_features & (1 << VIRTIO_F_NOTIFY_ON_EMPTY)) &&

         !vq->inuse && vring_avail_idx(vq) == vq->last_avail_idx)) {

        return true;

    }



    if (!(vdev->guest_features & (1 << VIRTIO_RING_F_EVENT_IDX))) {

        return !(vring_avail_flags(vq) & VRING_AVAIL_F_NO_INTERRUPT);

    }



    v = vq->signalled_used_valid;

    vq->signalled_used_valid = true;

    old = vq->signalled_used;

    new = vq->signalled_used = vring_used_idx(vq);

    return !v || vring_need_event(vring_used_event(vq), new, old);

}","static bool vring_notify(VirtIODevice *vdev, VirtQueue *vq) {
uint16_t old, new; bool v; /* Always notify when queue is empty (when feature acknowledge) */ if (((vdev->guest_features & (1 << VIRTIO_F_NOTIFY_ON_EMPTY)) && !vq->inuse && vring_avail_idx(vq) == vq->last_avail_idx)) {
return true; } if (!(vdev->guest_features & (1 << VIRTIO_RING_F_EVENT_IDX))) {
return !(vring_avail_flags(vq) & VRING_AVAIL_F_NO_INTERRUPT); } v = vq->signalled_used_valid; vq->signalled_used_valid = true; old = vq->signalled_used; new = vq->signalled_used = vring_used_idx(vq); return !v || vring_need_event(vring_used_event(vq), new, old); }",qemu,1
"static char *idebus_get_fw_dev_path(DeviceState *dev)

{

    char path[30];



    snprintf(path, sizeof(path), ""%s@%d"", qdev_fw_name(dev),

             ((IDEBus*)dev->parent_bus)->bus_id);



    return strdup(path);

}
","static char *idebus_get_fw_dev_path(DeviceState *dev) {
char path[30]; snprintf(path, sizeof(path), ""%s@%d"", qdev_fw_name(dev), ((IDEBus*)dev->parent_bus)->bus_id); return strdup(path); } ",qemu,0
"static void write_palette(const char *key, QObject *obj, void *opaque)

{

    struct palette_cb_priv *priv = opaque;

    VncState *vs = priv->vs;

    uint32_t bytes = vs->clientds.pf.bytes_per_pixel;

    uint8_t idx = qint_get_int(qobject_to_qint(obj));



    if (bytes == 4) {

        uint32_t color = tight_palette_buf2rgb(32, (uint8_t *)key);



        ((uint32_t*)priv->header)[idx] = color;

    } else {

        uint16_t color = tight_palette_buf2rgb(16, (uint8_t *)key);



        ((uint16_t*)priv->header)[idx] = color;

    }

}
","static void write_palette(const char *key, QObject *obj, void *opaque) {
struct palette_cb_priv *priv = opaque; VncState *vs = priv->vs; uint32_t bytes = vs->clientds.pf.bytes_per_pixel; uint8_t idx = qint_get_int(qobject_to_qint(obj)); if (bytes == 4) {
uint32_t color = tight_palette_buf2rgb(32, (uint8_t *)key); ((uint32_t*)priv->header)[idx] = color; } else {
uint16_t color = tight_palette_buf2rgb(16, (uint8_t *)key); ((uint16_t*)priv->header)[idx] = color; } } ",qemu,0
"static void lan9118_writel(void *opaque, target_phys_addr_t offset,

                           uint64_t val, unsigned size)

{

    lan9118_state *s = (lan9118_state *)opaque;

    offset &= 0xff;



    //DPRINTF(""Write reg 0x%02x = 0x%08x\n"", (int)offset, val);

    if (offset >= 0x20 && offset < 0x40) {

        /* TX FIFO */

        tx_fifo_push(s, val);

        return;

    }

    switch (offset) {

    case CSR_IRQ_CFG:

        /* TODO: Implement interrupt deassertion intervals.  */

        val &= (IRQ_EN | IRQ_POL | IRQ_TYPE);

        s->irq_cfg = (s->irq_cfg & IRQ_INT) | val;

        break;

    case CSR_INT_STS:

        s->int_sts &= ~val;

        break;

    case CSR_INT_EN:

        s->int_en = val & ~RESERVED_INT;

        s->int_sts |= val & SW_INT;

        break;

    case CSR_FIFO_INT:

        DPRINTF(""FIFO INT levels %08x\n"", val);

        s->fifo_int = val;

        break;

    case CSR_RX_CFG:

        if (val & 0x8000) {

            /* RX_DUMP */

            s->rx_fifo_used = 0;

            s->rx_status_fifo_used = 0;

            s->rx_packet_size_tail = s->rx_packet_size_head;

            s->rx_packet_size[s->rx_packet_size_head] = 0;

        }

        s->rx_cfg = val & 0xcfff1ff0;

        break;

    case CSR_TX_CFG:

        if (val & 0x8000) {

            s->tx_status_fifo_used = 0;

        }

        if (val & 0x4000) {

            s->txp->state = TX_IDLE;

            s->txp->fifo_used = 0;

            s->txp->cmd_a = 0xffffffff;

        }

        s->tx_cfg = val & 6;

        break;

    case CSR_HW_CFG:

        if (val & 1) {

            /* SRST */

            lan9118_reset(&s->busdev.qdev);

        } else {

            s->hw_cfg = (val & 0x003f300) | (s->hw_cfg & 0x4);

        }

        break;

    case CSR_RX_DP_CTRL:

        if (val & 0x80000000) {

            /* Skip forward to next packet.  */

            s->rxp_pad = 0;

            s->rxp_offset = 0;

            if (s->rxp_size == 0) {

                /* Pop a word to start the next packet.  */

                rx_fifo_pop(s);

                s->rxp_pad = 0;

                s->rxp_offset = 0;

            }

            s->rx_fifo_head += s->rxp_size;

            if (s->rx_fifo_head >= s->rx_fifo_size) {

                s->rx_fifo_head -= s->rx_fifo_size;

            }

        }

        break;

    case CSR_PMT_CTRL:

        if (val & 0x400) {

            phy_reset(s);

        }

        s->pmt_ctrl &= ~0x34e;

        s->pmt_ctrl |= (val & 0x34e);

        break;

    case CSR_GPIO_CFG:

        /* Probably just enabling LEDs.  */

        s->gpio_cfg = val & 0x7777071f;

        break;

    case CSR_GPT_CFG:

        if ((s->gpt_cfg ^ val) & GPT_TIMER_EN) {

            if (val & GPT_TIMER_EN) {

                ptimer_set_count(s->timer, val & 0xffff);

                ptimer_run(s->timer, 0);

            } else {

                ptimer_stop(s->timer);

                ptimer_set_count(s->timer, 0xffff);

            }

        }

        s->gpt_cfg = val & (GPT_TIMER_EN | 0xffff);

        break;

    case CSR_WORD_SWAP:

        /* Ignored because we're in 32-bit mode.  */

        s->word_swap = val;

        break;

    case CSR_MAC_CSR_CMD:

        s->mac_cmd = val & 0x4000000f;

        if (val & 0x80000000) {

            if (val & 0x40000000) {

                s->mac_data = do_mac_read(s, val & 0xf);

                DPRINTF(""MAC read %d = 0x%08x\n"", val & 0xf, s->mac_data);

            } else {

                DPRINTF(""MAC write %d = 0x%08x\n"", val & 0xf, s->mac_data);

                do_mac_write(s, val & 0xf, s->mac_data);

            }

        }

        break;

    case CSR_MAC_CSR_DATA:

        s->mac_data = val;

        break;

    case CSR_AFC_CFG:

        s->afc_cfg = val & 0x00ffffff;

        break;

    case CSR_E2P_CMD:

        lan9118_eeprom_cmd(s, (val >> 28) & 7, val & 0x7f);

        break;

    case CSR_E2P_DATA:

        s->e2p_data = val & 0xff;

        break;



    default:

        hw_error(""lan9118_write: Bad reg 0x%x = %x\n"", (int)offset, (int)val);

        break;

    }

    lan9118_update(s);

}
","static void lan9118_writel(void *opaque, target_phys_addr_t offset, uint64_t val, unsigned size) {
lan9118_state *s = (lan9118_state *)opaque; offset &= 0xff; //DPRINTF(""Write reg 0x%02x = 0x%08x\n"", (int)offset, val); if (offset >= 0x20 && offset < 0x40) {
/* TX FIFO */ tx_fifo_push(s, val); return; } switch (offset) {
case CSR_IRQ_CFG: /* TODO: Implement interrupt deassertion intervals. */ val &= (IRQ_EN | IRQ_POL | IRQ_TYPE); s->irq_cfg = (s->irq_cfg & IRQ_INT) | val; break; case CSR_INT_STS: s->int_sts &= ~val; break; case CSR_INT_EN: s->int_en = val & ~RESERVED_INT; s->int_sts |= val & SW_INT; break; case CSR_FIFO_INT: DPRINTF(""FIFO INT levels %08x\n"", val); s->fifo_int = val; break; case CSR_RX_CFG: if (val & 0x8000) {
/* RX_DUMP */ s->rx_fifo_used = 0; s->rx_status_fifo_used = 0; s->rx_packet_size_tail = s->rx_packet_size_head; s->rx_packet_size[s->rx_packet_size_head] = 0; } s->rx_cfg = val & 0xcfff1ff0; break; case CSR_TX_CFG: if (val & 0x8000) {
s->tx_status_fifo_used = 0; } if (val & 0x4000) {
s->txp->state = TX_IDLE; s->txp->fifo_used = 0; s->txp->cmd_a = 0xffffffff; } s->tx_cfg = val & 6; break; case CSR_HW_CFG: if (val & 1) {
/* SRST */ lan9118_reset(&s->busdev.qdev); } else {
s->hw_cfg = (val & 0x003f300) | (s->hw_cfg & 0x4); } break; case CSR_RX_DP_CTRL: if (val & 0x80000000) {
/* Skip forward to next packet. */ s->rxp_pad = 0; s->rxp_offset = 0; if (s->rxp_size == 0) {
/* Pop a word to start the next packet. */ rx_fifo_pop(s); s->rxp_pad = 0; s->rxp_offset = 0; } s->rx_fifo_head += s->rxp_size; if (s->rx_fifo_head >= s->rx_fifo_size) {
s->rx_fifo_head -= s->rx_fifo_size; } } break; case CSR_PMT_CTRL: if (val & 0x400) {
phy_reset(s); } s->pmt_ctrl &= ~0x34e; s->pmt_ctrl |= (val & 0x34e); break; case CSR_GPIO_CFG: /* Probably just enabling LEDs. */ s->gpio_cfg = val & 0x7777071f; break; case CSR_GPT_CFG: if ((s->gpt_cfg ^ val) & GPT_TIMER_EN) {
if (val & GPT_TIMER_EN) {
ptimer_set_count(s->timer, val & 0xffff); ptimer_run(s->timer, 0); } else {
ptimer_stop(s->timer); ptimer_set_count(s->timer, 0xffff); } } s->gpt_cfg = val & (GPT_TIMER_EN | 0xffff); break; case CSR_WORD_SWAP: /* Ignored because we're in 32-bit mode. */ s->word_swap = val; break; case CSR_MAC_CSR_CMD: s->mac_cmd = val & 0x4000000f; if (val & 0x80000000) {
if (val & 0x40000000) {
s->mac_data = do_mac_read(s, val & 0xf); DPRINTF(""MAC read %d = 0x%08x\n"", val & 0xf, s->mac_data); } else {
DPRINTF(""MAC write %d = 0x%08x\n"", val & 0xf, s->mac_data); do_mac_write(s, val & 0xf, s->mac_data); } } break; case CSR_MAC_CSR_DATA: s->mac_data = val; break; case CSR_AFC_CFG: s->afc_cfg = val & 0x00ffffff; break; case CSR_E2P_CMD: lan9118_eeprom_cmd(s, (val >> 28) & 7, val & 0x7f); break; case CSR_E2P_DATA: s->e2p_data = val & 0xff; break; default: hw_error(""lan9118_write: Bad reg 0x%x = %x\n"", (int)offset, (int)val); break; } lan9118_update(s); } ",qemu,0
"static void test_visitor_in_errors(TestInputVisitorData *data,

                                   const void *unused)

{

    TestStruct *p = NULL;

    Error *err = NULL;

    Visitor *v;

    strList *q = NULL;

    UserDefTwo *r = NULL;

    WrapAlternate *s = NULL;



    v = visitor_input_test_init(data, ""{ 'integer': false, 'boolean': 'foo', ""

                                ""'string': -42 }"");



    visit_type_TestStruct(v, NULL, &p, &err);

    error_free_or_abort(&err);

    g_assert(!p);



    v = visitor_input_test_init(data, ""[ '1', '2', false, '3' ]"");

    visit_type_strList(v, NULL, &q, &err);

    error_free_or_abort(&err);

    assert(!q);



    v = visitor_input_test_init(data, ""{ 'str':'hi' }"");

    visit_type_UserDefTwo(v, NULL, &r, &err);

    error_free_or_abort(&err);

    assert(!r);



    v = visitor_input_test_init(data, ""{ }"");

    visit_type_WrapAlternate(v, NULL, &s, &err);

    error_free_or_abort(&err);

    assert(!s);

}
","static void test_visitor_in_errors(TestInputVisitorData *data, const void *unused) {
TestStruct *p = NULL; Error *err = NULL; Visitor *v; strList *q = NULL; UserDefTwo *r = NULL; WrapAlternate *s = NULL; v = visitor_input_test_init(data, "" {
'integer': false, 'boolean': 'foo', "" ""'string': -42 }""); visit_type_TestStruct(v, NULL, &p, &err); error_free_or_abort(&err); g_assert(!p); v = visitor_input_test_init(data, ""[ '1', '2', false, '3' ]""); visit_type_strList(v, NULL, &q, &err); error_free_or_abort(&err); assert(!q); v = visitor_input_test_init(data, "" {
'str':'hi' }""); visit_type_UserDefTwo(v, NULL, &r, &err); error_free_or_abort(&err); assert(!r); v = visitor_input_test_init(data, "" {
}""); visit_type_WrapAlternate(v, NULL, &s, &err); error_free_or_abort(&err); assert(!s); } ",qemu,0
"int dyngen_code_search_pc(TCGContext *s, uint8_t *gen_code_buf,

                          const uint8_t *searched_pc)

{

    return tcg_gen_code_common(s, gen_code_buf, 1, searched_pc);

}
","int dyngen_code_search_pc(TCGContext *s, uint8_t *gen_code_buf, const uint8_t *searched_pc) {
return tcg_gen_code_common(s, gen_code_buf, 1, searched_pc); } ",qemu,1
"void qemu_put_be16(QEMUFile *f, unsigned int v)

{

    qemu_put_byte(f, v >> 8);

    qemu_put_byte(f, v);

}
","void qemu_put_be16(QEMUFile *f, unsigned int v) {
qemu_put_byte(f, v >> 8); qemu_put_byte(f, v); } ",qemu,1
"int inet_listen_opts(QemuOpts *opts, int port_offset, Error **errp)

{

    struct addrinfo ai,*res,*e;

    const char *addr;

    char port[33];

    char uaddr[INET6_ADDRSTRLEN+1];

    char uport[33];

    int slisten, rc, to, port_min, port_max, p;



    memset(&ai,0, sizeof(ai));

    ai.ai_flags = AI_PASSIVE | AI_ADDRCONFIG;

    ai.ai_family = PF_UNSPEC;

    ai.ai_socktype = SOCK_STREAM;



    if ((qemu_opt_get(opts, ""host"") == NULL) ||

        (qemu_opt_get(opts, ""port"") == NULL)) {

        error_setg(errp, ""host and/or port not specified"");

        return -1;

    }

    pstrcpy(port, sizeof(port), qemu_opt_get(opts, ""port""));

    addr = qemu_opt_get(opts, ""host"");



    to = qemu_opt_get_number(opts, ""to"", 0);

    if (qemu_opt_get_bool(opts, ""ipv4"", 0))

        ai.ai_family = PF_INET;

    if (qemu_opt_get_bool(opts, ""ipv6"", 0))

        ai.ai_family = PF_INET6;



    /* lookup */

    if (port_offset) {

        unsigned long long baseport;

        if (parse_uint_full(port, &baseport, 10) < 0) {

            error_setg(errp, ""can't convert to a number: %s"", port);

            return -1;

        }

        if (baseport > 65535 ||

            baseport + port_offset > 65535) {

            error_setg(errp, ""port %s out of range"", port);

            return -1;

        }

        snprintf(port, sizeof(port), ""%d"", (int)baseport + port_offset);

    }

    rc = getaddrinfo(strlen(addr) ? addr : NULL, port, &ai, &res);

    if (rc != 0) {

        error_setg(errp, ""address resolution failed for %s:%s: %s"", addr, port,

                   gai_strerror(rc));

        return -1;

    }



    /* create socket + bind */

    for (e = res; e != NULL; e = e->ai_next) {

        getnameinfo((struct sockaddr*)e->ai_addr,e->ai_addrlen,

		        uaddr,INET6_ADDRSTRLEN,uport,32,

		        NI_NUMERICHOST | NI_NUMERICSERV);

        slisten = qemu_socket(e->ai_family, e->ai_socktype, e->ai_protocol);

        if (slisten < 0) {

            if (!e->ai_next) {

                error_setg_errno(errp, errno, ""Failed to create socket"");

            }

            continue;

        }



        socket_set_fast_reuse(slisten);

#ifdef IPV6_V6ONLY

        if (e->ai_family == PF_INET6) {

            /* listen on both ipv4 and ipv6 */

            const int off = 0;

            qemu_setsockopt(slisten, IPPROTO_IPV6, IPV6_V6ONLY, &off,

                            sizeof(off));

        }

#endif



        port_min = inet_getport(e);

        port_max = to ? to + port_offset : port_min;

        for (p = port_min; p <= port_max; p++) {

            inet_setport(e, p);

            if (bind(slisten, e->ai_addr, e->ai_addrlen) == 0) {

                goto listen;

            }

            if (p == port_max) {

                if (!e->ai_next) {

                    error_setg_errno(errp, errno, ""Failed to bind socket"");

                }

            }

        }

        closesocket(slisten);

    }

    freeaddrinfo(res);

    return -1;



listen:

    if (listen(slisten,1) != 0) {

        error_setg_errno(errp, errno, ""Failed to listen on socket"");

        closesocket(slisten);

        freeaddrinfo(res);

        return -1;

    }

    qemu_opt_set(opts, ""host"", uaddr, &error_abort);

    qemu_opt_set_number(opts, ""port"", inet_getport(e) - port_offset,

                        &error_abort);

    qemu_opt_set_bool(opts, ""ipv6"", e->ai_family == PF_INET6,

                      &error_abort);

    qemu_opt_set_bool(opts, ""ipv4"", e->ai_family != PF_INET6,

                      &error_abort);

    freeaddrinfo(res);

    return slisten;

}
","int inet_listen_opts(QemuOpts *opts, int port_offset, Error **errp) {
struct addrinfo ai,*res,*e; const char *addr; char port[33]; char uaddr[INET6_ADDRSTRLEN+1]; char uport[33]; int slisten, rc, to, port_min, port_max, p; memset(&ai,0, sizeof(ai)); ai.ai_flags = AI_PASSIVE | AI_ADDRCONFIG; ai.ai_family = PF_UNSPEC; ai.ai_socktype = SOCK_STREAM; if ((qemu_opt_get(opts, ""host"") == NULL) || (qemu_opt_get(opts, ""port"") == NULL)) {
error_setg(errp, ""host and/or port not specified""); return -1; } pstrcpy(port, sizeof(port), qemu_opt_get(opts, ""port"")); addr = qemu_opt_get(opts, ""host""); to = qemu_opt_get_number(opts, ""to"", 0); if (qemu_opt_get_bool(opts, ""ipv4"", 0)) ai.ai_family = PF_INET; if (qemu_opt_get_bool(opts, ""ipv6"", 0)) ai.ai_family = PF_INET6; /* lookup */ if (port_offset) {
unsigned long long baseport; if (parse_uint_full(port, &baseport, 10) < 0) {
error_setg(errp, ""can't convert to a number: %s"", port); return -1; } if (baseport > 65535 || baseport + port_offset > 65535) {
error_setg(errp, ""port %s out of range"", port); return -1; } snprintf(port, sizeof(port), ""%d"", (int)baseport + port_offset); } rc = getaddrinfo(strlen(addr) ? addr : NULL, port, &ai, &res); if (rc != 0) {
error_setg(errp, ""address resolution failed for %s:%s: %s"", addr, port, gai_strerror(rc)); return -1; } /* create socket + bind */ for (e = res; e != NULL; e = e->ai_next) {
getnameinfo((struct sockaddr*)e->ai_addr,e->ai_addrlen, uaddr,INET6_ADDRSTRLEN,uport,32, NI_NUMERICHOST | NI_NUMERICSERV); slisten = qemu_socket(e->ai_family, e->ai_socktype, e->ai_protocol); if (slisten < 0) {
if (!e->ai_next) {
error_setg_errno(errp, errno, ""Failed to create socket""); } continue; } socket_set_fast_reuse(slisten); #ifdef IPV6_V6ONLY if (e->ai_family == PF_INET6) {
/* listen on both ipv4 and ipv6 */ const int off = 0; qemu_setsockopt(slisten, IPPROTO_IPV6, IPV6_V6ONLY, &off, sizeof(off)); } #endif port_min = inet_getport(e); port_max = to ? to + port_offset : port_min; for (p = port_min; p <= port_max; p++) {
inet_setport(e, p); if (bind(slisten, e->ai_addr, e->ai_addrlen) == 0) {
goto listen; } if (p == port_max) {
if (!e->ai_next) {
error_setg_errno(errp, errno, ""Failed to bind socket""); } } } closesocket(slisten); } freeaddrinfo(res); return -1; listen: if (listen(slisten,1) != 0) {
error_setg_errno(errp, errno, ""Failed to listen on socket""); closesocket(slisten); freeaddrinfo(res); return -1; } qemu_opt_set(opts, ""host"", uaddr, &error_abort); qemu_opt_set_number(opts, ""port"", inet_getport(e) - port_offset, &error_abort); qemu_opt_set_bool(opts, ""ipv6"", e->ai_family == PF_INET6, &error_abort); qemu_opt_set_bool(opts, ""ipv4"", e->ai_family != PF_INET6, &error_abort); freeaddrinfo(res); return slisten; } ",qemu,1
"static void apic_common_class_init(ObjectClass *klass, void *data)

{

    ICCDeviceClass *idc = ICC_DEVICE_CLASS(klass);

    DeviceClass *dc = DEVICE_CLASS(klass);



    dc->vmsd = &vmstate_apic_common;

    dc->reset = apic_reset_common;

    dc->props = apic_properties_common;

    idc->realize = apic_common_realize;

    /*

     * Reason: APIC and CPU need to be wired up by

     * x86_cpu_apic_create()

     */

    dc->cannot_instantiate_with_device_add_yet = true;

}
","static void apic_common_class_init(ObjectClass *klass, void *data) {
ICCDeviceClass *idc = ICC_DEVICE_CLASS(klass); DeviceClass *dc = DEVICE_CLASS(klass); dc->vmsd = &vmstate_apic_common; dc->reset = apic_reset_common; dc->props = apic_properties_common; idc->realize = apic_common_realize; /* * Reason: APIC and CPU need to be wired up by * x86_cpu_apic_create() */ dc->cannot_instantiate_with_device_add_yet = true; } ",qemu,0
"void do_fctiw (void)

{

    union {

        double d;

        uint64_t i;

    } p;



    /* XXX: higher bits are not supposed to be significant.

     *     to make tests easier, return the same as a real PowerPC 750 (aka G3)

     */

    p.i = float64_to_int32(FT0, &env->fp_status);

    p.i |= 0xFFF80000ULL << 32;

    FT0 = p.d;

}
","void do_fctiw (void) {
union {
double d; uint64_t i; } p; /* XXX: higher bits are not supposed to be significant. * to make tests easier, return the same as a real PowerPC 750 (aka G3) */ p.i = float64_to_int32(FT0, &env->fp_status); p.i |= 0xFFF80000ULL << 32; FT0 = p.d; } ",qemu,1
"vcard_free(VCard *vcard)

{

    VCardApplet *current_applet = NULL;

    VCardApplet *next_applet = NULL;



    if (vcard == NULL) {

        return;

    }

    vcard->reference_count--;

    if (vcard->reference_count != 0) {

        return;

    }

    if (vcard->vcard_private_free) {

        (*vcard->vcard_private_free)(vcard->vcard_private);

        vcard->vcard_private_free = 0;

        vcard->vcard_private = 0;

    }

    for (current_applet = vcard->applet_list; current_applet;

                                        current_applet = next_applet) {

        next_applet = current_applet->next;

        vcard_delete_applet(current_applet);

    }

    vcard_buffer_response_delete(vcard->vcard_buffer_response);

    g_free(vcard);

}
","vcard_free(VCard *vcard) {
VCardApplet *current_applet = NULL; VCardApplet *next_applet = NULL; if (vcard == NULL) {
return; } vcard->reference_count--; if (vcard->reference_count != 0) {
return; } if (vcard->vcard_private_free) {
(*vcard->vcard_private_free)(vcard->vcard_private); vcard->vcard_private_free = 0; vcard->vcard_private = 0; } for (current_applet = vcard->applet_list; current_applet; current_applet = next_applet) {
next_applet = current_applet->next; vcard_delete_applet(current_applet); } vcard_buffer_response_delete(vcard->vcard_buffer_response); g_free(vcard); } ",qemu,0
"static int local_lstat(FsContext *fs_ctx, V9fsPath *fs_path, struct stat *stbuf)

{

    int err;

    char *buffer;

    char *path = fs_path->data;



    buffer = rpath(fs_ctx, path);

    err =  lstat(buffer, stbuf);

    if (err) {

        goto err_out;

    }

    if (fs_ctx->export_flags & V9FS_SM_MAPPED) {

        /* Actual credentials are part of extended attrs */

        uid_t tmp_uid;

        gid_t tmp_gid;

        mode_t tmp_mode;

        dev_t tmp_dev;

        if (getxattr(buffer, ""user.virtfs.uid"", &tmp_uid, sizeof(uid_t)) > 0) {

            stbuf->st_uid = le32_to_cpu(tmp_uid);

        }

        if (getxattr(buffer, ""user.virtfs.gid"", &tmp_gid, sizeof(gid_t)) > 0) {

            stbuf->st_gid = le32_to_cpu(tmp_gid);

        }

        if (getxattr(buffer, ""user.virtfs.mode"",

                    &tmp_mode, sizeof(mode_t)) > 0) {

            stbuf->st_mode = le32_to_cpu(tmp_mode);

        }

        if (getxattr(buffer, ""user.virtfs.rdev"", &tmp_dev, sizeof(dev_t)) > 0) {

            stbuf->st_rdev = le64_to_cpu(tmp_dev);

        }

    } else if (fs_ctx->export_flags & V9FS_SM_MAPPED_FILE) {

        local_mapped_file_attr(fs_ctx, path, stbuf);

    }



err_out:

    g_free(buffer);

    return err;

}
","static int local_lstat(FsContext *fs_ctx, V9fsPath *fs_path, struct stat *stbuf) {
int err; char *buffer; char *path = fs_path->data; buffer = rpath(fs_ctx, path); err = lstat(buffer, stbuf); if (err) {
goto err_out; } if (fs_ctx->export_flags & V9FS_SM_MAPPED) {
/* Actual credentials are part of extended attrs */ uid_t tmp_uid; gid_t tmp_gid; mode_t tmp_mode; dev_t tmp_dev; if (getxattr(buffer, ""user.virtfs.uid"", &tmp_uid, sizeof(uid_t)) > 0) {
stbuf->st_uid = le32_to_cpu(tmp_uid); } if (getxattr(buffer, ""user.virtfs.gid"", &tmp_gid, sizeof(gid_t)) > 0) {
stbuf->st_gid = le32_to_cpu(tmp_gid); } if (getxattr(buffer, ""user.virtfs.mode"", &tmp_mode, sizeof(mode_t)) > 0) {
stbuf->st_mode = le32_to_cpu(tmp_mode); } if (getxattr(buffer, ""user.virtfs.rdev"", &tmp_dev, sizeof(dev_t)) > 0) {
stbuf->st_rdev = le64_to_cpu(tmp_dev); } } else if (fs_ctx->export_flags & V9FS_SM_MAPPED_FILE) {
local_mapped_file_attr(fs_ctx, path, stbuf); } err_out: g_free(buffer); return err; } ",qemu,1
"static inline uint32_t xen_vcpu_eport(shared_iopage_t *shared_page, int i)

{

    return shared_page->vcpu_iodata[i].vp_eport;

}
","static inline uint32_t xen_vcpu_eport(shared_iopage_t *shared_page, int i) {
return shared_page->vcpu_iodata[i].vp_eport; } ",qemu,0
"static uint64_t bonito_ldma_readl(void *opaque, target_phys_addr_t addr,

                                  unsigned size)

{

    uint32_t val;

    PCIBonitoState *s = opaque;



    val = ((uint32_t *)(&s->bonldma))[addr/sizeof(uint32_t)];



    return val;

}
","static uint64_t bonito_ldma_readl(void *opaque, target_phys_addr_t addr, unsigned size) {
uint32_t val; PCIBonitoState *s = opaque; val = ((uint32_t *)(&s->bonldma))[addr/sizeof(uint32_t)]; return val; } ",qemu,0
"static void put_uint16(QEMUFile *f, void *pv, size_t size)

{

    uint16_t *v = pv;

    qemu_put_be16s(f, v);

}
","static void put_uint16(QEMUFile *f, void *pv, size_t size) {
uint16_t *v = pv; qemu_put_be16s(f, v); } ",qemu,1
"static void dcr_write_sdram (void *opaque, int dcrn, uint32_t val)

{

    ppc4xx_sdram_t *sdram;



    sdram = opaque;

    switch (dcrn) {

    case SDRAM0_CFGADDR:

        sdram->addr = val;

        break;

    case SDRAM0_CFGDATA:

        switch (sdram->addr) {

        case 0x00: /* SDRAM_BESR0 */

            sdram->besr0 &= ~val;

            break;

        case 0x08: /* SDRAM_BESR1 */

            sdram->besr1 &= ~val;

            break;

        case 0x10: /* SDRAM_BEAR */

            sdram->bear = val;

            break;

        case 0x20: /* SDRAM_CFG */

            val &= 0xFFE00000;

            if (!(sdram->cfg & 0x80000000) && (val & 0x80000000)) {

#ifdef DEBUG_SDRAM

                printf(""%s: enable SDRAM controller\n"", __func__);

#endif

                /* validate all RAM mappings */

                sdram_map_bcr(sdram);

                sdram->status &= ~0x80000000;

            } else if ((sdram->cfg & 0x80000000) && !(val & 0x80000000)) {

#ifdef DEBUG_SDRAM

                printf(""%s: disable SDRAM controller\n"", __func__);

#endif

                /* invalidate all RAM mappings */

                sdram_unmap_bcr(sdram);

                sdram->status |= 0x80000000;

            }

            if (!(sdram->cfg & 0x40000000) && (val & 0x40000000))

                sdram->status |= 0x40000000;

            else if ((sdram->cfg & 0x40000000) && !(val & 0x40000000))

                sdram->status &= ~0x40000000;

            sdram->cfg = val;

            break;

        case 0x24: /* SDRAM_STATUS */

            /* Read-only register */

            break;

        case 0x30: /* SDRAM_RTR */

            sdram->rtr = val & 0x3FF80000;

            break;

        case 0x34: /* SDRAM_PMIT */

            sdram->pmit = (val & 0xF8000000) | 0x07C00000;

            break;

        case 0x40: /* SDRAM_B0CR */

            sdram_set_bcr(&sdram->bcr[0], val, sdram->cfg & 0x80000000);

            break;

        case 0x44: /* SDRAM_B1CR */

            sdram_set_bcr(&sdram->bcr[1], val, sdram->cfg & 0x80000000);

            break;

        case 0x48: /* SDRAM_B2CR */

            sdram_set_bcr(&sdram->bcr[2], val, sdram->cfg & 0x80000000);

            break;

        case 0x4C: /* SDRAM_B3CR */

            sdram_set_bcr(&sdram->bcr[3], val, sdram->cfg & 0x80000000);

            break;

        case 0x80: /* SDRAM_TR */

            sdram->tr = val & 0x018FC01F;

            break;

        case 0x94: /* SDRAM_ECCCFG */

            sdram->ecccfg = val & 0x00F00000;

            break;

        case 0x98: /* SDRAM_ECCESR */

            val &= 0xFFF0F000;

            if (sdram->eccesr == 0 && val != 0)

                qemu_irq_raise(sdram->irq);

            else if (sdram->eccesr != 0 && val == 0)

                qemu_irq_lower(sdram->irq);

            sdram->eccesr = val;

            break;

        default: /* Error */

            break;

        }

        break;

    }

}
","static void dcr_write_sdram (void *opaque, int dcrn, uint32_t val) {
ppc4xx_sdram_t *sdram; sdram = opaque; switch (dcrn) {
case SDRAM0_CFGADDR: sdram->addr = val; break; case SDRAM0_CFGDATA: switch (sdram->addr) {
case 0x00: /* SDRAM_BESR0 */ sdram->besr0 &= ~val; break; case 0x08: /* SDRAM_BESR1 */ sdram->besr1 &= ~val; break; case 0x10: /* SDRAM_BEAR */ sdram->bear = val; break; case 0x20: /* SDRAM_CFG */ val &= 0xFFE00000; if (!(sdram->cfg & 0x80000000) && (val & 0x80000000)) {
#ifdef DEBUG_SDRAM printf(""%s: enable SDRAM controller\n"", __func__); #endif /* validate all RAM mappings */ sdram_map_bcr(sdram); sdram->status &= ~0x80000000; } else if ((sdram->cfg & 0x80000000) && !(val & 0x80000000)) {
#ifdef DEBUG_SDRAM printf(""%s: disable SDRAM controller\n"", __func__); #endif /* invalidate all RAM mappings */ sdram_unmap_bcr(sdram); sdram->status |= 0x80000000; } if (!(sdram->cfg & 0x40000000) && (val & 0x40000000)) sdram->status |= 0x40000000; else if ((sdram->cfg & 0x40000000) && !(val & 0x40000000)) sdram->status &= ~0x40000000; sdram->cfg = val; break; case 0x24: /* SDRAM_STATUS */ /* Read-only register */ break; case 0x30: /* SDRAM_RTR */ sdram->rtr = val & 0x3FF80000; break; case 0x34: /* SDRAM_PMIT */ sdram->pmit = (val & 0xF8000000) | 0x07C00000; break; case 0x40: /* SDRAM_B0CR */ sdram_set_bcr(&sdram->bcr[0], val, sdram->cfg & 0x80000000); break; case 0x44: /* SDRAM_B1CR */ sdram_set_bcr(&sdram->bcr[1], val, sdram->cfg & 0x80000000); break; case 0x48: /* SDRAM_B2CR */ sdram_set_bcr(&sdram->bcr[2], val, sdram->cfg & 0x80000000); break; case 0x4C: /* SDRAM_B3CR */ sdram_set_bcr(&sdram->bcr[3], val, sdram->cfg & 0x80000000); break; case 0x80: /* SDRAM_TR */ sdram->tr = val & 0x018FC01F; break; case 0x94: /* SDRAM_ECCCFG */ sdram->ecccfg = val & 0x00F00000; break; case 0x98: /* SDRAM_ECCESR */ val &= 0xFFF0F000; if (sdram->eccesr == 0 && val != 0) qemu_irq_raise(sdram->irq); else if (sdram->eccesr != 0 && val == 0) qemu_irq_lower(sdram->irq); sdram->eccesr = val; break; default: /* Error */ break; } break; } } ",qemu,0
"static ssize_t qcow2_crypto_hdr_init_func(QCryptoBlock *block, size_t headerlen,

                                          void *opaque, Error **errp)

{

    BlockDriverState *bs = opaque;

    BDRVQcow2State *s = bs->opaque;

    int64_t ret;

    int64_t clusterlen;



    ret = qcow2_alloc_clusters(bs, headerlen);

    if (ret < 0) {

        error_setg_errno(errp, -ret,

                         ""Cannot allocate cluster for LUKS header size %zu"",

                         headerlen);

        return -1;

    }



    s->crypto_header.length = headerlen;

    s->crypto_header.offset = ret;



    /* Zero fill remaining space in cluster so it has predictable

     * content in case of future spec changes */

    clusterlen = size_to_clusters(s, headerlen) * s->cluster_size;


    ret = bdrv_pwrite_zeroes(bs->file,

                             ret + headerlen,

                             clusterlen - headerlen, 0);

    if (ret < 0) {

        error_setg_errno(errp, -ret, ""Could not zero fill encryption header"");

        return -1;

    }



    return ret;

}","static ssize_t qcow2_crypto_hdr_init_func(QCryptoBlock *block, size_t headerlen, void *opaque, Error **errp) {
BlockDriverState *bs = opaque; BDRVQcow2State *s = bs->opaque; int64_t ret; int64_t clusterlen; ret = qcow2_alloc_clusters(bs, headerlen); if (ret < 0) {
error_setg_errno(errp, -ret, ""Cannot allocate cluster for LUKS header size %zu"", headerlen); return -1; } s->crypto_header.length = headerlen; s->crypto_header.offset = ret; /* Zero fill remaining space in cluster so it has predictable * content in case of future spec changes */ clusterlen = size_to_clusters(s, headerlen) * s->cluster_size; ret = bdrv_pwrite_zeroes(bs->file, ret + headerlen, clusterlen - headerlen, 0); if (ret < 0) {
error_setg_errno(errp, -ret, ""Could not zero fill encryption header""); return -1; } return ret; }",qemu,1
"static size_t curl_size_cb(void *ptr, size_t size, size_t nmemb, void *opaque)

{

    CURLState *s = ((CURLState*)opaque);

    size_t realsize = size * nmemb;

    size_t fsize;



    if(sscanf(ptr, ""Content-Length: %zd"", &fsize) == 1) {

        s->s->len = fsize;

    }



    return realsize;

}
","static size_t curl_size_cb(void *ptr, size_t size, size_t nmemb, void *opaque) {
CURLState *s = ((CURLState*)opaque); size_t realsize = size * nmemb; size_t fsize; if(sscanf(ptr, ""Content-Length: %zd"", &fsize) == 1) {
s->s->len = fsize; } return realsize; } ",qemu,1
"static void replication_start(ReplicationState *rs, ReplicationMode mode,
                              Error **errp)
{
    BlockDriverState *bs = rs->opaque;
    BDRVReplicationState *s;
    BlockDriverState *top_bs;
    int64_t active_length, hidden_length, disk_length;
    AioContext *aio_context;
    Error *local_err = NULL;
    BlockJob *job;
    aio_context = bdrv_get_aio_context(bs);
    aio_context_acquire(aio_context);
    s = bs->opaque;
    if (s->stage != BLOCK_REPLICATION_NONE) {
        error_setg(errp, ""Block replication is running or done"");
        aio_context_release(aio_context);
        return;
    }
    if (s->mode != mode) {
        error_setg(errp, ""The parameter mode's value is invalid, needs %d,""
                   "" but got %d"", s->mode, mode);
        aio_context_release(aio_context);
        return;
    }
    switch (s->mode) {
    case REPLICATION_MODE_PRIMARY:
        break;
    case REPLICATION_MODE_SECONDARY:
        s->active_disk = bs->file;
        if (!s->active_disk || !s->active_disk->bs ||
                                    !s->active_disk->bs->backing) {
            error_setg(errp, ""Active disk doesn't have backing file"");
            aio_context_release(aio_context);
            return;
        }
        s->hidden_disk = s->active_disk->bs->backing;
        if (!s->hidden_disk->bs || !s->hidden_disk->bs->backing) {
            error_setg(errp, ""Hidden disk doesn't have backing file"");
            aio_context_release(aio_context);
            return;
        }
        s->secondary_disk = s->hidden_disk->bs->backing;
        if (!s->secondary_disk->bs || !bdrv_has_blk(s->secondary_disk->bs)) {
            error_setg(errp, ""The secondary disk doesn't have block backend"");
            aio_context_release(aio_context);
            return;
        }
        /* verify the length */
        active_length = bdrv_getlength(s->active_disk->bs);
        hidden_length = bdrv_getlength(s->hidden_disk->bs);
        disk_length = bdrv_getlength(s->secondary_disk->bs);
        if (active_length < 0 || hidden_length < 0 || disk_length < 0 ||
            active_length != hidden_length || hidden_length != disk_length) {
            error_setg(errp, ""Active disk, hidden disk, secondary disk's length""
                       "" are not the same"");
            aio_context_release(aio_context);
            return;
        }
        if (!s->active_disk->bs->drv->bdrv_make_empty ||
            !s->hidden_disk->bs->drv->bdrv_make_empty) {
            error_setg(errp,
                       ""Active disk or hidden disk doesn't support make_empty"");
            aio_context_release(aio_context);
            return;
        }
        /* reopen the backing file in r/w mode */
        reopen_backing_file(bs, true, &local_err);
        if (local_err) {
            error_propagate(errp, local_err);
            aio_context_release(aio_context);
            return;
        }
        /* start backup job now */
        error_setg(&s->blocker,
                   ""Block device is in use by internal backup job"");
        top_bs = bdrv_lookup_bs(s->top_id, s->top_id, NULL);
        if (!top_bs || !bdrv_is_root_node(top_bs) ||
            !check_top_bs(top_bs, bs)) {
            error_setg(errp, ""No top_bs or it is invalid"");
            reopen_backing_file(bs, false, NULL);
            aio_context_release(aio_context);
            return;
        }
        bdrv_op_block_all(top_bs, s->blocker);
        bdrv_op_unblock(top_bs, BLOCK_OP_TYPE_DATAPLANE, s->blocker);
        job = backup_job_create(NULL, s->secondary_disk->bs, s->hidden_disk->bs,
                                0, MIRROR_SYNC_MODE_NONE, NULL, false,
                                BLOCKDEV_ON_ERROR_REPORT,
                                BLOCKDEV_ON_ERROR_REPORT, BLOCK_JOB_INTERNAL,
                                backup_job_completed, bs, NULL, &local_err);
        if (local_err) {
            error_propagate(errp, local_err);
            backup_job_cleanup(bs);
            aio_context_release(aio_context);
            return;
        }
        block_job_start(job);
        break;
    default:
        aio_context_release(aio_context);
        abort();
    }
    s->stage = BLOCK_REPLICATION_RUNNING;
    if (s->mode == REPLICATION_MODE_SECONDARY) {
        secondary_do_checkpoint(s, errp);
    }
    s->error = 0;
    aio_context_release(aio_context);
}","static void replication_start(ReplicationState *rs, ReplicationMode mode, Error **errp) {
BlockDriverState *bs = rs->opaque; BDRVReplicationState *s; BlockDriverState *top_bs; int64_t active_length, hidden_length, disk_length; AioContext *aio_context; Error *local_err = NULL; BlockJob *job; aio_context = bdrv_get_aio_context(bs); aio_context_acquire(aio_context); s = bs->opaque; if (s->stage != BLOCK_REPLICATION_NONE) {
error_setg(errp, ""Block replication is running or done""); aio_context_release(aio_context); return; } if (s->mode != mode) {
error_setg(errp, ""The parameter mode's value is invalid, needs %d,"" "" but got %d"", s->mode, mode); aio_context_release(aio_context); return; } switch (s->mode) {
case REPLICATION_MODE_PRIMARY: break; case REPLICATION_MODE_SECONDARY: s->active_disk = bs->file; if (!s->active_disk || !s->active_disk->bs || !s->active_disk->bs->backing) {
error_setg(errp, ""Active disk doesn't have backing file""); aio_context_release(aio_context); return; } s->hidden_disk = s->active_disk->bs->backing; if (!s->hidden_disk->bs || !s->hidden_disk->bs->backing) {
error_setg(errp, ""Hidden disk doesn't have backing file""); aio_context_release(aio_context); return; } s->secondary_disk = s->hidden_disk->bs->backing; if (!s->secondary_disk->bs || !bdrv_has_blk(s->secondary_disk->bs)) {
error_setg(errp, ""The secondary disk doesn't have block backend""); aio_context_release(aio_context); return; } /* verify the length */ active_length = bdrv_getlength(s->active_disk->bs); hidden_length = bdrv_getlength(s->hidden_disk->bs); disk_length = bdrv_getlength(s->secondary_disk->bs); if (active_length < 0 || hidden_length < 0 || disk_length < 0 || active_length != hidden_length || hidden_length != disk_length) {
error_setg(errp, ""Active disk, hidden disk, secondary disk's length"" "" are not the same""); aio_context_release(aio_context); return; } if (!s->active_disk->bs->drv->bdrv_make_empty || !s->hidden_disk->bs->drv->bdrv_make_empty) {
error_setg(errp, ""Active disk or hidden disk doesn't support make_empty""); aio_context_release(aio_context); return; } /* reopen the backing file in r/w mode */ reopen_backing_file(bs, true, &local_err); if (local_err) {
error_propagate(errp, local_err); aio_context_release(aio_context); return; } /* start backup job now */ error_setg(&s->blocker, ""Block device is in use by internal backup job""); top_bs = bdrv_lookup_bs(s->top_id, s->top_id, NULL); if (!top_bs || !bdrv_is_root_node(top_bs) || !check_top_bs(top_bs, bs)) {
error_setg(errp, ""No top_bs or it is invalid""); reopen_backing_file(bs, false, NULL); aio_context_release(aio_context); return; } bdrv_op_block_all(top_bs, s->blocker); bdrv_op_unblock(top_bs, BLOCK_OP_TYPE_DATAPLANE, s->blocker); job = backup_job_create(NULL, s->secondary_disk->bs, s->hidden_disk->bs, 0, MIRROR_SYNC_MODE_NONE, NULL, false, BLOCKDEV_ON_ERROR_REPORT, BLOCKDEV_ON_ERROR_REPORT, BLOCK_JOB_INTERNAL, backup_job_completed, bs, NULL, &local_err); if (local_err) {
error_propagate(errp, local_err); backup_job_cleanup(bs); aio_context_release(aio_context); return; } block_job_start(job); break; default: aio_context_release(aio_context); abort(); } s->stage = BLOCK_REPLICATION_RUNNING; if (s->mode == REPLICATION_MODE_SECONDARY) {
secondary_do_checkpoint(s, errp); } s->error = 0; aio_context_release(aio_context); }",qemu,1
"static int usbredir_post_load(void *priv, int version_id)
{
    USBRedirDevice *dev = priv;
    switch (dev->device_info.speed) {
    case usb_redir_speed_low:
        dev->dev.speed = USB_SPEED_LOW;
        break;
    case usb_redir_speed_full:
        dev->dev.speed = USB_SPEED_FULL;
        break;
    case usb_redir_speed_high:
        dev->dev.speed = USB_SPEED_HIGH;
        break;
    case usb_redir_speed_super:
        dev->dev.speed = USB_SPEED_SUPER;
        break;
    default:
        dev->dev.speed = USB_SPEED_FULL;
    dev->dev.speedmask = (1 << dev->dev.speed);
    usbredir_setup_usb_eps(dev);
    usbredir_check_bulk_receiving(dev);","static int usbredir_post_load(void *priv, int version_id) {
USBRedirDevice *dev = priv; switch (dev->device_info.speed) {
case usb_redir_speed_low: dev->dev.speed = USB_SPEED_LOW; break; case usb_redir_speed_full: dev->dev.speed = USB_SPEED_FULL; break; case usb_redir_speed_high: dev->dev.speed = USB_SPEED_HIGH; break; case usb_redir_speed_super: dev->dev.speed = USB_SPEED_SUPER; break; default: dev->dev.speed = USB_SPEED_FULL; dev->dev.speedmask = (1 << dev->dev.speed); usbredir_setup_usb_eps(dev); usbredir_check_bulk_receiving(dev);",qemu,1
"void virtio_scsi_common_realize(DeviceState *dev, Error **errp,

                                VirtIOHandleOutput ctrl,

                                VirtIOHandleOutput evt,

                                VirtIOHandleOutput cmd)

{

    VirtIODevice *vdev = VIRTIO_DEVICE(dev);

    VirtIOSCSICommon *s = VIRTIO_SCSI_COMMON(dev);

    int i;



    virtio_init(vdev, ""virtio-scsi"", VIRTIO_ID_SCSI,

                sizeof(VirtIOSCSIConfig));



    if (s->conf.num_queues == 0 ||

            s->conf.num_queues > VIRTIO_QUEUE_MAX - 2) {

        error_setg(errp, ""Invalid number of queues (= %"" PRIu32 ""), ""

                         ""must be a positive integer less than %d."",

                   s->conf.num_queues, VIRTIO_QUEUE_MAX - 2);

        virtio_cleanup(vdev);

        return;

    }

    s->cmd_vqs = g_new0(VirtQueue *, s->conf.num_queues);

    s->sense_size = VIRTIO_SCSI_SENSE_DEFAULT_SIZE;

    s->cdb_size = VIRTIO_SCSI_CDB_DEFAULT_SIZE;



    s->ctrl_vq = virtio_add_queue_aio(vdev, VIRTIO_SCSI_VQ_SIZE, ctrl);

    s->event_vq = virtio_add_queue_aio(vdev, VIRTIO_SCSI_VQ_SIZE, evt);

    for (i = 0; i < s->conf.num_queues; i++) {

        s->cmd_vqs[i] = virtio_add_queue_aio(vdev, VIRTIO_SCSI_VQ_SIZE, cmd);

    }



    if (s->conf.iothread) {

        virtio_scsi_set_iothread(VIRTIO_SCSI(s), s->conf.iothread);

    }

}
","void virtio_scsi_common_realize(DeviceState *dev, Error **errp, VirtIOHandleOutput ctrl, VirtIOHandleOutput evt, VirtIOHandleOutput cmd) {
VirtIODevice *vdev = VIRTIO_DEVICE(dev); VirtIOSCSICommon *s = VIRTIO_SCSI_COMMON(dev); int i; virtio_init(vdev, ""virtio-scsi"", VIRTIO_ID_SCSI, sizeof(VirtIOSCSIConfig)); if (s->conf.num_queues == 0 || s->conf.num_queues > VIRTIO_QUEUE_MAX - 2) {
error_setg(errp, ""Invalid number of queues (= %"" PRIu32 ""), "" ""must be a positive integer less than %d."", s->conf.num_queues, VIRTIO_QUEUE_MAX - 2); virtio_cleanup(vdev); return; } s->cmd_vqs = g_new0(VirtQueue *, s->conf.num_queues); s->sense_size = VIRTIO_SCSI_SENSE_DEFAULT_SIZE; s->cdb_size = VIRTIO_SCSI_CDB_DEFAULT_SIZE; s->ctrl_vq = virtio_add_queue_aio(vdev, VIRTIO_SCSI_VQ_SIZE, ctrl); s->event_vq = virtio_add_queue_aio(vdev, VIRTIO_SCSI_VQ_SIZE, evt); for (i = 0; i < s->conf.num_queues; i++) {
s->cmd_vqs[i] = virtio_add_queue_aio(vdev, VIRTIO_SCSI_VQ_SIZE, cmd); } if (s->conf.iothread) {
virtio_scsi_set_iothread(VIRTIO_SCSI(s), s->conf.iothread); } } ",qemu,0
"static void test_validate_union_anon(TestInputVisitorData *data,

                                     const void *unused)

{

    UserDefAnonUnion *tmp = NULL;

    Visitor *v;

    Error *errp = NULL;



    v = validate_test_init(data, ""42"");



    visit_type_UserDefAnonUnion(v, &tmp, NULL, &errp);

    g_assert(!error_is_set(&errp));

    qapi_free_UserDefAnonUnion(tmp);

}
","static void test_validate_union_anon(TestInputVisitorData *data, const void *unused) {
UserDefAnonUnion *tmp = NULL; Visitor *v; Error *errp = NULL; v = validate_test_init(data, ""42""); visit_type_UserDefAnonUnion(v, &tmp, NULL, &errp); g_assert(!error_is_set(&errp)); qapi_free_UserDefAnonUnion(tmp); } ",qemu,0
"static void pci_nic_uninit(PCIDevice *pci_dev)

{

    EEPRO100State *s = DO_UPCAST(EEPRO100State, dev, pci_dev);



    vmstate_unregister(&pci_dev->qdev, s->vmstate, s);


    eeprom93xx_free(&pci_dev->qdev, s->eeprom);

    qemu_del_nic(s->nic);

}","static void pci_nic_uninit(PCIDevice *pci_dev) {
EEPRO100State *s = DO_UPCAST(EEPRO100State, dev, pci_dev); vmstate_unregister(&pci_dev->qdev, s->vmstate, s); eeprom93xx_free(&pci_dev->qdev, s->eeprom); qemu_del_nic(s->nic); }",qemu,1
